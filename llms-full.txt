# Bazel Documentation – llms-full.txt
> Complete Markdown snapshot of the Bazel documentation tree. Each section is canonicalized to the published URL for easier ingestion.

- Canonical domain: https://bazel.build/
- Docs snapshot: HEAD
- Generated: 2025-11-12 16:41:01 UTC
- Source repo: https://github.com/bazel-contrib/bazel-docs

---

## { Fast, Correct }: Choose two
- URL: https://bazel.build/
- Source: index.mdx
- Slug: /
- Summary: From startup to enterprise, choose the Bazel open source project to build and test your multi-language, multi-platform projects.

<div class="w-11/12 mx-auto px-6 py-8">
<div class="bg-gradient-to-r from-green-600 to-green-800 p-6 rounded-lg">
<Columns cols={4}>
  <Card
    title="Build better"
    icon="gauge-max"
    href="/basics"
  >
Rebuild only what is necessary. Get fast incremental builds with Bazel's advanced local and distributed caching, optimized dependency analysis, and parallel execution.
  </Card>
  <Card
    title="Multilingual magic"
    icon="language"
    href="/tutorials"
  >
   Build and test using Java, C++, Go, Android, iOS and many other languages and platforms. Bazel runs on Windows, macOS, and Linux.
  </Card>
    <Card
    title="Simply scalable"
    icon="arrows-maximize"
    href="/remote-execution"
  >
    Scale your organization, codebase, and Continuous Integration systems. Bazel handles codebases of any size, whether in multiple repositories or a huge monorepo.
  </Card>
  <Card
    title="Endlessly extensible"
    icon="gear"
    href="/writing-rules"
  >
    Add support for new languages and platforms with Bazel's extension language. Share and re-use language rules written by the growing Bazel community.
  </Card>
</Columns>
</div>

<div className="text-2xl">
Essential Bazel
</div>

Build and test software of any size, quickly and reliably. Industry leaders like Google, Stripe, and Dropbox trust Bazel to build heavy-duty, mission-critical infrastructure, services, and applications.

<Columns cols={3}>

<Card title="Get started" cta="Get Started" img="/images/essential_start.svg">
Learn what Bazel is, why it is a good choice for your project, and how you can get started using it quickly.
</Card>
<Card title="User's guide" cta="Read the docs" img="/images/essential_guide.svg">
Learn how to use Bazel with documentation and tutorials covering topics from foundational to expert.

</Card>
<Card title="Reference guide" cta="Search the guide" img="/images/essential_reference.svg">
Use these resources to efficiently look up the commands, queries, and terminology necessary to working with Bazel.

</Card>


</Columns>


<div className="justify-items-center">
<div className="max-w-96">
<Card title="Release Notes" img="/images/release_notes.svg" cta="View notes" link="https://github.com/bazelbuild/bazel/releases/latest">
Bazel is always evolving — check the release notes to see what's changed in the latest releases, and how that affects your builds.
</Card>
</div>
</div>

<div className="text-2xl">
What's New?
</div>

Catch up on the latest documentation, community events, and programs.
<Columns cols={3}>

<Card title="Roadmap" img="/images/new_1.svg" href="/about/roadmap">
Read our new public roadmap to see what is coming down the pipeline.
</Card>
<Card title="Community updates" img="/images/new_3.svg" href="/community/update">
Tune in for our new monthly community update livestream.
</Card>
<Card title="Query quickstart tutorial" img="/images/new_2.svg" href="/query/quickstart">
Get started with the Bazel query language with this new guided scenario.
</Card>


</Columns>
</div>

---

## FAQ
- URL: https://bazel.build/about/faq
- Source: about/faq.mdx
- Slug: /about/faq

If you have questions or need support, see [Getting Help](/help).

## What is Bazel?

Bazel is a tool that automates software builds and tests. Supported build tasks include running compilers and linkers to produce executable programs and libraries, and assembling deployable packages for Android, iOS and other target environments. Bazel is similar to other tools like Make, Ant, Gradle, Buck, Pants and Maven.

## What is special about Bazel?

Bazel was designed to fit the way software is developed at Google. It has the following features:

*   Multi-language support: Bazel supports [many languages](/reference/be/overview), and can be extended to support arbitrary programming languages.
*   High-level build language: Projects are described in the `BUILD` language, a concise text format that describes a project as sets of small interconnected libraries, binaries and tests. In contrast, with tools like Make, you have to describe individual files and compiler invocations.
*   Multi-platform support: The same tool and the same `BUILD` files can be used to build software for different architectures, and even different platforms. At Google, we use Bazel to build everything from server applications running on systems in our data centers to client apps running on mobile phones.
*   Reproducibility: In `BUILD` files, each library, test and binary must specify its direct dependencies completely. Bazel uses this dependency information to know what must be rebuilt when you make changes to a source file, and which tasks can run in parallel. This means that all builds are incremental and will always produce the same result.
*   Scalable: Bazel can handle large builds; at Google, it is common for a server binary to have 100k source files, and builds where no files were changed take about ~200ms.

## Why doesn’t Google use...?

*   Make, Ninja: These tools give very exact control over what commands get invoked to build files, but it’s up to the user to write rules that are correct.
    * Users interact with Bazel on a higher level. For example, Bazel has built-in rules for “Java test”, “C++ binary”, and notions such as “target platform” and “host platform”. These rules have been battle tested to be foolproof.
*   Ant and Maven: Ant and Maven are primarily geared toward Java, while Bazel handles multiple languages. Bazel encourages subdividing codebases in smaller reusable units, and can rebuild only ones that need rebuilding. This speeds up development when working with larger codebases.
*   Gradle: Bazel configuration files are much more structured than Gradle’s, letting Bazel understand exactly what each action does. This allows for more parallelism and better reproducibility.
*   Pants, Buck: Both tools were created and developed by ex-Googlers at Twitter and Foursquare, and Facebook respectively. They have been modeled after Bazel, but their feature sets are different, so they aren’t viable alternatives for us.

## Where did Bazel come from?

Bazel is a flavor of the tool that Google uses to build its server software internally. It has expanded to build other software as well, like mobile apps (iOS, Android) that connect to our servers.

## Did you rewrite your internal tool as open-source? Is it a fork?

Bazel shares most of its code with the internal tool and its rules are used for millions of builds every day.

## Why did Google build Bazel?

A long time ago, Google built its software using large, generated Makefiles. These led to slow and unreliable builds, which began to interfere with our developers’ productivity and the company’s agility. Bazel was a way to solve these problems.

## Does Bazel require a build cluster?

Bazel runs build operations locally by default. However, Bazel can also connect to a build cluster for even faster builds and tests. See our documentation on [remote execution and caching](/remote/rbe) and [remote caching](/remote/caching) for further details.

## How does the Google development process work?

For our server code base, we use the following development workflow:

*   All our server code is in a single, gigantic version control system.
*   Everybody builds their software with Bazel.
*   Different teams own different parts of the source tree, and make their components available as `BUILD` targets.
*   Branching is primarily used for managing releases, so everybody develops their software at the head revision.

Bazel is a cornerstone of this philosophy: since Bazel requires all dependencies to be fully specified, we can predict which programs and tests are affected by a change, and vet them before submission.

More background on the development process at Google can be found on the [eng tools blog](http://google-engtools.blogspot.com/).

## Why did you open up Bazel?

Building software should be fun and easy. Slow and unpredictable builds take the fun out of programming.

## Why would I want to use Bazel?

*   Bazel may give you faster build times because it can recompile only the files that need to be recompiled. Similarly, it can skip re-running tests that it knows haven’t changed.
*   Bazel produces deterministic results. This eliminates skew between incremental and clean builds, laptop and CI system, etc.
*   Bazel can build different client and server apps with the same tool from the same workspace. For example, you can change a client/server protocol in a single commit, and test that the updated mobile app works with the updated server, building both with the same tool, reaping all the aforementioned benefits of Bazel.

## Can I see examples?

Yes; see a [simple example](https://github.com/bazelbuild/bazel/blob/master/examples/cpp/BUILD)
or read the [Bazel source code](https://github.com/bazelbuild/bazel/blob/master/src/BUILD) for a more complex example.


## What is Bazel best at?

Bazel shines at building and testing projects with the following properties:

*   Projects with a large codebase
*   Projects written in (multiple) compiled languages
*   Projects that deploy on multiple platforms
*   Projects that have extensive tests

## Where can I run Bazel?

Bazel runs on Linux, macOS (OS X), and Windows.

Porting to other UNIX platforms should be relatively easy, as long as a JDK is available for the platform.

## What should I not use Bazel for?

*   Bazel tries to be smart about caching. This means that it is not good for running build operations whose outputs should not be cached. For example, the following steps should not be run from Bazel:
    *   A compilation step that fetches data from the internet.
    *   A test step that connects to the QA instance of your site.
    *   A deployment step that changes your site’s cloud configuration.
*   If your build consists of a few long, sequential steps, Bazel may not be able to help much. You’ll get more speed by breaking long steps into smaller, discrete targets that Bazel can run in parallel.

## How stable is Bazel’s feature set?

The core features (C++, Java, and shell rules) have extensive use inside Google, so they are thoroughly tested and have very little churn. Similarly, we test new versions of Bazel across hundreds of thousands of targets every day to find regressions, and we release new versions multiple times every month.

In short, except for features marked as experimental, Bazel should Just Work. Changes to non-experimental rules will be backward compatible. A more detailed list of feature support statuses can be found in our [support document](/contribute/support).

## How stable is Bazel as a binary?

Inside Google, we make sure that Bazel crashes are very rare. This should also hold for our open source codebase.

## How can I start using Bazel?

See [Getting Started](/start/).

## Doesn’t Docker solve the reproducibility problems?

With Docker you can easily create sandboxes with fixed OS releases, for example, Ubuntu 12.04, Fedora 21. This solves the problem of reproducibility for the system environment – that is, “which version of /usr/bin/c++ do I need?”

Docker does not address reproducibility with regard to changes in the source code. Running Make with an imperfectly written Makefile inside a Docker container can still yield unpredictable results.

Inside Google, we check tools into source control for reproducibility. In this way, we can vet changes to tools (“upgrade GCC to 4.6.1”) with the same mechanism as changes to base libraries (“fix bounds check in OpenSSL”).

## Can I build binaries for deployment on Docker?

With Bazel, you can build standalone, statically linked binaries in C/C++, and self-contained jar files for Java. These run with few dependencies on normal UNIX systems, and as such should be simple to install inside a Docker container.

Bazel has conventions for structuring more complex programs, for example, a Java program that consumes a set of data files, or runs another program as subprocess. It is possible to package up such environments as standalone archives, so they can be deployed on different systems, including Docker images.

## Can I build Docker images with Bazel?

Yes, you can use our [Docker rules](https://github.com/bazelbuild/rules_docker) to build reproducible Docker images.

## Will Bazel make my builds reproducible automatically?

For Java and C++ binaries, yes, assuming you do not change the toolchain. If you have build steps that involve custom recipes (for example, executing binaries through a shell script inside a rule), you will need to take some extra care:

*   Do not use dependencies that were not declared. Sandboxed execution (–spawn\_strategy=sandboxed, only on Linux) can help find undeclared dependencies.
*   Avoid storing timestamps and user-IDs in generated files. ZIP files and other archives are especially prone to this.
*   Avoid connecting to the network. Sandboxed execution can help here too.
*   Avoid processes that use random numbers, in particular, dictionary traversal is randomized in many programming languages.

## Do you have binary releases?

Yes, you can find the latest [release binaries](https://github.com/bazelbuild/bazel/releases/latest) and review our [release policy](/release/)

## I use Eclipse/IntelliJ/XCode. How does Bazel interoperate with IDEs?

For IntelliJ, check out the [IntelliJ with Bazel plugin](https://ij.bazel.build/).

For XCode, check out [Tulsi](http://tulsi.bazel.build/).

For Eclipse, check out [E4B plugin](https://github.com/bazelbuild/e4b).

For other IDEs, check out the [blog post](https://blog.bazel.build/2016/06/10/ide-support.html) on how these plugins work.

## I use Jenkins/CircleCI/TravisCI. How does Bazel interoperate with CI systems?

Bazel returns a non-zero exit code if the build or test invocation fails, and this should be enough for basic CI integration. Since Bazel does not need clean builds for correctness, the CI system should not be configured to clean before starting a build/test run.

Further details on exit codes are in the [User Manual](/docs/user-manual).

## What future features can we expect in Bazel?

See our [Roadmaps](/about/roadmap).

## Can I use Bazel for my INSERT LANGUAGE HERE project?

Bazel is extensible. Anyone can add support for new languages. Many languages are supported: see the [build encyclopedia](/reference/be/overview) for a list of recommendations and [awesomebazel.com](https://awesomebazel.com/) for a more comprehensive list.

If you would like to develop extensions or learn how they work, see the documentation for [extending Bazel](/extending/concepts).

## Can I contribute to the Bazel code base?

See our [contribution guidelines](/contribute/).

## Why isn’t all development done in the open?

We still have to refactor the interfaces between the public code in Bazel and our internal extensions frequently. This makes it hard to do much development in the open.

## Are you done open sourcing Bazel?

Open sourcing Bazel is a work-in-progress. In particular, we’re still working on open sourcing:

*   Many of our unit and integration tests (which should make contributing patches easier).
*   Full IDE integration.

Beyond code, we’d like to eventually have all code reviews, bug tracking, and design decisions happen publicly, with the Bazel community involved. We are not there yet, so some changes will simply appear in the Bazel repository without clear explanation. Despite this lack of transparency, we want to support external developers and collaborate. Thus, we are opening up the code, even though some of the development is still happening internal to Google. Please let us know if anything seems unclear or unjustified as we transition to an open model.

## Are there parts of Bazel that will never be open sourced?

Yes, some of the code base either integrates with Google-specific technology or we have been looking for an excuse to get rid of (or is some combination of the two). These parts of the code base are not available on GitHub and probably never will be.

## How do I contact the team?

We are reachable at bazel-discuss@googlegroups.com.

## Where do I report bugs?

Open an issue [on GitHub](https://github.com/bazelbuild/bazel/issues).

## What’s up with the word “Blaze” in the codebase?

This is an internal name for the tool. Please refer to Blaze as Bazel.

## Why do other Google projects (Android, Chrome) use other build tools?

Until the first (Alpha) release, Bazel was not available externally, so open source projects such as Chromium and Android could not use it. In addition, the original lack of Windows support was a problem for building Windows applications, such as Chrome. Since the project has matured and become more stable, the [Android Open Source Project](https://source.android.com/) is in the process of migrating to Bazel.

## How do you pronounce “Bazel”?

The same way as “basil” (the herb) in US English: “BAY-zel”. It rhymes with “hazel”. IPA: /ˈbeɪzˌəl/

---

## Intro to Bazel
- URL: https://bazel.build/about/intro
- Source: about/intro.mdx
- Slug: /about/intro

Bazel is an open-source build and test tool similar to Make, Maven, and Gradle.
It uses a human-readable, high-level build language. Bazel supports projects in
multiple languages and builds outputs for multiple platforms. Bazel supports
large codebases across multiple repositories, and large numbers of users.

## Benefits

Bazel offers the following advantages:

*   **High-level build language.** Bazel uses an abstract, human-readable
    language to describe the build properties of your project at a high
    semantical level. Unlike other tools, Bazel operates on the *concepts*
    of libraries, binaries, scripts, and data sets, shielding you from the
    complexity of writing individual calls to tools such as compilers and
    linkers.

*   **Bazel is fast and reliable.** Bazel caches all previously done work and
    tracks changes to both file content and build commands. This way, Bazel
    knows when something needs to be rebuilt, and rebuilds only that. To further
    speed up your builds, you can set up your project to build in a  highly
    parallel and incremental fashion.

*   **Bazel is multi-platform.** Bazel runs on Linux, macOS, and Windows. Bazel
    can build binaries and deployable packages for multiple platforms, including
    desktop, server, and mobile, from the same project.

*   **Bazel scales.** Bazel maintains agility while handling builds with 100k+
    source files. It works with multiple repositories and user bases in the tens
    of thousands.

*   **Bazel is extensible.** Many [languages](/rules) are
    supported, and you can extend Bazel to support any other language or
    framework.

## Using Bazel

To build or test a project with Bazel, you typically do the following:

1.  **Set up Bazel.** Download and [install Bazel](/install).

2.  **Set up a project [workspace](/concepts/build-ref#workspaces)**, which is a
    directory where Bazel looks for build inputs and `BUILD` files, and where it
    stores build outputs.

3.  **Write a `BUILD` file**, which tells Bazel what to build and how to
    build it.

    You write your `BUILD` file by declaring build targets using
    [Starlark](/rules/language), a domain-specific language. (See example
    [here](https://github.com/bazelbuild/bazel/blob/master/examples/cpp/BUILD).)

    A build target specifies a set of input artifacts that Bazel will build plus
    their dependencies, the build rule Bazel will use to build it, and options
    that configure the build rule.

    A build rule specifies the build tools Bazel will use, such as compilers and
    linkers, and their configurations. Bazel ships with a number of build rules
    covering the most common artifact types in the supported languages on
    supported platforms.

4. **Run Bazel** from the [command line](/reference/command-line-reference). Bazel
   places your outputs within the workspace.

In addition to building, you can also use Bazel to run
[tests](/reference/test-encyclopedia) and [query](/query/guide) the build
to trace dependencies in your code.

## Bazel build process

When running a build or a test, Bazel does the following:

1.  **Loads** the `BUILD` files relevant to the target.

2.  **Analyzes** the inputs and their
    [dependencies](/concepts/dependencies), applies the specified build
    rules, and produces an [action](/extending/concepts#evaluation-model)
    graph.

3.  **Executes** the build actions on the inputs until the final build outputs
    are produced.

Since all previous build work is cached, Bazel can identify and reuse cached
artifacts and only rebuild or retest what's changed. To further enforce
correctness, you can set up Bazel to run builds and tests
[hermetically](/basics/hermeticity) through sandboxing, minimizing skew
and maximizing [reproducibility](/run/build#correct-incremental-rebuilds).

### Action graph

The action graph represents the build artifacts, the relationships between them,
and the build actions that Bazel will perform. Thanks to this graph, Bazel can
[track](/run/build#build-consistency) changes to
file content as well as changes to actions, such as build or test commands, and
know what build work has previously been done. The graph also enables you to
easily [trace dependencies](/query/guide) in your code.

## Getting started tutorials

To get started with Bazel, see [Getting Started](/start/) or jump
directly to the Bazel tutorials:

*   [Tutorial: Build a C++ Project](/start/cpp)
*   [Tutorial: Build a Java Project](/start/java)
*   [Tutorial: Build an Android Application](/start/android-app)
*   [Tutorial: Build an iOS Application](/start/ios-app)

---

## Bazel roadmap
- URL: https://bazel.build/about/roadmap
- Source: about/roadmap.mdx
- Slug: /about/roadmap

As Bazel continues to evolve in response to your needs, we want to share our
2025 roadmap update.

We plan to bring Bazel 9.0
[long term support (LTS)](https://bazel.build/release/versioning) to you in late
2025.

## Full transition to Bzlmod

[Bzlmod](https://bazel.build/docs/bzlmod) has been the standard external
dependency system in Bazel since Bazel 7, replacing the legacy WORKSPACE system.
As of March 2025, the [Bazel Central Registry](https://registry.bazel.build/)
hosts more than 650 modules.

With Bazel 9, we will completely remove WORKSPACE functionality, and Bzlmod will
be the only way to introduce external dependencies in Bazel. To minimize the
migration cost for the community, we'll focus on further improving our migration
[guide](https://bazel.build/external/migration) and
[tool](https://github.com/bazelbuild/bazel-central-registry/tree/main/tools#migrate_to_bzlmodpy).

Additionally, we aim to implement an improved shared repository cache (see
[#12227](https://github.com/bazelbuild/bazel/issues/12227))
with garbage collection, and may backport it to Bazel 8. The Bazel Central
Registry will also support verifying SLSA attestations.

## Migration of Android, C++, Java, Python, and Proto rules

With Bazel 8, we have migrated support for Android, Java, Python, and Proto
rules out of the Bazel codebase into Starlark rules in their corresponding
repositories. To ease the migration, we implemented the autoload features in
Bazel, which can be controlled with
[--incompatible_autoload_externally](https://github.com/bazelbuild/bazel/issues/23043)
and [--incompatible_disable_autoloads_in_main_repo](https://github.com/bazelbuild/bazel/issues/25755)
flags.

With Bazel 9, we aim to disable autoloads by default and require every project
to explicitly load required rules in BUILD files.

We will rewrite most of C++ language support to Starlark, detach it from Bazel
binary and move it into the [/rules_cc](https://github.com/bazelbuild/rules_cc)
repository. This is the last remaining major language support that is still part
of Bazel.

We're also porting unit tests for C++, Java, and Proto rules to Starlark, moving
them to repositories next to the implementation to increase velocity of rule
authors.

## Starlark improvements

Bazel will have the ability to evaluate symbolic macros lazily. This means that
a symbolic macro won't run if the targets it declares are not requested,
improving performance for very large packages.

Starlark will have an experimental type system, similar to Python's type
annotations. We expect the type system to stabilize _after_ Bazel 9 is launched.

## Configurability

Our main focus is reducing the cost and confusion of build flags.

We're [experimenting](https://github.com/bazelbuild/bazel/issues/24839) with a
new project configuration model that doesn't make users have to know which build
and test flags to set where. So `$ bazel test //foo` automatically sets the
right flags based on `foo`'s project's policy. This will likely remain
experimental in 9.0 but guiding feedback is welcome.

[Flag scoping](https://github.com/bazelbuild/bazel/issues/24042) lets you strip
out Starlark flags when they leave project boundaries, so they don't break
caching on transitive dependencies that don't need them. This makes builds that
use [transitions](https://bazel.build/extending/config#user-defined-transitions)
cheaper and faster.
[Here's](https://github.com/gregestren/snippets/tree/master/project_scoped_flags)
an example. We're extending the idea to control which flags propagate to
[exec configurations](https://bazel.build/extending/rules#configurations) and
are considering even more flexible support like custom Starlark to determine
which dependency edges should propagate flags.

We're up-prioritizing effort to move built-in language flags out of Bazel and
into Starlark, where they can live with related rule definitions.

## Remote execution improvements

We plan to add support for asynchronous execution, speeding up remote execution
by increasing parallelism.

---

To follow updates to the roadmap and discuss planned features, join the
community Slack server at [slack.bazel.build](https://slack.bazel.build/).

*This roadmap is intended to help inform the community about the team's
intentions for Bazel 9.0. Priorities are subject to change in response to
developer and customer feedback, or to new market opportunities.*

---

## Bazel Vision
- URL: https://bazel.build/about/vision
- Source: about/vision.mdx
- Slug: /about/vision

Any software developer can efficiently build, test, and package
any project, of any size or complexity, with tooling that's easy to adopt and
extend.

*   **Engineers can take build fundamentals for granted.** Software developers
    focus on the creative process of authoring code because the mechanical
    process of build and test is solved. When customizing the build system to
    support new languages or unique organizational needs, users focus on the
    aspects of extensibility that are unique to their use case, without having
    to reinvent the basic plumbing.

*   **Engineers can easily contribute to any project.** A developer who wants to
    start working on a new project can simply clone the project and run the
    build. There's no need for local configuration - it just works. With
    cross-platform remote execution, they can work on any machine anywhere and
    fully test their changes against all platforms the project targets.
    Engineers can quickly configure the build for a new project or incrementally
    migrate an existing build.

*   **Projects can scale to any size codebase, any size team.** Fast,
    incremental testing allows teams to fully validate every change before it is
    committed. This remains true even as repos grow, projects span multiple
    repos, and multiple languages are introduced. Infrastructure does not force
    developers to trade test coverage for build speed.

**We believe Bazel has the potential to fulfill this vision.**

Bazel was built from the ground up to enable builds that are reproducible (a
given set of inputs will always produce the same outputs) and portable (a build
can be run on any machine without affecting the output).

These characteristics support safe incrementality (rebuilding only changed
inputs doesn't introduce the risk of corruption) and distributability (build
actions are isolated and can be offloaded). By minimizing the work needed to do
a correct build and parallelizing that work across multiple cores and remote
systems, Bazel can make any build fast.

Bazel's abstraction layer — instructions specific to languages, platforms, and
toolchains implemented in a simple extensibility language — allows it to be
easily applied to any context.

## Bazel core competencies

1.  Bazel supports **multi-language, multi-platform** builds and tests. You can
    run a single command to build and test your entire source tree, no matter
    which combination of languages and platforms you target.
1.  Bazel builds are **fast and correct**. Every build and test run is
    incremental, on your developers' machines and on CI.
1.  Bazel provides a **uniform, extensible language** to define builds for any
    language or platform.
1.  Bazel allows your builds **to scale** by connecting to remote execution and
    caching services.
1.  Bazel works across **all major development platforms** (Linux, MacOS, and
    Windows).
1.  We accept that adopting Bazel requires effort, but **gradual adoption** is
    possible. Bazel interfaces with de-facto standard tools for a given
    language/platform.

## Serving language communities

Software engineering evolves in the context of language communities — typically,
self-organizing groups of people who use common tools and practices.

To be of use to members of a language community, high-quality Bazel rules must be
available that integrate with the workflows and conventions of that community.

Bazel is committed to be extensible and open, and to support good rulesets for
any language.

###  Requirements of a good ruleset

1.  The rules need to support efficient **building and testing** for the
    language, including code coverage.
1.  The rules need to **interface with a widely-used "package manager"** for the
    language (such as Maven for Java), and support incremental migration paths
    from other widely-used build systems.
1.  The rules need to be **extensible and interoperable**, following
    ["Bazel sandwich"](https://github.com/bazelbuild/bazel-website/blob/master/designs/_posts/2016-08-04-extensibility-for-native-rules.md)
    principles.
1.  The rules need to be **remote-execution ready**. In practice, this means
    **configurable using the [toolchains](/extending/toolchains) mechanism**.
1.  The rules (and Bazel) need to interface with a **widely-used IDE** for the
    language, if there is one.
1.  The rules need to have **thorough, usable documentation,** with introductory
    material for new users, comprehensive docs for expert users.

Each of these items is essential and only together do they deliver on Bazel's
competencies for their particular ecosystem.

They are also, by and large, sufficient - once all are fulfilled, Bazel fully
delivers its value to members of that language community.

---

## Why Bazel?
- URL: https://bazel.build/about/why
- Source: about/why.mdx
- Slug: /about/why

Bazel is a [fast](#fast), [correct](#correct), and [extensible](#extensible)
build tool with [integrated testing](#integrated-testing) that supports multiple
[languages](#multi-language), [repositories](#multi-repository), and
[platforms](#multi-platform) in an industry-leading [ecosystem](#ecosystem).

## Bazel is fast

Bazel knows exactly what input files each build command needs, avoiding
unnecessary work by re-running only when the set of input files have
changed between each build.
It runs build commands with as much parallelism as possible, either within the
same computer or on [remote build nodes](/remote/rbe). If the structure of build
allows for it, it can run thousands of build or test commands at the same time.

This is supported by multiple caching layers, in memory, on disk and on the
remote build farm, if available. At Google, we routinely achieve cache hit rates
north of 99%.

## Bazel is correct

Bazel ensures that your binaries are built *only* from your own
source code. Bazel actions run in individual sandboxes and Bazel tracks
every input file of the build, only and always re-running build
commands when it needs to. This keeps your binaries up-to-date so that the
[same source code always results in the same binary](/basics/hermeticity), bit
by bit.

Say goodbye to endless `make clean` invocations and to chasing phantom bugs
that were in fact resolved in source code that never got built.

## Bazel is extensible

Harness the full power of Bazel by writing your own rules and macros to
customize Bazel for your specific needs across a wide range of projects.

Bazel rules are written in [Starlark](/rules/language), our
in-house programming language that's a subset of Python. Starlark makes
rule-writing accessible to most developers, while also creating rules that can
be used across the ecosystem.

## Integrated testing

Bazel's [integrated test runner](/docs/user-manual#running-tests)
knows and runs only those tests needing to be re-run, using remote execution
(if available) to run them in parallel. Detect flakes early by using remote
execution to quickly run a test thousands of times.

Bazel [provides facilities](/remote/bep) to upload test results to a central
location, thereby facilitating efficient communication of test outcomes, be it
on CI or by individual developers.

## Multi-language support

Bazel supports many common programming languages including C++, Java,
Kotlin, Python, Go, and Rust. You can build multiple binaries (for example,
backend, web UI and mobile app) in the same Bazel invocation without being
constrained to one language's idiomatic build tool.

## Multi-repository support

Bazel can [gather source code from multiple locations](/external/overview): you
don't need to vendor your dependencies (but you can!), you can instead point
Bazel to the location of your source code or prebuilt artifacts (e.g. a git
repository or Maven Central), and it takes care of the rest.

## Multi-platform support

Bazel can simultaneously build projects for multiple platforms including Linux,
macOS, Windows, and Android. It also provides powerful
[cross-compilation capabilities](/extending/platforms) to build code for one
platform while running the build on another.

## Wide ecosystem

[Industry leaders](/community/users) love Bazel, building a large
community of developers who use and contribute to Bazel. Find a tools, services
and documentation, including [consulting and SaaS offerings](/community/experts)
Bazel can use. Explore extensions like support for programming languages in
our [open source software repositories](/rules).

---

## Breaking down build performance
- URL: https://bazel.build/advanced/performance/build-performance-breakdown
- Source: advanced/performance/build-performance-breakdown.mdx
- Slug: /advanced/performance/build-performance-breakdown

Bazel is complex and does a lot of different things over the course of a build,
some of which can have an impact on build performance. This page attempts to map
some of these Bazel concepts to their implications on build performance. While
not extensive, we have included some examples of how to detect build performance
issues through [extracting metrics](/configure/build-performance-metrics)
and what you can do to fix them. With this, we hope you can apply these concepts
when investigating build performance regressions.

### Clean vs Incremental builds

A clean build is one that builds everything from scratch, while an incremental
build reuses some already completed work.

We suggest looking at clean and incremental builds separately, especially when
you are collecting / aggregating metrics that are dependent on the state of
Bazel’s caches (for example
[build request size metrics](#deterministic-build-metrics-as-a-proxy-for-build-performance)
). They also represent two different user experiences. As compared to starting
a clean build from scratch (which takes longer due to a cold cache), incremental
builds happen far more frequently as developers iterate on code (typically
faster since the cache is usually already warm).

You can use the `CumulativeMetrics.num_analyses` field in the BEP to classify
builds. If `num_analyses <= 1`, it is a clean build; otherwise, we can broadly
categorize it to likely be an incremental build - the user could have switched
to different flags or different targets causing an effectively clean build. Any
more rigorous definition of incrementality will likely have to come in the form
of a heuristic, for example looking at the number of packages loaded
(`PackageMetrics.packages_loaded`).

### Deterministic build metrics as a proxy for build performance

Measuring build performance can be difficult due to the non-deterministic nature
of certain metrics (for example Bazel’s CPU time or queue times on a remote
cluster). As such, it can be useful to use deterministic metrics as a proxy for
the amount of work done by Bazel, which in turn affects its performance.

The size of a build request can have a significant implication on build
performance. A larger build could represent more work in analyzing and
constructing the build graphs. Organic growth of builds comes naturally with
development, as more dependencies are added/created, and thus grow in complexity
and become more expensive to build.

We can slice this problem into the various build phases, and use the following
metrics as proxy metrics for work done at each phase:

1. `PackageMetrics.packages_loaded`: the number of packages successfully loaded.
  A regression here represents more work that needs to be done to read and parse
  each additional BUILD file in the loading phase.
   - This is often due to the addition of dependencies and having to load their
     transitive closure.
   - Use [query](/query/quickstart) / [cquery](/query/cquery) to find
     where new dependencies might have been added.

2. `TargetMetrics.targets_configured`: representing the number of targets and
  aspects configured in the build. A regression represents more work in
  constructing and traversing the configured target graph.
   - This is often due to the addition of dependencies and having to construct
     the graph of their transitive closure.
   - Use [cquery](/query/cquery) to find where new
     dependencies might have been added.

3. `ActionSummary.actions_created`: represents the actions created in the build,
  and a regression represents more work in constructing the action graph. Note
  that this also includes unused actions that might not have been executed.
   - Use [aquery](/query/aquery) for debugging regressions;
     we suggest starting with
     [`--output=summary`](/reference/command-line-reference#flag--output)
     before further drilling down with
     [`--skyframe_state`](/reference/command-line-reference#flag--skyframe_state).

4. `ActionSummary.actions_executed`: the number of actions executed, a
  regression directly represents more work in executing these actions.
   - The [BEP](/remote/bep) writes out the action statistics
     `ActionData` that shows the most executed action types. By default, it
     collects the top 20 action types, but you can pass in the
     [`--experimental_record_metrics_for_all_mnemonics`](/reference/command-line-reference#flag--experimental_record_metrics_for_all_mnemonics)
     to collect this data for all action types that were executed.
   - This should help you to figure out what kind of actions were executed
     (additionally).

5. `BuildGraphSummary.outputArtifactCount`: the number of artifacts created by
  the executed actions.
   - If the number of actions executed did not increase, then it is likely that
     a rule implementation was changed.


These metrics are all affected by the state of the local cache, hence you will
want to ensure that the builds you extract these metrics from are
**clean builds**.

We have noted that a regression in any of these metrics can be accompanied by
regressions in wall time, cpu time and memory usage.

### Usage of local resources

Bazel consumes a variety of resources on your local machine (both for analyzing
the build graph and driving the execution, and for running local actions), this
can affect the performance / availability of your machine in performing the
build, and also other tasks.

#### Time spent

Perhaps the metrics most susceptible to noise (and can vary greatly from build
to build) is time; in particular - wall time, cpu time and system time. You can
use [bazel-bench](https://github.com/bazelbuild/bazel-bench) to get
a benchmark for these metrics, and with a sufficient number of `--runs`, you can
increase the statistical significance of your measurement.

- **Wall time** is the real world time elapsed.
   - If _only_ wall time regresses, we suggest collecting a
     [JSON trace profile](/advanced/performance/json-trace-profile) and looking
     for differences. Otherwise, it would likely be more efficient to
     investigate other regressed metrics as they could have affected the wall
     time.

- **CPU time** is the time spent by the CPU executing user code.
   - If the CPU time regresses across two project commits, we suggest collecting
     a Starlark CPU profile. You should probably also use `--nobuild` to
     restrict the build to the analysis phase since that is where most of the
     CPU heavy work is done.

- System time is the time spent by the CPU in the kernel.
   - If system time regresses, it is mostly correlated with I/O when Bazel reads
     files from your file system.

#### System-wide load profiling

Using the
[`--experimental_collect_load_average_in_profiler`](https://github.com/bazelbuild/bazel/blob/6.0.0/src/main/java/com/google/devtools/build/lib/runtime/CommonCommandOptions.java#L306-L312)
flag introduced in Bazel 6.0, the
[JSON trace profiler](/advanced/performance/json-trace-profile) collects the
system load average during the invocation.

![Profile that includes system load average](/docs/images/json-trace-profile-system-load-average.png "Profile that includes system load average")

**Figure 1.** Profile that includes system load average.

A high load during a Bazel invocation can be an indication that Bazel schedules
too many local actions in parallel for your machine. You might want to look into
adjusting
[`--local_cpu_resources`](/reference/command-line-reference#flag--local_cpu_resources)
and [`--local_ram_resources`](/reference/command-line-reference#flag--local_ram_resources),
especially in container environments (at least until
[#16512](https://github.com/bazelbuild/bazel/pull/16512) is merged).


#### Monitoring Bazel memory usage

There are two main sources to get Bazel’s memory usage, Bazel `info` and the
[BEP](/remote/bep).

- `bazel info used-heap-size-after-gc`: The amount of used memory in bytes after
  a call to `System.gc()`.
   - [Bazel bench](https://github.com/bazelbuild/bazel-bench)
     provides benchmarks for this metric as well.
   - Additionally, there are `peak-heap-size`, `max-heap-size`, `used-heap-size`
     and `committed-heap-size` (see
     [documentation](/docs/user-manual#configuration-independent-data)), but are
     less relevant.

- [BEP](/remote/bep)’s
  `MemoryMetrics.peak_post_gc_heap_size`: Size of the peak JVM heap size in
  bytes post GC (requires setting
  [`--memory_profile`](/reference/command-line-reference#flag--memory_profile)
  that attempts to force a full GC).

A regression in memory usage is usually a result of a regression in
[build request size metrics](#deterministic_build_metrics_as_a_proxy_for_build_performance),
which are often due to addition of dependencies or a change in the rule
implementation.

To analyze Bazel’s memory footprint on a more granular level, we recommend using
the [built-in memory profiler](/rules/performance#memory-profiling)
for rules.

#### Memory profiling of persistent workers

While [persistent workers](/remote/persistent) can help to speed up builds
significantly (especially for interpreted languages) their memory footprint can
be problematic. Bazel collects metrics on its workers, in particular, the
`WorkerMetrics.WorkerStats.worker_memory_in_kb` field tells how much memory
workers use (by mnemonic).

The [JSON trace profiler](/advanced/performance/json-trace-profile) also
collects persistent worker memory usage during the invocation by passing in the
[`--experimental_collect_system_network_usage`](https://github.com/bazelbuild/bazel/blob/6.0.0/src/main/java/com/google/devtools/build/lib/runtime/CommonCommandOptions.java#L314-L320)
flag (new in Bazel 6.0).

![Profile that includes workers memory usage](/docs/images/json-trace-profile-workers-memory-usage.png "Profile that includes workers memory usage")

**Figure 2.** Profile that includes workers memory usage.

Lowering the value of
[`--worker_max_instances`](/reference/command-line-reference#flag--worker_max_instances)
(default 4) might help to reduce
the amount of memory used by persistent workers. We are actively working on
making Bazel’s resource manager and scheduler smarter so that such fine tuning
will be required less often in the future.

### Monitoring network traffic for remote builds

In remote execution, Bazel downloads artifacts that were built as a result of
executing actions. As such, your network bandwidth can affect the performance
of your build.

If you are using remote execution for your builds, you might want to consider
monitoring the network traffic during the invocation using the
`NetworkMetrics.SystemNetworkStats` proto from the [BEP](/remote/bep)
(requires passing `--experimental_collect_system_network_usage`).

Furthermore, [JSON trace profiles](/advanced/performance/json-trace-profile)
allow you to view system-wide network usage throughout the course of the build
by passing the `--experimental_collect_system_network_usage` flag (new in Bazel
6.0).

![Profile that includes system-wide network usage](/docs/images/json-trace-profile-network-usage.png "Profile that includes system-wide network usage")

**Figure 3.** Profile that includes system-wide network usage.

A high but rather flat network usage when using remote execution might indicate
that network is the bottleneck in your build; if you are not using it already,
consider turning on Build without the Bytes by passing
[`--remote_download_minimal`](/reference/command-line-reference#flag--remote_download_minimal).
This will speed up your builds by avoiding the download of unnecessary intermediate artifacts.

Another option is to configure a local
[disk cache](/reference/command-line-reference#flag--disk_cache) to save on
download bandwidth.

---

## Extracting build performance metrics
- URL: https://bazel.build/advanced/performance/build-performance-metrics
- Source: advanced/performance/build-performance-metrics.mdx
- Slug: /advanced/performance/build-performance-metrics

Probably every Bazel user has experienced builds that were slow or slower than
anticipated. Improving the performance of individual builds has particular value
for targets with significant impact, such as:

1. Core developer targets that are frequently iterated on and (re)built.

2. Common libraries widely depended upon by other targets.

3. A representative target from a class of targets (e.g. custom rules),
  diagnosing and fixing issues in one build might help to resolve issues at the
  larger scale.

An important step to improving the performance of builds is to understand where
resources are spent. This page lists different metrics you can collect.
[Breaking down build performance](/configure/build-performance-breakdown) showcases
how you can use these metrics to detect and fix build performance issues.

There are a few main ways to extract metrics from your Bazel builds, namely:

## Build Event Protocol (BEP)

Bazel outputs a variety of protocol buffers
[`build_event_stream.proto`](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/buildeventstream/proto/build_event_stream.proto)
through the [Build Event Protocol (BEP)](/remote/bep), which
can be aggregated by a backend specified by you. Depending on your use cases,
you might decide to aggregate the metrics in various ways, but here we will go
over some concepts and proto fields that would be useful in general to consider.

## Bazel’s query / cquery / aquery commands

Bazel provides 3 different query modes ([query](/query/quickstart),
[cquery](/query/cquery) and [aquery](/query/aquery)) that allow users
to query the target graph, configured target graph and action graph
respectively. The query language provides a
[suite of functions](/query/language#functions) usable across the different
query modes, that allows you to customize your queries according to your needs.

## JSON Trace Profiles

For every build-like Bazel invocation, Bazel writes a trace profile in JSON
format. The [JSON trace profile](/advanced/performance/json-trace-profile) can
be very useful to quickly understand what Bazel spent time on during the
invocation.

## Execution Log

The [execution log](/remote/cache-remote) can help you to troubleshoot and fix
missing remote cache hits due to machine and environment differences or
non-deterministic actions. If you pass the flag
[`--experimental_execution_log_spawn_metrics`](/reference/command-line-reference#flag--experimental_execution_log_spawn_metrics)
(available from Bazel 5.2) it will also contain detailed spawn metrics, both for
locally and remotely executed actions. You can use these metrics for example to
make comparisons between local and remote machine performance or to find out
which part of the spawn execution is consistently slower than expected (for
example due to queuing).

## Execution Graph Log

While the JSON trace profile contains the critical path information, sometimes
you need additional information on the dependency graph of the executed actions.
Starting with Bazel 6.0, you can pass the flags
`--experimental_execution_graph_log` and
`--experimental_execution_graph_log_dep_type=all` to write out a log about the
executed actions and their inter-dependencies.

This information can be used to understand the drag that is added by a node on
the critical path. The drag is the amount of time that can potentially be saved
by removing a particular node from the execution graph.

The data helps you predict the impact of changes to the build and action graph
before you actually do them.

## Benchmarking with bazel-bench

[Bazel bench](https://github.com/bazelbuild/bazel-bench) is a
benchmarking tool for Git projects to benchmark build performance in the
following cases:

* **Project benchmark:** Benchmarking two git commits against each other at a
 single Bazel version. Used to detect regressions in your build (often through
 the addition of dependencies).

* **Bazel benchmark:** Benchmarking two versions of Bazel against each other at
 a single git commit. Used to detect regressions within Bazel itself (if you
 happen to maintain / fork Bazel).

Benchmarks monitor wall time, CPU  time and system time and Bazel’s retained
heap size.

It is also recommended to run Bazel bench on dedicated, physical machines that
are not running other processes so as to reduce sources of variability.

---

## Optimize Iteration Speed
- URL: https://bazel.build/advanced/performance/iteration-speed
- Source: advanced/performance/iteration-speed.mdx
- Slug: /advanced/performance/iteration-speed

This page describes how to optimize Bazel's build performance when running Bazel
repeatedly.

## Bazel's Runtime State

A Bazel invocation involves several interacting parts.

*   The `bazel` command line interface (CLI) is the user-facing front-end tool
    and receives commands from the user.

*   The CLI tool starts a [*Bazel server*](https://bazel.build/run/client-server)
    for each distinct [output base](https://bazel.build/remote/output-directories).
    The Bazel server is generally persistent, but will shut down after some idle
    time so as to not waste resources.

*   The Bazel server performs the loading and analysis steps for a given command
    (`build`, `run`, `cquery`, etc.), in which it constructs the necessary parts
    of the build graph in memory. The resulting data structures are retained in
    the Bazel server as part of the *analysis cache*.

*   The Bazel server can also perform the action execution, or it can send
    actions off for remote execution if it is set up to do so. The results of
    action executions are also cached, namely in the *action cache* (or
    *execution cache*, which may be either local or remote, and it may be shared
    among Bazel servers).

*   The result of the Bazel invocation is made available in the output tree.

## Running Bazel Iteratively

In a typical developer workflow, it is common to build (or run) a piece of code
repeatedly, often at a very high frequency (e.g. to resolve some compilation
error or investigate a failing test). In this situation, it is important that
repeated invocations of `bazel` have as little overhead as possible relative to
the underlying, repeated action (e.g. invoking a compiler, or executing a test).

With this in mind, we take another look at Bazel's runtime state:

The analysis cache is a critical piece of data. A significant amount of time can
be spent just on the loading and analysis phases of a cold run (i.e. a run just
after the Bazel server was started or when the analysis cache was discarded).
For a single, successful cold build (e.g. for a production release) this cost is
bearable, but for repeatedly building the same target it is important that this
cost be amortized and not repeated on each invocation.

The analysis cache is rather volatile. First off, it is part of the in-process
state of the Bazel server, so losing the server loses the cache. But the cache
is also *invalidated* very easily: for example, many `bazel` command line flags
cause the cache to be discarded. This is because many flags affect the build
graph (e.g. because of
[configurable attributes](https://bazel.build/configure/attributes)). Some flag
changes can also cause the Bazel server to be restarted (e.g. changing
[startup options](https://bazel.build/docs/user-manual#startup-options)).

A good execution cache is also valuable for build performance. An execution
cache can be kept locally
[on disk](https://bazel.build/remote/caching#disk-cache), or
[remotely](https://bazel.build/remote/caching). The cache can be shared among
Bazel servers, and indeed among developers.

## Avoid discarding the analysis cache

Bazel will print a warning if either the analysis cache was discarded or the
server was restarted. Either of these should be avoided during iterative use:

*   Be mindful of changing `bazel` flags in the middle of an iterative
    workflow. For example, mixing a `bazel build -c opt` with a `bazel cquery`
    causes each command to discard the analysis cache of the other. In general,
    try to use a fixed set of flags for the duration of a particular workflow.

*   Losing the Bazel server loses the analysis cache. The Bazel server has a
    [configurable](https://bazel.build/docs/user-manual#max-idle-secs) idle
    time, after which it shuts down. You can configure this time via your
    bazelrc file to suit your needs. The server also restarted when startup
    flags change, so, again, avoid changing those flags if possible.

*   <a id="avoid-ctrl-c">Beware</a> that the Bazel server is killed if you press
    Ctrl-C repeatedly while Bazel is running. It is tempting to try to save time
    by interrupting a running build that is no longer needed, but only press
    Ctrl-C once to request a graceful end of the current invocation.

*   If you want to use multiple sets of flags from the same workspace, you can
    use multiple, distinct output bases, switched with the `--output_base`
    flag. Each output base gets its own Bazel server.

To make this condition an error rather than a warning, you can use the
`--noallow_analysis_cache_discard` flag (introduced in Bazel 6.4.0)

---

## JSON Trace Profile
- URL: https://bazel.build/advanced/performance/json-trace-profile
- Source: advanced/performance/json-trace-profile.mdx
- Slug: /advanced/performance/json-trace-profile

The JSON trace profile can be very useful to quickly understand what Bazel spent
time on during the invocation.

By default, for all build-like commands and query, Bazel writes a profile into
the output base named `command-$INVOCATION_ID.profile.gz`, where
`$INVOCATION_ID` is the invocation identifier of the command. Bazel also creates
a symlink called `command.profile.gz` in the output base that points the profile
of the latest command. You can configure whether a profile is written with the
[`--generate_json_trace_profile`](/reference/command-line-reference#flag--generate_json_trace_profile)
flag, and the location it is written to with the
[`--profile`](/docs/user-manual#profile) flag. Locations ending with `.gz` are
compressed with GZIP. Bazel keeps the last 5 profiles, configurable by
[`--profiles_to_retain`](/reference/command-line-reference#flag--generate_json_trace_profile),
in the output base by default for post-build analysis. Explicitly passing a
profile path with `--profile` disables automatic garbage collection.

## Tools

You can load this profile into `chrome://tracing` or analyze and
post-process it with other tools.

### `chrome://tracing`

To visualize the profile, open `chrome://tracing` in a Chrome browser tab,
click "Load" and pick the (potentially compressed) profile file. For more
detailed results, click the boxes in the lower left corner.

Example profile:

![Example profile](/docs/images/json-trace-profile.png "Example profile")

**Figure 1.** Example profile.

You can use these keyboard controls to navigate:

*   Press `1` for "select" mode. In this mode, you can select
    particular boxes to inspect the event details (see lower left corner).
    Select multiple events to get a summary and aggregated statistics.
*   Press `2` for "pan" mode. Then drag the mouse to move the view. You
    can also use `a`/`d` to move left/right.
*   Press `3` for "zoom" mode. Then drag the mouse to zoom. You can
    also use `w`/`s` to zoom in/out.
*   Press `4` for "timing" mode where you can measure the distance
    between two events.
*   Press `?` to learn about all controls.

### Bazel Invocation Analyzer

The open-source
[Bazel Invocation Analyzer](https://github.com/EngFlow/bazel_invocation_analyzer)
consumes a profile format and prints suggestions on how to improve
the build’s performance. This analysis can be performed using its CLI or on
[https://analyzer.engflow.com](https://analyzer.engflow.com).

### `jq`

`jq` is like `sed` for JSON data. An example usage of `jq` to extract all
durations of the sandbox creation step in local action execution:

```
$ zcat $(../bazel-6.0.0rc1-linux-x86_64 info output_base)/command.profile.gz | jq '.traceEvents | .[] | select(.name == "sandbox.createFileSystem") | .dur'
6378
7247
11850
13756
6555
7445
8487
15520
[...]
```

## Profile information

The profile contains multiple rows. Usually the bulk of rows represent Bazel
threads and their corresponding events, but some special rows are also included.

The special rows included depend on the version of Bazel invoked when the
profile was created, and may be customized by different flags.

Figure 1 shows a profile created with Bazel v5.3.1 and includes these rows:

*   `action count`: Displays how many concurrent actions were in flight. Click
    on it to see the actual value. Should go up to the value of
    [`--jobs`](/reference/command-line-reference#flag--jobs) in clean
    builds.
*   `CPU usage (Bazel)`: For each second of the build, displays the amount of
    CPU that was used by Bazel (a value of 1 equals one core being 100% busy).
*   `Critical Path`: Displays one block for each action on the critical path.
*   `Main Thread`: Bazel’s main thread. Useful to get a high-level picture of
    what Bazel is doing, for example "Launch Blaze", "evaluateTargetPatterns",
    and "runAnalysisPhase".
*   `Garbage Collector`: Displays minor and major Garbage Collection (GC)
    pauses.

## Common performance issues

When analyzing performance profiles, look for:

*   Slower than expected analysis phase (`runAnalysisPhase`), especially on
    incremental builds. This can be a sign of a poor rule implementation, for
    example one that flattens depsets. Package loading can be slow by an
    excessive amount of targets, complex macros or recursive globs.
*   Individual slow actions, especially those on the critical path. It might be
    possible to split large actions into multiple smaller actions or reduce the
    set of (transitive) dependencies to speed them up. Also check for an unusual
    high non-`PROCESS_TIME` (such as `REMOTE_SETUP` or `FETCH`).
*   Bottlenecks, that is a small number of threads is busy while all others are
    idling / waiting for the result (see around 22s and 29s in Figure 1).
    Optimizing this will most likely require touching the rule implementations
    or Bazel itself to introduce more parallelism. This can also happen when
    there is an unusual amount of GC.

## Profile file format

The top-level object contains metadata (`otherData`) and the actual tracing data
(`traceEvents`). The metadata contains extra info, for example the invocation ID
and date of the Bazel invocation.

Example:

```json
{
  "otherData": {
    "build_id": "101bff9a-7243-4c1a-8503-9dc6ae4c3b05",
    "date": "Wed Oct 26 08:22:35 CEST 2022",
    "profile_finish_ts": "1677666095162000",
    "output_base": "/usr/local/google/_bazel_johndoe/573d4be77eaa72b91a3dfaa497bf8cd0"
  },
  "traceEvents": [
    {"name":"thread_name","ph":"M","pid":1,"tid":0,"args":{"name":"Critical Path"}},
    ...
    {"cat":"build phase marker","name":"Launch Blaze","ph":"X","ts":-1306000,"dur":1306000,"pid":1,"tid":21},
    ...
    {"cat":"package creation","name":"foo","ph":"X","ts":2685358,"dur":784,"pid":1,"tid":246},
    ...
    {"name":"thread_name","ph":"M","pid":1,"tid":11,"args":{"name":"Garbage Collector"}},
    {"cat":"gc notification","name":"minor GC","ph":"X","ts":825986,"dur":11000,"pid":1,"tid":11},
    ...
    {"cat":"action processing","name":"Compiling foo/bar.c","ph":"X","ts":54413389,"dur":357594,"pid":1,"args":{"mnemonic":"CppCompile"},"tid":341},
 ]
}
```

Timestamps (`ts`) and durations (`dur`) in the trace events are given in
microseconds. The category (`cat`) is one of enum values of `ProfilerTask`.
Note that some events are merged together if they are very short and close to
each other; pass
[`--noslim_profile`](/reference/command-line-reference#flag--slim_profile)
if you would like to prevent event merging.

See also the
[Chrome Trace Event Format Specification](https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview).

---

## Optimize Memory
- URL: https://bazel.build/advanced/performance/memory
- Source: advanced/performance/memory.mdx
- Slug: /advanced/performance/memory

This page describes how to limit and reduce the memory Bazel uses.

## Running Bazel with Limited RAM

In certain situations, you may want Bazel to use minimal memory. You can set the
maximum heap via the startup flag
[`--host_jvm_args`](/docs/user-manual#host-jvm-args),
like `--host_jvm_args=-Xmx2g`.

### Trade incremental build speeds for memory

If your builds are too big, Bazel may throw an `OutOfMemoryError` (OOM) when
it doesn't have enough memory. You can make Bazel use less memory, at the cost
of slower incremental builds, by passing the following command flags:
[`--discard_analysis_cache`](/docs/user-manual#discard-analysis-cache),
[`--nokeep_state_after_build`](/reference/command-line-reference#flag--keep_state_after_build),
and
[`--notrack_incremental_state`](/reference/command-line-reference#flag--track_incremental_state).

These flags will minimize the memory that Bazel uses in a build, at the cost of
making future builds slower than a standard incremental build would be.

You can also pass any one of these flags individually:

 * `--discard_analysis_cache` will reduce the memory used during execution (not
analysis). Incremental builds will not have to redo package loading, but will
have to redo analysis and execution (although the on-disk action cache can
prevent most re-execution).
 * `--notrack_incremental_state` will not store any edges in Bazel's internal
 dependency graph, so that it is unusable for incremental builds. The next build
 will discard that data, but it is preserved until then, for internal debugging,
 unless `--nokeep_state_after_build` is specified.
 * `--nokeep_state_after_build` will discard all data after the build, so that
 incremental builds have to build from scratch (except for the on-disk action
 cache). Alone, it does not affect the high-water mark of the current build.

### Trade build flexibility for memory with Skyfocus (Experimental)

If you want to make Bazel use less memory *and* retain incremental build speeds,
you can tell Bazel the working set of files that you will be modifying, and
Bazel will only keep state needed to correctly incrementally rebuild changes to
those files. This feature is called **Skyfocus**.

To use Skyfocus, pass the `--experimental_enable_skyfocus` flag:

```sh
bazel build //pkg:target --experimental_enable_skyfocus
```

By default, the working set will be the set of files next to the target being
built. In the example, all files in `//pkg` will be kept in the working set, and
changes to files outside of the working set will be disallowed, until you issue
`bazel clean` or restart the Bazel server.

If you want to specify an exact set of files or directories, use the
`--experimental_working_set` flag, like so:

```sh
bazel build //pkg:target --experimental_enable_skyfocus
--experimental_working_set=path/to/another/dir,path/to/tests/dir
```

You can also pass `--experimental_skyfocus_dump_post_gc_stats` to show the
memory reduction amount:

Putting it altogether, you should see something like this:

```none
$ bazel test //pkg:target //tests/... --experimental_enable_skyfocus --experimental_working_set dir1,dir2,dir3/subdir --experimental_skyfocus_dump_post_gc_stats
INFO: --experimental_enable_skyfocus is enabled. Blaze will reclaim memory not needed to build the working set. Run 'blaze dump --skyframe=working_set' to show the working set, after this command.
WARNING: Changes outside of the working set will cause a build error.
INFO: Analyzed 149 targets (4533 packages loaded, 169438 targets configured).
INFO: Found 25 targets and 124 test targets...
INFO: Updated working set successfully.
INFO: Focusing on 334 roots, 3 leafs... (use --experimental_skyfocus_dump_keys to show them)
INFO: Heap: 1237MB -> 676MB (-45.31%)
INFO: Elapsed time: 192.670s ...
INFO: Build completed successfully, 62303 total actions
```

For this example, using Skyfocus allowed Bazel to drop 561MB (45%) of memory,
and incremental builds to handle changes to files under `dir1`, `dir2`, and
`dir3/subdir` will retain their fast speeds, with the tradeoff that Bazel cannot
rebuild changed files outside of these directories.

## Memory Profiling

Bazel comes with a built-in memory profiler that can help you check your rule’s
memory use. Read more about this process on the
[Memory Profiling section](/rules/performance#memory-profiling) of our
documentation on how to improve the performance of custom rules.

---

## Build Basics
- URL: https://bazel.build/basics
- Source: basics/index.mdx
- Slug: /basics

A build system is one of the most important parts of an engineering organization
because each developer interacts with it potentially dozens or hundreds of times
per day. A fully featured build system is necessary to enable developer
productivity as an organization scales. For individual developers, it's
straightforward to just compile your code and so a build system might seem
excessive. But at a larger scale, having a build system helps with managing
shared dependencies, such as relying on another part of the code base, or an
external resource, such as a library. Build systems help to make sure that you
have everything you need to build your code before it starts building. Build
systems also increase velocity when they're set up to help engineers share
resources and results.

This section covers some history and basics of building and build systems,
including design decisions that went into making Bazel. If you're
familiar with artifact-based build systems, such as Bazel, Buck, and Pants, you
can skip this section, but it's a helpful overview to understand why
artifact-based build systems are excellent at enabling scale.

Note: Much of this section's content comes from the _Build Systems and
Build Philosophy_ chapter of the
[_Software Engineering at Google_ book](https://abseil.io/resources/swe-book/html/ch18.html).
Thank you to the original author, Erik Kuefler, for allowing its reuse and
modification here!

*   **[Why a Build System?](/basics/build-systems)**

    If you haven't used a build system before, start here. This page covers why
    you should use a build system, and why compilers and build scripts aren't
    the best choice once your organization starts to scale beyond a few
    developers.

*   **[Task-Based Build Systems](/basics/task-based-builds)**

    This page discusses task-based build systems (such as Make, Maven, and
    Gradle) and some of their challenges.

*   **[Artifact-Based Build Systems](/basics/artifact-based-builds)**

    This page discusses artifact-based build systems in response to the pain
    points of task-based build systems.

*   **[Distributed Builds](/basics/distributed-builds)**

    This page covers distributed builds, or builds that are executed outside of
    your local machine. This requires more robust infrastructure to share
    resources and build results (and is where the true wizardry happens!)

*   **[Dependency Management](/basics/dependencies)**

    This page covers some complications of dependencies at a large scale and
    strategies to counteract those complications.

---

## Artifact-Based Build Systems
- URL: https://bazel.build/basics/artifact-based-builds
- Source: basics/artifact-based-builds.mdx
- Slug: /basics/artifact-based-builds

This page covers artifact-based build systems and the philosophy behind their
creation. Bazel is an artifact-based build system. While task-based build
systems are good step above build scripts, they give too much power to
individual engineers by letting them define their own tasks.

Artifact-based build systems have a small number of tasks defined by the system
that engineers can configure in a limited way. Engineers still tell the system
**what** to build, but the build system determines **how** to build it. As with
task-based build systems, artifact-based build systems, such as Bazel, still
have buildfiles, but the contents of those buildfiles are very different. Rather
than being an imperative set of commands in a Turing-complete scripting language
describing how to produce an output, buildfiles in Bazel are a declarative
manifest describing a set of artifacts to build, their dependencies, and a
limited set of options that affect how they’re built. When engineers run `bazel`
on the command line, they specify a set of targets to build (the **what**), and
Bazel is responsible for configuring, running, and scheduling the compilation
steps (the **how**). Because the build system now has full control over what
tools to run when, it can make much stronger guarantees that allow it to be far
more efficient while still guaranteeing correctness.

## A functional perspective

It’s easy to make an analogy between artifact-based build systems and functional
programming. Traditional imperative programming languages (such as, Java, C, and
Python) specify lists of statements to be executed one after another, in the
same way that task-based build systems let programmers define a series of steps
to execute. Functional programming languages (such as, Haskell and ML), in
contrast, are structured more like a series of mathematical equations. In
functional languages, the programmer describes a computation to perform, but
leaves the details of when and exactly how that computation is executed to the
compiler.

This maps to the idea of declaring a manifest in an artifact-based build system
and letting the system figure out how to execute the build. Many problems can't
be easily expressed using functional programming, but the ones that do benefit
greatly from it: the language is often able to trivially parallelize such
programs and make strong guarantees about their correctness that would be
impossible in an imperative language. The easiest problems to express using
functional programming are the ones that simply involve transforming one piece
of data into another using a series of rules or functions. And that’s exactly
what a build system is: the whole system is effectively a mathematical function
that takes source files (and tools like the compiler) as inputs and produces
binaries as outputs. So, it’s not surprising that it works well to base a build
system around the tenets of functional programming.

## Understanding artifact-based build systems

Google's build system, Blaze, was the first artifact-based build system. Bazel
is the open-sourced version of Blaze.

Here’s what a buildfile (normally named `BUILD`) looks like in Bazel:

```python
java_binary(
    name = "MyBinary",
    srcs = ["MyBinary.java"],
    deps = [
        ":mylib",
    ],
)
java_library(
    name = "mylib",
    srcs = ["MyLibrary.java", "MyHelper.java"],
    visibility = ["//java/com/example/myproduct:__subpackages__"],
    deps = [
        "//java/com/example/common",
        "//java/com/example/myproduct/otherlib",
    ],
)
```

In Bazel, `BUILD` files define targets—the two types of targets here are
`java_binary` and `java_library`. Every target corresponds to an artifact that
can be created by the system: binary targets produce binaries that can be
executed directly, and library targets produce libraries that can be used by
binaries or other libraries. Every target has:

*   `name`: how the target is referenced on the command line and by other
    targets
*   `srcs`: the source files to be compiled to create the artifact for the target
*   `deps`: other targets that must be built before this target and linked into
    it

Dependencies can either be within the same package (such as `MyBinary`’s
dependency on `:mylib`) or on a different package in the same source hierarchy
(such as `mylib`’s dependency on `//java/com/example/common`).

As with task-based build systems, you perform builds using Bazel’s command-line
tool. To build the `MyBinary` target, you run `bazel build :MyBinary`. After
entering that command for the first time in a clean repository, Bazel:

1.  Parses every `BUILD` file in the workspace to create a graph of dependencies
    among artifacts.
1.  Uses the graph to determine the transitive dependencies of `MyBinary`; that
    is, every target that `MyBinary` depends on and every target that those
    targets depend on, recursively.
1.  Builds each of those dependencies, in order. Bazel starts by building each
    target that has no other dependencies and keeps track of which dependencies
    still need to be built for each target. As soon as all of a target’s
    dependencies are built, Bazel starts building that target. This process
    continues until every one of `MyBinary`’s transitive dependencies have been
    built.
1.  Builds `MyBinary` to produce a final executable binary that links in all of
    the dependencies that were built in step 3.

Fundamentally, it might not seem like what’s happening here is that much
different than what happened when using a task-based build system. Indeed, the
end result is the same binary, and the process for producing it involved
analyzing a bunch of steps to find dependencies among them, and then running
those steps in order. But there are critical differences. The first one appears
in step 3: because Bazel knows that each target only produces a Java library, it
knows that all it has to do is run the Java compiler rather than an arbitrary
user-defined script, so it knows that it’s safe to run these steps in parallel.
This can produce an order of magnitude performance improvement over building
targets one at a time on a multicore machine, and is only possible because the
artifact-based approach leaves the build system in charge of its own execution
strategy so that it can make stronger guarantees about parallelism.

The benefits extend beyond parallelism, though. The next thing that this
approach gives us becomes apparent when the developer types `bazel
build :MyBinary` a second time without making any changes: Bazel exits in less
than a second with a message saying that the target is up to date. This is
possible due to the functional programming paradigm we talked about
earlier—Bazel knows that each target is the result only of running a Java
compiler, and it knows that the output from the Java compiler depends only on
its inputs, so as long as the inputs haven’t changed, the output can be reused.
And this analysis works at every level; if `MyBinary.java` changes, Bazel knows
to rebuild `MyBinary` but reuse `mylib`. If a source file for
`//java/com/example/common` changes, Bazel knows to rebuild that library,
`mylib`, and `MyBinary`, but reuse `//java/com/example/myproduct/otherlib`.
Because Bazel knows about the properties of the tools it runs at every step,
it’s able to rebuild only the minimum set of artifacts each time while
guaranteeing that it won’t produce stale builds.

Reframing the build process in terms of artifacts rather than tasks is subtle
but powerful. By reducing the flexibility exposed to the programmer, the build
system can know more about what is being done at every step of the build. It can
use this knowledge to make the build far more efficient by parallelizing build
processes and reusing their outputs. But this is really just the first step, and
these building blocks of parallelism and reuse form the basis for a distributed
and highly scalable build system.

## Other nifty Bazel tricks

Artifact-based build systems fundamentally solve the problems with parallelism
and reuse that are inherent in task-based build systems. But there are still a
few problems that came up earlier that we haven’t addressed. Bazel has clever
ways of solving each of these, and we should discuss them before moving on.

### Tools as dependencies

One problem we ran into earlier was that builds depended on the tools installed
on our machine, and reproducing builds across systems could be difficult due to
different tool versions or locations. The problem becomes even more difficult
when your project uses languages that require different tools based on which
platform they’re being built on or compiled for (such as, Windows versus Linux),
and each of those platforms requires a slightly different set of tools to do the
same job.

Bazel solves the first part of this problem by treating tools as dependencies to
each target. Every `java_library` in the workspace implicitly depends on a Java
compiler, which defaults to a well-known compiler. Whenever Bazel builds a
`java_library`, it checks to make sure that the specified compiler is available
at a known location. Just like any other dependency, if the Java compiler
changes, every artifact that depends on it is rebuilt.

Bazel solves the second part of the problem, platform independence, by setting
[build configurations](/run/build#build-config-cross-compilation). Rather than
targets depending directly on their tools, they depend on types of configurations:

*   **Host configuration**: building tools that run during the build
*   **Target configuration**: building the binary you ultimately requested

### Extending the build system

Bazel comes with targets for several popular programming languages out of the
box, but engineers will always want to do more—part of the benefit of task-based
systems is their flexibility in supporting any kind of build process, and it
would be better not to give that up in an artifact-based build system.
Fortunately, Bazel allows its supported target types to be extended by
[adding custom rules](/extending/rules).

To define a rule in Bazel, the rule author declares the inputs that the rule
requires (in the form of attributes passed in the `BUILD` file) and the fixed
set of outputs that the rule produces. The author also defines the actions that
will be generated by that rule. Each action declares its inputs and outputs,
runs a particular executable or writes a particular string to a file, and can be
connected to other actions via its inputs and outputs. This means that actions
are the lowest-level composable unit in the build system—an action can do
whatever it wants so long as it uses only its declared inputs and outputs, and
Bazel takes care of scheduling actions and caching their results as appropriate.

The system isn’t foolproof given that there’s no way to stop an action developer
from doing something like introducing a nondeterministic process as part of
their action. But this doesn’t happen very often in practice, and pushing the
possibilities for abuse all the way down to the action level greatly decreases
opportunities for errors. Rules supporting many common languages and tools are
widely available online, and most projects will never need to define their own
rules. Even for those that do, rule definitions only need to be defined in one
central place in the repository, meaning most engineers will be able to use
those rules without ever having to worry about their implementation.

### Isolating the environment

Actions sound like they might run into the same problems as tasks in other
systems—isn’t it still possible to write actions that both write to the same
file and end up conflicting with one another? Actually, Bazel makes these
conflicts impossible by using _[sandboxing](/docs/sandboxing)_. On supported
systems, every action is isolated from every other action via a filesystem
sandbox. Effectively, each action can see only a restricted view of the
filesystem that includes the inputs it has declared and any outputs it has
produced. This is enforced by systems such as LXC on Linux, the same technology
behind Docker. This means that it’s impossible for actions to conflict with one
another because they are unable to read any files they don’t declare, and any
files that they write but don’t declare will be thrown away when the action
finishes. Bazel also uses sandboxes to restrict actions from communicating via
the network.

### Making external dependencies deterministic

There’s still one problem remaining: build systems often need to download
dependencies (whether tools or libraries) from external sources rather than
directly building them. This can be seen in the example via the
`@com_google_common_guava_guava//jar` dependency, which downloads a `JAR` file
from Maven.

Depending on files outside of the current workspace is risky. Those files could
change at any time, potentially requiring the build system to constantly check
whether they’re fresh. If a remote file changes without a corresponding change
in the workspace source code, it can also lead to unreproducible builds—a build
might work one day and fail the next for no obvious reason due to an unnoticed
dependency change. Finally, an external dependency can introduce a huge security
risk when it is owned by a third party: if an attacker is able to infiltrate
that third-party server, they can replace the dependency file with something of
their own design, potentially giving them full control over your build
environment and its output.

The fundamental problem is that we want the build system to be aware of these
files without having to check them into source control. Updating a dependency
should be a conscious choice, but that choice should be made once in a central
place rather than managed by individual engineers or automatically by the
system. This is because even with a “Live at Head” model, we still want builds
to be deterministic, which implies that if you check out a commit from last
week, you should see your dependencies as they were then rather than as they are
now.

Bazel and some other build systems address this problem by requiring a
workspacewide manifest file that lists a _cryptographic hash_ for every external
dependency in the workspace. The hash is a concise way to uniquely represent the
file without checking the entire file into source control. Whenever a new
external dependency is referenced from a workspace, that dependency’s hash is
added to the manifest, either manually or automatically. When Bazel runs a
build, it checks the actual hash of its cached dependency against the expected
hash defined in the manifest and redownloads the file only if the hash differs.

If the artifact we download has a different hash than the one declared in the
manifest, the build will fail unless the hash in the manifest is updated. This
can be done automatically, but that change must be approved and checked into
source control before the build will accept the new dependency. This means that
there’s always a record of when a dependency was updated, and an external
dependency can’t change without a corresponding change in the workspace source.
It also means that, when checking out an older version of the source code, the
build is guaranteed to use the same dependencies that it was using at the point
when that version was checked in (or else it will fail if those dependencies are
no longer available).

Of course, it can still be a problem if a remote server becomes unavailable or
starts serving corrupt data—this can cause all of your builds to begin failing
if you don’t have another copy of that dependency available. To avoid this
problem, we recommend that, for any nontrivial project, you mirror all of its
dependencies onto servers or services that you trust and control. Otherwise you
will always be at the mercy of a third party for your build system’s
availability, even if the checked-in hashes guarantee its security.

---

## Why a Build System?
- URL: https://bazel.build/basics/build-systems
- Source: basics/build-systems.mdx
- Slug: /basics/build-systems

This page discusses what build systems are, what they do, why you should use a
build system, and why compilers and build scripts aren't the best choice as your
organization starts to scale. It's intended for developers who don't have much
experience with a build system.

## What is a build system?

Fundamentally, all build systems have a straightforward purpose: they transform
the source code written by engineers into executable binaries that can be read
by machines. Build systems aren't just for human-authored code; they also allow
machines to create builds automatically, whether for testing or for releases to
production. In an organization with thousands of engineers, it's common that
most builds are triggered automatically rather than directly by engineers.

### Can't I just use a compiler?

The need for a build system might not be immediately obvious. Most engineers
don't use a build system while learning to code: most start by invoking tools
like `gcc` or `javac` directly from the command line, or the equivalent in an
integrated development environment (IDE). As long as all the source code is in
the same directory, a command like this works fine:

```posix-terminal
javac *.java
```

This instructs the Java compiler to take every Java source file in the current
directory and turn it into a binary class file. In the simplest case, this is
all you need.

However, as soon as code expands, the complications begin. `javac` is smart
enough to look in subdirectories of the current directory to find code to
import. But it has no way of finding code stored in _other parts_ of the
filesystem (perhaps a library shared by several projects). It also only knows
how to build Java code. Large systems often involve different pieces written in
a variety of programming languages with webs of dependencies among those pieces,
meaning no compiler for a single language can possibly build the entire system.

Once you're dealing with code from multiple languages or multiple compilation
units, building code is no longer a one-step process. Now you must evaluate what
your code depends on and build those pieces in the proper order, possibly using
a different set of tools for each piece. If any dependencies change, you must
repeat this process to avoid depending on stale binaries. For a codebase of even
moderate size, this process quickly becomes tedious and error-prone.

The compiler also doesn’t know anything about how to handle external
dependencies, such as third-party `JAR` files in Java. Without a build system,
you could manage this by downloading the dependency from the internet, sticking
it in a `lib` folder on the hard drive, and configuring the compiler to read
libraries from that directory. Over time, it's difficult to maintain the
updates, versions, and source of these external dependencies.

### What about shell scripts?

Suppose that your hobby project starts out simple enough that you can build it
using just a compiler, but you begin running into some of the problems described
previously. Maybe you still don’t think you need a build system and can automate
away the tedious parts using some simple shell scripts that take care of
building things in the correct order. This helps out for a while, but pretty
soon you start running into even more problems:

*   It becomes tedious. As your system grows more complex, you begin spending
    almost as much time working on your build scripts as on real code. Debugging
    shell scripts is painful, with more and more hacks being layered on top of
    one another.

*   It’s slow. To make sure you weren’t accidentally relying on stale libraries,
    you have your build script build every dependency in order every time you
    run it. You think about adding some logic to detect which parts need to be
    rebuilt, but that sounds awfully complex and error prone for a script. Or
    you think about specifying which parts need to be rebuilt each time, but
    then you’re back to square one.

*   Good news: it’s time for a release! Better go figure out all the arguments
    you need to pass to the jar command to make your final build. And remember
    how to upload it and push it out to the central repository. And build and
    push the documentation updates, and send out a notification to users. Hmm,
    maybe this calls for another script...

*   Disaster! Your hard drive crashes, and now you need to recreate your entire
    system. You were smart enough to keep all of your source files in version
    control, but what about those libraries you downloaded? Can you find them
    all again and make sure they were the same version as when you first
    downloaded them? Your scripts probably depended on particular tools being
    installed in particular places—can you restore that same environment so that
    the scripts work again? What about all those environment variables you set a
    long time ago to get the compiler working just right and then forgot about?

*   Despite the problems, your project is successful enough that you’re able to
    begin hiring more engineers. Now you realize that it doesn’t take a disaster
    for the previous problems to arise—you need to go through the same painful
    bootstrapping process every time a new developer joins your team. And
    despite your best efforts, there are still small differences in each
    person’s system. Frequently, what works on one person’s machine doesn’t work
    on another’s, and each time it takes a few hours of debugging tool paths or
    library versions to figure out where the difference is.

*   You decide that you need to automate your build system. In theory, this is
    as simple as getting a new computer and setting it up to run your build
    script every night using cron. You still need to go through the painful
    setup process, but now you don’t have the benefit of a human brain being
    able to detect and resolve minor problems. Now, every morning when you get
    in, you see that last night’s build failed because yesterday a developer
    made a change that worked on their system but didn’t work on the automated
    build system. Each time it’s a simple fix, but it happens so often that you
    end up spending a lot of time each day discovering and applying these simple
    fixes.

*   Builds become slower and slower as the project grows. One day, while waiting
    for a build to complete, you gaze mournfully at the idle desktop of your
    coworker, who is on vacation, and wish there were a way to take advantage of
    all that wasted computational power.

You’ve run into a classic problem of scale. For a single developer working on at
most a couple hundred lines of code for at most a week or two (which might have
been the entire experience thus far of a junior developer who just graduated
university), a compiler is all you need. Scripts can maybe take you a little bit
farther. But as soon as you need to coordinate across multiple developers and
their machines, even a perfect build script isn’t enough because it becomes very
difficult to account for the minor differences in those machines. At this point,
this simple approach breaks down and it’s time to invest in a real build system.

---

## Dependency Management
- URL: https://bazel.build/basics/dependencies
- Source: basics/dependencies.mdx
- Slug: /basics/dependencies

In looking through the previous pages, one theme repeats over and over: managing
your own code is fairly straightforward, but managing its dependencies is much
more difficult. There are all sorts of dependencies: sometimes there’s a
dependency on a task (such as “push the documentation before I mark a release as
complete”), and sometimes there’s a dependency on an artifact (such as “I need
to have the latest version of the computer vision library to build my code”).
Sometimes, you have internal dependencies on another part of your codebase, and
sometimes you have external dependencies on code or data owned by another team
(either in your organization or a third party). But in any case, the idea of “I
need that before I can have this” is something that recurs repeatedly in the
design of build systems, and managing dependencies is perhaps the most
fundamental job of a build system.

## Dealing with Modules and Dependencies

Projects that use artifact-based build systems like Bazel are broken into a set
of modules, with modules expressing dependencies on one another via `BUILD`
files. Proper organization of these modules and dependencies can have a huge
effect on both the performance of the build system and how much work it takes to
maintain.

## Using Fine-Grained Modules and the 1:1:1 Rule

The first question that comes up when structuring an artifact-based build is
deciding how much functionality an individual module should encompass. In Bazel,
a _module_ is represented by a target specifying a buildable unit like a
`java_library` or a `go_binary`. At one extreme, the entire project could be
contained in a single module by putting one `BUILD` file at the root and
recursively globbing together all of that project’s source files. At the other
extreme, nearly every source file could be made into its own module, effectively
requiring each file to list in a `BUILD` file every other file it depends on.

Most projects fall somewhere between these extremes, and the choice involves a
trade-off between performance and maintainability. Using a single module for the
entire project might mean that you never need to touch the `BUILD` file except
when adding an external dependency, but it means that the build system must
always build the entire project all at once. This means that it won’t be able to
parallelize or distribute parts of the build, nor will it be able to cache parts
that it’s already built. One-module-per-file is the opposite: the build system
has the maximum flexibility in caching and scheduling steps of the build, but
engineers need to expend more effort maintaining lists of dependencies whenever
they change which files reference which.

Though the exact granularity varies by language (and often even within
language), Google tends to favor significantly smaller modules than one might
typically write in a task-based build system. A typical production binary at
Google often depends on tens of thousands of targets, and even a moderate-sized
team can own several hundred targets within its codebase. For languages like
Java that have a strong built-in notion of packaging, each directory usually
contains a single package, target, and `BUILD` file (Pants, another build system
based on Bazel, calls this the 1:1:1 rule). Languages with weaker packaging
conventions frequently define multiple targets per `BUILD` file.

The benefits of smaller build targets really begin to show at scale because they
lead to faster distributed builds and a less frequent need to rebuild targets.
The advantages become even more compelling after testing enters the picture, as
finer-grained targets mean that the build system can be much smarter about
running only a limited subset of tests that could be affected by any given
change. Because Google believes in the systemic benefits of using smaller
targets, we’ve made some strides in mitigating the downside by investing in
tooling to automatically manage `BUILD` files to avoid burdening developers.

Some of these tools, such as `buildifier` and `buildozer`, are available with
Bazel in the [`buildtools`
directory](https://github.com/bazelbuild/buildtools).

## Minimizing Module Visibility

Bazel and other build systems allow each target to specify a visibility — a
property that determines which other targets may depend on it. A private target
can only be referenced within its own `BUILD` file. A target may grant broader
visibility to the targets of an explicitly defined list of `BUILD` files, or, in
the case of public visibility, to every target in the workspace.

As with most programming languages, it is usually best to minimize visibility as
much as possible. Generally, teams at Google will make targets public only if
those targets represent widely used libraries available to any team at Google.
Teams that require others to coordinate with them before using their code will
maintain an allowlist of customer targets as their target’s visibility. Each
team’s internal implementation targets will be restricted to only directories
owned by the team, and most `BUILD` files will have only one target that isn’t
private.

## Managing Dependencies

Modules need to be able to refer to one another. The downside of breaking a
codebase into fine-grained modules is that you need to manage the dependencies
among those modules (though tools can help automate this). Expressing these
dependencies usually ends up being the bulk of the content in a `BUILD` file.

### Internal dependencies

In a large project broken into fine-grained modules, most dependencies are
likely to be internal; that is, on another target defined and built in the same
source repository. Internal dependencies differ from external dependencies in
that they are built from source rather than downloaded as a prebuilt artifact
while running the build. This also means that there’s no notion of “version” for
internal dependencies—a target and all of its internal dependencies are always
built at the same commit/revision in the repository. One issue that should be
handled carefully with regard to internal dependencies is how to treat
transitive dependencies (Figure 1). Suppose target A depends on target B, which
depends on a common library target C. Should target A be able to use classes
defined in target C?

[![Transitive
dependencies](/images/transitive-dependencies.png)](/images/transitive-dependencies.png)

**Figure 1**. Transitive dependencies

As far as the underlying tools are concerned, there’s no problem with this; both
B and C will be linked into target A when it is built, so any symbols defined in
C are known to A. Bazel allowed this for many years, but as Google grew, we
began to see problems. Suppose that B was refactored such that it no longer
needed to depend on C. If B’s dependency on C was then removed, A and any other
target that used C via a dependency on B would break. Effectively, a target’s
dependencies became part of its public contract and could never be safely
changed. This meant that dependencies accumulated over time and builds at Google
started to slow down.

Google eventually solved this issue by introducing a “strict transitive
dependency mode” in Bazel. In this mode, Bazel detects whether a target tries to
reference a symbol without depending on it directly and, if so, fails with an
error and a shell command that can be used to automatically insert the
dependency. Rolling this change out across Google’s entire codebase and
refactoring every one of our millions of build targets to explicitly list their
dependencies was a multiyear effort, but it was well worth it. Our builds are
now much faster given that targets have fewer unnecessary dependencies, and
engineers are empowered to remove dependencies they don’t need without worrying
about breaking targets that depend on them.

As usual, enforcing strict transitive dependencies involved a trade-off. It made
build files more verbose, as frequently used libraries now need to be listed
explicitly in many places rather than pulled in incidentally, and engineers
needed to spend more effort adding dependencies to `BUILD` files. We’ve since
developed tools that reduce this toil by automatically detecting many missing
dependencies and adding them to a `BUILD` files without any developer
intervention. But even without such tools, we’ve found the trade-off to be well
worth it as the codebase scales: explicitly adding a dependency to `BUILD` file
is a one-time cost, but dealing with implicit transitive dependencies can cause
ongoing problems as long as the build target exists. Bazel [enforces strict
transitive
dependencies](https://blog.bazel.build/2017/06/28/sjd-unused_deps.html)
on Java code by default.

### External dependencies

If a dependency isn’t internal, it must be external. External dependencies are
those on artifacts that are built and stored outside of the build system. The
dependency is imported directly from an artifact repository (typically accessed
over the internet) and used as-is rather than being built from source. One of
the biggest differences between external and internal dependencies is that
external dependencies have versions, and those versions exist independently of
the project’s source code.

### Automatic versus manual dependency management

Build systems can allow the versions of external dependencies to be managed
either manually or automatically. When managed manually, the buildfile
explicitly lists the version it wants to download from the artifact repository,
often using a [semantic version string](https://semver.org/) such
as `1.1.4`. When managed automatically, the source file specifies a range of
acceptable versions, and the build system always downloads the latest one. For
example, Gradle allows a dependency version to be declared as “1.+” to specify
that any minor or patch version of a dependency is acceptable so long as the
major version is 1.

Automatically managed dependencies can be convenient for small projects, but
they’re usually a recipe for disaster on projects of nontrivial size or that are
being worked on by more than one engineer. The problem with automatically
managed dependencies is that you have no control over when the version is
updated. There’s no way to guarantee that external parties won’t make breaking
updates (even when they claim to use semantic versioning), so a build that
worked one day might be broken the next with no easy way to detect what changed
or to roll it back to a working state. Even if the build doesn’t break, there
can be subtle behavior or performance changes that are impossible to track down.

In contrast, because manually managed dependencies require a change in source
control, they can be easily discovered and rolled back, and it’s possible to
check out an older version of the repository to build with older dependencies.
Bazel requires that versions of all dependencies be specified manually. At even
moderate scales, the overhead of manual version management is well worth it for
the stability it provides.

### The One-Version Rule

Different versions of a library are usually represented by different artifacts,
so in theory there’s no reason that different versions of the same external
dependency couldn’t both be declared in the build system under different names.
That way, each target could choose which version of the dependency it wanted to
use. This causes a lot of problems in practice, so Google enforces a strict
[One-Version
Rule](https://opensource.google/docs/thirdparty/oneversion/) for
all third-party dependencies in our codebase.

The biggest problem with allowing multiple versions is the diamond dependency
issue. Suppose that target A depends on target B and on v1 of an external
library. If target B is later refactored to add a dependency on v2 of the same
external library, target A will break because it now depends implicitly on two
different versions of the same library. Effectively, it’s never safe to add a
new dependency from a target to any third-party library with multiple versions,
because any of that target’s users could already be depending on a different
version. Following the One-Version Rule makes this conflict impossible—if a
target adds a dependency on a third-party library, any existing dependencies
will already be on that same version, so they can happily coexist.

### Transitive external dependencies

Dealing with the transitive dependencies of an external dependency can be
particularly difficult. Many artifact repositories such as Maven Central, allow
artifacts to specify dependencies on particular versions of other artifacts in
the repository. Build tools like Maven or Gradle often recursively download each
transitive dependency by default, meaning that adding a single dependency in
your project could potentially cause dozens of artifacts to be downloaded in
total.

This is very convenient: when adding a dependency on a new library, it would be
a big pain to have to track down each of that library’s transitive dependencies
and add them all manually. But there’s also a huge downside: because different
libraries can depend on different versions of the same third-party library, this
strategy necessarily violates the One-Version Rule and leads to the diamond
dependency problem. If your target depends on two external libraries that use
different versions of the same dependency, there’s no telling which one you’ll
get. This also means that updating an external dependency could cause seemingly
unrelated failures throughout the codebase if the new version begins pulling in
conflicting versions of some of its dependencies.

Bazel did not use to automatically download transitive dependencies. It used to
employ a `WORKSPACE` file that required all transitive dependencies to be
listed, which led to a lot of pain when managing external dependencies. Bazel
has since added support for automatic transitive external dependency management
in the form of the `MODULE.bazel` file. See [external dependency
overview](/external/overview) for more details.

Yet again, the choice here is one between convenience and scalability. Small
projects might prefer not having to worry about managing transitive dependencies
themselves and might be able to get away with using automatic transitive
dependencies. This strategy becomes less and less appealing as the organization
and codebase grows, and conflicts and unexpected results become more and more
frequent. At larger scales, the cost of manually managing dependencies is much
less than the cost of dealing with issues caused by automatic dependency
management.

### Caching build results using external dependencies

External dependencies are most often provided by third parties that release
stable versions of libraries, perhaps without providing source code. Some
organizations might also choose to make some of their own code available as
artifacts, allowing other pieces of code to depend on them as third-party rather
than internal dependencies. This can theoretically speed up builds if artifacts
are slow to build but quick to download.

However, this also introduces a lot of overhead and complexity: someone needs to
be responsible for building each of those artifacts and uploading them to the
artifact repository, and clients need to ensure that they stay up to date with
the latest version. Debugging also becomes much more difficult because different
parts of the system will have been built from different points in the
repository, and there is no longer a consistent view of the source tree.

A better way to solve the problem of artifacts taking a long time to build is to
use a build system that supports remote caching, as described earlier. Such a
build system saves the resulting artifacts from every build to a location that
is shared across engineers, so if a developer depends on an artifact that was
recently built by someone else, the build system automatically downloads it
instead of building it. This provides all of the performance benefits of
depending directly on artifacts while still ensuring that builds are as
consistent as if they were always built from the same source. This is the
strategy used internally by Google, and Bazel can be configured to use a remote
cache.

### Security and reliability of external dependencies

Depending on artifacts from third-party sources is inherently risky. There’s an
availability risk if the third-party source (such as an artifact repository)
goes down, because your entire build might grind to a halt if it’s unable to
download an external dependency. There’s also a security risk: if the
third-party system is compromised by an attacker, the attacker could replace the
referenced artifact with one of their own design, allowing them to inject
arbitrary code into your build. Both problems can be mitigated by mirroring any
artifacts you depend on onto servers you control and blocking your build system
from accessing third-party artifact repositories like Maven Central. The
trade-off is that these mirrors take effort and resources to maintain, so the
choice of whether to use them often depends on the scale of the project. The
security issue can also be completely prevented with little overhead by
requiring the hash of each third-party artifact to be specified in the source
repository, causing the build to fail if the artifact is tampered with. Another
alternative that completely sidesteps the issue is to vendor your project’s
dependencies. When a project vendors its dependencies, it checks them into
source control alongside the project’s source code, either as source or as
binaries. This effectively means that all of the project’s external dependencies
are converted to internal dependencies. Google uses this approach internally,
checking every third-party library referenced throughout Google into a
`third_party` directory at the root of Google’s source tree. However, this works
at Google only because Google’s source control system is custom built to handle
an extremely large monorepo, so vendoring might not be an option for all
organizations.

---

## Distributed Builds
- URL: https://bazel.build/basics/distributed-builds
- Source: basics/distributed-builds.mdx
- Slug: /basics/distributed-builds

When you have a large codebase, chains of dependencies can become very deep.
Even simple binaries can often depend on tens of thousands of build targets. At
this scale, it’s simply impossible to complete a build in a reasonable amount
of time on a single machine: no build system can get around the fundamental
laws of physics imposed on a machine’s hardware. The only way to make this work
is with a build system that supports distributed builds wherein the units of
work being done by the system are spread across an arbitrary and scalable
number of machines. Assuming we’ve broken the system’s work into small enough
units (more on this later), this would allow us to complete any build of any
size as quickly as we’re willing to pay for. This scalability is the holy grail
we’ve been working toward by defining an artifact-based build system.

## Remote caching

The simplest type of distributed build is one that only leverages _remote
caching_, which is shown in Figure 1.

[![Distributed build with remote caching](/images/distributed-build-remote-cache.png)](/images/distributed-build-remote-cache.png)

**Figure 1**. A distributed build showing remote caching

Every system that performs builds, including both developer workstations and
continuous integration systems, shares a reference to a common remote cache
service. This service might be a fast and local short-term storage system like
Redis or a cloud service like Google Cloud Storage. Whenever a user needs to
build an artifact, whether directly or as a dependency, the system first checks
with the remote cache to see if that artifact already exists there. If so, it
can download the artifact instead of building it. If not, the system builds the
artifact itself and uploads the result back to the cache. This means that
low-level dependencies that don’t change very often can be built once and shared
across users rather than having to be rebuilt by each user. At Google, many
artifacts are served from a cache rather than built from scratch, vastly
reducing the cost of running our build system.

For a remote caching system to work, the build system must guarantee that builds
are completely reproducible. That is, for any build target, it must be possible
to determine the set of inputs to that target such that the same set of inputs
will produce exactly the same output on any machine. This is the only way to
ensure that the results of downloading an artifact are the same as the results
of building it oneself. Note that this requires that each artifact in the cache
be keyed on both its target and a hash of its inputs—that way, different
engineers could make different modifications to the same target at the same
time, and the remote cache would store all of the resulting artifacts and serve
them appropriately without conflict.

Of course, for there to be any benefit from a remote cache, downloading an
artifact needs to be faster than building it. This is not always the case,
especially if the cache server is far from the machine doing the build. Google’s
network and build system is carefully tuned to be able to quickly share build
results.

## Remote execution

Remote caching isn’t a true distributed build. If the cache is lost or if you
make a low-level change that requires everything to be rebuilt, you still need
to perform the entire build locally on your machine. The true goal is to support
remote execution, in which the actual work of doing the build can be spread
across any number of workers. Figure 2 depicts a remote execution system.

[![Remote execution system](/images/remote-execution-system.png)](/images/remote-execution-system.png)

**Figure 2**. A remote execution system

The build tool running on each user’s machine (where users are either human
engineers or automated build systems) sends requests to a central build master.
The build master breaks the requests into their component actions and schedules
the execution of those actions over a scalable pool of workers. Each worker
performs the actions asked of it with the inputs specified by the user and
writes out the resulting artifacts. These artifacts are shared across the other
machines executing actions that require them until the final output can be
produced and sent to the user.

The trickiest part of implementing such a system is managing the communication
between the workers, the master, and the user’s local machine. Workers might
depend on intermediate artifacts produced by other workers, and the final output
needs to be sent back to the user’s local machine. To do this, we can build on
top of the distributed cache described previously by having each worker write
its results to and read its dependencies from the cache. The master blocks
workers from proceeding until everything they depend on has finished, in which
case they’ll be able to read their inputs from the cache. The final product is
also cached, allowing the local machine to download it. Note that we also need a
separate means of exporting the local changes in the user’s source tree so that
workers can apply those changes before building.

For this to work, all of the parts of the artifact-based build systems described
earlier need to come together. Build environments must be completely
self-describing so that we can spin up workers without human intervention. Build
processes themselves must be completely self-contained because each step might
be executed on a different machine. Outputs must be completely deterministic so
that each worker can trust the results it receives from other workers. Such
guarantees are extremely difficult for a task-based system to provide, which
makes it nigh-impossible to build a reliable remote execution system on top of
one.

## Distributed builds at Google

Since 2008, Google has been using a distributed build system that employs both
remote caching and remote execution, which is illustrated in Figure 3.

[![High-level build system](/images/high-level-build-system.png)](/images/high-level-build-system.png)

**Figure 3**. Google’s distributed build system

Google’s remote cache is called ObjFS. It consists of a backend that stores
build outputs in Bigtables distributed throughout our fleet of production
machines and a frontend FUSE daemon named objfsd that runs on each developer’s
machine. The FUSE daemon allows engineers to browse build outputs as if they
were normal files stored on the workstation, but with the file content
downloaded on-demand only for the few files that are directly requested by the
user. Serving file contents on-demand greatly reduces both network and disk
usage, and the system is able to build twice as fast compared to when we stored
all build output on the developer’s local disk.

Google’s remote execution system is called Forge. A Forge client in Blaze
(Bazel's internal equivalent) called
the Distributor sends requests for each action to a job running in our
datacenters called the Scheduler. The Scheduler maintains a cache of action
results, allowing it to return a response immediately if the action has already
been created by any other user of the system. If not, it places the action into
a queue. A large pool of Executor jobs continually read actions from this queue,
execute them, and store the results directly in the ObjFS Bigtables. These
results are available to the executors for future actions, or to be downloaded
by the end user via objfsd.

The end result is a system that scales to efficiently support all builds
performed at Google. And the scale of Google’s builds is truly massive: Google
runs millions of builds executing millions of test cases and producing petabytes
of build outputs from billions of lines of source code every day. Not only does
such a system let our engineers build complex codebases quickly, it also allows
us to implement a huge number of automated tools and systems that rely on our
build.

---

## Hermeticity
- URL: https://bazel.build/basics/hermeticity
- Source: basics/hermeticity.mdx
- Slug: /basics/hermeticity

This page covers hermeticity, the benefits of using hermetic builds, and
strategies for identifying non-hermetic behavior in your builds.

## Overview

When given the same input source code and product configuration, a hermetic
build system always returns the same output by isolating the build from changes
to the host system.

In order to isolate the build, hermetic builds are insensitive to libraries and
other software installed on the local or remote host machine. They depend on
specific versions of build tools, such as compilers, and dependencies, such as
libraries. This makes the build process self-contained as it doesn't rely on
services external to the build environment.

The two important aspects of hermeticity are:

* **Isolation**: Hermetic build systems treat tools as source code. They
  download copies of tools and manage their storage and use inside managed file
  trees. This creates isolation between the host machine and local user,
  including installed versions of languages.
* **Source identity**: Hermetic build systems try to ensure the sameness of
  inputs. Code repositories, such as Git, identify sets of code mutations with a
  unique hash code. Hermetic build systems use this hash to identify changes to
  the build's input.

## Benefits

The major benefits of hermetic builds are:

* **Speed**: The output of an action can be cached, and the action need not be
  run again unless inputs change.
* **Parallel execution**: For given input and output, the build system can
  construct a graph of all actions to calculate efficient and parallel
  execution. The build system loads the rules and calculates an action graph
  and hash inputs to look up in the cache.
* **Multiple builds**: You can build multiple hermetic builds on the same
  machine, each build using different tools and versions.
* **Reproducibility**: Hermetic builds are good for troubleshooting because you
  know the exact conditions that produced the build.

## Identifying non-hermeticity

If you are preparing to switch to Bazel, migration is easier if you improve
your existing builds' hermeticity in advance. Some common sources of
non-hermeticity in builds are:

* Arbitrary processing in `.mk` files
* Actions or tooling that create files non-deterministically, usually involving
  build IDs or timestamps
* System binaries that differ across hosts (such as `/usr/bin` binaries, absolute
  paths, system C++ compilers for native C++ rules autoconfiguration)
* Writing to the source tree during the build. This prevents the same source
  tree from being used for another target. The first build writes to the source
  tree, fixing the source tree for target A. Then trying to build target B may
  fail.

## Troubleshooting non-hermetic builds

Starting with local execution, issues that affect local cache hits reveal
non-hermetic actions.

* Ensure null sequential builds: If you run `make` and get a successful build,
  running the build again should not rebuild any targets. If you run each build
  step twice or on different systems, compare a hash of the file contents and
  get results that differ, the build is not reproducible.
* Run steps to
  [debug local cache hits](/remote/cache-remote#troubleshooting-cache-hits)
  from a variety of potential client machines to ensure that you catch any
  cases of client environment leaking into the actions.
* Execute a build within a docker container that contains nothing but the
  checked-out source tree and explicit list of host tools. Build breakages and
  error messages will catch implicit system dependencies.
* Discover and fix hermeticity problems using
  [remote execution rules](/remote/rules#overview).
* Enable strict [sandboxing](/docs/sandboxing)
  at the per-action level, since actions in a build can be stateful and affect
  the build or the output.
* [Workspace rules](/remote/workspace)
  allow developers to add dependencies to external workspaces, but they are
  rich enough to allow arbitrary processing to happen in the process. You can
  get a log of some potentially non-hermetic actions in Bazel workspace rules by
  adding the flag
  `--experimental_workspace_rules_log_file=<var>PATH</var>` to
  your Bazel command.

Note: Make your build fully hermetic when mixing remote and local execution,
using Bazel’s “dynamic strategy” functionality. Running Bazel inside the remote
Docker container will enable the build to execute the same in both environments.

## Hermeticity with Bazel

For more information about how other projects have had success using hermetic
builds with Bazel, see these  BazelCon talks:

*   [Building Real-time Systems with Bazel](https://www.youtube.com/watch?v=t_3bckhV_YI) (SpaceX)
*   [Bazel Remote Execution and Remote Caching](https://www.youtube.com/watch?v=_bPyEbAyC0s) (Uber and TwoSigma)
*   [Faster Builds With Remote Execution and Caching](https://www.youtube.com/watch?v=MyuJRUwT5LI)
*   [Fusing Bazel: Faster Incremental Builds](https://www.youtube.com/watch?v=rQd9Zd1ONOw)
*   [Remote Execution vs Local Execution](https://www.youtube.com/watch?v=C8wHmIln--g)
*   [Improving the Usability of Remote Caching](https://www.youtube.com/watch?v=u5m7V3ZRHLA) (IBM)
*   [Building Self Driving Cars with Bazel](https://www.youtube.com/watch?v=Gh4SJuYUoQI&list=PLxNYxgaZ8Rsf-7g43Z8LyXct9ax6egdSj&index=4&t=0s) (BMW)
*   [Building Self Driving Cars with Bazel + Q&A](https://www.youtube.com/watch?v=fjfFe98LTm8&list=PLxNYxgaZ8Rsf-7g43Z8LyXct9ax6egdSj&index=29) (GM Cruise)

---

## Task-Based Build Systems
- URL: https://bazel.build/basics/task-based-builds
- Source: basics/task-based-builds.mdx
- Slug: /basics/task-based-builds

This page covers task-based build systems, how they work and some of the
complications that can occur with task-based systems. After shell scripts,
task-based build systems are the next logical evolution of building.


## Understanding task-based build systems

In a task-based build system, the fundamental unit of work is the task. Each
task is a script that can execute any sort of logic, and tasks specify other
tasks as dependencies that must run before them. Most major build systems in use
today, such as Ant, Maven, Gradle, Grunt, and Rake, are task based. Instead of
shell scripts, most modern build systems require engineers to create build files
that describe how to perform the build.

Take this example from the
[Ant manual](https://ant.apache.org/manual/using.html):

```xml
<project name="MyProject" default="dist" basedir=".">
   <description>
     simple example build file
   </description>
   <!-- set global properties for this build -->
   <property name="src" location="src"/>
   <property name="build" location="build"/>
   <property name="dist" location="dist"/>

   <target name="init">
     <!-- Create the time stamp -->
     <tstamp/>
     <!-- Create the build directory structure used by compile -->
     <mkdir dir="${build}"/>
   </target>
   <target name="compile" depends="init"
       description="compile the source">
     <!-- Compile the Java code from ${src} into ${build} -->
     <javac srcdir="${src}" destdir="${build}"/>
   </target>
   <target name="dist" depends="compile"
       description="generate the distribution">
     <!-- Create the distribution directory -->
     <mkdir dir="${dist}/lib"/>
     <!-- Put everything in ${build} into the MyProject-${DSTAMP}.jar file -->
     <jar jarfile="${dist}/lib/MyProject-${DSTAMP}.jar" basedir="${build}"/>
   </target>
   <target name="clean"
       description="clean up">
     <!-- Delete the ${build} and ${dist} directory trees -->
     <delete dir="${build}"/>
     <delete dir="${dist}"/>
   </target>
</project>
```

The buildfile is written in XML and defines some simple metadata about the build
along with a list of tasks (the `<target>` tags in the XML). (Ant uses the word
_target_ to represent a _task_, and it uses the word _task_ to refer to
_commands_.) Each task executes a list of possible commands defined by Ant,
which here include creating and deleting directories, running `javac`, and
creating a JAR file. This set of commands can be extended by user-provided
plug-ins to cover any sort of logic. Each task can also define the tasks it
depends on via the depends attribute. These dependencies form an acyclic graph,
as seen in Figure 1.

[![Acrylic graph showing dependencies](/images/task-dependencies.png)](/images/task-dependencies.png)

Figure 1. An acyclic graph showing dependencies

Users perform builds by providing tasks to Ant’s command-line tool. For example,
when a user types `ant dist`, Ant takes the following steps:

1.  Loads a file named `build.xml` in the current directory and parses it to
    create the graph structure shown in Figure 1.
1.  Looks for the task named `dist` that was provided on the command line and
    discovers that it has a dependency on the task named `compile`.
1.  Looks for the task named `compile` and discovers that it has a dependency on
    the task named `init`.
1.  Looks for the task named `init` and discovers that it has no dependencies.
1.  Executes the commands defined in the `init` task.
1.  Executes the commands defined in the `compile` task given that all of that
    task’s dependencies have been run.
1.  Executes the commands defined in the `dist` task given that all of that
    task’s dependencies have been run.

In the end, the code executed by Ant when running the `dist` task is equivalent
to the following shell script:

```posix-terminal
./createTimestamp.sh

mkdir build/

javac src/* -d build/

mkdir -p dist/lib/

jar cf dist/lib/MyProject-$(date --iso-8601).jar build/*
```

When the syntax is stripped away, the buildfile and the build script actually
aren’t too different. But we’ve already gained a lot by doing this. We can
create new buildfiles in other directories and link them together. We can easily
add new tasks that depend on existing tasks in arbitrary and complex ways. We
need only pass the name of a single task to the `ant` command-line tool, and it
determines everything that needs to be run.

Ant is an old piece of software, originally released in 2000. Other tools like
Maven and Gradle have improved on Ant in the intervening years and essentially
replaced it by adding features like automatic management of external
dependencies and a cleaner syntax without any XML. But the nature of these newer
systems remains the same: they allow engineers to write build scripts in a
principled and modular way as tasks and provide tools for executing those tasks
and managing dependencies among them.

## The dark side of task-based build systems

Because these tools essentially let engineers define any script as a task, they
are extremely powerful, allowing you to do pretty much anything you can imagine
with them. But that power comes with drawbacks, and task-based build systems can
become difficult to work with as their build scripts grow more complex. The
problem with such systems is that they actually end up giving _too much power to
engineers and not enough power to the system_. Because the system has no idea
what the scripts are doing, performance suffers, as it must be very conservative
in how it schedules and executes build steps. And there’s no way for the system
to confirm that each script is doing what it should, so scripts tend to grow in
complexity and end up being another thing that needs debugging.

### Difficulty of parallelizing build steps

Modern development workstations are quite powerful, with multiple cores that are
capable of executing several build steps in parallel. But task-based systems are
often unable to parallelize task execution even when it seems like they should
be able to. Suppose that task A depends on tasks B and C. Because tasks B and C
have no dependency on each other, is it safe to run them at the same time so
that the system can more quickly get to task A? Maybe, if they don’t touch any
of the same resources. But maybe not—perhaps both use the same file to track
their statuses and running them at the same time causes a conflict. There’s no
way in general for the system to know, so either it has to risk these conflicts
(leading to rare but very difficult-to-debug build problems), or it has to
restrict the entire build to running on a single thread in a single process.
This can be a huge waste of a powerful developer machine, and it completely
rules out the possibility of distributing the build across multiple machines.

### Difficulty performing incremental builds

A good build system allows engineers to perform reliable incremental builds such
that a small change doesn’t require the entire codebase to be rebuilt from
scratch. This is especially important if the build system is slow and unable to
parallelize build steps for the aforementioned reasons. But unfortunately,
task-based build systems struggle here, too. Because tasks can do anything,
there’s no way in general to check whether they’ve already been done. Many tasks
simply take a set of source files and run a compiler to create a set of
binaries; thus, they don’t need to be rerun if the underlying source files
haven’t changed. But without additional information, the system can’t say this
for sure—maybe the task downloads a file that could have changed, or maybe it
writes a timestamp that could be different on each run. To guarantee
correctness, the system typically must rerun every task during each build. Some
build systems try to enable incremental builds by letting engineers specify the
conditions under which a task needs to be rerun. Sometimes this is feasible, but
often it’s a much trickier problem than it appears. For example, in languages
like C++ that allow files to be included directly by other files, it’s
impossible to determine the entire set of files that must be watched for changes
without parsing the input sources. Engineers often end up taking shortcuts, and
these shortcuts can lead to rare and frustrating problems where a task result is
reused even when it shouldn’t be. When this happens frequently, engineers get
into the habit of running clean before every build to get a fresh state,
completely defeating the purpose of having an incremental build in the first
place. Figuring out when a task needs to be rerun is surprisingly subtle, and is
a job better handled by machines than humans.

### Difficulty maintaining and debugging scripts

Finally, the build scripts imposed by task-based build systems are often just
difficult to work with. Though they often receive less scrutiny, build scripts
are code just like the system being built, and are easy places for bugs to hide.
Here are some examples of bugs that are very common when working with a
task-based build system:

*   Task A depends on task B to produce a particular file as output. The owner
    of task B doesn’t realize that other tasks rely on it, so they change it to
    produce output in a different location. This can’t be detected until someone
    tries to run task A and finds that it fails.
*   Task A depends on task B, which depends on task C, which is producing a
    particular file as output that’s needed by task A. The owner of task B
    decides that it doesn’t need to depend on task C any more, which causes task
    A to fail even though task B doesn’t care about task C at all!
*   The developer of a new task accidentally makes an assumption about the
    machine running the task, such as the location of a tool or the value of
    particular environment variables. The task works on their machine, but fails
    whenever another developer tries it.
*   A task contains a nondeterministic component, such as downloading a file
    from the internet or adding a timestamp to a build. Now, people get
    potentially different results each time they run the build, meaning that
    engineers won’t always be able to reproduce and fix one another’s failures
    or failures that occur on an automated build system.
*   Tasks with multiple dependencies can create race conditions. If task A
    depends on both task B and task C, and task B and C both modify the same
    file, task A gets a different result depending on which one of tasks B and C
    finishes first.

There’s no general-purpose way to solve these performance, correctness, or
maintainability problems within the task-based framework laid out here. So long
as engineers can write arbitrary code that runs during the build, the system
can’t have enough information to always be able to run builds quickly and
correctly. To solve the problem, we need to take some power out of the hands of
engineers and put it back in the hands of the system and reconceptualize the
role of the system not as running tasks, but as producing artifacts.

This approach led to the creation of artifact-based build systems, like Blaze
and Bazel.

---

## Bazel Brand Guidelines
- URL: https://bazel.build/brand
- Source: brand/index.mdx
- Slug: /brand

The Bazel trademark and logo ("Bazel Trademarks") are trademarks of Google, and
are treated separately from the copyright or patent license grants contained in
the Apache-licensed Bazel repositories on GitHub. Any use of the Bazel
Trademarks other than those permitted in these guidelines must be approved in
advance.

## Purpose of the Brand Guidelines

These guidelines exist to ensure that the Bazel project can share its technology
under open source licenses while making sure that the "Bazel" brand is protected
as a meaningful source identifier in a way that's consistent with trademark law.
By adhering to these guidelines, you help to promote the freedom to use and
develop high-quality Bazel technology.

## Acceptable Uses

Given the open nature of Bazel, you may use the Bazel trademark to refer to the
project without prior written permission. Examples of these approved references
include the following:

*   To refer to the Bazel Project itself;
*   To link to bazel.build;
*   To refer to unmodified source code or other files shared by the Bazel
    repositories on GitHub;
*   In blog posts, news articles, or educational materials about Bazel;
*   To accurately identify that your design or implementation is based on, is
    for use with, or is compatible with Bazel technology.

Examples:

*   \[Your Product\] for Bazel
*   \[Your Product\] is compatible with Bazel
*   \[XYZ\] Conference for Bazel Users

## General Guidelines

*   The Bazel name may never be used or registered in a manner that would cause
    confusion as to Google's sponsorship, affiliation, or endorsement.
*   Don't use the Bazel name as part of your company name, product name, domain
    name, or social media profile.
*   Other than as permitted by these guidelines, the Bazel name should not be
    combined with other trademarks, terms, or source identifiers.
*   Don't remove, distort or alter any element of the Bazel Trademarks. That
    includes modifying the Bazel Trademark, for example, through hyphenation,
    combination or abbreviation. Do not shorten, abbreviate, or create acronyms
    out of the Bazel Trademarks.
*   Don't display the word Bazel using any different stylization, color, or font
    from the surrounding text.
*   Don't use the term Bazel as a verb or use it in possessive form.
*   Don't use the Bazel logo on any website, product UI, or promotional
    materials without prior written permission from
    [product@bazel.build](mailto:product@bazel.build).

## Usage for Events and Community Groups

The Bazel word mark may be used referentially in events, community groups, or
other gatherings related to the Bazel build system, but it may not be used in a
manner that implies official status or endorsement.

Examples of appropriate naming conventions are:

*   \[XYZ\] Bazel User Group
*   Bazel Community Day at \[XYZ\]
*   \[XYZ\] Conference for Bazel Users

where \[XYZ\] represents the location and optionally other wordings.

Any naming convention that may imply official status or endorsement requires
review for approval from [product@bazel.build](mailto:product@bazel.build).

Examples of naming conventions that require prior written permission:

*   BazelCon
*   Bazel Conference

## Contact Us

Please do not hesitate to contact us at
[product@bazel.build](mailto:product@bazel.build) if you are unsure whether your
intended use of the Bazel Trademarks is in compliance with these guidelines, or
to ask for permission to use the Bazel Trademarks, clearly describing the
intended usage and duration.

---

## Sharing Variables
- URL: https://bazel.build/build/share-variables
- Source: build/share-variables.mdx
- Slug: /build/share-variables

`BUILD` files are intended to be simple and declarative. They will typically
consist of a series of target declarations. As your code base and your `BUILD`
files get larger, you will probably notice some duplication, such as:

``` python
cc_library(
  name = "foo",
  copts = ["-DVERSION=5"],
  srcs = ["foo.cc"],
)

cc_library(
  name = "bar",
  copts = ["-DVERSION=5"],
  srcs = ["bar.cc"],
  deps = [":foo"],
)
```

Code duplication in `BUILD` files is usually fine. This can make the file more
readable: each declaration can be read and understood without any context. This
is important, not only for humans, but also for external tools. For example, a
tool might be able to read and update `BUILD` files to add missing dependencies.
Code refactoring and code reuse might prevent this kind of automated
modification.

If it is useful to share values (for example, if values must be kept in sync),
you can introduce a variable:

``` python
COPTS = ["-DVERSION=5"]

cc_library(
  name = "foo",
  copts = COPTS,
  srcs = ["foo.cc"],
)

cc_library(
  name = "bar",
  copts = COPTS,
  srcs = ["bar.cc"],
  deps = [":foo"],
)
```

Multiple declarations now use the value `COPTS`. By convention, use uppercase
letters to name global constants.

## Sharing variables across multiple BUILD files

If you need to share a value across multiple `BUILD` files, you have to put it
in a `.bzl` file. `.bzl` files contain definitions (variables and functions)
that can be used in `BUILD` files.

In `path/to/variables.bzl`, write:

``` python
COPTS = ["-DVERSION=5"]
```

Then, you can update your `BUILD` files to access the variable:

``` python
load("//path/to:variables.bzl", "COPTS")

cc_library(
  name = "foo",
  copts = COPTS,
  srcs = ["foo.cc"],
)

cc_library(
  name = "bar",
  copts = COPTS,
  srcs = ["bar.cc"],
  deps = [":foo"],
)
```

---

## BUILD Style Guide
- URL: https://bazel.build/build/style-guide
- Source: build/style-guide.mdx
- Slug: /build/style-guide

## Prefer DAMP BUILD files over DRY

The DRY principle — "Don't Repeat Yourself" — encourages uniqueness by
introducing abstractions such as variables and functions to avoid redundancy in
code.

In contrast, the DAMP principle — "Descriptive and Meaningful Phrases" —
encourages readability over uniqueness to make files easier to understand and
maintain.

`BUILD` files aren't code, they are configurations. They aren't tested like
code, but do need to be maintained by people and tools. That makes DAMP better
for them than DRY.

## BUILD.bazel file formatting

`BUILD` file formatting follows the same approach as Go, where a standardized
tool takes care of most formatting issues.
[Buildifier](https://github.com/bazelbuild/buildifier) is a tool that parses and
emits the source code in a standard style. Every `BUILD` file is therefore
formatted in the same automated way, which makes formatting a non-issue during
code reviews. It also makes it easier for tools to understand, edit, and
generate `BUILD` files.

`BUILD` file formatting must match the output of `buildifier`.

### Formatting example

```python
# Test code implementing the Foo controller.
package(default_testonly = True)

py_test(
    name = "foo_test",
    srcs = glob(["*.py"]),
    data = [
        "//data/production/foo:startfoo",
        "//foo",
        "//third_party/java/jdk:jdk-k8",
    ],
    flaky = True,
    deps = [
        ":check_bar_lib",
        ":foo_data_check",
        ":pick_foo_port",
        "//pyglib",
        "//testing/pybase",
    ],
)
```

## File structure

**Recommendation**: Use the following order (every element is optional):

*   Package description (a comment)

*   All `load()` statements

*   The `package()` function.

*   Calls to rules and macros

Buildifier makes a distinction between a standalone comment and a comment
attached to an element. If a comment is not attached to a specific element, use
an empty line after it. The distinction is important when doing automated
changes (for example, to keep or remove a comment when deleting a rule).

```python
# Standalone comment (such as to make a section in a file)

# Comment for the cc_library below
cc_library(name = "cc")
```

## References to targets in the current package

Files should be referred to by their paths relative to the package directory
(without ever using up-references, such as `..`). Generated files should be
prefixed with "`:`" to indicate that they are not sources. Source files
should not be prefixed with `:`. Rules should be prefixed with `:`. For
example, assuming `x.cc` is a source file:

```python
cc_library(
    name = "lib",
    srcs = ["x.cc"],
    hdrs = [":gen_header"],
)

genrule(
    name = "gen_header",
    srcs = [],
    outs = ["x.h"],
    cmd = "echo 'int x();' > $@",
)
```

## Target naming

Target names should be descriptive. If a target contains one source file,
the target should generally have a name derived from that source (for example, a
`cc_library` for `chat.cc` could be named `chat`, or a `java_library` for
`DirectMessage.java` could be named `direct_message`).

The eponymous target for a package (the target with the same name as the
containing directory) should provide the functionality described by the
directory name. If there is no such target, do not create an eponymous
target.

Prefer using the short name when referring to an eponymous target (`//x`
instead of `//x:x`). If you are in the same package, prefer the local
reference (`:x` instead of `//x`).

Avoid using "reserved" target names which have special meaning. This includes
`all`, `__pkg__`, and `__subpackages__`, these names have special
semantics and can cause confusion and unexpected behaviors when they are used.

In the absence of a prevailing team convention these are some non-binding
recommendations that are broadly used at Google:

* In general, use ["snake_case"](https://en.wikipedia.org/wiki/Snake_case)
    * For a `java_library` with one `src` this means using a name that is not
      the same as the filename without the extension
    * For Java `*_binary` and `*_test` rules, use
      ["Upper CamelCase"](https://en.wikipedia.org/wiki/Camel_case).
      This allows for the target name to match one of the `src`s. For
      `java_test`, this makes it possible for the `test_class` attribute to be
      inferred from the name of the target.
* If there are multiple variants of a particular target then add a suffix to
  disambiguate (such as. `:foo_dev`, `:foo_prod` or `:bar_x86`, `:bar_x64`)
* Suffix `_test` targets with `_test`, `_unittest`, `Test`, or `Tests`
* Avoid meaningless suffixes like `_lib` or `_library` (unless necessary to
  avoid conflicts between a `_library` target and its corresponding `_binary`)
* For proto related targets:
    * `proto_library` targets should have names ending in `_proto`
    * Languages specific `*_proto_library` rules should match the underlying
      proto but replace `_proto` with a language specific suffix such as:
         * **`cc_proto_library`**: `_cc_proto`
         * **`java_proto_library`**: `_java_proto`
         * **`java_lite_proto_library`**: `_java_proto_lite`

## Visibility

Visibility should be scoped as tightly as possible, while still allowing access
by tests and reverse dependencies. Use `__pkg__` and `__subpackages__` as
appropriate.

Avoid setting package `default_visibility` to `//visibility:public`.
`//visibility:public` should be individually set only for targets in the
project's public API. These could be libraries that are designed to be depended
on by external projects or binaries that could be used by an external project's
build process.

## Dependencies

Dependencies should be restricted to direct dependencies (dependencies
needed by the sources listed in the rule). Do not list transitive dependencies.

Package-local dependencies should be listed first and referred to in a way
compatible with the
[References to targets in the current package](#targets-current-package)
section above (not by their absolute package name).

Prefer to list dependencies directly, as a single list. Putting the "common"
dependencies of several targets into a variable reduces maintainability, makes
it impossible for tools to change the dependencies of a target, and can lead to
unused dependencies.

## Globs

Indicate "no targets" with `[]`. Do not use a glob that matches nothing: it
is more error-prone and less obvious than an empty list.

### Recursive

Do not use recursive globs to match source files (for example,
`glob(["**/*.java"])`).

Recursive globs make `BUILD` files difficult to reason about because they skip
subdirectories containing `BUILD` files.

Recursive globs are generally less efficient than having a `BUILD` file per
directory with a dependency graph defined between them as this enables better
remote caching and parallelism.

It is good practice to author a `BUILD` file in each directory and define a
dependency graph between them.

### Non-recursive

Non-recursive globs are generally acceptable.

## Avoid list comprehensions

Avoid using list comprehensions at the top level of a `BUILD.bazel` file.
Automate repetitive calls by creating each named target with a separate
top-level rule or macro call. Give each a short `name` parameter for clarity.

List comprehension reduces the following:

*   Maintainability. It's difficult or impossible for human maintainers and
    large scale automated changes to update list comprehensions correctly.
*   Discoverability. Since the pattern doesn't have `name` parameters,
    it's hard to find the rule by name.

A common application of the list comprehension pattern is to generate tests. For
example:

```build {.bad}
[[java_test(
    name = "test_%s_%s" % (backend, count),
    srcs = [ ... ],
    deps = [ ... ],
    ...
) for backend in [
    "fake",
    "mock",
]] for count in [
    1,
    10,
]]
```

We recommend using simpler alternatives. For example, define a macro that
generates one test and invoke it for each top-level `name`:

```build
my_java_test(name = "test_fake_1",
    ...)
my_java_test(name = "test_fake_10",
    ...)
...
```

## Don't use deps variables

Don't use list variables to encapsulate common dependencies:

```build {.bad}
COMMON_DEPS = [
  "//d:e",
  "//x/y:z",
]

cc_library(name = "a",
    srcs = ["a.cc"],
    deps = COMMON_DEPS + [ ... ],
)

cc_library(name = "b",
    srcs = ["b.cc"],
    deps = COMMON_DEPS + [ ... ],
)
```

Similarly, don't use a library target with
[`exports`](/reference/be/java#java_library.exports) to group dependencies.

Instead, list the dependencies separately for each target:

```build {.good}
cc_library(name = "a",
    srcs = ["a.cc"],
    deps = [
      "//a:b",
      "//x/y:z",
      ...
    ],
)

cc_library(name = "b",
    srcs = ["b.cc"],
    deps = [
      "//a:b",
      "//x/y:z",
      ...
    ],
)
```

Let [Gazelle](https://github.com/bazel-contrib/bazel-gazelle) and other tools
maintain them. There will be repetition, but you won't have to think about how
to manage the dependencies.

## Prefer literal strings

Although Starlark provides string operators for concatenation (`+`) and
formatting (`%`), use them with caution. It is tempting to factor out common
string parts to make expressions more concise or break long lines. However,

*   It is harder to read broken-up string values at a glance.

*   Automated tools such as
    [buildozer][buildozer] and Code Search have trouble finding values and
    updating them correctly when the values broken up.

*   In `BUILD` files, readability is more important than avoiding repetition
    (see [DAMP versus DRY](#prefer-damp-build-files-over-dry)).

*   This Style Guide
    [warns against splitting label-valued strings](#other-conventions)
    and
    [explicitly permits long lines](#differences-python-style-guide).

*   Buildifier automatically fuses concatenated strings when it detects that
    they are labels.

Therefore, prefer explicit, literal strings over concatenated or formatted
strings, especially in label-type attributes such as `name` and `deps`. For
example, this `BUILD` fragment:

```build {.bad}
NAME = "foo"
PACKAGE = "//a/b"

proto_library(
  name = "%s_proto" % NAME,
  deps = [PACKAGE + ":other_proto"],
  alt_dep = "//surprisingly/long/chain/of/package/names:" +
            "extravagantly_long_target_name",
)
```

would be better rewritten as

```build {.good}
proto_library(
  name = "foo_proto",
  deps = ["//a/b:other_proto"],
  alt_dep = "//surprisingly/long/chain/of/package/names:extravagantly_long_target_name",
)
```

[buildozer]: https://github.com/bazelbuild/buildtools/blob/main/buildozer/README.md

## Limit the symbols exported by each `.bzl` file

Minimize the number of symbols (rules, macros, constants, functions) exported by
each public `.bzl` (Starlark) file. We recommend that a file should export
multiple symbols only if they are certain to be used together. Otherwise, split
it into multiple `.bzl` files, each with its own [bzl_library][bzl_library].

Excessive symbols can cause `.bzl` files to grow into broad "libraries" of
symbols, causing changes to single files to force Bazel to rebuild many targets.

[bzl_library]: https://github.com/bazelbuild/bazel-skylib/blob/main/README.md#bzl_library

## Other conventions

 * Use uppercase and underscores to declare constants (such as `GLOBAL_CONSTANT`),
   use lowercase and underscores to declare variables (such as `my_variable`).

 * Labels should never be split, even if they are longer than 79 characters.
   Labels should be string literals whenever possible. *Rationale*: It makes
   find and replace easy. It also improves readability.

 * The value of the name attribute should be a literal constant string (except
   in macros). *Rationale*: External tools use the name attribute to refer a
   rule. They need to find rules without having to interpret code.

 * When setting boolean-type attributes, use boolean values, not integer values.
   For legacy reasons, rules still convert integers to booleans as needed,
   but this is discouraged. *Rationale*: `flaky = 1` could be misread as saying
   "deflake this target by rerunning it once". `flaky = True` unambiguously says
   "this test is flaky".

## Differences with Python style guide

Although compatibility with
[Python style guide](https://www.python.org/dev/peps/pep-0008/)
is a goal, there are a few differences:

 * No strict line length limit. Long comments and long strings are often split
   to 79 columns, but it is not required. It should not be enforced in code
   reviews or presubmit scripts. *Rationale*: Labels can be long and exceed this
   limit. It is common for `BUILD` files to be generated or edited by tools,
   which does not go well with a line length limit.

 * Implicit string concatenation is not supported. Use the `+` operator.
   *Rationale*: `BUILD` files contain many string lists. It is easy to forget a
   comma, which leads to a complete different result. This has created many bugs
   in the past. [See also this discussion.](https://lwn.net/Articles/551438/)

 * Use spaces around the `=` sign for keywords arguments in rules. *Rationale*:
   Named arguments are much more frequent than in Python and are always on a
   separate line. Spaces improve readability. This convention has been around
   for a long time, and it is not worth modifying all existing `BUILD` files.

 * By default, use double quotation marks for strings. *Rationale*: This is not
   specified in the Python style guide, but it recommends consistency. So we
   decided to use only double-quoted strings. Many languages use double-quotes
   for string literals.

 * Use a single blank line between two top-level definitions. *Rationale*: The
   structure of a `BUILD` file is not like a typical Python file. It has only
   top-level statements. Using a single-blank line makes `BUILD` files shorter.

---

## Bazel Community Experts
- URL: https://bazel.build/community/experts
- Source: community/experts.mdx
- Slug: /community/experts

Companies who have contributed significantly to the Bazel community and can help with your project

---

<Columns cols={2}>
<Card title="Aspect Build" img="/community/images/aspect-logo-2.png" cta="Learn more" href="https://www.aspect.build/services">
Leveraging the insights we gathered from helping over 50 companies to navigate Bazel, we developed Aspect Workflows, our Developer Platform for Bazel Monorepos. Our platform is designed to enhance the developer experience with Bazel, notably accelerating build times on CI while reducing compute costs. Aspect Workflows seamlessly integrates with your existing CI and cloud infrastructure, while aligning with your current configuration. This approach empowers users to concentrate on innovating and delivering exceptional products, while we manage the underlying infrastructure and ensure CI stays fast. Aspect offers consulting, support, and training to help your team succeed with Bazel and we are the authors and maintainers of the canonical open-source rulesets for JavaScript and OCI images.

</Card>
<Card title="Bitrise" img="/community/images/bitrise-logo.png" cta="Learn more" href="https://bitrise.io/why/features/mobile-build-caching-for-better-build-test-performance">
Bitrise is the world’s leading mobile-first CI/CD Platform. Our Bitrise Build Cache & CDN product enables enterprises and high-growth organizations to optimize and elevate their Bazel projects. Bitrise brings to market an uncontested build and test caching solution available nowhere else. The first of its kind, fully managed by Bitrise requiring no setup or maintenance, you can use our out-of-the-box cache Steps to enjoy a deeply integrated CI/CD & Bazel experience or integrate our cache directly into your builds on any other platform.

</Card>
</Columns>

<Columns cols={2}>
<Card title="BuildBuddy" img="/community/images/buildbuddy-logo.svg" cta="Learn more" href="https://www.buildbuddy.io/">
BuildBuddy provides an open-core suite of enterprise features for Bazel. Included are a Remote Build Execution service, a shared build artifact cache, and a build result UI. It's available as a fully-managed cloud service or as an easy to deploy on-prem solution. The service is free to use for individuals and open-source projects. BuildBuddy is based in San Francisco, backed by Y Combinator, and founded by two ex-Googlers deeply passionate about making developers more productive.

</Card>
<Card title="Codethink" img="/community/images/codethink-logo.svg" cta="Learn more" href="https://www.codethink.co.uk/">
Codethink is an Open Source software consultancy providing development, integration, test automation and whole-of-life maintenance services for advanced software on production embedded devices and cloud infrastructure. Codethink works with international-scale organisations exploiting Bazel and remote execution solutions to increase continuous delivery productivity and throughput.

</Card>
</Columns>

<Columns cols={2}>
<Card title="EngFlow" img="/community/images/engflow-logo.svg" cta="Learn more" href="https://www.engflow.com/">
EngFlow helps companies accelerate builds for Bazel, Goma, and Android Platform. Builds, tests and CI are typically 5 to 10x faster when using EngFlow’s remote build execution, caching, and build results UI. Customers also benefit from the company’s unparalleled Bazel expertise. Created by the engineer who led the development of Bazel, EngFlow upstreams Bazel changes in the interest of customers, and has partnerships with other Bazel experts.

</Card>
<Card title="EPAM" img="/community/images/epam-logo.png" cta="Learn more" href="https://www.epam.com/">
Since 1993 EPAM has leveraged its ‘Engineering DNA’ to become a leading global product development, digital platform engineering, and top digital and product design agency. Due to its tight partnership with Google, EPAM has extensive Bazel expertise and is ready to assess readiness and perform complex migration to Bazel in order to streamline development and CI/CD processes for any technology and complexity including custom rules and tools development, remote cache and execution setup.

</Card>
</Columns>

<Columns cols={2}>
<Card title="SUM Global" img="/community/images/sumglobal-logo.png" cta="Learn more" href="http://sumglobal.com/bazel-build">
SUM Global Technology is an IT consulting firm. We help create high performance CI/CD build infrastructures. Combining decades of build domain knowledge with a deep technical expertise using Bazel, SUM Global works with your organization to create and enhance the software delivery process. We specialize in Java, Android and Angular builds as well as conversions from other tools. We can help with Google Remote Build Execution.

</Card>
<Card title="Tweag" img="/community/images/tweag-logo.png" cta="Learn more" href="https://www.tweag.io/">
Tweag I/O is one of Bazel's earliest adopters, an active contributor of new features and new open source extensions since early 2018. Our specialty: partner with you to help you achieve near byte-for-byte reproducibility, fully traceable all the way to production and conveniently auditable builds that can be cached correctly and run fast. Tweag engineers combine extensive knowledge of Bazel with a capacity for deep technical understanding of your architecture, because no two builds are the same. We get you ready for Bazel with our Bazel Readiness Assessment, perform migrations, and improve existing setups.

</Card>
</Columns>

---

## Bazel Product Partners
- URL: https://bazel.build/community/partners
- Source: community/partners.mdx
- Slug: /community/partners

We categorize Bazel product partners as organizations that build open source or paid tooling that interfaces with Bazel. These tools aim to improve Bazel end user experience and accelerate developer productivity.
Interested organizations can write to product@bazel.build to get added to the product partnership program.

---

<Columns cols={2}>
<Card title="Aspect Build" img="/community/images/aspect-logo-2.png" cta="Learn more" href="https://www.aspect.build/">
Leveraging the insights we gathered from helping over 50 companies to navigate Bazel, we developed Aspect Workflows, our Developer Platform for Bazel Monorepos. Our platform is designed to enhance the developer experience with Bazel, notably accelerating build times on CI while reducing compute costs. Aspect Workflows seamlessly integrates with your existing CI and cloud infrastructure, while aligning with your current configuration. This approach empowers users to concentrate on innovating and delivering exceptional products, while we manage the underlying infrastructure and ensure CI stays fast. Aspect offers consulting, support, and training to help your team succeed with Bazel and we are the authors and maintainers of the canonical open-source rulesets for JavaScript and OCI images.

</Card>
<Card title="Bitrise" img="/community/images/Bitrise.png" cta="Learn more" href="https://bitrise.io/why/features/mobile-build-caching-for-better-build-test-performance">
Bitrise Inc. is the world’s leading mobile-first CI/CD Platform. Our Bitrise Build Cache & CDN product enables enterprises and high-growth organizations to optimize and elevate their Bazel projects. Bitrise brings to market an uncontested build and test caching solution available nowhere else. The first of its kind, fully managed by Bitrise requiring no setup or maintenance, you can use our out-of-the-box cache Steps to enjoy a deeply integrated CI/CD & Bazel experience or integrate our cache directly into your builds on any other platform..

</Card>
</Columns>

<Columns cols={2}>
<Card title="BuildBuddy" img="/community/images/buildbuddy-logo.svg" cta="Learn more" href="https://www.buildbuddy.io/">
BuildBuddy provides an open-core suite of enterprise features for Bazel. Included are a Remote Build Execution service, a shared build cache, Bazel-optimized CI workflows, and a build & test result UI for debugging and analytics.It's available as a fully-managed cloud service or as an easy to deploy on-prem solution. The service is free to use for small teams and open-source projects. BuildBuddy is based in San Francisco, backed by Y Combinator, and founded by two ex-Googlers deeply passionate about making developers more productive.

</Card>
<Card title="EngFlow" img="/community/images/engflow-logo.svg" cta="Learn more" href="https://www.engflow.com/product/demo">
EngFlow is the build and test acceleration company created by core Bazel engineers and funded by Andreessen Horowitz. EngFlow’s secure (audited: SOC 2 type 2) remote execution, caching, and observability platform scales from 1 to 100,000+ cores, reduces time by 5-10x and cloud costs by 20-50%. Whether deployed on your cloud or on EngFlow’s: our global Bazel experts provide 24x7 coverage, support small and large teams, no hidden costs, and SSO included.

</Card>
</Columns>

<Columns cols={2}>
<Card title="Gradle Inc." img="/community/images/develocity.png" cta="Learn more" href="https://gradle.com/gradle-enterprise-solutions/bazel-build-system/">
Develocity is a multi-build-system platform for improving developer productivity and happiness. It does this by providing a comprehensive and end-to-end solution for build & test observability, acceleration, and failure analytics, and currently supports the Bazel, Apache Maven, Gradle, and SBT build systems. Specifically, Develocity for Bazel supports Build Cache to speed up build and test feedback cycles; Build Scan® that is like an X-ray for your build to make troubleshooting more efficient; and Failure Analytics to improve toolchain reliability.

</Card>
<Card title="Tweag" img="/community/images/tweag-logo.png" cta="Learn more" href="https://www.tweag.io/group/scalable-builds/">
Tweag is one of Bazel's earliest adopters, an active contributor of new features and new open source extensions since early 2018. We have been helping companies and teams achieve near byte-for-byte reproducibility, fully traceable all the way to production and conveniently auditable builds that can be cached correctly and run fast. Besides our consulting work, we also have an array of tools and extensions. Skyscope to visualize and explore complex Bazel build graphs with hundreds of thousands of nodes in your web browser. Open source Bazel extensions to achieve fully reproducible builds with the power of Nix, manage shell tools in a principled way, and build Haskell projects with Bazel. Finally, our open source Gazelle extensions to automate your Bazel migration and build maintenance.

</Card>
</Columns>

<Columns cols={2}>
<Card title="Nativelink" img="/community/images/nativelink.svg" cta="Learn more" href="https://www.nativelink.com/">
NativeLink is the 100% free, open source, and permissively licensed build and test acceleration project written in Rust. It is created by a team of open source maintainers and the company is funded by Wellington Management, an asset manager with over one trillion dollars under management. Whether deployed on your infrastructure or on NativeLink’s cloud, our global Bazel, Reclient, and compiler experts provide 24x7 coverage, support small and large teams, and SSO included.

</Card>
<Card title="VirtusLab" img="/community/images/virtuslab.svg" cta="Learn more" href="https://virtuslab.com/expertise/monorepo-expertise-with-bazel/">
VirtusLab specializes in managing monorepos and migrating codebases to Bazel. We offer a smooth transition that enhances your software’s creation, testing, and release processes. Our engineers will assist you in adopting industry-standard tooling and practices, including top-tier security measures. We believe that a seamless Developer Experience requires great tooling. By partnering with us, you will optimize build times, reduce costs, and empower your development teams to reach their peak performance.

</Card>
</Columns>

<Columns cols={2}>
<Card title="Buildkite" img="/community/images/buildkite.svg" cta="Learn more" href="https://buildkite.com/">
Based in San Francisco and Sydney, Buildkite is a fast-growing software delivery provider that offers the industry’s first and only Scale-Out Delivery Platform. Buildkite's Scale-Out Delivery platform is the only solution that provides the flexibility and scale required by the world's most demanding companies for delivering software across a broad range of use cases, including AI/ML workloads and mobile application development. Global innovation leaders including Airbnb, Block, Canva, Cruise, Culture Amp, Elastic, Lyft, PagerDuty, Pinterest, PlanetScale, Rippling, Shopify, Slack, Tinder, Twilio, Uber, and Wayfair have standardized on Buildkite for software delivery. Buildkite delivers seamless Bazel integration, enabling teams to scale CI/CD for complex monorepos. Buildkite sponsors CI for the Bazel project as part of its commitment to developers, driving faster builds and broader adoption.

</Card>
</Columns>

---

## Recommended Rules
- URL: https://bazel.build/community/recommended-rules
- Source: community/recommended-rules.mdx
- Slug: /community/recommended-rules

In the documentation, we provide a list of
[recommended rules](/rules).

This is a set of high quality rules, which will provide a good experience to our
users. We make a distinction between the supported rules, and the hundreds of
rules you can find on the Internet.

## Nomination

If a ruleset meets the requirements below, a rule maintainer can nominate it
to be part of the _recommended rules_ by filing a
[GitHub issue](https://github.com/bazelbuild/bazel/).

After a review by the [Bazel core team](/contribute/policy), it
will be recommended on the Bazel website.

## Requirements for the rule maintainers

*   The ruleset provides an important feature, useful to a large number of Bazel
    users (for example, support for a widely popular language).
*   The ruleset is well maintained. There must be at least two active maintainers.
*   The ruleset is well documented, with examples, and easy to use.
*   The ruleset follows the best practices and is performant (see
    [the performance guide](/rules/performance)).
*   The ruleset has sufficient test coverage.
*   The ruleset is tested on
    [BuildKite](https://github.com/bazelbuild/continuous-integration/blob/master/buildkite/README.md)
    with the latest version of Bazel. Tests should always pass (when used as a
    presubmit check).
*   The ruleset is also tested with the upcoming incompatible changes. Breakages
    should be fixed within two weeks. Migration issues should be reported to the
    Bazel team quickly.

## Requirements for Bazel developers

*   Recommended rules are frequently tested with Bazel at head (at least once a
    day).
*   No change in Bazel may break a recommended rule (with the default set of
    flags). If it happens, the change should be fixed or rolled back.

## Demotion

If there is a concern that a particular ruleset is no longer meeting the
requirements, a [GitHub issue](https://github.com/bazelbuild/bazel/) should be
filed.

Rule maintainers will be contacted and need to respond in 2 weeks. Based on the
outcome, Bazel core team might make a decision to demote the rule set.

---

## Remote Execution Services
- URL: https://bazel.build/community/remote-execution-services
- Source: community/remote-execution-services.mdx
- Slug: /community/remote-execution-services

Use the following services to run Bazel with remote execution:

*   Manual

    * Use the [gRPC protocol](https://github.com/bazelbuild/remote-apis)
      directly to create your own remote execution service.

*   Self-service

    * [Buildbarn](https://github.com/buildbarn)
    * [Buildfarm](https://github.com/bazelbuild/bazel-buildfarm)
    * [BuildGrid](https://gitlab.com/BuildGrid/buildgrid)
    * [NativeLink](https://github.com/TraceMachina/nativelink)

*   Commercial

    * [Aspect Build](https://www.aspect.build/) – Self-hosted remote cache and remote execution services.
    * [Bitrise](https://bitrise.io/why/features/mobile-build-caching-for-better-build-test-performance) - Providing the world's leading mobile-first CI/CD and remote build caching platform.
    * [BuildBuddy](https://www.buildbuddy.io) - Remote build execution,
      caching, and results UI.
    * [EngFlow Remote Execution](https://www.engflow.com) - Remote execution
      and remote caching service with Build and Test UI. Can be self-hosted or hosted.
    * [NativeLink](https://github.com/TraceMachina/nativelink) - Remote build execution, caching, analytics, and simulation.

---

## Bazel Special Interest Groups
- URL: https://bazel.build/community/sig
- Source: community/sig.mdx
- Slug: /community/sig

Bazel hosts Special Interest Groups (SIGs) to focus collaboration on particular
areas and to support communication and coordination between [Bazel owners,
maintainers, and contributors](/contribute/policy). This policy
applies to [`bazelbuild`](http://github.com/bazelbuild).

SIGs do their work in public. The ideal scope for a SIG covers a well-defined
domain, where the majority of participation is from the community. SIGs may
focus on community maintained repositories in `bazelbuild` (such as language
rules) or focus on areas of code in the Bazel repository (such as Remote
Execution).

While not all SIGs will have the same level of energy, breadth of scope, or
governance models, there should be sufficient evidence that there are community
members willing to engage and contribute should the interest group be
established. Before joining, review the group's work, and then get in touch
with the SIG leader. Membership policies vary on a per-SIG basis.

See the complete list of
[Bazel SIGs](https://github.com/bazelbuild/community/tree/main/sigs).

### Non-goals: What a SIG is not

SIGs are intended to facilitate collaboration on shared work. A SIG is
therefore:

-   *Not a support forum:* a mailing list and a SIG is not the same thing
-   *Not immediately required:* early on in a project's life, you may not know
    if you have shared work or collaborators
-   *Not free labor:* energy is required to grow and coordinate the work
    collaboratively

Bazel Owners take a conservative approach to SIG creation—thanks to the ease of
starting projects on GitHub, there are many avenues where collaboration can
happen without the need for a SIG.

## SIG lifecycle

This section covers how to create a SIG.

### Research and consultation

To propose a new SIG group, first gather evidence for approval, as specified
below. Some possible avenues to consider are:

-   A well-defined problem or set of problems the group would solve
-   Consultation with community members who would benefit, assessing both the
    benefit and their willingness to commit
-   For existing projects, evidence from issues and PRs that contributors care
    about the topic
-   Potential goals for the group to achieve
-   Resource requirements of running the group

Even if the need for a SIG seems self-evident, the research and consultation is
still important to the success of the group.

### Create the new group

The new group should follow the below process for chartering. In particular, it
must demonstrate:

-   A clear purpose and benefit to Bazel (either around a sub-project or
    application area)
-   Two or more contributors willing to act as group leads, existence of other
    contributors, and evidence of demand for the group
-   Each group needs to use at least one publicly accessible mailing list. A SIG
    may reuse one of the public lists, such as
    [bazel-discuss](https://groups.google.com/g/bazel-discuss), ask for a list
    for @bazel.build, or create their own list
-   Resources the SIG initially requires (usually, mailing list and regular
    video call.)
-   SIGs can serve documents and files from their directory in
    [`bazelbuild/community`](https://github.com/bazelbuild/community)
    or from their own repository in the
    [`bazelbuild`](https://github.com/bazelbuild) GitHub
    organization. SIGs may link to external resources if they choose to organize
    their work outside of the `bazelbuild` GitHub organization
-   Bazel Owners approve or reject SIG applications and consult other
    stakeholders as necessary

Before entering the formal parts of the process, you should consult with
the Bazel product team, at product@bazel.build. Most SIGs require conversation
and iteration before approval.

The formal request for the new group is done by submitting a charter as a PR to
[`bazelbuild/community`](https://github.com/bazelbuild/community),
and including the request in the comments on the PR following the template
below. On approval, the PR for the group is merged and the required resources
created.

### Template Request for New SIG

To request a new SIG, use the template in the community repo:
[SIG-request-template.md](https://github.com/bazelbuild/community/blob/main/governance/SIG-request-template.md).

### Chartering

To establish a group, you need a charter and must follow the Bazel
[code of conduct](https://github.com/bazelbuild/bazel/blob/HEAD/CODE_OF_CONDUCT.md).
Archives of the group will be public. Membership may either be open to all
without approval, or available on request, pending approval of the group
administrator.

The charter must nominate an administrator. As well as an administrator, the
group must include at least one person as lead (these may be the same person),
who serves as point of contact for coordination as required with the Bazel
product team.

Group creators must post their charter to the group mailing list. The community
repository in the Bazel GitHub organization archives such documents and
policies. As groups evolve their practices and conventions, they should update
their charters within the relevant part of the community repository.

### Collaboration and inclusion

While not mandated, the group should choose to make use of collaboration
via scheduled conference calls or chat channels to conduct meetings. Any such
meetings should be advertised on the mailing list, and notes posted to the
mailing list afterwards. Regular meetings help drive accountability and progress
in a SIG.

Bazel product team members may proactively monitor and encourage the group to
discussion and action as appropriate.

### Launch a SIG

Required activities:

-   Notify Bazel general discussion groups
    ([bazel-discuss](https://groups.google.com/g/bazel-discuss),
    [bazel-dev](https://groups.google.com/g/bazel-dev)).

Optional activities:

-   Create a blog post for the Bazel blog

### Health and termination of SIGs

The Bazel owners make a best effort to ensure the health of SIGs. Bazel owners
occasionally request the SIG lead to report on the SIG's work, to inform the
broader Bazel community of the group's activity.

If a SIG no longer has a useful purpose or interested community, it may be
archived and cease operation. The Bazel product team reserves the right to
archive such inactive SIGs to maintain the overall health of the project,
though it is a less preferable outcome. A SIG may also opt to disband if
it recognizes it has reached the end of its useful life.

## Note

*This content has been adopted from Tensorflow’s
[SIG playbook](https://www.tensorflow.org/community/sig_playbook)
with modifications.*

---

## Who''s Using Bazel
- URL: https://bazel.build/community/users
- Source: community/users.mdx
- Slug: /community/users

Note: Using Bazel? You can add your company on
[StackShare](https://stackshare.io/bazel). To add yourself to this page,
contact [product@bazel.build](mailto:product@bazel.build).

This page lists companies and OSS projects that are known to use Bazel.
This does not constitute an endorsement.

## Companies using Bazel

### [acqio](https://acqio.com.br)

<img src="/community/images/acqio_logo.svg" width="150" align="right" />

Acqio is a Fintech that provides payment products and services for small and
medium merchants. Acqio has a handful of monorepos and uses Bazel along with
Kubernetes to deliver fast and reliable microservices.

### [Adobe](https://www.adobe.com/)

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Adobe_logo_and_wordmark_%282017%29.svg/440px-Adobe_logo_and_wordmark_%282017%29.svg.png" width="150" align="right" />

Adobe has released Bazel [rules](https://github.com/adobe/rules_gitops) for
continuous, GitOps driven Kubernetes deployments.

### [Asana](https://asana.com)

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Asana_logo.svg/256px-Asana_logo.svg.png" width="100" align="right" />

Asana is a web and mobile application designed to help teams track their work.
In their own words:

> Bazel has increased reliability, stability, and speed for all of builds/tests
at Asana. We no longer need to clean because of incorrect caches.

### [Ascend.io](https://ascend.io)

Ascend is a Palo Alto startup that offers solutions for large data sets
analysis. Their motto is _Big data is hard. We make it easy_.

### [ASML](https://asml.com)

<img src="https://upload.wikimedia.org/wikipedia/en/6/6c/ASML_Holding_N.V._logo.svg" width="150" align="right" />

ASML is an innovation leader in the semiconductor industry. We provide chipmakers
with everything they need – hardware, software and services – to mass produce
patterns on silicon through lithography.

### [Beeswax](https://www.beeswax.com/)

> Beeswax is a New York based startup that provides real time bidding as
service. Bazel powers their Jenkins based continuous integration and deployment
framework. Beeswax loves Bazel because it is blazingly fast, correct and well
supported across many languages and platforms.

### [Braintree](https://www.braintreepayments.com)

<img src="https://upload.wikimedia.org/wikipedia/commons/0/00/Braintree-logo1.png" width="150" align="right" />

Braintree, a PayPal subsidiary, develops payment solutions for websites and
applications. They use Bazel for parts of their internal build and Paul Gross
even posted a
[nice piece about how their switch to Bazel went](https://www.pgrs.net/2015/09/01/migrating-from-gradle-to-bazel/).

### [Canva](https://www.canva.com/)
<img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Canva_Logo.svg" width="90" align="right" />

Canva leverages Bazel to manage its large polyglot codebase, which includes
Java, TypeScript, Scala, Python, and more. Migration to Bazel has delivered
significant developer and compute infrastructure efficiencies, for example 5-6x
decreases in average CI build times, and it continues to become the foundation
of fast, reproducible, and standardised software builds at the company.

### [CarGurus](https://www.cargurus.com)
<img src="https://www.cargurus.com/gfx/reskin/logos/logo_CarGurus.svg" width="150" align="right" />

CarGurus is on a mission to build the world's most trusted and transparent
automotive marketplace and uses Bazel to build their polyglot monorepo.

### [Compass](https://www.compass.com)

Compass is a tech-driven real estate platform. With an elite team of real
estate, technology and business professionals, we aim to be the best and most
trusted source for home seekers.

### [Databricks](https://databricks.com)

<img src="https://databricks.com/wp-content/uploads/2021/10/db-nav-logo.svg" width="100" align="right" />
Databricks provides cloud-based integrated workspaces based on Apache Spark™.

> The Databricks codebase is a Monorepo, containing the Scala code that powers
most of our services, Javascript for front-end UI, Python for scripting,
Jsonnet to configure our infrastructure, and much more [...] Even though our
monorepo contains a million lines of Scala, working with code within is fast
and snappy.
([Speedy Scala Builds with Bazel at Databricks](https://databricks.com/blog/2019/02/27/speedy-scala-builds-with-bazel-at-databricks.html))

### [Dataform](https://dataform.co)

Dataform provides scalable analytics for data teams. They maintain a handful of
NPM packages and a documentation site in one single monorepo and they do it all
with Bazel.

After the migration to Bazel, they
[reported many benefits](https://github.com/bazelbuild/rules_nodejs#user-testimonials),
including:

> * Faster CI: we enabled the remote build caching which has reduced our average build time from 30 minutes to 5 (for the entire repository).
> * Improvements to local development: no more random bash scripts that you forget to run, incremental builds reduced to seconds from minutes
> * Developer setup time: New engineers can build all our code with just 3 dependencies - bazel, docker and the JVM. The last engineer to join our team managed to build all our code in < 30 minutes on a brand new, empty laptop

### [Deep Silver FISHLABS](https://www.dsfishlabs.com)
Deep Silver FISHLABS is a developer of high-end 3D games. They use Bazel with
C++/Python/Go/C as a base for their internal build tooling and especially for
baking and deploying all their 3D Assets.

### [Dropbox](https://www.dropbox.com/)
<img src="/community/images/dropbox.png" width="150" align="right" />
At Dropbox, Bazel is a key component to our distributed build and test
environment. We use Bazel to combine TypeScript/Python/Go/C/Rust into reliable
production releases.

### [Engel & Völkers](https://www.engelvoelkers.com)

Engel & Völkers AG is a privately owned German company that, via a series of
franchised offices, provides services related to real estate transactions.

> One of our internal project has seen a decrease of compilation time from 11
minutes to roughly 1 minute, this was an impressive achievement and we are
currently working on bringing Bazel to more projects.
([Experimenting with Google Cloud Build and Bazel](https://www.engelvoelkers.com/en/tech/engineering/software-engineering/experimenting-with-google-cloud-build-and-bazel/))

### [Etsy](https://www.etsy.com/)
<img src="https://upload.wikimedia.org/wikipedia/commons/a/aa/Etsy_logo_lg_rgb.png" width="150" align="right" />

Etsy is an e-commerce website focused on handmade or vintage items and supplies,
as well as unique factory-manufactured items.

They use Bazel to build and test its Java-based search platform. Bazel produces
both packages for bare metal servers and repeatable Docker images.

### [Evertz.io](https://www.evertz.io/)

Evertz.io is a multi-tenant, serverless SaaS platform for offering cost
effective, multi-regional services worldwide to the Broadcast Media Industry,
created by [Evertz Microsystems](https://en.wikipedia.org/wiki/Evertz_Microsystems).

The website is fully built and deployed with an Angular and Bazel workflow
([source](https://twitter.com/MattMackay/status/1113947685508341762)).

### [FINDMINE](http://www.findmine.com)
<img src="https://www.findmine.com/static/assets/landpage/findmine-color-logo.png" width="150" align="right" />

FINDMINE is a automation technology for the retail industry that uses machine
learning to scale the currently manual and tedious process of product curation.
We use Bazel to mechanize our entire python package building, testing, and
deployment process.

### [Flexport](https://www.flexport.com/)

Flexport is a tech-enabled global freight forwarder; our mission is to make
global trade easier for everyone. At Flexport, we use Bazel to build/test our
Java/JavaScript services and client libraries and to generate Java and Ruby
code from protobuf definitions.
[Read about how we run individual JUnit 5 tests in isolation with Bazel.](https://flexport.engineering/connecting-bazel-and-junit5-by-transforming-arguments-46440c6ea068)

### [Foursquare](https://foursquare.com)
<img src="https://upload.wikimedia.org/wikipedia/commons/9/99/FSQ_logo.png" width="150" align="right" />

Foursquare's mission is to create technology that constructs meaningful
bridges between digital spaces and physical places. We manage millions of
lines of primarily Scala and Python code powering data-intensive
applications, including complex codegen and container build processes, with
Bazel.

### [GermanTechJobs](https://germantechjobs.de)
<img src="https://upload.wikimedia.org/wikipedia/commons/9/98/GermanTechJobs_Logo.png" width="150" align="right" />

Bazel has simplified our workflows 10-fold and enabled shipping features at
scale.

### [Google](https://google.com)
<img src="https://upload.wikimedia.org/wikipedia/commons/2/2f/Google_2015_logo.svg" width="150" align="right" />

Bazel was designed to be able to scale to Google's needs and meet Google's
requirements of reproducibility and platform/language support. All software at
Google is built using Bazel. Google uses Bazel and its rules for millions of
builds every day.

### [Huawei](http://www.huawei.com/)

> Huawei Technologies is using Bazel in about 30 projects, they are Java/Scala/Go
projects, except for Go projects, others originally were built by Maven. We
write a simple tool to translate a Maven-built project into Bazel-built one.
More and more projects will use Bazel in recent future.

### [IMC Trading](https://imc.com)
<img src="https://upload.wikimedia.org/wikipedia/commons/1/17/IMC_Logo.svg" width="150" align="right" />

> IMC is a global proprietary trading firm and market maker headquarted in
Amsterdam. We are using Bazel to continuously build and test our
Java/C++/Python/SystemVerilog projects.

### [Improbable.io](https://improbable.io/)

Improbable.io develops SpatialOS, a distributed operating system that enables
creating huge simulations inhabited by millions of complex entities.

### [Interaxon](https://www.choosemuse.com/)

InteraXon is a thought-controlled computing firm that creates hardware and
software platforms to convert brainwaves into digital signals.

### [Jupiter](https://jupiter.co/)

Jupiter is a company that provides delivery of groceries and household
essentials every week.

They use Bazel in their backend code, specifically to compile protos and Kotlin
to JVM binaries, using remote caching.
([source](https://starship.jupiter.co/jupiter-stack/))

### [Just](https://gojust.com/)

Just is an enterprise financial technology company, headquartered in Norway,
creating software solutions to transform how global corporate treasurers manage
risk and liquidity. Their entire application stack is built with Bazel.

### [Line](https://line.me/)

Line provides an app for instant communications, which is the most popular
messaging application in Japan.
They use Bazel on their codebase consisting of about 60% Swift and 40%
C/C++/Objective-C/Objective-C++
([source](https://twitter.com/thi_dt/status/1253334262020886532)).

> After switching to Bazel, we were able to achieve a huge improvement in the
build times. This brought a significant improvement in the turn-around time
during a QA period. Distributing a new build to our testers no longer means
another hour waiting for building and testing.
([Improving Build Performance of LINE for iOS with Bazel](https://engineering.linecorp.com/en/blog/improving-build-performance-line-ios-bazel/))

### [LingoChamp](https://www.liulishuo.com/en)

<img src="/community/images/liulishuo.png" width="100" align="right" />
LingoChamp provides professional solutions to English learners. We use Bazel
for our go, java and python projects.

### [LinkedIn](https://linkedin.com/)

<img src="/community/images/Linkedin-Logo.png" width="100" align="right" />
LinkedIn, a subsidiary of Microsoft, is the world’s largest professional social
network. LinkedIn uses Bazel for building its iOS Apps.

### [Lucid Software](https://lucid.co/)

<img src="/community/images/Lucid_Software-logo.svg" width="150" align="right" />

Lucid Software is a leader in visual collaboration, helping teams see and build the
future from idea to reality. With its products—[Lucidchart](https://www.lucidchart.com/),
[Lucidspark](https://lucidspark.com/), and [Lucidscale](https://lucidscale.com/)—teams
can align around a shared vision, clarify complexity, and collaborate visually, no
matter where they’re located.

Lucid uses Bazel to build millions of lines of Scala and TypeScript.
Migrating to Bazel has tremendously sped up its builds, reduced external
dependencies on the build environment, and simplified developers' experience
with the build system. Bazel has improved developer productivity at Lucid and
unlocked further growth.

### [Lyft](https://www.lyft.com/)

Lyft is using Bazel for their iOS ([source](https://twitter.com/SmileyKeith/status/1116486751806033920)) and Android Apps.

### [Meetup](http://www.meetup.com/)

Meetup is an online social networking portal that facilitates offline group
meetings.
The Meetup engineering team contributes to
[rules_scala](https://github.com/bazelbuild/rules_scala) and is the
maintainer of [rules_avro](https://github.com/meetup/rules_avro)
and [rules_openapi](https://github.com/meetup/rules_openapi).


### [Nvidia](https://www.nvidia.com/)

> At Nvidia we have been using dazel(docker bazel) for python to work around
some of bazel's python short comings. Everything else runs in normal bazel
(Mostly Go / Scala/ C++/ Cuda)
([source](https://twitter.com/rwhitcomb/status/1080887723433447424))


### [Peloton Technology](http://www.peloton-tech.com)

Peloton Technology is an automated vehicle technology company that tackles truck
accidents and fuel use. They use Bazel to _enable reliable builds for automotive
safety systems_.

### [Pigweed](https://pigweed.dev)

<img src="https://pigweed.dev/_static/pw_logo.svg" width="100" align="right" />

Pigweed is an open-source solution for sustained, robust, and rapid embedded
product development for large teams. Pigweed has shipped in millions of
devices, including Google's suite of Pixel devices, Nest thermostats,
[satellites](https://www.spinlaunch.com/), and [autonomous aerial
drones](https://www.flyzipline.com/).

Pigweed [uses Bazel as its primary build
system](https://pigweed.dev/seed/0111-build-systems.html). The [Bazel for
Embedded][pw-bazel-great] blog post discusses why we think it's a great build
system for embedded projects!

[pw-bazel-great]: https://blog.bazel.build/2024/08/08/bazel-for-embedded.html#why-bazel-for-embedded

### [Pinterest](https://www.pinterest.com/)

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Pinterest_Logo.svg/200px-Pinterest_Logo.svg.png" width="150" align="right" />

Pinterest is the world’s catalog of ideas. They use Bazel to build various
backend services (Java/C++) and the iOS application (Objective-C/C++).

> We identified Bazel was the best fit for our goals to build a foundation for
an order of magnitude improvement in performance, eliminate variability in
build environments and adopt incrementally. As a result, we’re now shipping all
our iOS releases using Bazel.
[Developing fast & reliable iOS builds at Pinterest](https://medium.com/@Pinterest_Engineering/developing-fast-reliable-ios-builds-at-pinterest-part-one-cb1810407b92)

### [PubRef](https://github.com/pubref)

PubRef is an emerging scientific publishing platform.  They use Bazel with
[rules_closure](https://github.com/bazelbuild/rules_closure) to build the
frontend, native java rules to build the main backend,
[rules_go](https://github.com/bazelbuild/rules_go),
[rules_node](https://github.com/pubref/rules_node), and
[rules_kotlin](https://github.com/pubref/rules_kotlin) to build assorted
backend services.  [rules_protobuf](https://github.com/pubref/rules_protobuf) is
used to assist with gRPC-based communication between backend services.
PubRef.org is based in Boulder, CO.

### [Redfin](https://redfin.com/)
Redfin is a next-generation real estate brokerage with full-service local
agents. They use Bazel to build and deploy the website and various backend
services.

> With the conversion mostly behind us, things are greatly improved! Our CI
builds are faster (*way* faster: they used to take 40–90 minutes, and now dev
builds average 5–6 minutes). Reliability is far higher, too. This is harder to
quantify, but the shift from unexplained build failures being something that
“just happens” to being viewed as real problems to be solved has put us on a
virtuous cycle of ever-increasing reliability.
([We Switched from Maven to Bazel and Builds Got 10x Faster](https://redfin.engineering/we-switched-from-maven-to-bazel-and-builds-got-10x-faster-b265a7845854))

### [Ritual](https://ritual.co)
<img src="https://lh3.googleusercontent.com/7Ir6j25ROnsXhtQXveOzup33cizxLf-TiifSC1cI6op0bQVB-WePmPjJOfXUBQ0L3KpkheObAiS28e-TS8hZtDzxOIc" width="150" align="right" />

Ritual is a mobile pick up app, connecting restaurants with customers to offer
a simple, time-saving tool to get the food and beverages they want, without the
wait. Ritual uses Bazel for their backend services.

### [Snap](https://www.snap.com/en-US/)

Snap, the developer of Snapchat messaging app, has migrated from Buck to Bazel
in 2020 ([source](https://twitter.com/wew/status/1326957862816509953)). For more
details about their process, see their [engineering blog](https://eng.snap.com/blog/).

### [Stripe](https://stripe.com)
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Stripe_Logo%2C_revised_2016.svg/320px-Stripe_Logo%2C_revised_2016.svg.png" width="150" align="right" />

Stripe provides mobile payment solutions. They use Bazel in their build and test pipelines, as detailed in their [engineering blog](https://stripe.com/blog/fast-secure-builds-choose-two).

### [Tinder](https://tinder.com)
<img src="https://policies.tinder.com/static/b0327365f4c0a31c4337157c10e9fadf/c1b63/tinder_full_color_watermark.png" width="150" align="right" />

Tinder migrated its iOS app from CocoaPods to Bazel
in 2021 ([source](https://medium.com/tinder/bazel-hermetic-toolchain-and-tooling-migration-c244dc0d3ae)).

### [Tink](https://tink.com/)
<img src="https://cdn.tink.se/tink-logos/LOW/Tink_Black.png" width="150" align="right" />

Tink is a european fintech, building the best way to connect to banks across
Europe.

They are using Bazel to build their backend services from a polyglot monorepo.
Engineers at Tink are organizing the [bazel build //stockholm/...](https://www.meetup.com/BazelSTHLM/)
meetup group.

### [Tokopedia](https://www.tokopedia.com/)

Tokopedia is an Indonesian technology company specializing in e-commerce, with
over 90 million monthly active users and over 7 million merchants on the
platform.

They wrote the article
[How Tokopedia Achieved 1000% Faster iOS Build Time](https://medium.com/tokopedia-engineering/how-tokopedia-achieved-1000-faster-ios-build-time-7664b2d8ae5),
where they explain how Bazel sped up their builds. The build duration went from
55 minutes to 10 minutes by using Bazel, and down to 5 minutes with remote
caching.

### [Trunk.io](https://trunk.io/merge/trunk-merge-and-bazel)
<img src="/community/images/trunk-logo-dark.svg" width="150" align="right" />

Trunk is a San Francisco-based company backed by Andreessen Horowitz and Initialized Capital. Trunk offers a powerful pull request merge service with first-class support for the Bazel build system. By leveraging Bazel's understanding of dependencies within a codebase, Trunk's merge service intelligently creates parallel merge lanes, allowing independent changes to be tested and merged simultaneously.

> Trunk’s internal monorepo builds modern C++ 20 and typescript all while leveraging bazel graph knowledge to selectively test and merge code.

### [Twitter](https://twitter.com/)

Twitter has made the decision to migrate from Pants to Bazel as their primary
build tool
([source](https://groups.google.com/forum/#!msg/pants-devel/PHVIbVDLhx8/LpSKIP5cAwAJ)).

### [Two Sigma](https://www.twosigma.com/)
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Two_Sigma_logo.svg/2880px-Two_Sigma_logo.svg.png" width="150" align="right" />

Two Sigma is a New York-headquartered technology company dedicated to finding
value in the world’s data.

### [TypeDB](https://typedb.com)
<img src="/community/images/typedb.png" alt="TypeDB Logo" width="150" align="right" />

TypeDB is a database technology that can be used to intuitively model
interconnected data. Through its type-theoretic and polymorphic query language,
TypeQL, the data can be accessed with simple, human-readable queries that run at
lightspeed.

Bazel enables the TypeDB team to build a highly-orchestrated CI and distribution
pipeline that manages many repositories in a wide variety of languages, and
deploys to numerous platforms seamlessly. The TypeDB team has also released
Bazel rules for assembling and deploying software distributions.

### [Uber](https://www.uber.com)

Uber is a ride-hailing company. With 900 active developers, Uber’s Go monorepo
is likely one of the largest Go repositories using Bazel. See the article
[Building Uber’s Go Monorepo with Bazel](https://eng.uber.com/go-monorepo-bazel/)
to learn more about their experience.

### [Uber Advanced Technologies Group](https://www.uber.com/info/atg/)
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Uber_logo.svg/220px-Uber_logo.svg.png" width="150" align="right" />

Uber Advanced Technologies Group is focused on autonomous vehicle efforts at
Uber, including trucking/freight and autonomous ride sharing. The organization
uses Bazel as its primary build system.

### [Vistar Media](http://vistarmedia.com)
Vistar Media is an advertising platform that enables brands to reach consumers
based on their behavior in the physical world. Their engineering team is
primarily based out of Philadelphia and is using Bazel for builds, deploys, to
speed up testing, and to consolidate repositories written with a variety of
different technologies.

### [VMware](https://www.vmware.com/)
VMware uses Bazel to produce deterministic, reliable builds while developing
innovative products for their customers.

### [Wix](https://www.wix.com/)

Wix is a cloud-based web development platform. Their backend uses Java and Scala
code. They use remote execution with Google Cloud Build.

> We have seen about 5 times faster clean builds when running with bazel remote
execution which utilizes bazel’s great build/test parallelism capabilities when
it dispatches build/test actions to a worker farm. Average build times are more
than 10 times faster due to the utilization of bazel’s aggressive caching
mechanism.
([Migrating to Bazel from Maven or Gradle? 5 crucial questions you should ask yourself](https://medium.com/wix-engineering/migrating-to-bazel-from-maven-or-gradle-5-crucial-questions-you-should-ask-yourself-f23ac6bca070))

### [Zenly](https://zen.ly/)

Zenly is a live map of your friends and family. It’s the most fun way to meet up
— or just see what’s up! — so you can feel together, even when you're apart.


***

## Open source projects using Bazel

### [Abseil](https://abseil.io/)

Abseil is an open-source collection of C++ code (compliant to C++11) designed
to augment the C++ standard library.

### [Angular](https://angular.io)

<img src="https://upload.wikimedia.org/wikipedia/commons/c/cf/Angular_full_color_logo.svg" width="120" align="right" />

Angular is a popular web framework.
Angular is [built with Bazel](https://github.com/angular/angular/blob/master/docs/BAZEL.md).

### [Apollo](https://github.com/ApolloAuto/apollo)

Apollo is a high performance, flexible architecture which accelerates the
development, testing, and deployment of Autonomous Vehicles.

### [brpc](https://github.com/brpc/brpc)

An industrial-grade RPC framework used throughout Baidu, with 1,000,000+
instances(not counting clients) and thousands kinds of services, called
"baidu-rpc" inside Baidu.

### [cert-manager](https://github.com/jetstack/cert-manager)

cert-manager is a Kubernetes add-on to automate the management and issuance of
TLS certificates from various issuing sources. It will ensure certificates are
valid and up to date periodically, and attempt to renew certificates at an
appropriate time before expiry.

### [CallBuilder](https://github.com/google/CallBuilder)

A Java code generator that allows you to create a builder by writing one
function.

### [CPPItertools](https://github.com/ryanhaining/cppitertools)

C++ library providing range-based for loop add-ons inspired by the Python
builtins and itertools library. Like itertools and the Python3 builtins, this
library uses lazy evaluation wherever possible.

### [Copybara](https://github.com/google/copybara)

Copybara is a tool for transforming and moving code between repositories.

### [Dagger](https://google.github.io/dagger/)

Dagger is a fully static, compile-time dependency injection framework for both
Java and Android.

### [DAML](https://github.com/digital-asset/daml)

DAML is a smart contract language for building future-proof distributed
applications on a safe, privacy-aware runtime.

### [DeepMind Lab](https://github.com/deepmind/lab)

A customisable 3D platform for agent-based AI research.

### [Drake](https://github.com/RobotLocomotion/drake)

Drake is a C++ toolbox started at MIT and now led by the Toyota Research
Institute. It is a collection of tools for analyzing the dynamics of our robots
and building control systems for them, with a heavy emphasis on
optimization-based design/analysis.

### [Envoy](https://github.com/lyft/envoy)

C++ L7 proxy and communication bus

### [Error Prone](https://github.com/google/error-prone)

Catches common Java mistakes as compile-time errors. (Migration to Bazel is in
progress.)

### [Extensible Service Proxy](https://github.com/cloudendpoints/esp)

Extensible Service Proxy, a.k.a. ESP is a proxy which enables API management
capabilities for JSON/REST or gRPC API services. The current implementation is
based on an NGINX HTTP reverse proxy server.

### [FFruit](https://gitlab.com/perezd/ffruit/)

FFruit is a free & open source Android application to the popular service
[Falling Fruit](https://fallingfruit.org).

### [Gerrit Code Review](https://gerritcodereview.com)

Gerrit is a code review and project management tool for Git based projects.

### [Gitiles](https://gerrit.googlesource.com/gitiles/)

Gitiles is a simple repository browser for Git repositories, built on JGit.

### [Grakn](https://github.com/graknlabs/grakn)

Grakn (https://grakn.ai/) is the knowledge graph engine to organise complex
networks of data and make it queryable.

### [GRPC](http://www.grpc.io)
A language-and-platform-neutral remote procedure call system.
(Bazel is a supported, although not primary, build system.)

### [gVisor](https://github.com/google/gvisor)
gVisor is a container runtime sandbox.

### [Guetzli](https://github.com/google/guetzli/)

Guetzli is a JPEG encoder that aims for excellent compression density at high
visual quality.

### [Gulava](http://www.github.com/google/gulava/)

A Java code generator that lets you write Prolog-style predicates and use them
seamlessly from normal Java code.

### [Heron](https://github.com/apache/incubator-heron)

Heron is a realtime, distributed, fault-tolerant stream processing engine from
Twitter.

### [Internet Computer Protocol](https://internetcomputer.org/)

<img src="https://internetcomputer.org/img/IC_logo_horizontal_white.svg" width="120" align="right" />

The Internet Computer Protocol is a publicly available blockchain network that
enables replicated execution of general-purpose computation, serving hundreds
of thousands of applications and their users.

### [Jazzer](https://github.com/CodeIntelligenceTesting/jazzer)

<img src="https://www.code-intelligence.com/hubfs/Logos/CI%20Logos/Jazzer_einfach.png" width="120" align="right" />

Jazzer is a fuzzer for Java and other JVM-based languages that integrates with JUnit 5.

### [JGit](https://eclipse.org/jgit/)

JGit is a lightweight, pure Java library implementing the Git version control
system.

### [Jsonnet](https://jsonnet.org/)

An elegant, formally-specified config generation language for JSON.
(Bazel is a supported build system.)

### [Kubernetes](https://github.com/kubernetes/kubernetes)

<img src="https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png" width="80" align="right" />
Kubernetes is an open source system for managing containerized applications
across multiple hosts, providing basic mechanisms for deployment, maintenance,
and scaling of applications.

### [Kythe](https://github.com/google/kythe)

An ecosystem for building tools that work with code.

### [ls-lint](https://github.com/loeffel-io/ls-lint)

<img src="https://raw.githubusercontent.com/loeffel-io/ls-lint/master/assets/logo/ls-lint.png" width="75" align="right" />

An extremely fast directory and filename linter - Bring some structure to your
project file system.

### [Nomulus](https://github.com/google/nomulus)

Top-level domain name registry service on Google App Engine.

### [ONOS : Open Network Operating System](https://github.com/opennetworkinglab/onos)

<img src="https://upload.wikimedia.org/wikipedia/en/thumb/d/d3/Logo_for_the_ONOS_open_source_project.png/175px-Logo_for_the_ONOS_open_source_project.png" width="120" align="right" />
ONOS is the only SDN controller platform that supports the transition from
legacy “brown field” networks to SDN “green field” networks. This enables
exciting new capabilities, and disruptive deployment and operational cost points
for network operators.

### [PetitParser for Java](https://github.com/petitparser/java-petitparser)

Grammars for programming languages are traditionally specified statically.
They are hard to compose and reuse due to ambiguities that inevitably arise.
PetitParser combines ideas from scannnerless parsing, parser combinators,
parsing expression grammars and packrat parsers to model grammars and parsers
as objects that can be reconfigured dynamically.

### [PlaidML](https://github.com/plaidml/plaidml)

PlaidML is a framework for making deep learning work everywhere.

### [Project V](https://www.v2ray.com/)

<img src="https://www.v2ray.com/resources/v2ray_1024.png" width="100" align="right" />
Project V is a set of tools to help you build your own privacy network over
internet.

### [Prysmatic Labs Ethereum 2.0 Implementation](https://github.com/prysmaticlabs/prysm)

Prysm is a sharding client for Ethereum 2.0, a blockchain-based distributed
computing platform.

### [Ray](https://github.com/ray-project/ray)

Ray is a flexible, high-performance distributed execution framework.

### [Resty](https://github.com/go-resty/resty)

Resty is a Simple HTTP and REST client library for Go (inspired by Ruby
rest-client).

### [Roughtime](https://roughtime.googlesource.com/roughtime)

Roughtime is a project that aims to provide secure time synchronisation.

### [Selenium](https://github.com/SeleniumHQ/selenium)

Selenium is a portable framework for testing web applications.

### [Semantic](https://github.com/github/semantic)

Semantic is a Haskell library and command line tool for parsing, analyzing, and
comparing source code. It is developed by GitHub (and used for example for the
code navigation).

### [Served](https://github.com/meltwater/served)

Served is a C++ library for building high performance RESTful web servers.

### [Sonnet](https://github.com/deepmind/sonnet)

Sonnet is a library built on top of TensorFlow for building complex neural
networks.

### [Sorbet](https://github.com/sorbet/sorbet)

Sorbet is a fast, powerful type checker for a subset of Ruby. It scales to
codebases with millions of lines of code and can be adopted incrementally.

### [Spotify](https://spotify.com)

Spotify is using Bazel to build their iOS and Android Apps ([source](https://twitter.com/BalestraPatrick/status/1573355078995566594)).

### [Tink](https://github.com/google/tink)

Tink is a multi-language, cross-platform, open source library that provides
cryptographic APIs that are secure, easy to use correctly, and hard(er) to
misuse.

### [TensorFlow](http://tensorflow.org)
<img src="https://upload.wikimedia.org/wikipedia/commons/a/a4/TensorFlowLogo.png" width="150" align="right" />

An open source software library for machine intelligence.

### [Turbo Santa](https://github.com/turbo-santa/turbo-santa-common)

A platform-independent GameBoy emulator.

### [Wycheproof](https://github.com/google/wycheproof)

Project Wycheproof tests crypto libraries against known attacks.

### [XIOSim](https://github.com/s-kanev/XIOSim)

XIOSim is a detailed user-mode microarchitectural simulator for the x86
architecture.

### [ZhihuDailyPurify](https://github.com/izzyleung/ZhihuDailyPurify)

ZhihuDailyPurify is a light weight version of Zhihu Daily, a Chinese
question-and-answer webs.

---

## Repositories, workspaces, packages, and targets
- URL: https://bazel.build/concepts/build-ref
- Source: concepts/build-ref.mdx
- Slug: /concepts/build-ref

Bazel builds software from source code organized in directory trees called
repositories. A defined set of repositories comprises the workspace. Source
files in repositories are organized in a nested hierarchy of packages, where
each package is a directory that contains a set of related source files and one
`BUILD` file. The `BUILD` file specifies what software outputs can be built from
the source.

### Repositories

Source files used in a Bazel build are organized in _repositories_ (often
shortened to _repos_). A repo is a directory tree with a boundary marker file at
its root; such a boundary marker file could be `MODULE.bazel`, `REPO.bazel`, or
in legacy contexts, `WORKSPACE` or `WORKSPACE.bazel`.

The repo in which the current Bazel command is being run is called the _main
repo_. Other, (external) repos are defined by _repo rules_; see [external
dependencies overview](/external/overview) for more information.

## Workspace

A _workspace_ is the environment shared by all Bazel commands run from the same
main repo. It encompasses the main repo and the set of all defined external
repos.

Note that historically the concepts of "repository" and "workspace" have been
conflated; the term "workspace" has often been used to refer to the main
repository, and sometimes even used as a synonym of "repository".

## Packages

The primary unit of code organization in a repository is the _package_. A
package is a collection of related files and a specification of how they can be
used to produce output artifacts.

A package is defined as a directory containing a
[`BUILD` file](/concepts/build-files) named either `BUILD` or `BUILD.bazel`. A
package includes all files in its directory, plus all subdirectories beneath it,
except those which themselves contain a `BUILD` file. From this definition, no
file or directory may be a part of two different packages.

For example, in the following directory tree there are two packages, `my/app`,
and the subpackage `my/app/tests`. Note that `my/app/data` is not a package, but
a directory belonging to package `my/app`.

```
src/my/app/BUILD
src/my/app/app.cc
src/my/app/data/input.txt
src/my/app/tests/BUILD
src/my/app/tests/test.cc
```

## Targets

A package is a container of _targets_, which are defined in the package's
`BUILD` file. Most targets are one of two principal kinds, _files_ and _rules_.

Files are further divided into two kinds. _Source files_ are usually written by
the efforts of people, and checked in to the repository. _Generated files_,
sometimes called derived files or output files, are not checked in, but are
generated from source files.

The second kind of target is declared with a _rule_. Each rule instance
specifies the relationship between a set of input and a set of output files. The
inputs to a rule may be source files, but they also may be the outputs of other
rules.

Whether the input to a rule is a source file or a generated file is in most
cases immaterial; what matters is only the contents of that file. This fact
makes it easy to replace a complex source file with a generated file produced by
a rule, such as happens when the burden of manually maintaining a highly
structured file becomes too tiresome, and someone writes a program to derive it.
No change is required to the consumers of that file. Conversely, a generated
file may easily be replaced by a source file with only local changes.

The inputs to a rule may also include _other rules_. The precise meaning of such
relationships is often quite complex and language- or rule-dependent, but
intuitively it is simple: a C++ library rule A might have another C++ library
rule B for an input. The effect of this dependency is that B's header files are
available to A during compilation, B's symbols are available to A during
linking, and B's runtime data is available to A during execution.

An invariant of all rules is that the files generated by a rule always belong to
the same package as the rule itself; it is not possible to generate files into
another package. It is not uncommon for a rule's inputs to come from another
package, though.

Package groups are sets of packages whose purpose is to limit accessibility of
certain rules. Package groups are defined by the `package_group` function. They
have three properties: the list of packages they contain, their name, and other
package groups they include. The only allowed ways to refer to them are from the
`visibility` attribute of rules or from the `default_visibility` attribute of
the `package` function; they do not generate or consume files. For more
information, refer to the [`package_group`
documentation](/reference/be/functions#package_group).

<a class="button button-with-icon button-primary" href="/concepts/labels">
  Labels<span class="material-icons icon-after" aria-hidden="true">arrow_forward</span>
</a>

---

## Migrating to Platforms
- URL: https://bazel.build/concepts/platforms
- Source: concepts/platforms.mdx
- Slug: /concepts/platforms

Bazel has sophisticated [support](#background) for modeling
[platforms][Platforms] and [toolchains][Toolchains] for multi-architecture and
cross-compiled builds.

This page summarizes the state of this support.

Key Point: Bazel's platform and toolchain APIs are available today. Not all
languages support them. Use these APIs with your project if you can. Bazel is
migrating all major languages so eventually all builds will be platform-based.

See also:

* [Platforms][Platforms]
* [Toolchains][Toolchains]
* [Background][Background]

## Status

### C++

C++ rules use platforms to select toolchains when
`--incompatible_enable_cc_toolchain_resolution` is set.

This means you can configure a C++ project with:

```posix-terminal
bazel build //:my_cpp_project --platforms=//:myplatform
```

instead of the legacy:

```posix-terminal
bazel build //:my_cpp_project` --cpu=... --crosstool_top=...  --compiler=...
```

This will be enabled by default in Bazel 7.0 ([#7260](https://github.com/bazelbuild/bazel/issues/7260)).

To test your C++ project with platforms, see
[Migrating Your Project](#migrating-your-project) and
[Configuring C++ toolchains].

### Java

Java rules use platforms to select toolchains.

This replaces legacy flags `--java_toolchain`, `--host_java_toolchain`,
`--javabase`, and `--host_javabase`.

See [Java and Bazel](/docs/bazel-and-java) for details.

### Android

Android rules use platforms to select toolchains when
`--incompatible_enable_android_toolchain_resolution` is set.

This means you can configure an Android project with:

```posix-terminal
bazel build //:my_android_project --android_platforms=//:my_android_platform
```

instead of with legacy flags like  `--android_crosstool_top`, `--android_cpu`,
and `--fat_apk_cpu`.

This will be enabled by default in Bazel 7.0 ([#16285](https://github.com/bazelbuild/bazel/issues/16285)).

To test your Android project with platforms, see
[Migrating Your Project](#migrating-your-project).

### Apple

[Apple rules] do not support platforms and are not yet scheduled
for support.

You can still use platform APIs with Apple builds (for example, when building
with a mixture of Apple rules and pure C++) with [platform
mappings](#platform-mappings).

### Other languages

* [Go rules] fully support platforms
* [Rust rules] fully support platforms.

If you own a language rule set, see [Migrating your rule set] for adding
support.

## Background

*Platforms* and *toolchains* were introduced to standardize how software
projects target different architectures and cross-compile.

This was
[inspired][Inspiration]
by the observation that language maintainers were already doing this in ad
hoc, incompatible ways. For example, C++ rules used `--cpu` and
 `--crosstool_top` to declare a target CPU and toolchain. Neither of these
correctly models a "platform". This produced awkward and incorrect builds.

Java, Android, and other languages evolved their own flags for similar purposes,
none of which interoperated with each other. This made cross-language builds
confusing and complicated.

Bazel is intended for large, multi-language, multi-platform projects. This
demands more principled support for these concepts, including a clear
standard API.

### Need for migration

Upgrading to the new API requires two efforts: releasing the API and upgrading
rule logic to use it.

The first is done but the second is ongoing. This consists of ensuring
language-specific platforms and toolchains are defined, language logic reads
toolchains through the new API instead of old flags like `--crosstool_top`, and
`config_setting`s select on the new API instead of old flags.

This work is straightforward but requires a distinct effort for each language,
plus fair warning for project owners to test against upcoming changes.

This is why this is an ongoing migration.

### Goal

This migration is complete when all projects build with the form:

```posix-terminal
bazel build //:myproject --platforms=//:myplatform
```

This implies:

1. Your project's rules choose the right toolchains for `//:myplatform`.
1. Your project's dependencies choose the right toolchains for `//:myplatform`.
1. `//:myplatform` references
[common declarations][Common Platform Declarations]
of `CPU`, `OS`, and other generic, language-independent properties
1. All relevant [`select()`s][select()] properly match `//:myplatform`.
1. `//:myplatform` is defined in a clear, accessible place: in your project's
repo if the platform is unique to your project, or some common place all
consuming projects can find it

Old flags like `--cpu`, `--crosstool_top`, and `--fat_apk_cpu` will be
deprecated and removed as soon as it's safe to do so.

Ultimately, this will be the *sole* way to configure architectures.


## Migrating your project

If you build with languages that support platforms, your build should already
work with an invocation like:

```posix-terminal
bazel build //:myproject --platforms=//:myplatform
```

See [Status](#status) and your language's documentation for precise details.

If a language requires a flag to enable platform support, you also need to set
that flag. See [Status](#status) for details.

For your project to build, you need to check the following:

1. `//:myplatform` must exist. It's generally the project owner's responsibility
   to define platforms because different projects target different machines.
   See [Default platforms](#default-platforms).

1. The toolchains you want to use must exist. If using stock toolchains, the
   language owners should include instructions for how to register them. If
   writing your own custom toolchains, you need to [register](https://bazel.build/extending/toolchains#registering-building-toolchains) them in your
   `MODULE.bazel` file or with [`--extra_toolchains`](https://bazel.build/reference/command-line-reference#flag--extra_toolchains).

1. `select()`s and [configuration transitions][Starlark transitions] must
  resolve properly. See [select()](#select) and [Transitions](#transitions).

1. If your build mixes languages that do and don't support platforms, you may
   need platform mappings to help the legacy languages work with the new API.
   See [Platform mappings](#platform-mappings) for details.

If you still have problems, [reach out](#questions) for support.

### Default platforms

Project owners should define explicit
[platforms][Defining Constraints and Platforms] to describe the architectures
they want to build for. These are then triggered with `--platforms`.

When `--platforms` isn't set, Bazel defaults to a `platform` representing the
local build machine. This is auto-generated at `@platforms//host` (aliased as
`@bazel_tools//tools:host_platform`)
so there's no need to explicitly define it. It maps the local machine's `OS`
and `CPU` with `constraint_value`s declared in
[`@platforms`](https://github.com/bazelbuild/platforms).

### `select()`

Projects can [`select()`][select()] on
[`constraint_value` targets][constraint_value Rule] but not complete
platforms. This is intentional so `select()` supports as wide a variety of
machines as possible. A library with `ARM`-specific sources should support *all*
`ARM`-powered machines unless there's reason to be more specific.

To select on one or more `constraint_value`s, use:

```python
config_setting(
    name = "is_arm",
    constraint_values = [
        "@platforms//cpu:arm",
    ],
)
```

This is equivalent to traditionally selecting on `--cpu`:

```python
config_setting(
    name = "is_arm",
    values = {
        "cpu": "arm",
    },
)
```

More details [here][select() Platforms].

`select`s on `--cpu`, `--crosstool_top`, etc. don't understand `--platforms`.
When migrating your project to platforms, you must either convert them to
`constraint_values` or use [platform mappings](#platform-mappings) to support
both styles during migration.

### Transitions

[Starlark transitions][Starlark transitions] change
flags down parts of your build graph. If your project uses a transition that
sets `--cpu`, `--crossstool_top`, or other legacy flags, rules that read
`--platforms` won't see these changes.

When migrating your project to platforms, you must either convert changes like
`return { "//command_line_option:cpu": "arm" }` to `return {
"//command_line_option:platforms": "//:my_arm_platform" }` or use [platform
mappings](#platform-mappings) to support both styles during migration.
window.

## Migrating your rule set

If you own a rule set and want to support platforms, you need to:

1. Have rule logic resolve toolchains with the toolchain API. See
   [toolchain API][Toolchains] (`ctx.toolchains`).

1. Optional: define an `--incompatible_enable_platforms_for_my_language` flag so
   rule logic alternately resolves toolchains through the new API or old flags
   like `--crosstool_top` during migration testing.

1. Define the relevant properties that make up platform components. See
   [Common platform properties](#common-platform-properties)

1. Define standard toolchains and make them accessible to users through your
   rule's registration instructions ([details](https://bazel.build/extending/toolchains#registering-building-toolchains))

1. Ensure [`select()`s](#select) and
   [configuration transitions](#transitions) support platforms. This is the
   biggest challenge. It's particularly challenging for multi-language projects
   (which may fail if *all* languages can't read `--platforms`).

If you need to mix with rules that don't support platforms, you may need
[platform mappings](#platform-mappings) to bridge the gap.

### Common platform properties

Common, cross-language platform properties like `OS` and `CPU` should be
declared in [`@platforms`](https://github.com/bazelbuild/platforms).
This encourages sharing, standardization, and cross-language compatibility.

Properties unique to your rules should be declared in your rule's repo. This
lets you maintain clear ownership over the specific concepts your rules are
responsible for.

If your rules use custom-purpose OSes or CPUs, these should be declared in your
rule's repo vs.
[`@platforms`](https://github.com/bazelbuild/platforms).

## Platform mappings

*Platform mappings* is a temporary API that lets platform-aware logic mix with
legacy logic in the same build. This is a blunt tool that's only intended to
smooth incompatibilities with different migration timeframes.

Caution: Only use this if necessary, and expect to eventually  eliminate it.

A platform mapping is a map of either a `platform()` to a
corresponding set of legacy flags or the reverse. For example:

```python
platforms:
  # Maps "--platforms=//platforms:ios" to "--ios_multi_cpus=x86_64 --apple_platform_type=ios".
  //platforms:ios
    --ios_multi_cpus=x86_64
    --apple_platform_type=ios

flags:
  # Maps "--ios_multi_cpus=x86_64 --apple_platform_type=ios" to "--platforms=//platforms:ios".
  --ios_multi_cpus=x86_64
  --apple_platform_type=ios
    //platforms:ios

  # Maps "--cpu=darwin_x86_64 --apple_platform_type=macos" to "//platform:macos".
  --cpu=darwin_x86_64
  --apple_platform_type=macos
    //platforms:macos
```

Bazel uses this to guarantee all settings, both platform-based and
legacy, are consistently applied throughout the build, including through
[transitions](#transitions).

By default Bazel reads mappings from the `platform_mappings` file in your
workspace root. You can also set
`--platform_mappings=//:my_custom_mapping`.

See the [platform mappings design] for details.

## API review

A [`platform`][platform Rule] is a collection of
[`constraint_value` targets][constraint_value Rule]:

```python
platform(
    name = "myplatform",
    constraint_values = [
        "@platforms//os:linux",
        "@platforms//cpu:arm",
    ],
)
```

A [`constraint_value`][constraint_value Rule] is a machine
property. Values of the same "kind" are grouped under a common
[`constraint_setting`][constraint_setting Rule]:

```python
constraint_setting(name = "os")
constraint_value(
    name = "linux",
    constraint_setting = ":os",
)
constraint_value(
    name = "mac",
    constraint_setting = ":os",
)
```

A [`toolchain`][Toolchains] is a [Starlark rule][Starlark rule]. Its
attributes declare a language's tools (like `compiler =
"//mytoolchain:custom_gcc"`). Its [providers][Starlark Provider] pass
this information to rules that need to build with these tools.

Toolchains declare the `constraint_value`s of machines they can
[target][target_compatible_with Attribute]
(`target_compatible_with = ["@platforms//os:linux"]`) and machines their tools can
[run on][exec_compatible_with Attribute]
(`exec_compatible_with = ["@platforms//os:mac"]`).

When building `$ bazel build //:myproject --platforms=//:myplatform`, Bazel
automatically selects a toolchain that can run on the build machine and
build binaries for `//:myplatform`. This is known as *toolchain resolution*.

The set of available toolchains can be registered in the `MODULE.bazel` file
with [`register_toolchains`][register_toolchains Function] or at the
command line with [`--extra_toolchains`][extra_toolchains Flag].

For more information see [here][Toolchains].

## Questions

For general support and questions about the migration timeline, contact
[bazel-discuss] or the owners of the appropriate rules.

For discussions on the design and evolution of the platform/toolchain APIs,
contact [bazel-dev].

## See also

* [Configurable Builds - Part 1]
* [Platforms]
* [Toolchains]
* [Bazel Platforms Cookbook]
* [Platforms examples]
* [Example C++ toolchain]

[Android Rules]: /docs/bazel-and-android
[Apple Rules]: https://github.com/bazelbuild/rules_apple
[Background]: #background
[Bazel platforms Cookbook]: https://docs.google.com/document/d/1UZaVcL08wePB41ATZHcxQV4Pu1YfA1RvvWm8FbZHuW8/
[bazel-dev]: https://groups.google.com/forum/#!forum/bazel-dev
[bazel-discuss]: https://groups.google.com/forum/#!forum/bazel-discuss
[Common Platform Declarations]: https://github.com/bazelbuild/platforms
[constraint_setting Rule]: /reference/be/platforms-and-toolchains#constraint_setting
[constraint_value Rule]: /reference/be/platforms-and-toolchains#constraint_value
[Configurable Builds - Part 1]: https://blog.bazel.build/2019/02/11/configurable-builds-part-1.html
[Configuring C++ toolchains]: /tutorials/ccp-toolchain-config
[Defining Constraints and Platforms]: /extending/platforms#constraints-platforms
[Example C++ toolchain]: https://github.com/gregestren/snippets/tree/master/custom_cc_toolchain_with_platforms
[exec_compatible_with Attribute]: /reference/be/platforms-and-toolchains#toolchain.exec_compatible_with
[extra_toolchains Flag]: /reference/command-line-reference#flag--extra_toolchains
[Go Rules]: https://github.com/bazelbuild/rules_go
[Inspiration]: https://blog.bazel.build/2019/02/11/configurable-builds-part-1.html
[Migrating your rule set]: #migrating-your-rule-set
[Platforms]: /extending/platforms
[Platforms examples]: https://github.com/hlopko/bazel_platforms_examples
[platform mappings design]: https://docs.google.com/document/d/1Vg_tPgiZbSrvXcJ403vZVAGlsWhH9BUDrAxMOYnO0Ls/edit
[platform Rule]: /reference/be/platforms-and-toolchains#platform
[register_toolchains Function]: /rules/lib/globals/module#register_toolchains
[Rust rules]: https://github.com/bazelbuild/rules_rust
[select()]: /docs/configurable-attributes
[select() Platforms]: /docs/configurable-attributes#platforms
[Starlark provider]: /extending/rules#providers
[Starlark rule]: /extending/rules
[Starlark transitions]: /extending/config#user-defined-transitions
[target_compatible_with Attribute]: /reference/be/platforms-and-toolchains#toolchain.target_compatible_with
[Toolchains]: /extending/toolchains

---

## Visibility
- URL: https://bazel.build/concepts/visibility
- Source: concepts/visibility.mdx
- Slug: /concepts/visibility

This page covers Bazel's two visibility systems:
[target visibility](#target-visibility) and [load visibility](#load-visibility).

Both types of visibility help other developers distinguish between your
library's public API and its implementation details, and help enforce structure
as your workspace grows. You can also use visibility when deprecating a public
API to allow current users while denying new ones.

## Target visibility

**Target visibility** controls who may depend on your target — that is, who may
use your target's label inside an attribute such as `deps`. A target will fail
to build during the [analysis](/reference/glossary#analysis-phase) phase if it
violates the visibility of one of its dependencies.

Generally, a target `A` is visible to a target `B` if they are in the same
location, or if `A` grants visibility to `B`'s location. In the absence of
[symbolic macros](/extending/macros), the term "location" can be simplified
to just "package"; see [below](#symbolic-macros) for more on symbolic macros.

Visibility is specified by listing allowed packages. Allowing a package does not
necessarily mean that its subpackages are also allowed. For more details on
packages and subpackages, see [Concepts and terminology](/concepts/build-ref).

For prototyping, you can disable target visibility enforcement by setting the
flag `--check_visibility=false`. This shouldn't be done for production usage in
submitted code.

The primary way to control visibility is with a rule's
[`visibility`](/reference/be/common-definitions#common.visibility) attribute.
The following subsections describe the attribute's format, how to apply it to
various kinds of targets, and the interaction between the visibility system and
symbolic macros.

### Visibility specifications

All rule targets have a `visibility` attribute that takes a list of labels. Each
label has one of the following forms. With the exception of the last form, these
are just syntactic placeholders that don't correspond to any actual target.

*   `"//visibility:public"`: Grants access to all packages.

*   `"//visibility:private"`: Does not grant any additional access; only targets
    in this location's package can use this target.

*   `"//foo/bar:__pkg__"`: Grants access to `//foo/bar` (but not its
    subpackages).

*   `"//foo/bar:__subpackages__"`: Grants access to `//foo/bar` and all of its
    direct and indirect subpackages.

*   `"//some_pkg:my_package_group"`: Grants access to all of the packages that
    are part of the given [`package_group`](/reference/be/functions#package_group).

    *   Package groups use a
        [different syntax](/reference/be/functions#package_group.packages) for
        specifying packages. Within a package group, the forms
        `"//foo/bar:__pkg__"` and `"//foo/bar:__subpackages__"` are respectively
        replaced by `"//foo/bar"` and `"//foo/bar/..."`. Likewise,
        `"//visibility:public"` and `"//visibility:private"` are just `"public"`
        and `"private"`.

For example, if `//some/package:mytarget` has its `visibility` set to
`[":__subpackages__", "//tests:__pkg__"]`, then it could be used by any target
that is part of the `//some/package/...` source tree, as well as targets
declared in `//tests/BUILD`, but not by targets defined in
`//tests/integration/BUILD`.

**Best practice:** To make several targets visible to the same set
of packages, use a `package_group` instead of repeating the list in each
target's `visibility` attribute. This increases readability and prevents the
lists from getting out of sync.

**Best practice:** When granting visibility to another team's project, prefer
`__subpackages__` over `__pkg__` to avoid needless visibility churn as that
project evolves and adds new subpackages.

Note: The `visibility` attribute may not specify non-`package_group` targets.
Doing so triggers a "Label does not refer to a package group" or "Cycle in
dependency graph" error.

### Rule target visibility

A rule target's visibility is determined by taking its `visibility` attribute
-- or a suitable default if not given -- and appending the location where the
target was declared. For targets not declared in a symbolic macro, if the
package specifies a [`default_visibility`](/reference/be/functions#package.default_visibility),
this default is used; for all other packages and for targets declared in a
symbolic macro, the default is just `["//visibility:private"]`.

```starlark
# //mypkg/BUILD

package(default_visibility = ["//friend:__pkg__"])

cc_library(
    name = "t1",
    ...
    # No visibility explicitly specified.
    # Effective visibility is ["//friend:__pkg__", "//mypkg:__pkg__"].
    # If no default_visibility were given in package(...), the visibility would
    # instead default to ["//visibility:private"], and the effective visibility
    # would be ["//mypkg:__pkg__"].
)

cc_library(
    name = "t2",
    ...
    visibility = [":clients"],
    # Effective visibility is ["//mypkg:clients, "//mypkg:__pkg__"], which will
    # expand to ["//another_friend:__subpackages__", "//mypkg:__pkg__"].
)

cc_library(
    name = "t3",
    ...
    visibility = ["//visibility:private"],
    # Effective visibility is ["//mypkg:__pkg__"]
)

package_group(
    name = "clients",
    packages = ["//another_friend/..."],
)
```

**Best practice:** Avoid setting `default_visibility` to public. It may be
convenient for prototyping or in small codebases, but the risk of inadvertently
creating public targets increases as the codebase grows. It's better to be
explicit about which targets are part of a package's public interface.

### Generated file target visibility

A generated file target has the same visibility as the rule target that
generates it.

```starlark
# //mypkg/BUILD

java_binary(
    name = "foo",
    ...
    visibility = ["//friend:__pkg__"],
)
```

```starlark
# //friend/BUILD

some_rule(
    name = "bar",
    deps = [
        # Allowed directly by visibility of foo.
        "//mypkg:foo",
        # Also allowed. The java_binary's "_deploy.jar" implicit output file
        # target the same visibility as the rule target itself.
        "//mypkg:foo_deploy.jar",
    ]
    ...
)
```

### Source file target visibility

Source file targets can either be explicitly declared using
[`exports_files`](/reference/be/functions#exports_files), or implicitly created
by referring to their filename in a label attribute of a rule (outside of a
symbolic macro). As with rule targets, the location of the call to
`exports_files`, or the BUILD file that referred to the input file, is always
automatically appended to the file's visibility.

Files declared by `exports_files` can have their visibility set by the
`visibility` parameter to that function. If this parameter is not given, the visibility is public.

Note: `exports_files` may not be used to override the visibility of a generated
file.

For files that do not appear in a call to `exports_files`, the visibility
depends on the value of the flag
[`--incompatible_no_implicit_file_export`](https://github.com/bazelbuild/bazel/issues/10225):

*   If the flag is true, the visibility is private.

*   Else, the legacy behavior applies: The visibility is the same as the
    `BUILD` file's `default_visibility`, or private if a default visibility is
    not specified.

Avoid relying on the legacy behavior. Always write an `exports_files`
declaration whenever a source file target needs non-private visibility.

**Best practice:** When possible, prefer to expose a rule target rather than a
source file. For example, instead of calling `exports_files` on a `.java` file,
wrap the file in a non-private `java_library` target. Generally, rule targets
should only directly reference source files that live in the same package.

#### Example

File `//frobber/data/BUILD`:

```starlark
exports_files(["readme.txt"])
```

File `//frobber/bin/BUILD`:

```starlark
cc_binary(
  name = "my-program",
  data = ["//frobber/data:readme.txt"],
)
```

### Config setting visibility

Historically, Bazel has not enforced visibility for
[`config_setting`](/reference/be/general#config_setting) targets that are
referenced in the keys of a [`select()`](/reference/be/functions#select). There
are two flags to remove this legacy behavior:

*   [`--incompatible_enforce_config_setting_visibility`](https://github.com/bazelbuild/bazel/issues/12932)
    enables visibility checking for these targets. To assist with migration, it
    also causes any `config_setting` that does not specify a `visibility` to be
    considered public (regardless of package-level `default_visibility`).

*   [`--incompatible_config_setting_private_default_visibility`](https://github.com/bazelbuild/bazel/issues/12933)
    causes `config_setting`s that do not specify a `visibility` to respect the
    package's `default_visibility` and to fallback on private visibility, just
    like any other rule target. It is a no-op if
    `--incompatible_enforce_config_setting_visibility` is not set.

Avoid relying on the legacy behavior. Any `config_setting` that is intended to
be used outside the current package should have an explicit `visibility`, if the
package does not already specify a suitable `default_visibility`.

### Package group target visibility

`package_group` targets do not have a `visibility` attribute. They are always
publicly visible.

### Visibility of implicit dependencies

Some rules have [implicit dependencies](/extending/rules#private_attributes_and_implicit_dependencies) —
dependencies that are not spelled out in a `BUILD` file but are inherent to
every instance of that rule. For example, a `cc_library` rule might create an
implicit dependency from each of its rule targets to an executable target
representing a C++ compiler.

The visibility of such an implicit dependency is checked with respect to the
package containing the `.bzl` file in which the rule (or aspect) is defined. In
our example, the C++ compiler could be private so long as it lives in the same
package as the definition of the `cc_library` rule. As a fallback, if the
implicit dependency is not visible from the definition, it is checked with
respect to the `cc_library` target.

If you want to restrict the usage of a rule to certain packages, use
[load visibility](#load-visibility) instead.

### Visibility and symbolic macros

This section describes how the visibility system interacts with
[symbolic macros](/extending/macros).

#### Locations within symbolic macros

A key detail of the visibility system is how we determine the location of a
declaration. For targets that are not declared in a symbolic macro, the location
is just the package where the target lives -- the package of the `BUILD` file.
But for targets created in a symbolic macro, the location is the package
containing the `.bzl` file where the macro's definition (the
`my_macro = macro(...)` statement) appears. When a target is created inside
multiple nested targets, it is always the innermost symbolic macro's definition
that is used.

The same system is used to determine what location to check against a given
dependency's visibility. If the consuming target was created inside a macro, we
look at the innermost macro's definition rather than the package the consuming
target lives in.

This means that all macros whose code is defined in the same package are
automatically "friends" with one another. Any target directly created by a macro
defined in `//lib:defs.bzl` can be seen from any other macro defined in `//lib`,
regardless of what packages the macros are actually instantiated in. Likewise,
they can see, and can be seen by, targets declared directly in `//lib/BUILD` and
its legacy macros. Conversely, targets that live in the same package cannot
necessarily see one another if at least one of them is created by a symbolic
macro.

Within a symbolic macro's implementation function, the `visibility` parameter
has the effective value of the macro's `visibility` attribute after appending
the location where the macro was called. The standard way for a macro to export
one of its targets to its caller is to forward this value along to the target's
declaration, as in `some_rule(..., visibility = visibility)`. Targets that omit
this attribute won't be visible to the caller of the macro unless the caller
happens to be in the same package as the macro definition. This behavior
composes, in the sense that a chain of nested calls to submacros may each pass
`visibility = visibility`, re-exporting the inner macro's exported targets to
the caller at each level, without exposing any of the macros' implementation
details.

#### Delegating privileges to a submacro

The visibility model has a special feature to allow a macro to delegate its
permissions to a submacro. This is important for factoring and composing macros.

Suppose you have a macro `my_macro` that creates a dependency edge using a rule
`some_library` from another package:

```starlark
# //macro/defs.bzl
load("//lib:defs.bzl", "some_library")

def _impl(name, visibility, ...):
    ...
    native.genrule(
        name = name + "_dependency"
        ...
    )
    some_library(
        name = name + "_consumer",
        deps = [name + "_dependency"],
        ...
    )

my_macro = macro(implementation = _impl, ...)
```

```starlark
# //pkg/BUILD

load("//macro:defs.bzl", "my_macro")

my_macro(name = "foo", ...)
```

The `//pkg:foo_dependency` target has no `visibility` specified, so it is only
visible within `//macro`, which works fine for the consuming target. Now, what
happens if the author of `//lib` refactors `some_library` to instead be
implemented using a macro?

```starlark
# //lib:defs.bzl

def _impl(name, visibility, deps, ...):
    some_rule(
        # Main target, exported.
        name = name,
        visibility = visibility,
        deps = deps,
        ...)

some_library = macro(implementation = _impl, ...)
```

With this change, `//pkg:foo_consumer`'s location is now `//lib` rather than
`//macro`, so its usage of `//pkg:foo_dependency` violates the dependency's
visibility. The author of `my_macro` can't be expected to pass
`visibility = ["//lib"]` to the declaration of the dependency just to work
around this implementation detail.

For this reason, when a dependency of a target is also an attribute value of the
macro that declared the target, we check the dependency's visibility against the
location of the macro instead of the location of the consuming target.

In this example, to validate whether `//pkg:foo_consumer` can see
`//pkg:foo_dependency`, we see that `//pkg:foo_dependency` was also passed as an
input to the call to `some_library` inside of `my_macro`, and instead check the
dependency's visibility against the location of this call, `//macro`.

This process can repeat recursively, as long as a target or macro declaration is
inside of another symbolic macro taking the dependency's label in one of its
label-typed attributes.

Note: Visibility delegation does not work for labels that were not passed into
the macro, such as labels derived by string manipulation.

#### Finalizers

Targets declared in a rule finalizer (a symbolic macro with `finalizer = True`),
in addition to seeing targets following the usual symbolic macro visibility
rules, can *also* see all targets which are visible to the finalizer target's
package.

In other words, if you migrate a `native.existing_rules()`-based legacy macro to
a finalizer, the targets declared by the finalizer will still be able to see
their old dependencies.

It is possible to define targets that a finalizer can introspect using
`native.existing_rules()`, but which it cannot use as dependencies under the
visibility system. For example, if a macro-defined target is not visible to its
own package or to the finalizer macro's definition, and is not delegated to the
finalizer, the finalizer cannot see such a target. Note, however, that a
`native.existing_rules()`-based legacy macro will also be unable to see such a
target.

## Load visibility

**Load visibility** controls whether a `.bzl` file may be loaded from other
`BUILD` or `.bzl` files outside the current package.

In the same way that target visibility protects source code that is encapsulated
by targets, load visibility protects build logic that is encapsulated by `.bzl`
files. For instance, a `BUILD` file author might wish to factor some repetitive
target declarations into a macro in a `.bzl` file. Without the protection of
load visibility, they might find their macro reused by other collaborators in
the same workspace, so that modifying the macro breaks other teams' builds.

Note that a `.bzl` file may or may not have a corresponding source file target.
If it does, there is no guarantee that the load visibility and the target
visibility coincide. That is, the same `BUILD` file might be able to load the
`.bzl` file but not list it in the `srcs` of a [`filegroup`](/reference/be/general#filegroup),
or vice versa. This can sometimes cause problems for rules that wish to consume
`.bzl` files as source code, such as for documentation generation or testing.

For prototyping, you may disable load visibility enforcement by setting
`--check_bzl_visibility=false`. As with `--check_visibility=false`, this should
not be done for submitted code.

Load visibility is available as of Bazel 6.0.

### Declaring load visibility

To set the load visibility of a `.bzl` file, call the
[`visibility()`](/rules/lib/globals/bzl#visibility) function from within the file.
The argument to `visibility()` is a list of package specifications, just like
the [`packages`](/reference/be/functions#package_group.packages) attribute of
`package_group`. However, `visibility()` does not accept negative package
specifications.

The call to `visibility()` must only occur once per file, at the top level (not
inside a function), and ideally immediately following the `load()` statements.

Unlike target visibility, the default load visibility is always public. Files
that do not call `visibility()` are always loadable from anywhere in the
workspace. It is a good idea to add `visibility("private")` to the top of any
new `.bzl` file that is not specifically intended for use outside the package.

### Example

```starlark
# //mylib/internal_defs.bzl

# Available to subpackages and to mylib's tests.
visibility(["//mylib/...", "//tests/mylib/..."])

def helper(...):
    ...
```

```starlark
# //mylib/rules.bzl

load(":internal_defs.bzl", "helper")
# Set visibility explicitly, even though public is the default.
# Note the [] can be omitted when there's only one entry.
visibility("public")

myrule = rule(
    ...
)
```

```starlark
# //someclient/BUILD

load("//mylib:rules.bzl", "myrule")          # ok
load("//mylib:internal_defs.bzl", "helper")  # error

...
```

### Load visibility practices

This section describes tips for managing load visibility declarations.

#### Factoring visibilities

When multiple `.bzl` files should have the same visibility, it can be helpful to
factor their package specifications into a common list. For example:

```starlark
# //mylib/internal_defs.bzl

visibility("private")

clients = [
    "//foo",
    "//bar/baz/...",
    ...
]
```

```starlark
# //mylib/feature_A.bzl

load(":internal_defs.bzl", "clients")
visibility(clients)

...
```

```starlark
# //mylib/feature_B.bzl

load(":internal_defs.bzl", "clients")
visibility(clients)

...
```

This helps prevent accidental skew between the various `.bzl` files'
visibilities. It also is more readable when the `clients` list is large.

#### Composing visibilities

Sometimes a `.bzl` file might need to be visible to an allowlist that is
composed of multiple smaller allowlists. This is analogous to how a
`package_group` can incorporate other `package_group`s via its
[`includes`](/reference/be/functions#package_group.includes) attribute.

Suppose you are deprecating a widely used macro. You want it to be visible only
to existing users and to the packages owned by your own team. You might write:

```starlark
# //mylib/macros.bzl

load(":internal_defs.bzl", "our_packages")
load("//some_big_client:defs.bzl", "their_remaining_uses")

# List concatenation. Duplicates are fine.
visibility(our_packages + their_remaining_uses)
```

#### Deduplicating with package groups

Unlike target visibility, you cannot define a load visibility in terms of a
`package_group`. If you want to reuse the same allowlist for both target
visibility and load visibility, it's best to move the list of package
specifications into a .bzl file, where both kinds of declarations may refer to
it. Building off the example in [Factoring visibilities](#factoring-visibilities)
above, you might write:

```starlark
# //mylib/BUILD

load(":internal_defs", "clients")

package_group(
    name = "my_pkg_grp",
    packages = clients,
)
```

This only works if the list does not contain any negative package
specifications.

#### Protecting individual symbols

Any Starlark symbol whose name begins with an underscore cannot be loaded from
another file. This makes it easy to create private symbols, but does not allow
you to share these symbols with a limited set of trusted files. On the other
hand, load visibility gives you control over what other packages may see your
`.bzl file`, but does not allow you to prevent any non-underscored symbol from
being loaded.

Luckily, you can combine these two features to get fine-grained control.

```starlark
# //mylib/internal_defs.bzl

# Can't be public, because internal_helper shouldn't be exposed to the world.
visibility("private")

# Can't be underscore-prefixed, because this is
# needed by other .bzl files in mylib.
def internal_helper(...):
    ...

def public_util(...):
    ...
```

```starlark
# //mylib/defs.bzl

load(":internal_defs", "internal_helper", _public_util="public_util")
visibility("public")

# internal_helper, as a loaded symbol, is available for use in this file but
# can't be imported by clients who load this file.
...

# Re-export public_util from this file by assigning it to a global variable.
# We needed to import it under a different name ("_public_util") in order for
# this assignment to be legal.
public_util = _public_util
```

#### bzl-visibility Buildifier lint

There is a [Buildifier lint](https://github.com/bazelbuild/buildtools/blob/master/WARNINGS.md#bzl-visibility)
that provides a warning if users load a file from a directory named `internal`
or `private`, when the user's file is not itself underneath the parent of that
directory. This lint predates the load visibility feature and is unnecessary in
workspaces where `.bzl` files declare visibilities.

---

## Configurable Build Attributes
- URL: https://bazel.build/configure/attributes
- Source: configure/attributes.mdx
- Slug: /configure/attributes

**_Configurable attributes_**, commonly known as [`select()`](
/reference/be/functions#select), is a Bazel feature that lets users toggle the values
of build rule attributes at the command line.

This can be used, for example, for a multiplatform library that automatically
chooses the appropriate implementation for the architecture, or for a
feature-configurable binary that can be customized at build time.

## Example

```python
# myapp/BUILD

cc_binary(
    name = "mybinary",
    srcs = ["main.cc"],
    deps = select({
        ":arm_build": [":arm_lib"],
        ":x86_debug_build": [":x86_dev_lib"],
        "//conditions:default": [":generic_lib"],
    }),
)

config_setting(
    name = "arm_build",
    values = {"cpu": "arm"},
)

config_setting(
    name = "x86_debug_build",
    values = {
        "cpu": "x86",
        "compilation_mode": "dbg",
    },
)
```

This declares a `cc_binary` that "chooses" its deps based on the flags at the
command line. Specifically, `deps` becomes:

<table>
  <tr style="background: #E9E9E9; font-weight: bold">
    <td>Command</td>
    <td>deps =</td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary --cpu=arm</code></td>
    <td><code>[":arm_lib"]</code></td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary -c dbg --cpu=x86</code></td>
    <td><code>[":x86_dev_lib"]</code></td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary --cpu=ppc</code></td>
    <td><code>[":generic_lib"]</code></td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary -c dbg --cpu=ppc</code></td>
    <td><code>[":generic_lib"]</code></td>
  </tr>
</table>

`select()` serves as a placeholder for a value that will be chosen based on
*configuration conditions*, which are labels referencing [`config_setting`](/reference/be/general#config_setting)
targets. By using `select()` in a configurable attribute, the attribute
effectively adopts different values when different conditions hold.

Matches must be unambiguous: if multiple conditions match then either
*  They all resolve to the same value. For example, when running on linux x86, this is unambiguous
   `{"@platforms//os:linux": "Hello", "@platforms//cpu:x86_64": "Hello"}` because both branches resolve to "hello".
*  One's `values` is a strict superset of all others'. For example, `values = {"cpu": "x86", "compilation_mode": "dbg"}`
   is an unambiguous specialization of `values = {"cpu": "x86"}`.

The built-in condition [`//conditions:default`](#default-condition) automatically matches when
nothing else does.

While this example uses `deps`, `select()` works just as well on `srcs`,
`resources`, `cmd`, and most other attributes. Only a small number of attributes
are *non-configurable*, and these are clearly annotated. For example,
`config_setting`'s own
[`values`](/reference/be/general#config_setting.values) attribute is non-configurable.

## `select()` and dependencies

Certain attributes change the build parameters for all transitive dependencies
under a target. For example, `genrule`'s `tools` changes `--cpu` to the CPU of
the machine running Bazel (which, thanks to cross-compilation, may be different
than the CPU the target is built for). This is known as a
[configuration transition](/reference/glossary#transition).

Given

```python
#myapp/BUILD

config_setting(
    name = "arm_cpu",
    values = {"cpu": "arm"},
)

config_setting(
    name = "x86_cpu",
    values = {"cpu": "x86"},
)

genrule(
    name = "my_genrule",
    srcs = select({
        ":arm_cpu": ["g_arm.src"],
        ":x86_cpu": ["g_x86.src"],
    }),
    tools = select({
        ":arm_cpu": [":tool1"],
        ":x86_cpu": [":tool2"],
    }),
)

cc_binary(
    name = "tool1",
    srcs = select({
        ":arm_cpu": ["armtool.cc"],
        ":x86_cpu": ["x86tool.cc"],
    }),
)
```

running

```sh
$ bazel build //myapp:my_genrule --cpu=arm
```

on an `x86` developer machine binds the build to `g_arm.src`, `tool1`, and
`x86tool.cc`. Both of the `select`s attached to `my_genrule` use `my_genrule`'s
build parameters, which include `--cpu=arm`. The `tools` attribute changes
`--cpu` to `x86` for `tool1` and its transitive dependencies. The `select` on
`tool1` uses `tool1`'s build parameters, which include `--cpu=x86`.

## Configuration conditions

Each key in a configurable attribute is a label reference to a
[`config_setting`](/reference/be/general#config_setting) or
[`constraint_value`](/reference/be/platforms-and-toolchains#constraint_value).

`config_setting` is just a collection of
expected command line flag settings. By encapsulating these in a target, it's
easy to maintain "standard" conditions users can reference from multiple places.

`constraint_value` provides support for [multi-platform behavior](#platforms).

### Built-in flags

Flags like `--cpu` are built into Bazel: the build tool natively understands
them for all builds in all projects. These are specified with
[`config_setting`](/reference/be/general#config_setting)'s
[`values`](/reference/be/general#config_setting.values) attribute:

```python
config_setting(
    name = "meaningful_condition_name",
    values = {
        "flag1": "value1",
        "flag2": "value2",
        ...
    },
)
```

`flagN` is a flag name (without `--`, so `"cpu"` instead of `"--cpu"`). `valueN`
is the expected value for that flag. `:meaningful_condition_name` matches if
*every* entry in `values` matches. Order is irrelevant.

`valueN` is parsed as if it was set on the command line. This means:

*  `values = { "compilation_mode": "opt" }` matches `bazel build -c opt`
*  `values = { "force_pic": "true" }` matches `bazel build --force_pic=1`
*  `values = { "force_pic": "0" }` matches `bazel build --noforce_pic`

`config_setting` only supports flags that affect target behavior. For example,
[`--show_progress`](/docs/user-manual#show-progress) isn't allowed because
it only affects how Bazel reports progress to the user. Targets can't use that
flag to construct their results. The exact set of supported flags isn't
documented. In practice, most flags that "make sense" work.

### Custom flags

You can model your own project-specific flags with
[Starlark build settings][BuildSettings]. Unlike built-in flags, these are
defined as build targets, so Bazel references them with target labels.

These are triggered with [`config_setting`](/reference/be/general#config_setting)'s
[`flag_values`](/reference/be/general#config_setting.flag_values)
attribute:

```python
config_setting(
    name = "meaningful_condition_name",
    flag_values = {
        "//myflags:flag1": "value1",
        "//myflags:flag2": "value2",
        ...
    },
)
```

Behavior is the same as for [built-in flags](#built-in-flags). See [here](https://github.com/bazelbuild/examples/tree/HEAD/configurations/select_on_build_setting)
for a working example.

[`--define`](/reference/command-line-reference#flag--define)
is an alternative legacy syntax for custom flags (for example
`--define foo=bar`). This can be expressed either in the
[values](/reference/be/general#config_setting.values) attribute
(`values = {"define": "foo=bar"}`) or the
[define_values](/reference/be/general#config_setting.define_values) attribute
(`define_values = {"foo": "bar"}`). `--define` is only supported for backwards
compatibility. Prefer Starlark build settings whenever possible.

`values`, `flag_values`, and `define_values` evaluate independently. The
`config_setting` matches if all values across all of them match.

## The default condition

The built-in condition `//conditions:default` matches when no other condition
matches.

Because of the "exactly one match" rule, a configurable attribute with no match
and no default condition emits a `"no matching conditions"` error. This can
protect against silent failures from unexpected settings:

```python
# myapp/BUILD

config_setting(
    name = "x86_cpu",
    values = {"cpu": "x86"},
)

cc_library(
    name = "x86_only_lib",
    srcs = select({
        ":x86_cpu": ["lib.cc"],
    }),
)
```

```sh
$ bazel build //myapp:x86_only_lib --cpu=arm
ERROR: Configurable attribute "srcs" doesn't match this configuration (would
a default condition help?).
Conditions checked:
  //myapp:x86_cpu
```

For even clearer errors, you can set custom messages with `select()`'s
[`no_match_error`](#custom-error-messages) attribute.

## Platforms

While the ability to specify multiple flags on the command line provides
flexibility, it can also be burdensome to individually set each one every time
you want to build a target.
   [Platforms](/extending/platforms)
let you consolidate these into simple bundles.

```python
# myapp/BUILD

sh_binary(
    name = "my_rocks",
    srcs = select({
        ":basalt": ["pyroxene.sh"],
        ":marble": ["calcite.sh"],
        "//conditions:default": ["feldspar.sh"],
    }),
)

config_setting(
    name = "basalt",
    constraint_values = [
        ":black",
        ":igneous",
    ],
)

config_setting(
    name = "marble",
    constraint_values = [
        ":white",
        ":metamorphic",
    ],
)

# constraint_setting acts as an enum type, and constraint_value as an enum value.
constraint_setting(name = "color")
constraint_value(name = "black", constraint_setting = "color")
constraint_value(name = "white", constraint_setting = "color")
constraint_setting(name = "texture")
constraint_value(name = "smooth", constraint_setting = "texture")
constraint_setting(name = "type")
constraint_value(name = "igneous", constraint_setting = "type")
constraint_value(name = "metamorphic", constraint_setting = "type")

platform(
    name = "basalt_platform",
    constraint_values = [
        ":black",
        ":igneous",
    ],
)

platform(
    name = "marble_platform",
    constraint_values = [
        ":white",
        ":smooth",
        ":metamorphic",
    ],
)
```

The platform can be specified on the command line. It activates the
`config_setting`s that contain a subset of the platform's `constraint_values`,
allowing those `config_setting`s to match in `select()` expressions.

For example, in order to set the `srcs` attribute of `my_rocks` to `calcite.sh`,
you can simply run

```sh
bazel build //my_app:my_rocks --platforms=//myapp:marble_platform
```

Without platforms, this might look something like

```sh
bazel build //my_app:my_rocks --define color=white --define texture=smooth --define type=metamorphic
```

`select()` can also directly read `constraint_value`s:

```python
constraint_setting(name = "type")
constraint_value(name = "igneous", constraint_setting = "type")
constraint_value(name = "metamorphic", constraint_setting = "type")
sh_binary(
    name = "my_rocks",
    srcs = select({
        ":igneous": ["igneous.sh"],
        ":metamorphic" ["metamorphic.sh"],
    }),
)
```

This saves the need for boilerplate `config_setting`s when you only need to
check against single values.

Platforms are still under development. See the
[documentation](/concepts/platforms) for details.

## Combining `select()`s

`select` can appear multiple times in the same attribute:

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"] +
           select({
               ":armeabi_mode": ["armeabi_src.sh"],
               ":x86_mode": ["x86_src.sh"],
           }) +
           select({
               ":opt_mode": ["opt_extras.sh"],
               ":dbg_mode": ["dbg_extras.sh"],
           }),
)
```

Note: Some restrictions apply on what can be combined in the `select`s values:
 - Duplicate labels can appear in different paths of the same `select`.
 - Duplicate labels can *not* appear within the same path of a `select`.
 - Duplicate labels can *not* appear across multiple combined `select`s (no matter what path)

`select` cannot appear inside another `select`. If you need to nest `selects`
and your attribute takes other targets as values, use an intermediate target:

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":armeabi_mode": [":armeabi_lib"],
        ...
    }),
)

sh_library(
    name = "armeabi_lib",
    srcs = select({
        ":opt_mode": ["armeabi_with_opt.sh"],
        ...
    }),
)
```

If you need a `select` to match when multiple conditions match, consider [AND
chaining](#and-chaining).

## OR chaining

Consider the following:

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1": [":standard_lib"],
        ":config2": [":standard_lib"],
        ":config3": [":standard_lib"],
        ":config4": [":special_lib"],
    }),
)
```

Most conditions evaluate to the same dep. But this syntax is hard to read and
maintain. It would be nice to not have to repeat `[":standard_lib"]` multiple
times.

One option is to predefine the value as a BUILD variable:

```python
STANDARD_DEP = [":standard_lib"]

sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1": STANDARD_DEP,
        ":config2": STANDARD_DEP,
        ":config3": STANDARD_DEP,
        ":config4": [":special_lib"],
    }),
)
```

This makes it easier to manage the dependency. But it still causes unnecessary
duplication.

For more direct support, use one of the following:

### `selects.with_or`

The
[with_or](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md#selectswith_or)
macro in [Skylib](https://github.com/bazelbuild/bazel-skylib)'s
[`selects`](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md)
module supports `OR`ing conditions directly inside a `select`:

```python
load("@bazel_skylib//lib:selects.bzl", "selects")
```

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = selects.with_or({
        (":config1", ":config2", ":config3"): [":standard_lib"],
        ":config4": [":special_lib"],
    }),
)
```

### `selects.config_setting_group`


The
[config_setting_group](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md#selectsconfig_setting_group)
macro in [Skylib](https://github.com/bazelbuild/bazel-skylib)'s
[`selects`](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md)
module supports `OR`ing multiple `config_setting`s:

```python
load("@bazel_skylib//lib:selects.bzl", "selects")
```


```python
config_setting(
    name = "config1",
    values = {"cpu": "arm"},
)
config_setting(
    name = "config2",
    values = {"compilation_mode": "dbg"},
)
selects.config_setting_group(
    name = "config1_or_2",
    match_any = [":config1", ":config2"],
)
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1_or_2": [":standard_lib"],
        "//conditions:default": [":other_lib"],
    }),
)
```

Unlike `selects.with_or`, different targets can share `:config1_or_2` across
different attributes.

It's an error for multiple conditions to match unless one is an unambiguous
"specialization" of the others or they all resolve to the same value. See [here](#configurable-build-example) for details.

## AND chaining

If you need a `select` branch to match when multiple conditions match, use the
[Skylib](https://github.com/bazelbuild/bazel-skylib) macro
[config_setting_group](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md#selectsconfig_setting_group):

```python
config_setting(
    name = "config1",
    values = {"cpu": "arm"},
)
config_setting(
    name = "config2",
    values = {"compilation_mode": "dbg"},
)
selects.config_setting_group(
    name = "config1_and_2",
    match_all = [":config1", ":config2"],
)
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1_and_2": [":standard_lib"],
        "//conditions:default": [":other_lib"],
    }),
)
```

Unlike OR chaining, existing `config_setting`s can't be directly `AND`ed
inside a `select`. You have to explicitly wrap them in a `config_setting_group`.

## Custom error messages

By default, when no condition matches, the target the `select()` is attached to
fails with the error:

```sh
ERROR: Configurable attribute "deps" doesn't match this configuration (would
a default condition help?).
Conditions checked:
  //tools/cc_target_os:darwin
  //tools/cc_target_os:android
```

This can be customized with the [`no_match_error`](/reference/be/functions#select)
attribute:

```python
cc_library(
    name = "my_lib",
    deps = select(
        {
            "//tools/cc_target_os:android": [":android_deps"],
            "//tools/cc_target_os:windows": [":windows_deps"],
        },
        no_match_error = "Please build with an Android or Windows toolchain",
    ),
)
```

```sh
$ bazel build //myapp:my_lib
ERROR: Configurable attribute "deps" doesn't match this configuration: Please
build with an Android or Windows toolchain
```

## Rules compatibility

Rule implementations receive the *resolved values* of configurable
attributes. For example, given:

```python
# myapp/BUILD

some_rule(
    name = "my_target",
    some_attr = select({
        ":foo_mode": [":foo"],
        ":bar_mode": [":bar"],
    }),
)
```

```sh
$ bazel build //myapp/my_target --define mode=foo
```

Rule implementation code sees `ctx.attr.some_attr` as `[":foo"]`.

Macros can accept `select()` clauses and pass them through to native
rules. But *they cannot directly manipulate them*. For example, there's no way
for a macro to convert

```python
select({"foo": "val"}, ...)
```

to

```python
select({"foo": "val_with_suffix"}, ...)
```

This is for two reasons.

First, macros that need to know which path a `select` will choose *cannot work*
because macros are evaluated in Bazel's [loading phase](/run/build#loading),
which occurs before flag values are known.
This is a core Bazel design restriction that's unlikely to change any time soon.

Second, macros that just need to iterate over *all* `select` paths, while
technically feasible, lack a coherent UI. Further design is necessary to change
this.

## Bazel query and cquery

Bazel [`query`](/query/guide) operates over Bazel's
[loading phase](/reference/glossary#loading-phase).
This means it doesn't know what command line flags a target uses since those
flags aren't evaluated until later in the build (in the
[analysis phase](/reference/glossary#analysis-phase)).
So it can't determine which `select()` branches are chosen.

Bazel [`cquery`](/query/cquery) operates after Bazel's analysis phase, so it has
all this information and can accurately resolve `select()`s.

Consider:

```python
load("@bazel_skylib//rules:common_settings.bzl", "string_flag")
```
```python
# myapp/BUILD

string_flag(
    name = "dog_type",
    build_setting_default = "cat"
)

cc_library(
    name = "my_lib",
    deps = select({
        ":long": [":foo_dep"],
        ":short": [":bar_dep"],
    }),
)

config_setting(
    name = "long",
    flag_values = {":dog_type": "dachshund"},
)

config_setting(
    name = "short",
    flag_values = {":dog_type": "pug"},
)
```

`query` overapproximates `:my_lib`'s dependencies:

```sh
$ bazel query 'deps(//myapp:my_lib)'
//myapp:my_lib
//myapp:foo_dep
//myapp:bar_dep
```

while `cquery` shows its exact dependencies:

```sh
$ bazel cquery 'deps(//myapp:my_lib)' --//myapp:dog_type=pug
//myapp:my_lib
//myapp:bar_dep
```

## FAQ

### Why doesn't select() work in macros?

select() *does* work in rules! See [Rules compatibility](#rules-compatibility) for
details.

The key issue this question usually means is that select() doesn't work in
*macros*. These are different than *rules*. See the
documentation on [rules](/extending/rules) and [macros](/extending/macros)
to understand the difference.
Here's an end-to-end example:

Define a rule and macro:

```python
# myapp/defs.bzl

# Rule implementation: when an attribute is read, all select()s have already
# been resolved. So it looks like a plain old attribute just like any other.
def _impl(ctx):
    name = ctx.attr.name
    allcaps = ctx.attr.my_config_string.upper()  # This works fine on all values.
    print("My name is " + name + " with custom message: " + allcaps)

# Rule declaration:
my_custom_bazel_rule = rule(
    implementation = _impl,
    attrs = {"my_config_string": attr.string()},
)

# Macro declaration:
def my_custom_bazel_macro(name, my_config_string):
    allcaps = my_config_string.upper()  # This line won't work with select(s).
    print("My name is " + name + " with custom message: " + allcaps)
```

Instantiate the rule and macro:

```python
# myapp/BUILD

load("//myapp:defs.bzl", "my_custom_bazel_rule")
load("//myapp:defs.bzl", "my_custom_bazel_macro")

my_custom_bazel_rule(
    name = "happy_rule",
    my_config_string = select({
        "//third_party/bazel_platforms/cpu:x86_32": "first string",
        "//third_party/bazel_platforms/cpu:ppc": "second string",
    }),
)

my_custom_bazel_macro(
    name = "happy_macro",
    my_config_string = "fixed string",
)

my_custom_bazel_macro(
    name = "sad_macro",
    my_config_string = select({
        "//third_party/bazel_platforms/cpu:x86_32": "first string",
        "//third_party/bazel_platforms/cpu:ppc": "other string",
    }),
)
```

Building fails because `sad_macro` can't process the `select()`:

```sh
$ bazel build //myapp:all
ERROR: /myworkspace/myapp/BUILD:17:1: Traceback
  (most recent call last):
File "/myworkspace/myapp/BUILD", line 17
my_custom_bazel_macro(name = "sad_macro", my_config_stri..."}))
File "/myworkspace/myapp/defs.bzl", line 4, in
  my_custom_bazel_macro
my_config_string.upper()
type 'select' has no method upper().
ERROR: error loading package 'myapp': Package 'myapp' contains errors.
```

Building succeeds when you comment out `sad_macro`:

```sh
# Comment out sad_macro so it doesn't mess up the build.
$ bazel build //myapp:all
DEBUG: /myworkspace/myapp/defs.bzl:5:3: My name is happy_macro with custom message: FIXED STRING.
DEBUG: /myworkspace/myapp/hi.bzl:15:3: My name is happy_rule with custom message: FIRST STRING.
```

This is impossible to change because *by definition* macros are evaluated before
Bazel reads the build's command line flags. That means there isn't enough
information to evaluate select()s.

Macros can, however, pass `select()`s as opaque blobs to rules:

```python
# myapp/defs.bzl

def my_custom_bazel_macro(name, my_config_string):
    print("Invoking macro " + name)
    my_custom_bazel_rule(
        name = name + "_as_target",
        my_config_string = my_config_string,
    )
```

```sh
$ bazel build //myapp:sad_macro_less_sad
DEBUG: /myworkspace/myapp/defs.bzl:23:3: Invoking macro sad_macro_less_sad.
DEBUG: /myworkspace/myapp/defs.bzl:15:3: My name is sad_macro_less_sad with custom message: FIRST STRING.
```

### Why does select() always return true?

Because *macros* (but not rules) by definition
[can't evaluate `select()`s](#faq-select-macro), any attempt to do so
usually produces an error:

```sh
ERROR: /myworkspace/myapp/BUILD:17:1: Traceback
  (most recent call last):
File "/myworkspace/myapp/BUILD", line 17
my_custom_bazel_macro(name = "sad_macro", my_config_stri..."}))
File "/myworkspace/myapp/defs.bzl", line 4, in
  my_custom_bazel_macro
my_config_string.upper()
type 'select' has no method upper().
```

Booleans are a special case that fail silently, so you should be particularly
vigilant with them:

```sh
$ cat myapp/defs.bzl
def my_boolean_macro(boolval):
  print("TRUE" if boolval else "FALSE")

$ cat myapp/BUILD
load("//myapp:defs.bzl", "my_boolean_macro")
my_boolean_macro(
    boolval = select({
        "//third_party/bazel_platforms/cpu:x86_32": True,
        "//third_party/bazel_platforms/cpu:ppc": False,
    }),
)

$ bazel build //myapp:all --cpu=x86
DEBUG: /myworkspace/myapp/defs.bzl:4:3: TRUE.
$ bazel build //mypro:all --cpu=ppc
DEBUG: /myworkspace/myapp/defs.bzl:4:3: TRUE.
```

This happens because macros don't understand the contents of `select()`.
So what they're really evaluting is the `select()` object itself. According to
[Pythonic](https://docs.python.org/release/2.5.2/lib/truth.html) design
standards, all objects aside from a very small number of exceptions
automatically return true.

### Can I read select() like a dict?

Macros [can't](#faq-select-macro) evaluate select(s) because macros evaluate before
Bazel knows what the build's command line parameters are. Can they at least read
the `select()`'s dictionary to, for example, add a suffix to each value?

Conceptually this is possible, but it isn't yet a Bazel feature.
What you *can* do today is prepare a straight dictionary, then feed it into a
`select()`:

```sh
$ cat myapp/defs.bzl
def selecty_genrule(name, select_cmd):
  for key in select_cmd.keys():
    select_cmd[key] += " WITH SUFFIX"
  native.genrule(
      name = name,
      outs = [name + ".out"],
      srcs = [],
      cmd = "echo " + select(select_cmd + {"//conditions:default": "default"})
        + " > $@"
  )

$ cat myapp/BUILD
selecty_genrule(
    name = "selecty",
    select_cmd = {
        "//third_party/bazel_platforms/cpu:x86_32": "x86 mode",
    },
)

$ bazel build //testapp:selecty --cpu=x86 && cat bazel-genfiles/testapp/selecty.out
x86 mode WITH SUFFIX
```

If you'd like to support both `select()` and native types, you can do this:

```sh
$ cat myapp/defs.bzl
def selecty_genrule(name, select_cmd):
    cmd_suffix = ""
    if type(select_cmd) == "string":
        cmd_suffix = select_cmd + " WITH SUFFIX"
    elif type(select_cmd) == "dict":
        for key in select_cmd.keys():
            select_cmd[key] += " WITH SUFFIX"
        cmd_suffix = select(select_cmd + {"//conditions:default": "default"})

    native.genrule(
        name = name,
        outs = [name + ".out"],
        srcs = [],
        cmd = "echo " + cmd_suffix + "> $@",
    )
```

### Why doesn't select() work with bind()?

First of all, do not use `bind()`. It is deprecated in favor of `alias()`.

The technical answer is that [`bind()`](/reference/be/workspace#bind) is a repo
rule, not a BUILD rule.

Repo rules do not have a specific configuration, and aren't evaluated in
the same way as BUILD rules. Therefore, a `select()` in a `bind()` can't
actually evaluate to any specific branch.

Instead, you should use [`alias()`](/reference/be/general#alias), with a `select()` in
the `actual` attribute, to perform this type of run-time determination. This
works correctly, since `alias()` is a BUILD rule, and is evaluated with a
specific configuration.

```sh
$ cat WORKSPACE
workspace(name = "myapp")
bind(name = "openssl", actual = "//:ssl")
http_archive(name = "alternative", ...)
http_archive(name = "boringssl", ...)

$ cat BUILD
config_setting(
    name = "alt_ssl",
    define_values = {
        "ssl_library": "alternative",
    },
)

alias(
    name = "ssl",
    actual = select({
        "//:alt_ssl": "@alternative//:ssl",
        "//conditions:default": "@boringssl//:ssl",
    }),
)
```

With this setup, you can pass `--define ssl_library=alternative`, and any target
that depends on either `//:ssl` or `//external:ssl` will see the alternative
located at `@alternative//:ssl`.

But really, stop using `bind()`.

### Why doesn't my select() choose what I expect?

If `//myapp:foo` has a `select()` that doesn't choose the condition you expect,
use [cquery](/query/cquery) and `bazel config` to debug:

If `//myapp:foo` is the top-level target you're building, run:

```sh
$ bazel cquery //myapp:foo <desired build flags>
//myapp:foo (12e23b9a2b534a)
```

If you're building some other target `//bar` that depends on
//myapp:foo somewhere in its subgraph, run:

```sh
$ bazel cquery 'somepath(//bar, //myapp:foo)' <desired build flags>
//bar:bar   (3ag3193fee94a2)
//bar:intermediate_dep (12e23b9a2b534a)
//myapp:foo (12e23b9a2b534a)
```

The `(12e23b9a2b534a)` next to `//myapp:foo` is a *hash* of the
configuration that resolves `//myapp:foo`'s `select()`. You can inspect its
values with `bazel config`:

```sh
$ bazel config 12e23b9a2b534a
BuildConfigurationValue 12e23b9a2b534a
Fragment com.google.devtools.build.lib.analysis.config.CoreOptions {
  cpu: darwin
  compilation_mode: fastbuild
  ...
}
Fragment com.google.devtools.build.lib.rules.cpp.CppOptions {
  linkopt: [-Dfoo=bar]
  ...
}
...
```

Then compare this output against the settings expected by each `config_setting`.

`//myapp:foo` may exist in different configurations in the same build. See the
[cquery docs](/query/cquery) for guidance on using `somepath` to get the right
one.

Caution: To prevent restarting the Bazel server, invoke `bazel config` with the
same command line flags as the `bazel cquery`. The `config` command relies on
the configuration nodes from the still-running server of the previous command.

### Why doesn't `select()` work with platforms?

Bazel doesn't support configurable attributes checking whether a given platform
is the target platform because the semantics are unclear.

For example:

```py
platform(
    name = "x86_linux_platform",
    constraint_values = [
        "@platforms//cpu:x86",
        "@platforms//os:linux",
    ],
)

cc_library(
    name = "lib",
    srcs = [...],
    linkopts = select({
        ":x86_linux_platform": ["--enable_x86_optimizations"],
        "//conditions:default": [],
    }),
)
```

In this `BUILD` file, which `select()` should be used if the target platform has both the
`@platforms//cpu:x86` and `@platforms//os:linux` constraints, but is **not** the
`:x86_linux_platform` defined here? The author of the `BUILD` file and the user
who defined the separate platform may have different ideas.

#### What should I do instead?

Instead, define a `config_setting` that matches **any** platform with
these constraints:

```py
config_setting(
    name = "is_x86_linux",
    constraint_values = [
        "@platforms//cpu:x86",
        "@platforms//os:linux",
    ],
)

cc_library(
    name = "lib",
    srcs = [...],
    linkopts = select({
        ":is_x86_linux": ["--enable_x86_optimizations"],
        "//conditions:default": [],
    }),
)
```

This process defines specific semantics, making it clearer to users what
platforms meet the desired conditions.

#### What if I really, really want to `select` on the platform?

If your build requirements specifically require checking the platform, you
can flip the value of the `--platforms` flag in a `config_setting`:

```py
config_setting(
    name = "is_specific_x86_linux_platform",
    values = {
        "platforms": ["//package:x86_linux_platform"],
    },
)

cc_library(
    name = "lib",
    srcs = [...],
    linkopts = select({
        ":is_specific_x86_linux_platform": ["--enable_x86_optimizations"],
        "//conditions:default": [],
    }),
)
```

The Bazel team doesn't endorse doing this; it overly constrains your build and
confuses users when the expected condition does not match.

[BuildSettings]: /extending/config#user-defined-build-settings

---

## Best Practices
- URL: https://bazel.build/configure/best-practices
- Source: configure/best-practices.mdx
- Slug: /configure/best-practices

This page assumes you are familiar with Bazel and provides guidelines and
advice on structuring your projects to take full advantage of Bazel's features.

The overall goals are:

- To use fine-grained dependencies to allow parallelism and incrementality.
- To keep dependencies well-encapsulated.
- To make code well-structured and testable.
- To create a build configuration that is easy to understand and maintain.

These guidelines are not requirements: few projects will be able to adhere to
all of them.  As the man page for lint says, "A special reward will be presented
to the first person to produce a real program that produces no errors with
strict checking." However, incorporating as many of these principles as possible
should make a project more readable, less error-prone, and faster to build.

This page uses the requirement levels described in
[this RFC](https://www.ietf.org/rfc/rfc2119.txt).

## Running builds and tests

A project should always be able to run `bazel build //...` and
`bazel test //...` successfully on its stable branch. Targets that are necessary
but do not build under certain circumstances (such as,require specific build
flags, don't build on a certain platform, require license agreements) should be
tagged as specifically as possible (for example, "`requires-osx`"). This
tagging allows targets to be filtered at a more fine-grained level than the
"manual" tag and allows someone inspecting the `BUILD` file to understand what
a target's restrictions are.

## Third-party dependencies

You may declare third-party dependencies:

*   Either declare them as remote repositories in the `MODULE.bazel` file.
*   Or put them in a directory called `third_party/` under your workspace directory.

## Depending on binaries

Everything should be built from source whenever possible. Generally this means
that, instead of depending on a library `some-library.so`, you'd create a
`BUILD` file and build `some-library.so` from its sources, then depend on that
target.

Always building from source ensures that a build is not using a library that
was built with incompatible flags or a different architecture. There are also
some features like coverage, static analysis, or dynamic analysis that only
work on the source.

## Versioning

Prefer building all code from head whenever possible. When versions must be
used, avoid including the version in the target name (for example, `//guava`,
not `//guava-20.0`). This naming makes the library easier to update (only one
target needs to be updated). It's also more resilient to diamond dependency
issues: if one library depends on `guava-19.0` and one depends on `guava-20.0`,
you could end up with a library that tries to depend on two different versions.
If you created a misleading alias to point both targets to one `guava` library,
then the `BUILD` files are misleading.

## Using the `.bazelrc` file

For project-specific options, use the configuration file your
`<var>workspace</var>/.bazelrc` (see [bazelrc format](/run/bazelrc)).

If you want to support per-user options for your project that you **do not**
want to check into source control, include the line:

```
try-import %workspace%/user.bazelrc
```
(or any other file name) in your `<var>workspace</var>/.bazelrc`
and add `user.bazelrc` to your `.gitignore`.

The open-source
[bazelrc-preset.bzl](https://github.com/bazel-contrib/bazelrc-preset.bzl)

generates a custom bazelrc file that matches your Bazel version and provides a
preset of recommended flags.

## Packages

Every directory that contains buildable files should be a package. If a `BUILD`
file refers to files in subdirectories (such as, `srcs = ["a/b/C.java"]`) it's
a sign that a `BUILD` file should be added to that subdirectory. The longer
this structure exists, the more likely circular dependencies will be
inadvertently created, a target's scope will creep, and an increasing number
of reverse dependencies will have to be updated.

---

## Code coverage with Bazel
- URL: https://bazel.build/configure/coverage
- Source: configure/coverage.mdx
- Slug: /configure/coverage

Bazel features a `coverage` sub-command to produce code coverage
reports on repositories that can be tested with `bazel coverage`. Due
to the idiosyncrasies of the various language ecosystems, it is not
always trivial to make this work for a given project.

This page documents the general process for creating and viewing
coverage reports, and also features some language-specific notes for
languages whose configuration is well-known. It is best read by first
reading [the general section](#creating-a-coverage-report), and then
reading about the requirements for a specific language. Note also the
[remote execution section](#remote-execution), which requires some
additional considerations.

While a lot of customization is possible, this document focuses on
producing and consuming [`lcov`][lcov] reports, which is currently the
most well-supported route.

## Creating a coverage report

### Preparation

The basic workflow for creating coverage reports requires the
following:

- A basic repository with test targets
- A toolchain with the language-specific code coverage tools installed
- A correct "instrumentation" configuration

The former two are language-specific and mostly straightforward,
however the latter can be more difficult for complex projects.

"Instrumentation" in this case refers to the coverage tools that are
used for a specific target. Bazel allows turning this on for a
specific subset of files using the
[`--instrumentation_filter`](/reference/command-line-reference#flag--instrumentation_filter)
flag, which specifies a filter for targets that are tested with the
instrumentation enabled. To enable instrumentation for tests, the
[`--instrument_test_targets`](/reference/command-line-reference#flag--instrument_test_targets)
flag is required.

By default, bazel tries to match the target package(s), and prints the
relevant filter as an `INFO` message.

### Running coverage

To produce a coverage report, use [`bazel coverage
--combined_report=lcov
[target]`](/reference/command-line-reference#coverage). This runs the
tests for the target, generating coverage reports in the lcov format
for each file.

Once finished, bazel runs an action that collects all the produced
coverage files, and merges them into one, which is then finally
created under `$(bazel info
output_path)/_coverage/_coverage_report.dat`.

Coverage reports are also produced if tests fail, though note that
this does not extend to the failed tests - only passing tests are
reported.

### Viewing coverage

The coverage report is only output in the non-human-readable `lcov`
format. From this, we can use the `genhtml` utility (part of [the lcov
project][lcov]) to produce a report that can be viewed in a web
browser:

```console
genhtml --branch-coverage --output genhtml "$(bazel info output_path)/_coverage/_coverage_report.dat"
```

Note that `genhtml` reads the source code as well, to annotate missing
coverage in these files. For this to work, it is expected that
`genhtml` is executed in the root of the bazel project.

To view the result, simply open the `index.html` file produced in the
`genhtml` directory in any web browser.

For further help and information around the `genhtml` tool, or the
`lcov` coverage format, see [the lcov project][lcov].

## Remote execution

Running with remote test execution currently has a few caveats:

- The report combination action cannot yet run remotely. This is
  because Bazel does not consider the coverage output files as part of
  its graph (see [this issue][remote_report_issue]), and can therefore
  not correctly treat them as inputs to the combination action. To
  work around this, use `--strategy=CoverageReport=local`.
  - Note: It may be necessary to specify something like
    `--strategy=CoverageReport=local,remote` instead, if Bazel is set
    up to try `local,remote`, due to how Bazel resolves strategies.
- `--remote_download_minimal` and similar flags can also not be used
  as a consequence of the former.
- Bazel will currently fail to create coverage information if tests
  have been cached previously. To work around this,
  `--nocache_test_results` can be set specifically for coverage runs,
  although this of course incurs a heavy cost in terms of test times.
- `--experimental_split_coverage_postprocessing` and
  `--experimental_fetch_all_coverage_outputs`
  - Usually coverage is run as part of the test action, and so by
    default, we don't get all coverage back as outputs of the remote
    execution by default. These flags override the default and obtain
    the coverage data. See [this issue][split_coverage_issue] for more
    details.

## Language-specific configuration

### Java

Java should work out-of-the-box with the default configuration. The
[bazel toolchains][bazel_toolchains] contain everything necessary for
remote execution, as well, including JUnit.

### Python

See the [`rules_python` coverage docs](https://rules-python.readthedocs.io/en/latest/coverage.html)
for additional steps needed to enable coverage support in Python.

[lcov]: https://github.com/linux-test-project/lcov
[bazel_toolchains]: https://github.com/bazelbuild/bazel-toolchains
[remote_report_issue]: https://github.com/bazelbuild/bazel/issues/4685
[split_coverage_issue]: https://github.com/bazelbuild/bazel/issues/4685

---

## Using Bazel on Windows
- URL: https://bazel.build/configure/windows
- Source: configure/windows.mdx
- Slug: /configure/windows

This page covers Best Practices for using Bazel on Windows. For installation
instructions, see [Install Bazel on Windows](/install/windows).

## Known issues

Windows-related Bazel issues are marked with the "area-Windows" label on GitHub.
[GitHub-Windows].

[GitHub-Windows]: https://github.com/bazelbuild/bazel/issues?q=is%3Aopen+is%3Aissue+label%3Aarea-Windows

## Best practices

### Avoid long path issues

Some tools have the [Maximum Path Length Limitation](https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file#maximum-path-length-limitation) on Windows, including the MSVC compiler.
To avoid hitting this issue, you can specify a short output directory for Bazel by the [\-\-output_user_root](/reference/command-line-reference#flag--output_user_root) flag.

For example, add the following line to your bazelrc file:

```none
startup --output_user_root=C:/tmp
```

### Enable symlink support

Some features require Bazel to be able to create file symlinks on Windows,
either by enabling
[Developer Mode](https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development)
(on Windows 10 version 1703 or newer), or by running Bazel as an administrator.
This enables the following features:

* [\-\-windows_enable_symlinks](/reference/command-line-reference#flag--windows_enable_symlinks)
* [\-\-enable_runfiles](/reference/command-line-reference#flag--enable_runfiles)

To make it easier, add the following lines to your bazelrc file:

```none
startup --windows_enable_symlinks

build --enable_runfiles
```

**Note**: Creating symlinks on Windows is an expensive operation. The `--enable_runfiles` flag can potentially create a large amount of file symlinks. Only enable this feature when you need it.

{/* TODO(pcloudy): https://github.com/bazelbuild/bazel/issues/6402
                    Write a doc about runfiles library and add a link to it here */}

### Running Bazel: MSYS2 shell vs. command prompt vs. PowerShell

**Recommendation:** Run Bazel from the command prompt (`cmd.exe`) or from
PowerShell.

As of 2020-01-15, **do not** run Bazel from `bash` -- either
from MSYS2 shell, or Git Bash, or Cygwin, or any other Bash variant. While Bazel
may work for most use cases, some things are broken, like
[interrupting the build with Ctrl+C from MSYS2](https://github.com/bazelbuild/bazel/issues/10573)).
Also, if you choose to run under MSYS2, you need to disable MSYS2's
automatic path conversion, otherwise MSYS will convert command line arguments
that _look like_ Unix paths (such as `//foo:bar`) into Windows paths. See
[this StackOverflow answer](https://stackoverflow.com/a/49004265/7778502)
for details.

### Using Bazel without Bash (MSYS2)

#### Using bazel build without Bash

Bazel versions before 1.0 used to require Bash to build some rules.

Starting with Bazel 1.0, you can build any rule without Bash unless it is a:

- `genrule`, because genrules execute Bash commands
- `sh_binary` or `sh_test` rule, because these inherently need Bash
- Starlark rule that uses `ctx.actions.run_shell()` or `ctx.resolve_command()`

However, `genrule` is often used for simple tasks like
[copying a file](https://github.com/bazelbuild/bazel-skylib/blob/main/rules/copy_file.bzl)
or [writing a text file](https://github.com/bazelbuild/bazel-skylib/blob/main/rules/write_file.bzl).
Instead of using `genrule` (and depending on Bash) you may find a suitable rule
in the
[bazel-skylib repository](https://github.com/bazelbuild/bazel-skylib/tree/main/rules).
When built on Windows, **these rules do not require Bash**.

#### Using bazel test without Bash

Bazel versions before 1.0 used to require Bash to `bazel test` anything.

Starting with Bazel 1.0, you can test any rule without Bash, except when:

- you use `--run_under`
- the test rule itself requires Bash (because its executable is a shell script)

#### Using bazel run without Bash

Bazel versions before 1.0 used to require Bash to `bazel run` anything.

Starting with Bazel 1.0, you can run any rule without Bash, except when:

- you use `--run_under` or `--script_path`
- the test rule itself requires Bash (because its executable is a shell script)

#### Using sh\_binary and sh\_* rules, and ctx.actions.run_shell() without Bash

You need Bash to build and test `sh_*` rules, and to build and test Starlark
rules that use `ctx.actions.run_shell()` and `ctx.resolve_command()`. This
applies not only to rules in your project, but to rules in any of the external
repositories your project depends on (even transitively).

In the future, there may be an option to use Windows Subsystem for
Linux (WSL) to build these rules, but currently it is not a priority for
the Bazel-on-Windows subteam.

### Setting environment variables

Environment variables you set in the Windows Command Prompt (`cmd.exe`) are only
set in that command prompt session. If you start a new `cmd.exe`, you need to
set the variables again. To always set the variables when `cmd.exe` starts, you
can add them to the User variables or System variables in the `Control Panel >
System Properties > Advanced > Environment Variables...` dialog box.

## Build on Windows

### Build C++ with MSVC

To build C++ targets with MSVC, you need:

*   [The Visual C++ compiler](/install/windows#install-vc).

*   (Optional) The `BAZEL_VC` and `BAZEL_VC_FULL_VERSION` environment variable.

    Bazel automatically detects the Visual C++ compiler on your system.
    To tell Bazel to use a specific VC installation, you can set the
    following environment variables:

    For Visual Studio 2017 and 2019, set one of `BAZEL_VC`. Additionally you may also set `BAZEL_VC_FULL_VERSION`.

    *   `BAZEL_VC` the Visual C++ Build Tools installation directory

        ```
        set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC
        ```

    *   `BAZEL_VC_FULL_VERSION` (Optional) Only for Visual Studio 2017 and 2019, the full version
        number of your Visual C++ Build Tools. You can choose the exact Visual C++ Build Tools
        version via `BAZEL_VC_FULL_VERSION` if more than one version are installed, otherwise Bazel
        will choose the latest version.

        ```
        set BAZEL_VC_FULL_VERSION=14.16.27023
        ```

    For Visual Studio 2015 or older, set `BAZEL_VC`. (`BAZEL_VC_FULL_VERSION` is not supported.)

    *   `BAZEL_VC` the Visual C++ Build Tools installation directory

        ```
        set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC
        ```

*   The [Windows
    SDK](https://developer.microsoft.com/en-us/windows/downloads/windows-10-sdk).

    The Windows SDK contains header files and libraries you need when building
    Windows applications, including Bazel itself. By default, the latest Windows SDK installed will
    be used. You also can specify Windows SDK version by setting `BAZEL_WINSDK_FULL_VERSION`. You
    can use a full Windows 10 SDK number such as 10.0.10240.0, or specify 8.1 to use the Windows 8.1
    SDK (only one version of Windows 8.1 SDK is available). Please make sure you have the specified
    Windows SDK installed.

    **Requirement**: This is supported with VC 2017 and 2019. The standalone VC 2015 Build Tools doesn't
    support selecting Windows SDK, you'll need the full Visual Studio 2015 installation, otherwise
    `BAZEL_WINSDK_FULL_VERSION` will be ignored.

    ```
    set BAZEL_WINSDK_FULL_VERSION=10.0.10240.0
    ```

If everything is set up, you can build a C++ target now!

Try building a target from one of our [sample
projects](https://github.com/bazelbuild/bazel/tree/master/examples):

```
<code class="devsite-terminal"
            data-terminal-prefix="C:\projects\bazel> ">bazel build //examples/cpp:hello-world</code>
<code class="devsite-terminal"
            data-terminal-prefix="C:\projects\bazel> ">bazel-bin\examples\cpp\hello-world.exe</code>
```

By default, the built binaries target x64 architecture. To build for ARM64
architecture, use

```none
--platforms=//:windows_arm64  --extra_toolchains=@local_config_cc//:cc-toolchain-arm64_windows
```

You can introduce `@local_config_cc` in `MODULE.bazel` with

```python
bazel_dep(name = "rules_cc", version = "0.1.1")
cc_configure = use_extension("@rules_cc//cc:extensions.bzl", "cc_configure_extension")
use_repo(cc_configure, "local_config_cc")
```

To build and use Dynamically Linked Libraries (DLL files), see [this
example](https://github.com/bazelbuild/bazel/tree/master/examples/windows/dll).

**Command Line Length Limit**: To prevent the
[Windows command line length limit issue](https://github.com/bazelbuild/bazel/issues/5163),
enable the compiler parameter file feature via `--features=compiler_param_file`.

### Build C++ with Clang

From 0.29.0, Bazel supports building with LLVM's MSVC-compatible compiler driver (`clang-cl.exe`).

**Requirement**: To build with Clang, you have to install **both**
[LLVM](http://releases.llvm.org/download.html) and Visual C++ Build tools,
because although you use `clang-cl.exe` as compiler, you still need to link to
Visual C++ libraries.

Bazel can automatically detect LLVM installation on your system, or you can explicitly tell
Bazel where LLVM is installed by `BAZEL_LLVM`.

*   `BAZEL_LLVM` the LLVM installation directory

    ```posix-terminal
    set BAZEL_LLVM=C:\Program Files\LLVM
    ```

To enable the Clang toolchain for building C++, there are several situations.

* In Bazel 7.0.0 and newer: Add a platform target to your `BUILD file` (eg. the
  top level `BUILD` file):

    ```
    platform(
        name = "x64_windows-clang-cl",
        constraint_values = [
            "@platforms//cpu:x86_64",
            "@platforms//os:windows",
            "@bazel_tools//tools/cpp:clang-cl",
        ],
    )
    ```

    Then enable the Clang toolchain by specifying the following build flags:

    ```
    --extra_toolchains=@local_config_cc//:cc-toolchain-x64_windows-clang-cl --extra_execution_platforms=//:x64_windows-clang-cl
    ```

* In Bazel older than 7.0.0 but newer than 0.28: Enable the Clang toolchain by
  a build flag `--compiler=clang-cl`.

  If your build sets the flag
  [\-\-incompatible_enable_cc_toolchain_resolution]
  (https://github.com/bazelbuild/bazel/issues/7260)
  to `true`, then use the approach for Bazel 7.0.0.

* In Bazel 0.28 and older: Clang is not supported.

### Build Java

To build Java targets, you need:

*   [The Java SE Development Kit](/install/windows#install-jdk)

On Windows, Bazel builds two output files for `java_binary` rules:

*   a `.jar` file
*   a `.exe` file that can set up the environment for the JVM and run the binary

Try building a target from one of our [sample
projects](https://github.com/bazelbuild/bazel/tree/master/examples):

```
  <code class="devsite-terminal"
        data-terminal-prefix="C:\projects\bazel> ">bazel build //examples/java-native/src/main/java/com/example/<var>myproject</var>:hello-world</code>
  <code class="devsite-terminal"
        data-terminal-prefix="C:\projects\bazel> ">bazel-bin\examples\java-native\src\main\java\com\example\<var>myproject</var>\hello-world.exe</code>
```

### Build Python

To build Python targets, you need:

*   The [Python interpreter](/install/windows#install-python)

On Windows, Bazel builds two output files for `py_binary` rules:

*   a self-extracting zip file
*   an executable file that can launch the Python interpreter with the
    self-extracting zip file as the argument

You can either run the executable file (it has a `.exe` extension) or you can run
Python with the self-extracting zip file as the argument.

Try building a target from one of our [sample
projects](https://github.com/bazelbuild/bazel/tree/master/examples):

```
  <code class="devsite-terminal"
        data-terminal-prefix="C:\projects\bazel> ">bazel build //examples/py_native:bin</code>
  <code class="devsite-terminal"
        data-terminal-prefix="C:\projects\bazel> ">bazel-bin\examples\py_native\bin.exe</code>
  <code class="devsite-terminal"
        data-terminal-prefix="C:\projects\bazel> ">python bazel-bin\examples\py_native\bin.zip</code>
```

If you are interested in details about how Bazel builds Python targets on
Windows, check out this [design
doc](https://github.com/bazelbuild/bazel-website/blob/master/designs/_posts/2016-09-05-build-python-on-windows.md).

---

## Contributing to Bazel
- URL: https://bazel.build/contribute
- Source: contribute/index.mdx
- Slug: /contribute

There are many ways to help the Bazel project and ecosystem.

## Provide feedback

As you use Bazel, you may find things that can be improved.
You can help by [reporting issues](http://github.com/bazelbuild/bazel/issues)
when:

   - Bazel crashes or you encounter a bug that can [only be resolved using `bazel
     clean`](/run/build#correct-incremental-rebuilds).
   - The documentation is incomplete or unclear. You can also report issues
     from the page you are viewing by using the "Create issue"
     link at the top right corner of the page.
   - An error message could be improved.

## Participate in the community

You can engage with the Bazel community by:

   - Answering questions [on Stack Overflow](
     https://stackoverflow.com/questions/tagged/bazel).
   - Helping other users [on Slack](https://slack.bazel.build).
   - Improving documentation or [contributing examples](
     https://github.com/bazelbuild/examples).
   - Sharing your experience or your tips, for example, on a blog or social media.

## Contribute code

Bazel is a large project and making a change to the Bazel source code
can be difficult.

You can contribute to the Bazel ecosystem by:

   - Helping rules maintainers by contributing pull requests.
   - Creating new rules and open-sourcing them.
   - Contributing to Bazel-related tools, for example, migration tools.
   - Improving Bazel integration with other IDEs and tools.

Before making a change, [create a GitHub
issue](http://github.com/bazelbuild/bazel/issues)
or email [bazel-discuss@](mailto:bazel-discuss@googlegroups.com).

The most helpful contributions fix bugs or add features (as opposed
to stylistic, refactoring, or "cleanup" changes). Your change should
include tests and documentation, keeping in mind backward-compatibility,
portability, and the impact on memory usage and performance.

To learn about how to submit a change, see the
[patch acceptance process](/contribute/patch-acceptance).

## Bazel's code description

Bazel has a large codebase with code in multiple locations. See the [codebase guide](/contribute/codebase) for more details.

Bazel is organized as follows:

*  Client code is in `src/main/cpp` and provides the command-line interface.
*  Protocol buffers are in `src/main/protobuf`.
*  Server code is in `src/main/java` and `src/test/java`.
   *  Core code which is mostly composed of [SkyFrame](/reference/skyframe)
      and some utilities.
   *  Built-in rules are in `com.google.devtools.build.lib.rules` and in
     `com.google.devtools.build.lib.bazel.rules`. You might want to read about
     the [Challenges of Writing Rules](/rules/challenges) first.
*  Java native interfaces are in `src/main/native`.
*  Various tooling for language support are described in the list in the
   [compiling Bazel](/install/compile-source) section.


### Searching Bazel's source code

To quickly search through Bazel's source code, use
[Bazel Code Search](https://source.bazel.build/). You can navigate Bazel's
repositories, branches, and files. You can also view history, diffs, and blame
information. To learn more, see the
[Bazel Code Search User Guide](/contribute/search).

---

## Guide for rolling out breaking changes
- URL: https://bazel.build/contribute/breaking-changes
- Source: contribute/breaking-changes.mdx
- Slug: /contribute/breaking-changes

It is inevitable that we will make breaking changes to Bazel. We will have to
change our designs and fix the things that do not quite work. However, we need
to make sure that community and Bazel ecosystem can follow along. To that end,
Bazel project has adopted a
[backward compatibility policy](/release/backward-compatibility).
This document describes the process for Bazel contributors to make a breaking
change in Bazel to adhere to this policy.

1. Follow the [design document policy](/contribute/design-documents).

1. [File a GitHub issue.](#github-issue)

1. [Implement the change.](#implementation)

1. [Update labels.](#labels)

1. [Update repositories.](#update-repos)

1. [Flip the incompatible flag.](#flip-flag)

## GitHub issue

[File a GitHub issue](https://github.com/bazelbuild/bazel/issues)
in the Bazel repository.
[See example.](https://github.com/bazelbuild/bazel/issues/6611)

We recommend that:

* The title starts with the name of the flag (the flag name will start with
  `incompatible_`).

* You add the label
  [`incompatible-change`](https://github.com/bazelbuild/bazel/labels/incompatible-change).

* The description contains a description of the change and a link to relevant
  design documents.

* The description contains a migration recipe, to explain users how they should
  update their code. Ideally, when the change is mechanical, include a link to a
  migration tool.

* The description includes an example of the error message users will get if
  they don't migrate. This will make the GitHub issue more discoverable from
  search engines. Make sure that the error message is helpful and actionable.
  When possible, the error message should include the name of the incompatible
  flag.

For the migration tool, consider contributing to
[Buildifier](https://github.com/bazelbuild/buildtools/blob/master/buildifier/README.md).
It is able to apply automated fixes to `BUILD`, `WORKSPACE`, and `.bzl` files.
It may also report warnings.

## Implementation

Create a new flag in Bazel. The default value must be false. The help text
should contain the URL of the GitHub issue. As the flag name starts with
`incompatible_`, it needs metadata tags:

```java
      metadataTags = {
        OptionMetadataTag.INCOMPATIBLE_CHANGE,
      },
```

In the commit description, add a brief summary of the flag.
Also add [`RELNOTES:`](release-notes.md) in the following form:
`RELNOTES: --incompatible_name_of_flag has been added. See #xyz for details`

The commit should also update the relevant documentation, so that there is no
window of commits in which the code is inconsistent with the docs. Since our
documentation is versioned, changes to the docs will not be inadvertently
released prematurely.

## Labels

Once the commit is merged and the incompatible change is ready to be adopted, add the label
[`migration-ready`](https://github.com/bazelbuild/bazel/labels/migration-ready)
to the GitHub issue.

If a problem is found with the flag and users are not expected to migrate yet:
remove the flags `migration-ready`.

If you plan to flip the flag in the next major release, add label `breaking-change-X.0" to the issue.

## Updating repositories

Bazel CI tests a list of important projects at
[Bazel@HEAD + Downstream](https://buildkite.com/bazel/bazel-at-head-plus-downstream). Most of them are often
dependencies of other Bazel projects, therefore it's important to migrate them to unblock the migration for the broader community. To monitor the migration status of those projects, you can use the [`bazelisk-plus-incompatible-flags` pipeline](https://buildkite.com/bazel/bazelisk-plus-incompatible-flags).
Check how this pipeline works [here](https://github.com/bazelbuild/continuous-integration/tree/master/buildkite#checking-incompatible-changes-status-for-downstream-projects).

Our dev support team monitors the [`migration-ready`](https://github.com/bazelbuild/bazel/labels/migration-ready) label. Once you add this label to the GitHub issue, they will handle the following:

1. Create a comment in the GitHub issue to track the list of failures and downstream projects that need to be migrated ([see example](https://github.com/bazelbuild/bazel/issues/17032#issuecomment-1353077469))

1. File Github issues to notify the owners of every downstream project broken by your incompatible change ([see example](https://github.com/bazelbuild/intellij/issues/4208))

1. Follow up to make sure all issues are addressed before the target release date

Migrating projects in the downstream pipeline is NOT entirely the responsibility of the incompatible change author, but you can do the following to accelerate the migration and make life easier for both Bazel users and the Bazel Green Team.

1. Send PRs to fix downstream projects.

1. Reach out to the Bazel community for help on migration (e.g. [Bazel Rules Authors SIG](https://bazel-contrib.github.io/SIG-rules-authors/)).

## Flipping the flag

Before flipping the default value of the flag to true, please make sure that:

* Core repositories in the ecosystem are migrated.

    On the [`bazelisk-plus-incompatible-flags` pipeline](https://buildkite.com/bazel/bazelisk-plus-incompatible-flags),
    the flag should appear under `The following flags didn't break any passing Bazel team owned/co-owned projects`.

* All issues in the checklist are marked as fixed/closed.

* User concerns and questions have been resolved.

When the flag is ready to flip in Bazel, but blocked on internal migration at Google, please consider setting the flag value to false in the internal `blazerc` file to unblock the flag flip. By doing this, we can ensure Bazel users depend on the new behaviour by default as early as possible.

When changing the flag default to true, please:

* Use `RELNOTES[INC]` in the commit description, with the
    following format:
    `RELNOTES[INC]: --incompatible_name_of_flag is flipped to true. See #xyz for
    details`
    You can include additional information in the rest of the commit description.
* Use `Fixes #xyz` in the description, so that the GitHub issue gets closed
    when the commit is merged.
* Review and update documentation if needed.
* File a new issue `#abc` to track the removal of the flag.

## Removing the flag

After the flag is flipped at HEAD, it should be removed from Bazel eventually.
When you plan to remove the incompatible flag:

* Consider leaving more time for users to migrate if it's a major incompatible change.
  Ideally, the flag  should be available in at least one major release.
* For the commit that removes the flag, use `Fixes #abc` in the description
  so that the GitHub issue gets closed when the commit is merged.

---

## The Bazel codebase
- URL: https://bazel.build/contribute/codebase
- Source: contribute/codebase.mdx
- Slug: /contribute/codebase

This document is a description of the codebase and how Bazel is structured. It
is intended for people willing to contribute to Bazel, not for end-users.

## Introduction

The codebase of Bazel is large (~350KLOC production code and ~260 KLOC test
code) and no one is familiar with the whole landscape: everyone knows their
particular valley very well, but few know what lies over the hills in every
direction.

In order for people midway upon the journey not to find themselves within a
forest dark with the straightforward pathway being lost, this document tries to
give an overview of the codebase so that it's easier to get started with
working on it.

The public version of the source code of Bazel lives on GitHub at
[github.com/bazelbuild/bazel](http://github.com/bazelbuild/bazel). This is not
the "source of truth"; it's derived from a Google-internal source tree that
contains additional functionality that is not useful outside Google. The
long-term goal is to make GitHub the source of truth.

Contributions are accepted through the regular GitHub pull request mechanism,
and manually imported by a Googler into the internal source tree, then
re-exported back out to GitHub.

## Client/server architecture

The bulk of Bazel resides in a server process that stays in RAM between builds.
This allows Bazel to maintain state between builds.

This is why the Bazel command line has two kinds of options: startup and
command. In a command line like this:

```
    bazel --host_jvm_args=-Xmx8G build -c opt //foo:bar
```

Some options (`--host_jvm_args=`) are before the name of the command to be run
and some are after (`-c opt`); the former kind is called a "startup option" and
affects the server process as a whole, whereas the latter kind, the "command
option", only affects a single command.

Each server instance has a single associated workspace (collection of source
trees known as "repositories") and each workspace usually has a single active
server instance. This can be circumvented by specifying a custom output base
(see the "Directory layout" section for more information).

Bazel is distributed as a single ELF executable that is also a valid .zip file.
When you type `bazel`, the above ELF executable implemented in C++ (the
"client") gets control. It sets up an appropriate server process using the
following steps:

1.  Checks whether it has already extracted itself. If not, it does that. This
    is where the implementation of the server comes from.
2.  Checks whether there is an active server instance that works: it is running,
    it has the right startup options and uses the right workspace directory. It
    finds the running server by looking at the directory `$OUTPUT_BASE/server`
    where there is a lock file with the port the server is listening on.
3.  If needed, kills the old server process
4.  If needed, starts up a new server process

After a suitable server process is ready, the command that needs to be run is
communicated to it over a gRPC interface, then the output of Bazel is piped back
to the terminal. Only one command can be running at the same time. This is
implemented using an elaborate locking mechanism with parts in C++ and parts in
Java. There is some infrastructure for running multiple commands in parallel,
since the inability to run `bazel version` in parallel with another command
is somewhat embarrassing. The main blocker is the life cycle of `BlazeModule`s
and some state in `BlazeRuntime`.

At the end of a command, the Bazel server transmits the exit code the client
should return. An interesting wrinkle is the implementation of `bazel run`: the
job of this command is to run something Bazel just built, but it can't do that
from the server process because it doesn't have a terminal. So instead it tells
the client what binary it should `exec()` and with what arguments.

When one presses Ctrl-C, the client translates it to a Cancel call on the gRPC
connection, which tries to terminate the command as soon as possible. After the
third Ctrl-C, the client sends a SIGKILL to the server instead.

The source code of the client is under `src/main/cpp` and the protocol used to
communicate with the server is in `src/main/protobuf/command_server.proto` .

The main entry point of the server is `BlazeRuntime.main()` and the gRPC calls
from the client are handled by `GrpcServerImpl.run()`.

## Directory layout

Bazel creates a somewhat complicated set of directories during a build. A full
description is available in [Output directory layout](/remote/output-directories).

The "main repo" is the source tree Bazel is run in. It usually corresponds to
something you checked out from source control. The root of this directory is
known as the "workspace root".

Bazel puts all of its data under the "output user root". This is usually
`$HOME/.cache/bazel/_bazel_${USER}`, but can be overridden using the
`--output_user_root` startup option.

The "install base" is where Bazel is extracted to. This is done automatically
and each Bazel version gets a subdirectory based on its checksum under the
install base. It's at `$OUTPUT_USER_ROOT/install` by default and can be changed
using the `--install_base` command line option.

The "output base" is the place where the Bazel instance attached to a specific
workspace writes to. Each output base has at most one Bazel server instance
running at any time. It's usually at `$OUTPUT_USER_ROOT/<checksum of the path
to the workspace>`. It can be changed using the `--output_base` startup option,
which is, among other things, useful for getting around the limitation that only
one Bazel instance can be running in any workspace at any given time.

The output directory contains, among other things:

*   The fetched external repositories at `$OUTPUT_BASE/external`.
*   The exec root, a directory that contains symlinks to all the source
    code for the current build. It's located at `$OUTPUT_BASE/execroot`. During
    the build, the working directory is `$EXECROOT/<name of main
    repository>`. We are planning to change this to `$EXECROOT`, although it's a
    long term plan because it's a very incompatible change.
*   Files built during the build.

## The process of executing a command

Once the Bazel server gets control and is informed about a command it needs to
execute, the following sequence of events happens:

1.  `BlazeCommandDispatcher` is informed about the new request. It decides
    whether the command needs a workspace to run in (almost every command except
    for ones that don't have anything to do with source code, such as version or
    help) and whether another command is running.

2.  The right command is found. Each command must implement the interface
    `BlazeCommand` and must have the `@Command` annotation (this is a bit of an
    antipattern, it would be nice if all the metadata a command needs was
    described by methods on `BlazeCommand`)

3.  The command line options are parsed. Each command has different command line
    options, which are described in the `@Command` annotation.

4.  An event bus is created. The event bus is a stream for events that happen
    during the build. Some of these are exported to outside of Bazel under the
    aegis of the Build Event Protocol in order to tell the world how the build
    goes.

5.  The command gets control. The most interesting commands are those that run a
    build: build, test, run, coverage and so on: this functionality is
    implemented by `BuildTool`.

6.  The set of target patterns on the command line is parsed and wildcards like
    `//pkg:all` and `//pkg/...` are resolved. This is implemented in
    `AnalysisPhaseRunner.evaluateTargetPatterns()` and reified in Skyframe as
    `TargetPatternPhaseValue`.

7.  The loading/analysis phase is run to produce the action graph (a directed
    acyclic graph of commands that need to be executed for the build).

8.  The execution phase is run. This means running every action required to
    build the top-level targets that are requested are run.

## Command line options

The command line options for a Bazel invocation are described in an
`OptionsParsingResult` object, which in turn contains a map from "option
classes" to the values of the options. An "option class" is a subclass of
`OptionsBase` and groups command line options together that are related to each
other. For example:

1.  Options related to a programming language (`CppOptions` or `JavaOptions`).
    These should be a subclass of `FragmentOptions` and are eventually wrapped
    into a `BuildOptions` object.
2.  Options related to the way Bazel executes actions (`ExecutionOptions`)

These options are designed to be consumed in the analysis phase and (either
through `RuleContext.getFragment()` in Java or `ctx.fragments` in Starlark).
Some of them (for example, whether to do C++ include scanning or not) are read
in the execution phase, but that always requires explicit plumbing since
`BuildConfiguration` is not available then. For more information, see the
section "Configurations".

**WARNING:** We like to pretend that `OptionsBase` instances are immutable and
use them that way (such as a part of `SkyKeys`). This is not the case and
modifying them is a really good way to break Bazel in subtle ways that are hard
to debug. Unfortunately, making them actually immutable is a large endeavor.
(Modifying a `FragmentOptions` immediately after construction before anyone else
gets a chance to keep a reference to it and before `equals()` or `hashCode()` is
called on it is okay.)

Bazel learns about option classes in the following ways:

1.  Some are hard-wired into Bazel (`CommonCommandOptions`)
2.  From the `@Command` annotation on each Bazel command
3.  From `ConfiguredRuleClassProvider` (these are command line options related
    to individual programming languages)
4.  Starlark rules can also define their own options (see
    [here](/extending/config))

Each option (excluding Starlark-defined options) is a member variable of a
`FragmentOptions` subclass that has the `@Option` annotation, which specifies
the name and the type of the command line option along with some help text.

The Java type of the value of a command line option is usually something simple
(a string, an integer, a Boolean, a label, etc.). However, we also support
options of more complicated types; in this case, the job of converting from the
command line string to the data type falls to an implementation of
`com.google.devtools.common.options.Converter`.

## The source tree, as seen by Bazel

Bazel is in the business of building software, which happens by reading and
interpreting the source code. The totality of the source code Bazel operates on
is called "the workspace" and it is structured into repositories, packages and
rules.

### Repositories

A "repository" is a source tree on which a developer works; it usually
represents a single project. Bazel's ancestor, Blaze, operated on a monorepo,
that is, a single source tree that contains all source code used to run the build.
Bazel, in contrast, supports projects whose source code spans multiple
repositories. The repository from which Bazel is invoked is called the "main
repository", the others are called "external repositories".

A repository is marked by a repo boundary file (`MODULE.bazel`, `REPO.bazel`, or
in legacy contexts, `WORKSPACE` or `WORKSPACE.bazel`) in its root directory. The
main repo is the source tree where you're invoking Bazel from. External repos
are defined in various ways; see [external dependencies
overview](/external/overview) for more information.

Code of external repositories is symlinked or downloaded under
`$OUTPUT_BASE/external`.

When running the build, the whole source tree needs to be pieced together; this
is done by `SymlinkForest`, which symlinks every package in the main repository
to `$EXECROOT` and every external repository to either `$EXECROOT/external` or
`$EXECROOT/..`.

### Packages

Every repository is composed of packages, a collection of related files and
a specification of the dependencies. These are specified by a file called
`BUILD` or `BUILD.bazel`. If both exist, Bazel prefers `BUILD.bazel`; the reason
why `BUILD` files are still accepted is that Bazel's ancestor, Blaze, used this
file name. However, it turned out to be a commonly used path segment, especially
on Windows, where file names are case-insensitive.

Packages are independent of each other: changes to the `BUILD` file of a package
cannot cause other packages to change. The addition or removal of `BUILD` files
_can _change other packages, since recursive globs stop at package boundaries
and thus the presence of a `BUILD` file stops the recursion.

The evaluation of a `BUILD` file is called "package loading". It's implemented
in the class `PackageFactory`, works by calling the Starlark interpreter and
requires knowledge of the set of available rule classes. The result of package
loading is a `Package` object. It's mostly a map from a string (the name of a
target) to the target itself.

A large chunk of complexity during package loading is globbing: Bazel does not
require every source file to be explicitly listed and instead can run globs
(such as `glob(["**/*.java"])`). Unlike the shell, it supports recursive globs that
descend into subdirectories (but not into subpackages). This requires access to
the file system and since that can be slow, we implement all sorts of tricks to
make it run in parallel and as efficiently as possible.

Globbing is implemented in the following classes:

*   `LegacyGlobber`, a fast and blissfully Skyframe-unaware globber
*   `SkyframeHybridGlobber`, a version that uses Skyframe and reverts back to
    the legacy globber in order to avoid "Skyframe restarts" (described below)

The `Package` class itself contains some members that are exclusively used to
parse the "external" package (related to external dependencies) and which do not
make sense for real packages. This is
a design flaw because objects describing regular packages should not contain
fields that describe something else. These include:

*   The repository mappings
*   The registered toolchains
*   The registered execution platforms

Ideally, there would be more separation between parsing the "external" package
from parsing regular packages so that `Package` does not need to cater for the
needs of both. This is unfortunately difficult to do because the two are
intertwined quite deeply.

### Labels, Targets, and Rules

Packages are composed of targets, which have the following types:

1.  **Files:** things that are either the input or the output of the build. In
    Bazel parlance, we call them _artifacts_ (discussed elsewhere). Not all
    files created during the build are targets; it's common for an output of
    Bazel not to have an associated label.
2.  **Rules:** these describe steps to derive its outputs from its inputs. They
    are generally associated with a programming language (such as `cc_library`,
    `java_library` or `py_library`), but there are some language-agnostic ones
    (such as `genrule` or `filegroup`)
3.  **Package groups:** discussed in the [Visibility](#visibility) section.

The name of a target is called a _Label_. The syntax of labels is
`@repo//pac/kage:name`, where `repo` is the name of the repository the Label is
in, `pac/kage` is the directory its `BUILD` file is in and `name` is the path of
the file (if the label refers to a source file) relative to the directory of the
package. When referring to a target on the command line, some parts of the label
can be omitted:

1.  If the repository is omitted, the label is taken to be in the main
    repository.
2.  If the package part is omitted (such as `name` or `:name`), the label is taken
    to be in the package of the current working directory (relative paths
    containing uplevel references (..) are not allowed)

A kind of a rule (such as "C++ library") is called a "rule class". Rule classes may
be implemented either in Starlark (the `rule()` function) or in Java (so called
"native rules", type `RuleClass`). In the long term, every language-specific
rule will be implemented in Starlark, but some legacy rule families (such as Java
or C++) are still in Java for the time being.

Starlark rule classes need to be imported at the beginning of `BUILD` files
using the `load()` statement, whereas Java rule classes are "innately" known by
Bazel, by virtue of being registered with the `ConfiguredRuleClassProvider`.

Rule classes contain information such as:

1.  Its attributes (such as `srcs`, `deps`): their types, default values,
    constraints, etc.
2.  The configuration transitions and aspects attached to each attribute, if any
3.  The implementation of the rule
4.  The transitive info providers the rule "usually" creates

**Terminology note:** In the codebase, we often use "Rule" to mean the target
created by a rule class. But in Starlark and in user-facing documentation,
"Rule" should be used exclusively to refer to the rule class itself; the target
is just a "target". Also note that despite `RuleClass` having "class" in its
name, there is no Java inheritance relationship between a rule class and targets
of that type.

## Skyframe

The evaluation framework underlying Bazel is called Skyframe. Its model is that
everything that needs to be built during a build is organized into a directed
acyclic graph with edges pointing from any pieces of data to its dependencies,
that is, other pieces of data that need to be known to construct it.

The nodes in the graph are called `SkyValue`s and their names are called
`SkyKey`s. Both are deeply immutable; only immutable objects should be
reachable from them. This invariant almost always holds, and in case it doesn't
(such as for the individual options classes `BuildOptions`, which is a member of
`BuildConfigurationValue` and its `SkyKey`) we try really hard not to change
them or to change them in only ways that are not observable from the outside.
From this it follows that everything that is computed within Skyframe (such as
configured targets) must also be immutable.

The most convenient way to observe the Skyframe graph is to run `bazel dump
--skyframe=deps`, which dumps the graph, one `SkyValue` per line. It's best
to do it for tiny builds, since it can get pretty large.

Skyframe lives in the `com.google.devtools.build.skyframe` package. The
similarly-named package `com.google.devtools.build.lib.skyframe` contains the
implementation of Bazel on top of Skyframe. More information about Skyframe is
available [here](/reference/skyframe).

To evaluate a given `SkyKey` into a `SkyValue`, Skyframe will invoke the
`SkyFunction` corresponding to the type of the key. During the function's
evaluation, it may request other dependencies from Skyframe by calling the
various overloads of `SkyFunction.Environment.getValue()`. This has the
side-effect of registering those dependencies into Skyframe's internal graph, so
that Skyframe will know to re-evaluate the function when any of its dependencies
change. In other words, Skyframe's caching and incremental computation work at
the granularity of `SkyFunction`s and `SkyValue`s.

Whenever a `SkyFunction` requests a dependency that is unavailable, `getValue()`
will return null. The function should then yield control back to Skyframe by
itself returning null. At some later point, Skyframe will evaluate the
unavailable dependency, then restart the function from the beginning — only this
time the `getValue()` call will succeed with a non-null result.

A consequence of this is that any computation performed inside the `SkyFunction`
prior to the restart must be repeated. But this does not include work done to
evaluate dependency `SkyValues`, which are cached. Therefore, we commonly work
around this issue by:

1.  Declaring dependencies in batches (by using `getValuesAndExceptions()`) to
    limit the number of restarts.
2.  Breaking up a `SkyValue` into separate pieces computed by different
    `SkyFunction`s, so that they can be computed and cached independently. This
    should be done strategically, since it has the potential to increases memory
    usage.
3.  Storing state between restarts, either using
    `SkyFunction.Environment.getState()`, or keeping an ad hoc static cache
    "behind the back of Skyframe". With complex SkyFunctions, state management
    between restarts can get tricky, so
    [`StateMachine`s](/contribute/statemachine-guide) were introduced for a
    structured approach to logical concurrency, including hooks to suspend and
    resume hierarchical computations within a `SkyFunction`. Example:
    [`DependencyResolver#computeDependencies`][statemachine_example]
    uses a `StateMachine` with `getState()` to compute the potentially huge set
    of direct dependencies of a configured target, which otherwise can result in
    expensive restarts.

[statemachine_example]: https://developers.google.com/devsite/reference/markdown/links#reference_links

Fundamentally, Bazel need these types of workarounds because hundreds of
thousands of in-flight Skyframe nodes is common, and Java's support of
lightweight threads [does not outperform][virtual_threads] the
`StateMachine` implementation as of 2023.

[virtual_threads]: /contribute/statemachine-guide#epilogue_eventually_removing_callbacks

## Starlark

Starlark is the domain-specific language people use to configure and extend
Bazel. It's conceived as a restricted subset of Python that has far fewer types,
more restrictions on control flow, and most importantly, strong immutability
guarantees to enable concurrent reads. It is not Turing-complete, which
discourages some (but not all) users from trying to accomplish general
programming tasks within the language.

Starlark is implemented in the `net.starlark.java` package.
It also has an independent Go implementation
[here](https://github.com/google/starlark-go). The Java
implementation used in Bazel is currently an interpreter.

Starlark is used in several contexts, including:

1.  **`BUILD` files.** This is where new build targets are defined. Starlark
    code running in this context only has access to the contents of the `BUILD`
    file itself and `.bzl` files loaded by it.
2.  **The `MODULE.bazel` file.** This is where external dependencies are
    defined. Starlark code running in this context only has very limited access
    to a few predefined directives.
3.  **`.bzl` files.** This is where new build rules, repo rules, module
    extensions are defined. Starlark code here can define new functions and load
    from other `.bzl` files.

The dialects available for `BUILD` and `.bzl` files are slightly different
because they express different things. A list of differences is available
[here](/rules/language#differences-between-build-and-bzl-files).

More information about Starlark is available [here](/rules/language).

## The loading/analysis phase

The loading/analysis phase is where Bazel determines what actions are needed to
build a particular rule. Its basic unit is a "configured target", which is,
quite sensibly, a (target, configuration) pair.

It's called the "loading/analysis phase" because it can be split into two
distinct parts, which used to be serialized, but they can now overlap in time:

1.  Loading packages, that is, turning `BUILD` files into the `Package` objects
    that represent them
2.  Analyzing configured targets, that is, running the implementation of the
    rules to produce the action graph

Each configured target in the transitive closure of the configured targets
requested on the command line must be analyzed bottom-up; that is, leaf nodes
first, then up to the ones on the command line. The inputs to the analysis of
a single configured target are:

1.  **The configuration.** ("how" to build that rule; for example, the target
    platform but also things like command line options the user wants to be
    passed to the C++ compiler)
2.  **The direct dependencies.** Their transitive info providers are available
    to the rule being analyzed. They are called like that because they provide a
    "roll-up" of the information in the transitive closure of the configured
    target, such as all the .jar files on the classpath or all the .o files that
    need to be linked into a C++ binary)
3.  **The target itself**. This is the result of loading the package the target
    is in. For rules, this includes its attributes, which is usually what
    matters.
4.  **The implementation of the configured target.** For rules, this can either
    be in Starlark or in Java. All non-rule configured targets are implemented
    in Java.

The output of analyzing a configured target is:

1.  The transitive info providers that configured targets that depend on it can
    access
2.  The artifacts it can create and the actions that produce them.

The API offered to Java rules is `RuleContext`, which is the equivalent of the
`ctx` argument of Starlark rules. Its API is more powerful, but at the same
time, it's easier to do Bad Things™, for example to write code whose time or
space complexity is quadratic (or worse), to make the Bazel server crash with a
Java exception or to violate invariants (such as by inadvertently modifying an
`Options` instance or by making a configured target mutable)

The algorithm that determines the direct dependencies of a configured target
lives in `DependencyResolver.dependentNodeMap()`.

### Configurations

Configurations are the "how" of building a target: for what platform, with what
command line options, etc.

The same target can be built for multiple configurations in the same build. This
is useful, for example, when the same code is used for a tool that's run during
the build and for the target code and we are cross-compiling or when we are
building a fat Android app (one that contains native code for multiple CPU
architectures)

Conceptually, the configuration is a `BuildOptions` instance. However, in
practice, `BuildOptions` is wrapped by `BuildConfiguration` that provides
additional sundry pieces of functionality. It propagates from the top of the
dependency graph to the bottom. If it changes, the build needs to be
re-analyzed.

This results in anomalies like having to re-analyze the whole build if, for
example, the number of requested test runs changes, even though that only
affects test targets (we have plans to "trim" configurations so that this is
not the case, but it's not ready yet).

When a rule implementation needs part of the configuration, it needs to declare
it in its definition using `RuleClass.Builder.requiresConfigurationFragments()`
. This is both to avoid mistakes (such as Python rules using the Java fragment) and
to facilitate configuration trimming so that such as if Python options change, C++
targets don't need to be re-analyzed.

The configuration of a rule is not necessarily the same as that of its "parent"
rule. The process of changing the configuration in a dependency edge is called a
"configuration transition". It can happen in two places:

1.  On a dependency edge. These transitions are specified in
    `Attribute.Builder.cfg()` and are functions from a `Rule` (where the
    transition happens) and a `BuildOptions` (the original configuration) to one
    or more `BuildOptions` (the output configuration).
2.  On any incoming edge to a configured target. These are specified in
    `RuleClass.Builder.cfg()`.

The relevant classes are `TransitionFactory` and `ConfigurationTransition`.

Configuration transitions are used, for example:

1.  To declare that a particular dependency is used during the build and it
    should thus be built in the execution architecture
2.  To declare that a particular dependency must be built for multiple
    architectures (such as for native code in fat Android APKs)

If a configuration transition results in multiple configurations, it's called a
_split transition._

Configuration transitions can also be implemented in Starlark (documentation
[here](/extending/config))

### Transitive info providers

Transitive info providers are a way (and the _only _way) for configured targets
to learn things about other configured targets that they depend on, and the only
way to tell things about themselves to other configured targets that depend on
them. The reason why "transitive" is in their name is that this is usually some
sort of roll-up of the transitive closure of a configured target.

There is generally a 1:1 correspondence between Java transitive info providers
and Starlark ones (the exception is `DefaultInfo` which is an amalgamation of
`FileProvider`, `FilesToRunProvider` and `RunfilesProvider` because that API was
deemed to be more Starlark-ish than a direct transliteration of the Java one).
Their key is one of the following things:

1.  A Java Class object. This is only available for providers that are not
    accessible from Starlark. These providers are a subclass of
    `TransitiveInfoProvider`.
2.  A string. This is legacy and heavily discouraged since it's susceptible to
    name clashes. Such transitive info providers are direct subclasses of
    `build.lib.packages.Info` .
3.  A provider symbol. This can be created from Starlark using the `provider()`
    function and is the recommended way to create new providers. The symbol is
    represented by a `Provider.Key` instance in Java.

New providers implemented in Java should be implemented using `BuiltinProvider`.
`NativeProvider` is deprecated (we haven't had time to remove it yet) and
`TransitiveInfoProvider` subclasses cannot be accessed from Starlark.

### Configured targets

Configured targets are implemented as `RuleConfiguredTargetFactory`. There is a
subclass for each rule class implemented in Java. Starlark configured targets
are created through `StarlarkRuleConfiguredTargetUtil.buildRule()` .

Configured target factories should use `RuleConfiguredTargetBuilder` to
construct their return value. It consists of the following things:

1.  Their `filesToBuild`, the hazy concept of "the set of files this rule
    represents." These are the files that get built when the configured target
    is on the command line or in the srcs of a genrule.
2.  Their runfiles, regular and data.
3.  Their output groups. These are various "other sets of files" the rule can
    build. They can be accessed using the output\_group attribute of the
    filegroup rule in BUILD and using the `OutputGroupInfo` provider in Java.

### Runfiles

Some binaries need data files to run. A prominent example is tests that need
input files. This is represented in Bazel by the concept of "runfiles". A
"runfiles tree" is a directory tree of the data files for a particular binary.
It is created in the file system as a symlink tree with individual symlinks
pointing to the files in the source or output trees.

A set of runfiles is represented as a `Runfiles` instance. It is conceptually a
map from the path of a file in the runfiles tree to the `Artifact` instance that
represents it. It's a little more complicated than a single `Map` for two
reasons:

*   Most of the time, the runfiles path of a file is the same as its execpath.
    We use this to save some RAM.
*   There are various legacy kinds of entries in runfiles trees, which also need
    to be represented.

Runfiles are collected using `RunfilesProvider`: an instance of this class
represents the runfiles a configured target (such as a library) and its transitive
closure needs and they are gathered like a nested set (in fact, they are
implemented using nested sets under the cover): each target unions the runfiles
of its dependencies, adds some of its own, then sends the resulting set upwards
in the dependency graph. A `RunfilesProvider` instance contains two `Runfiles`
instances, one for when the rule is depended on through the "data" attribute and
one for every other kind of incoming dependency. This is because a target
sometimes presents different runfiles when depended on through a data attribute
than otherwise. This is undesired legacy behavior that we haven't gotten around
removing yet.

Runfiles of binaries are represented as an instance of `RunfilesSupport`. This
is different from `Runfiles` because `RunfilesSupport` has the capability of
actually being built (unlike `Runfiles`, which is just a mapping). This
necessitates the following additional components:

*   **The input runfiles manifest.** This is a serialized description of the
    runfiles tree. It is used as a proxy for the contents of the runfiles tree
    and Bazel assumes that the runfiles tree changes if and only if the contents
    of the manifest change.
*   **The output runfiles manifest.** This is used by runtime libraries that
    handle runfiles trees, notably on Windows, which sometimes doesn't support
    symbolic links.
*   **Command line arguments** for running the binary whose runfiles the
    `RunfilesSupport` object represents.

### Aspects

Aspects are a way to "propagate computation down the dependency graph". They are
described for users of Bazel
[here](/extending/aspects). A good
motivating example is protocol buffers: a `proto_library` rule should not know
about any particular language, but building the implementation of a protocol
buffer message (the "basic unit" of protocol buffers) in any programming
language should be coupled to the `proto_library` rule so that if two targets in
the same language depend on the same protocol buffer, it gets built only once.

Just like configured targets, they are represented in Skyframe as a `SkyValue`
and the way they are constructed is very similar to how configured targets are
built: they have a factory class called `ConfiguredAspectFactory` that has
access to a `RuleContext`, but unlike configured target factories, it also knows
about the configured target it is attached to and its providers.

The set of aspects propagated down the dependency graph is specified for each
attribute using the `Attribute.Builder.aspects()` function. There are a few
confusingly-named classes that participate in the process:

1.  `AspectClass` is the implementation of the aspect. It can be either in Java
    (in which case it's a subclass) or in Starlark (in which case it's an
    instance of `StarlarkAspectClass`). It's analogous to
    `RuleConfiguredTargetFactory`.
2.  `AspectDefinition` is the definition of the aspect; it includes the
    providers it requires, the providers it provides and contains a reference to
    its implementation, such as the appropriate `AspectClass` instance. It's
    analogous to `RuleClass`.
3.  `AspectParameters` is a way to parametrize an aspect that is propagated down
    the dependency graph. It's currently a string to string map. A good example
    of why it's useful is protocol buffers: if a language has multiple APIs, the
    information as to which API the protocol buffers should be built for should
    be propagated down the dependency graph.
4.  `Aspect` represents all the data that's needed to compute an aspect that
    propagates down the dependency graph. It consists of the aspect class, its
    definition and its parameters.
5.  `RuleAspect` is the function that determines which aspects a particular rule
    should propagate. It's a `Rule` -> `Aspect` function.

A somewhat unexpected complication is that aspects can attach to other aspects;
for example, an aspect collecting the classpath for a Java IDE will probably
want to know about all the .jar files on the classpath, but some of them are
protocol buffers. In that case, the IDE aspect will want to attach to the
(`proto_library` rule + Java proto aspect) pair.

The complexity of aspects on aspects is captured in the class
`AspectCollection`.

### Platforms and toolchains

Bazel supports multi-platform builds, that is, builds where there may be
multiple architectures where build actions run and multiple architectures for
which code is built. These architectures are referred to as _platforms_ in Bazel
parlance (full documentation
[here](/extending/platforms))

A platform is described by a key-value mapping from _constraint settings_ (such as
the concept of "CPU architecture") to _constraint values_ (such as a particular CPU
like x86\_64). We have a "dictionary" of the most commonly used constraint
settings and values in the `@platforms` repository.

The concept of _toolchain_ comes from the fact that depending on what platforms
the build is running on and what platforms are targeted, one may need to use
different compilers; for example, a particular C++ toolchain may run on a
specific OS and be able to target some other OSes. Bazel must determine the C++
compiler that is used based on the set execution and target platform
(documentation for toolchains
[here](/extending/toolchains)).

In order to do this, toolchains are annotated with the set of execution and
target platform constraints they support. In order to do this, the definition of
a toolchain are split into two parts:

1.  A `toolchain()` rule that describes the set of execution and target
    constraints a toolchain supports and tells what kind (such as C++ or Java) of
    toolchain it is (the latter is represented by the `toolchain_type()` rule)
2.  A language-specific rule that describes the actual toolchain (such as
    `cc_toolchain()`)

This is done in this way because we need to know the constraints for every
toolchain in order to do toolchain resolution and language-specific
`*_toolchain()` rules contain much more information than that, so they take more
time to load.

Execution platforms are specified in one of the following ways:

1.  In the MODULE.bazel file using the `register_execution_platforms()` function
2.  On the command line using the --extra\_execution\_platforms command line
    option

The set of available execution platforms is computed in
`RegisteredExecutionPlatformsFunction` .

The target platform for a configured target is determined by
`PlatformOptions.computeTargetPlatform()` . It's a list of platforms because we
eventually want to support multiple target platforms, but it's not implemented
yet.

The set of toolchains to be used for a configured target is determined by
`ToolchainResolutionFunction`. It is a function of:

*   The set of registered toolchains (in the MODULE.bazel file and the
    configuration)
*   The desired execution and target platforms (in the configuration)
*   The set of toolchain types that are required by the configured target (in
    `UnloadedToolchainContextKey)`
*   The set of execution platform constraints of the configured target (the
    `exec_compatible_with` attribute), in `UnloadedToolchainContextKey`

Its result is an `UnloadedToolchainContext`, which is essentially a map from
toolchain type (represented as a `ToolchainTypeInfo` instance) to the label of
the selected toolchain. It's called "unloaded" because it does not contain the
toolchains themselves, only their labels.

Then the toolchains are actually loaded using `ResolvedToolchainContext.load()`
and used by the implementation of the configured target that requested them.

We also have a legacy system that relies on there being one single "host"
configuration and target configurations being represented by various
configuration flags, such as `--cpu` . We are gradually transitioning to the above
system. In order to handle cases where people rely on the legacy configuration
values, we have implemented
[platform mappings](https://docs.google.com/document/d/1Vg_tPgiZbSrvXcJ403vZVAGlsWhH9BUDrAxMOYnO0Ls)
to translate between the legacy flags and the new-style platform constraints.
Their code is in `PlatformMappingFunction` and uses a non-Starlark "little
language".

### Constraints

Sometimes one wants to designate a target as being compatible with only a few
platforms. Bazel has (unfortunately) multiple mechanisms to achieve this end:

*   Rule-specific constraints
*   `environment_group()` / `environment()`
*   Platform constraints

Rule-specific constraints are mostly used within Google for Java rules; they are
on their way out and they are not available in Bazel, but the source code may
contain references to it. The attribute that governs this is called
`constraints=` .

#### environment_group() and environment()

These rules are a legacy mechanism and are not widely used.

All build rules can declare which "environments" they can be built for, where an
"environment" is an instance of the `environment()` rule.

There are various ways supported environments can be specified for a rule:

1.  Through the `restricted_to=` attribute. This is the most direct form of
    specification; it declares the exact set of environments the rule supports.
2.  Through the `compatible_with=` attribute. This declares environments a rule
    supports in addition to "standard" environments that are supported by
    default.
3.  Through the package-level attributes `default_restricted_to=` and
    `default_compatible_with=`.
4.  Through default specifications in `environment_group()` rules. Every
    environment belongs to a group of thematically related peers (such as "CPU
    architectures", "JDK versions" or "mobile operating systems"). The
    definition of an environment group includes which of these environments
    should be supported by "default" if not otherwise specified by the
    `restricted_to=` / `environment()` attributes. A rule with no such
    attributes inherits all defaults.
5.  Through a rule class default. This overrides global defaults for all
    instances of the given rule class. This can be used, for example, to make
    all `*_test` rules testable without each instance having to explicitly
    declare this capability.

`environment()` is implemented as a regular rule whereas `environment_group()`
is both a subclass of `Target` but not `Rule` (`EnvironmentGroup`) and a
function that is available by default from Starlark
(`StarlarkLibrary.environmentGroup()`) which eventually creates an eponymous
target. This is to avoid a cyclic dependency that would arise because each
environment needs to declare the environment group it belongs to and each
environment group needs to declare its default environments.

A build can be restricted to a certain environment with the
`--target_environment` command line option.

The implementation of the constraint check is in
`RuleContextConstraintSemantics` and `TopLevelConstraintSemantics`.

#### Platform constraints

The current "official" way to describe what platforms a target is compatible
with is by using the same constraints used to describe toolchains and platforms.
It was implemented in pull request
[#10945](https://github.com/bazelbuild/bazel/pull/10945).

### Visibility

If you work on a large codebase with a lot of developers (like at Google), you
want to take care to prevent everyone else from arbitrarily depending on your
code. Otherwise, as per [Hyrum's law](https://www.hyrumslaw.com/),
people _will_ come to rely on behaviors that you considered to be implementation
details.

Bazel supports this by the mechanism called _visibility_: you can limit which
targets can depend on a particular target using the
[visibility](/reference/be/common-definitions#common-attributes) attribute. This
attribute is a little special because, although it holds a list of labels, these
labels may encode a pattern over package names rather than a pointer to any
particular target. (Yes, this is a design flaw.)

This is implemented in the following places:

*   The `RuleVisibility` interface represents a visibility declaration. It can
    be either a constant (fully public or fully private) or a list of labels.
*   Labels can refer to either package groups (predefined list of packages), to
    packages directly (`//pkg:__pkg__`) or subtrees of packages
    (`//pkg:__subpackages__`). This is different from the command line syntax,
    which uses `//pkg:*` or `//pkg/...`.
*   Package groups are implemented as their own target (`PackageGroup`) and
    configured target (`PackageGroupConfiguredTarget`). We could probably
    replace these with simple rules if we wanted to. Their logic is implemented
    with the help of: `PackageSpecification`, which corresponds to a
    single pattern like `//pkg/...`; `PackageGroupContents`, which corresponds
    to a single `package_group`'s `packages` attribute; and
    `PackageSpecificationProvider`, which aggregates over a `package_group` and
    its transitive `includes`.
*   The conversion from visibility label lists to dependencies is done in
    `DependencyResolver.visitTargetVisibility` and a few other miscellaneous
    places.
*   The actual check is done in
    `CommonPrerequisiteValidator.validateDirectPrerequisiteVisibility()`

### Nested sets

Oftentimes, a configured target aggregates a set of files from its dependencies,
adds its own, and wraps the aggregate set into a transitive info provider so
that configured targets that depend on it can do the same. Examples:

*   The C++ header files used for a build
*   The object files that represent the transitive closure of a `cc_library`
*   The set of .jar files that need to be on the classpath for a Java rule to
    compile or run
*   The set of Python files in the transitive closure of a Python rule

If we did this the naive way by using, for example, `List` or `Set`, we'd end up with
quadratic memory usage: if there is a chain of N rules and each rule adds a
file, we'd have 1+2+...+N collection members.

In order to get around this problem, we came up with the concept of a
`NestedSet`. It's a data structure that is composed of other `NestedSet`
instances and some members of its own, thereby forming a directed acyclic graph
of sets. They are immutable and their members can be iterated over. We define
multiple iteration order (`NestedSet.Order`): preorder, postorder, topological
(a node always comes after its ancestors) and "don't care, but it should be the
same each time".

The same data structure is called `depset` in Starlark.

### Artifacts and Actions

The actual build consists of a set of commands that need to be run to produce
the output the user wants. The commands are represented as instances of the
class `Action` and the files are represented as instances of the class
`Artifact`. They are arranged in a bipartite, directed, acyclic graph called the
"action graph".

Artifacts come in two kinds: source artifacts (ones that are available
before Bazel starts executing) and derived artifacts (ones that need to be
built). Derived artifacts can themselves be multiple kinds:

1.  **Regular artifacts.** These are checked for up-to-dateness by computing
    their checksum, with mtime as a shortcut; we don't checksum the file if its
    ctime hasn't changed.
2.  **Unresolved symlink artifacts.** These are checked for up-to-dateness by
    calling readlink(). Unlike regular artifacts, these can be dangling
    symlinks. Usually used in cases where one then packs up some files into an
    archive of some sort.
3.  **Tree artifacts.** These are not single files, but directory trees. They
    are checked for up-to-dateness by checking the set of files in it and their
    contents. They are represented as a `TreeArtifact`.
4.  **Constant metadata artifacts.** Changes to these artifacts don't trigger a
    rebuild. This is used exclusively for build stamp information: we don't want
    to do a rebuild just because the current time changed.

There is no fundamental reason why source artifacts cannot be tree artifacts or
unresolved symlink artifacts; it's just that we haven't implemented it yet. At
the moment, source symlinks are always resolved, and while source directories
are supported, their contents are entirely opaque to build rules and thus don't
support the same kind of lazy command line expansion as tree artifacts do.

Actions are best understood as a command that needs to be run, the environment
it needs and the set of outputs it produces. The following things are the main
components of the description of an action:

*   The command line that needs to be run
*   The input artifacts it needs
*   The environment variables that need to be set
*   Annotations that describe the environment (such as platform) it needs to run in
    \

There are also a few other special cases, like writing a file whose content is
known to Bazel. They are a subclass of `AbstractAction`. Most of the actions are
a `SpawnAction` or a `StarlarkAction` (the same, they should arguably not be
separate classes), although Java and C++ have their own action types
(`JavaCompileAction`, `CppCompileAction` and `CppLinkAction`).

We eventually want to move everything to `SpawnAction`; `JavaCompileAction` is
pretty close, but C++ is a bit of a special-case due to .d file parsing and
include scanning.

The action graph is mostly "embedded" into the Skyframe graph: conceptually, the
execution of an action is represented as an invocation of
`ActionExecutionFunction`. The mapping from an action graph dependency edge to a
Skyframe dependency edge is described in
`ActionExecutionFunction.getInputDeps()` and `Artifact.key()` and has a few
optimizations in order to keep the number of Skyframe edges low:

*   Derived artifacts do not have their own `SkyValue`s. Instead,
    `Artifact.getGeneratingActionKey()` is used to find out the key for the
    action that generates it
*   Nested sets have their own Skyframe key.

### Shared actions

Some actions are generated by multiple configured targets; Starlark rules are
more limited since they are only allowed to put their derived actions into a
directory determined by their configuration and their package (but even so,
rules in the same package can conflict), but rules implemented in Java can put
derived artifacts anywhere.

This is considered to be a misfeature, but getting rid of it is really hard
because it produces significant savings in execution time when, for example, a
source file needs to be processed somehow and that file is referenced by
multiple rules (handwave-handwave). This comes at the cost of some RAM: each
instance of a shared action needs to be stored in memory separately.

If two actions generate the same output file, they must be exactly the same:
have the same inputs, the same outputs and run the same command line. This
equivalence relation is implemented in `Actions.canBeShared()` and it is
verified between the analysis and execution phases by looking at every Action.
This is implemented in `SkyframeActionExecutor.findAndStoreArtifactConflicts()`
and is one of the few places in Bazel that requires a "global" view of the
build.

## The execution phase

This is when Bazel actually starts running build actions, such as commands that
produce outputs.

The first thing Bazel does after the analysis phase is to determine what
Artifacts need to be built. The logic for this is encoded in
`TopLevelArtifactHelper`; roughly speaking, it's the `filesToBuild` of the
configured targets on the command line and the contents of a special output
group for the explicit purpose of expressing "if this target is on the command
line, build these artifacts".

The next step is creating the execution root. Since Bazel has the option to read
source packages from different locations in the file system (`--package_path`),
it needs to provide locally executed actions with a full source tree. This is
handled by the class `SymlinkForest` and works by taking note of every target
used in the analysis phase and building up a single directory tree that symlinks
every package with a used target from its actual location. An alternative would
be to pass the correct paths to commands (taking `--package_path` into account).
This is undesirable because:

*   It changes action command lines when a package is moved from a package path
    entry to another (used to be a common occurrence)
*   It results in different command lines if an action is run remotely than if
    it's run locally
*   It requires a command line transformation specific to the tool in use
    (consider the difference between such as Java classpaths and C++ include paths)
*   Changing the command line of an action invalidates its action cache entry
*   `--package_path` is slowly and steadily being deprecated

Then, Bazel starts traversing the action graph (the bipartite, directed graph
composed of actions and their input and output artifacts) and running actions.
The execution of each action is represented by an instance of the `SkyValue`
class `ActionExecutionValue`.

Since running an action is expensive, we have a few layers of caching that can
be hit behind Skyframe:

*   `ActionExecutionFunction.stateMap` contains data to make Skyframe restarts
    of `ActionExecutionFunction` cheap
*   The local action cache contains data about the state of the file system
*   Remote execution systems usually also contain their own cache

### The local action cache

This cache is another layer that sits behind Skyframe; even if an action is
re-executed in Skyframe, it can still be a hit in the local action cache. It
represents the state of the local file system and it's serialized to disk which
means that when one starts up a new Bazel server, one can get local action cache
hits even though the Skyframe graph is empty.

This cache is checked for hits using the method
`ActionCacheChecker.getTokenIfNeedToExecute()` .

Contrary to its name, it's a map from the path of a derived artifact to the
action that emitted it. The action is described as:

1.  The set of its input and output files and their checksum
2.  Its "action key", which is usually the command line that was executed, but
    in general, represents everything that's not captured by the checksum of the
    input files (such as for `FileWriteAction`, it's the checksum of the data
    that's written)

There is also a highly experimental "top-down action cache" that is still under
development, which uses transitive hashes to avoid going to the cache as many
times.

### Input discovery and input pruning

Some actions are more complicated than just having a set of inputs. Changes to
the set of inputs of an action come in two forms:

*   An action may discover new inputs before its execution or decide that some
    of its inputs are not actually necessary. The canonical example is C++,
    where it's better to make an educated guess about what header files a C++
    file uses from its transitive closure so that we don't heed to send every
    file to remote executors; therefore, we have an option not to register every
    header file as an "input", but scan the source file for transitively
    included headers and only mark those header files as inputs that are
    mentioned in `#include` statements (we overestimate so that we don't need to
    implement a full C preprocessor) This option is currently hard-wired to
    "false" in Bazel and is only used at Google.
*   An action may realize that some files were not used during its execution. In
    C++, this is called ".d files": the compiler tells which header files were
    used after the fact, and in order to avoid the embarrassment of having worse
    incrementality than Make, Bazel makes use of this fact. This offers a better
    estimate than the include scanner because it relies on the compiler.

These are implemented using methods on Action:

1.  `Action.discoverInputs()` is called. It should return a nested set of
    Artifacts that are determined to be required. These must be source artifacts
    so that there are no dependency edges in the action graph that don't have an
    equivalent in the configured target graph.
2.  The action is executed by calling `Action.execute()`.
3.  At the end of `Action.execute()`, the action can call
    `Action.updateInputs()` to tell Bazel that not all of its inputs were
    needed. This can result in incorrect incremental builds if a used input is
    reported as unused.

When an action cache returns a hit on a fresh Action instance (such as created
after a server restart), Bazel calls `updateInputs()` itself so that the set of
inputs reflects the result of input discovery and pruning done before.

Starlark actions can make use of the facility to declare some inputs as unused
using the `unused_inputs_list=` argument of
`ctx.actions.run()`.

### Various ways to run actions: Strategies/ActionContexts

Some actions can be run in different ways. For example, a command line can be
executed locally, locally but in various kinds of sandboxes, or remotely. The
concept that embodies this is called an `ActionContext` (or `Strategy`, since we
successfully went only halfway with a rename...)

The life cycle of an action context is as follows:

1.  When the execution phase is started, `BlazeModule` instances are asked what
    action contexts they have. This happens in the constructor of
    `ExecutionTool`. Action context types are identified by a Java `Class`
    instance that refers to a sub-interface of `ActionContext` and which
    interface the action context must implement.
2.  The appropriate action context is selected from the available ones and is
    forwarded to `ActionExecutionContext` and `BlazeExecutor` .
3.  Actions request contexts using `ActionExecutionContext.getContext()` and
    `BlazeExecutor.getStrategy()` (there should really be only one way to do
    it…)

Strategies are free to call other strategies to do their jobs; this is used, for
example, in the dynamic strategy that starts actions both locally and remotely,
then uses whichever finishes first.

One notable strategy is the one that implements persistent worker processes
(`WorkerSpawnStrategy`). The idea is that some tools have a long startup time
and should therefore be reused between actions instead of starting one anew for
every action (This does represent a potential correctness issue, since Bazel
relies on the promise of the worker process that it doesn't carry observable
state between individual requests)

If the tool changes, the worker process needs to be restarted. Whether a worker
can be reused is determined by computing a checksum for the tool used using
`WorkerFilesHash`. It relies on knowing which inputs of the action represent
part of the tool and which represent inputs; this is determined by the creator
of the Action: `Spawn.getToolFiles()` and the runfiles of the `Spawn` are
counted as parts of the tool.

More information about strategies (or action contexts!):

*   Information about various strategies for running actions is available
    [here](https://jmmv.dev/2019/12/bazel-strategies.html).
*   Information about the dynamic strategy, one where we run an action both
    locally and remotely to see whichever finishes first is available
    [here](https://jmmv.dev/series.html#Bazel%20dynamic%20execution).
*   Information about the intricacies of executing actions locally is available
    [here](https://jmmv.dev/2019/11/bazel-process-wrapper.html).

### The local resource manager

Bazel _can_ run many actions in parallel. The number of local actions that
_should_ be run in parallel differs from action to action: the more resources an
action requires, the less instances should be running at the same time to avoid
overloading the local machine.

This is implemented in the class `ResourceManager`: each action has to be
annotated with an estimate of the local resources it requires in the form of a
`ResourceSet` instance (CPU and RAM). Then when action contexts do something
that requires local resources, they call `ResourceManager.acquireResources()`
and are blocked until the required resources are available.

A more detailed description of local resource management is available
[here](https://jmmv.dev/2019/12/bazel-local-resources.html).

### The structure of the output directory

Each action requires a separate place in the output directory where it places
its outputs. The location of derived artifacts is usually as follows:

```
$EXECROOT/bazel-out/<configuration>/bin/<package>/<artifact name>
```

How is the name of the directory that is associated with a particular
configuration determined? There are two conflicting desirable properties:

1.  If two configurations can occur in the same build, they should have
    different directories so that both can have their own version of the same
    action; otherwise, if the two configurations disagree about such as the command
    line of an action producing the same output file, Bazel doesn't know which
    action to choose (an "action conflict")
2.  If two configurations represent "roughly" the same thing, they should have
    the same name so that actions executed in one can be reused for the other if
    the command lines match: for example, changes to the command line options to
    the Java compiler should not result in C++ compile actions being re-run.

So far, we have not come up with a principled way of solving this problem, which
has similarities to the problem of configuration trimming. A longer discussion
of options is available
[here](https://docs.google.com/document/d/1fZI7wHoaS-vJvZy9SBxaHPitIzXE_nL9v4sS4mErrG4/edit).
The main problematic areas are Starlark rules (whose authors usually aren't
intimately familiar with Bazel) and aspects, which add another dimension to the
space of things that can produce the "same" output file.

The current approach is that the path segment for the configuration is
`<CPU>-<compilation mode>` with various suffixes added so that configuration
transitions implemented in Java don't result in action conflicts. In addition, a
checksum of the set of Starlark configuration transitions is added so that users
can't cause action conflicts. It is far from perfect. This is implemented in
`OutputDirectories.buildMnemonic()` and relies on each configuration fragment
adding its own part to the name of the output directory.

## Tests

Bazel has rich support for running tests. It supports:

*   Running tests remotely (if a remote execution backend is available)
*   Running tests multiple times in parallel (for deflaking or gathering timing
    data)
*   Sharding tests (splitting test cases in same test over multiple processes
    for speed)
*   Re-running flaky tests
*   Grouping tests into test suites

Tests are regular configured targets that have a TestProvider, which describes
how the test should be run:

*   The artifacts whose building result in the test being run. This is a "cache
    status" file that contains a serialized `TestResultData` message
*   The number of times the test should be run
*   The number of shards the test should be split into
*   Some parameters about how the test should be run (such as the test timeout)

### Determining which tests to run

Determining which tests are run is an elaborate process.

First, during target pattern parsing, test suites are recursively expanded. The
expansion is implemented in `TestsForTargetPatternFunction`. A somewhat
surprising wrinkle is that if a test suite declares no tests, it refers to
_every_ test in its package. This is implemented in `Package.beforeBuild()` by
adding an implicit attribute called `$implicit_tests` to test suite rules.

Then, tests are filtered for size, tags, timeout and language according to the
command line options. This is implemented in `TestFilter` and is called from
`TargetPatternPhaseFunction.determineTests()` during target parsing and the
result is put into `TargetPatternPhaseValue.getTestsToRunLabels()`. The reason
why rule attributes which can be filtered for are not configurable is that this
happens before the analysis phase, therefore, the configuration is not
available.

This is then processed further in `BuildView.createResult()`: targets whose
analysis failed are filtered out and tests are split into exclusive and
non-exclusive tests. It's then put into `AnalysisResult`, which is how
`ExecutionTool` knows which tests to run.

In order to lend some transparency to this elaborate process, the `tests()`
query operator (implemented in `TestsFunction`) is available to tell which tests
are run when a particular target is specified on the command line. It's
unfortunately a reimplementation, so it probably deviates from the above in
multiple subtle ways.

### Running tests

The way the tests are run is by requesting cache status artifacts. This then
results in the execution of a `TestRunnerAction`, which eventually calls the
`TestActionContext` chosen by the `--test_strategy` command line option that
runs the test in the requested way.

Tests are run according to an elaborate protocol that uses environment variables
to tell tests what's expected from them. A detailed description of what Bazel
expects from tests and what tests can expect from Bazel is available
[here](/reference/test-encyclopedia). At the
simplest, an exit code of 0 means success, anything else means failure.

In addition to the cache status file, each test process emits a number of other
files. They are put in the "test log directory" which is the subdirectory called
`testlogs` of the output directory of the target configuration:

*   `test.xml`, a JUnit-style XML file detailing the individual test cases in
    the test shard
*   `test.log`, the console output of the test. stdout and stderr are not
    separated.
*   `test.outputs`, the "undeclared outputs directory"; this is used by tests
    that want to output files in addition to what they print to the terminal.

There are two things that can happen during test execution that cannot during
building regular targets: exclusive test execution and output streaming.

Some tests need to be executed in exclusive mode, for example not in parallel with
other tests. This can be elicited either by adding `tags=["exclusive"]` to the
test rule or running the test with `--test_strategy=exclusive` . Each exclusive
test is run by a separate Skyframe invocation requesting the execution of the
test after the "main" build. This is implemented in
`SkyframeExecutor.runExclusiveTest()`.

Unlike regular actions, whose terminal output is dumped when the action
finishes, the user can request the output of tests to be streamed so that they
get informed about the progress of a long-running test. This is specified by the
`--test_output=streamed` command line option and implies exclusive test
execution so that outputs of different tests are not interspersed.

This is implemented in the aptly-named `StreamedTestOutput` class and works by
polling changes to the `test.log` file of the test in question and dumping new
bytes to the terminal where Bazel rules.

Results of the executed tests are available on the event bus by observing
various events (such as `TestAttempt`, `TestResult` or `TestingCompleteEvent`).
They are dumped to the Build Event Protocol and they are emitted to the console
by `AggregatingTestListener`.

### Coverage collection

Coverage is reported by the tests in LCOV format in the files
`bazel-testlogs/$PACKAGE/$TARGET/coverage.dat` .

To collect coverage, each test execution is wrapped in a script called
`collect_coverage.sh` .

This script sets up the environment of the test to enable coverage collection
and determine where the coverage files are written by the coverage runtime(s).
It then runs the test. A test may itself run multiple subprocesses and consist
of parts written in multiple different programming languages (with separate
coverage collection runtimes). The wrapper script is responsible for converting
the resulting files to LCOV format if necessary, and merges them into a single
file.

The interposition of `collect_coverage.sh` is done by the test strategies and
requires `collect_coverage.sh` to be on the inputs of the test. This is
accomplished by the implicit attribute `:coverage_support` which is resolved to
the value of the configuration flag `--coverage_support` (see
`TestConfiguration.TestOptions.coverageSupport`)

Some languages do offline instrumentation, meaning that the coverage
instrumentation is added at compile time (such as C++) and others do online
instrumentation, meaning that coverage instrumentation is added at execution
time.

Another core concept is _baseline coverage_. This is the coverage of a library,
binary, or test if no code in it was run. The problem it solves is that if you
want to compute the test coverage for a binary, it is not enough to merge the
coverage of all of the tests because there may be code in the binary that is not
linked into any test. Therefore, what we do is to emit a coverage file for every
binary which contains only the files we collect coverage for with no covered
lines. The default baseline coverage file for a target is at
`bazel-testlogs/$PACKAGE/$TARGET/baseline_coverage.dat`, but rules are
encouraged to generate their own baseline coverage files with more meaningful
content than just the names of the source files.

We track two groups of files for coverage collection for each rule: the set of
instrumented files and the set of instrumentation metadata files.

The set of instrumented files is just that, a set of files to instrument. For
online coverage runtimes, this can be used at runtime to decide which files to
instrument. It is also used to implement baseline coverage.

The set of instrumentation metadata files is the set of extra files a test needs
to generate the LCOV files Bazel requires from it. In practice, this consists of
runtime-specific files; for example, gcc emits .gcno files during compilation.
These are added to the set of inputs of test actions if coverage mode is
enabled.

Whether or not coverage is being collected is stored in the
`BuildConfiguration`. This is handy because it is an easy way to change the test
action and the action graph depending on this bit, but it also means that if
this bit is flipped, all targets need to be re-analyzed (some languages, such as
C++ require different compiler options to emit code that can collect coverage,
which mitigates this issue somewhat, since then a re-analysis is needed anyway).

The coverage support files are depended on through labels in an implicit
dependency so that they can be overridden by the invocation policy, which allows
them to differ between the different versions of Bazel. Ideally, these
differences would be removed, and we standardized on one of them.

We also generate a "coverage report" which merges the coverage collected for
every test in a Bazel invocation. This is handled by
`CoverageReportActionFactory` and is called from `BuildView.createResult()` . It
gets access to the tools it needs by looking at the `:coverage_report_generator`
attribute of the first test that is executed.

## The query engine

Bazel has a
[little language](/query/guide)
used to ask it various things about various graphs. The following query kinds
are provided:

*   `bazel query` is used to investigate the target graph
*   `bazel cquery` is used to investigate the configured target graph
*   `bazel aquery` is used to investigate the action graph

Each of these is implemented by subclassing `AbstractBlazeQueryEnvironment`.
Additional additional query functions can be done by subclassing `QueryFunction`
. In order to allow streaming query results, instead of collecting them to some
data structure, a `query2.engine.Callback` is passed to `QueryFunction`, which
calls it for results it wants to return.

The result of a query can be emitted in various ways: labels, labels and rule
classes, XML, protobuf and so on. These are implemented as subclasses of
`OutputFormatter`.

A subtle requirement of some query output formats (proto, definitely) is that
Bazel needs to emit _all _the information that package loading provides so that
one can diff the output and determine whether a particular target has changed.
As a consequence, attribute values need to be serializable, which is why there
are only so few attribute types without any attributes having complex Starlark
values. The usual workaround is to use a label, and attach the complex
information to the rule with that label. It's not a very satisfying workaround
and it would be very nice to lift this requirement.

## The module system

Bazel can be extended by adding modules to it. Each module must subclass
`BlazeModule` (the name is a relic of the history of Bazel when it used to be
called Blaze) and gets information about various events during the execution of
a command.

They are mostly used to implement various pieces of "non-core" functionality
that only some versions of Bazel (such as the one we use at Google) need:

*   Interfaces to remote execution systems
*   New commands

The set of extension points `BlazeModule` offers is somewhat haphazard. Don't
use it as an example of good design principles.

## The event bus

The main way BlazeModules communicate with the rest of Bazel is by an event bus
(`EventBus`): a new instance is created for every build, various parts of Bazel
can post events to it and modules can register listeners for the events they are
interested in. For example, the following things are represented as events:

*   The list of build targets to be built has been determined
    (`TargetParsingCompleteEvent`)
*   The top-level configurations have been determined
    (`BuildConfigurationEvent`)
*   A target was built, successfully or not (`TargetCompleteEvent`)
*   A test was run (`TestAttempt`, `TestSummary`)

Some of these events are represented outside of Bazel in the
[Build Event Protocol](/remote/bep)
(they are `BuildEvent`s). This allows not only `BlazeModule`s, but also things
outside the Bazel process to observe the build. They are accessible either as a
file that contains protocol messages or Bazel can connect to a server (called
the Build Event Service) to stream events.

This is implemented in the `build.lib.buildeventservice` and
`build.lib.buildeventstream` Java packages.

## External repositories

Note: The information in this section is out of date, as code in this area has
undergone extensive change in the past couple of years. Please refer to
[external dependencies overview](/external/overview) for more up-to-date
information.

Whereas Bazel was originally designed to be used in a monorepo (a single source
tree containing everything one needs to build), Bazel lives in a world where
this is not necessarily true. "External repositories" are an abstraction used to
bridge these two worlds: they represent code that is necessary for the build but
is not in the main source tree.

### The WORKSPACE file

The set of external repositories is determined by parsing the WORKSPACE file.
For example, a declaration like this:

```
    local_repository(name="foo", path="/foo/bar")
```

Results in the repository called `@foo` being available. Where this gets
complicated is that one can define new repository rules in Starlark files, which
can then be used to load new Starlark code, which can be used to define new
repository rules and so on…

To handle this case, the parsing of the WORKSPACE file (in
`WorkspaceFileFunction`) is split up into chunks delineated by `load()`
statements. The chunk index is indicated by `WorkspaceFileKey.getIndex()` and
computing `WorkspaceFileFunction` until index X means evaluating it until the
Xth `load()` statement.

### Fetching repositories

Before the code of the repository is available to Bazel, it needs to be
_fetched_. This results in Bazel creating a directory under
`$OUTPUT_BASE/external/<repository name>`.

Fetching the repository happens in the following steps:

1.  `PackageLookupFunction` realizes that it needs a repository and creates a
    `RepositoryName` as a `SkyKey`, which invokes `RepositoryLoaderFunction`
2.  `RepositoryLoaderFunction` forwards the request to
    `RepositoryDelegatorFunction` for unclear reasons (the code says it's to
    avoid re-downloading things in case of Skyframe restarts, but it's not a
    very solid reasoning)
3.  `RepositoryDelegatorFunction` finds out the repository rule it's asked to
    fetch by iterating over the chunks of the WORKSPACE file until the requested
    repository is found
4.  The appropriate `RepositoryFunction` is found that implements the repository
    fetching; it's either the Starlark implementation of the repository or a
    hard-coded map for repositories that are implemented in Java.

There are various layers of caching since fetching a repository can be very
expensive:

1.  There is a cache for downloaded files that is keyed by their checksum
    (`RepositoryCache`). This requires the checksum to be available in the
    WORKSPACE file, but that's good for hermeticity anyway. This is shared by
    every Bazel server instance on the same workstation, regardless of which
    workspace or output base they are running in.
2.  A "marker file" is written for each repository under `$OUTPUT_BASE/external`
    that contains a checksum of the rule that was used to fetch it. If the Bazel
    server restarts but the checksum does not change, it's not re-fetched. This
    is implemented in `RepositoryDelegatorFunction.DigestWriter` .
3.  The `--distdir` command line option designates another cache that is used to
    look up artifacts to be downloaded. This is useful in enterprise settings
    where Bazel should not fetch random things from the Internet. This is
    implemented by `DownloadManager` .

Once a repository is downloaded, the artifacts in it are treated as source
artifacts. This poses a problem because Bazel usually checks for up-to-dateness
of source artifacts by calling stat() on them, and these artifacts are also
invalidated when the definition of the repository they are in changes. Thus,
`FileStateValue`s for an artifact in an external repository need to depend on
their external repository. This is handled by `ExternalFilesHelper`.

### Repository mappings

It can happen that multiple repositories want to depend on the same repository,
but in different versions (this is an instance of the "diamond dependency
problem"). For example, if two binaries in separate repositories in the build
want to depend on Guava, they will presumably both refer to Guava with labels
starting `@guava//` and expect that to mean different versions of it.

Therefore, Bazel allows one to re-map external repository labels so that the
string `@guava//` can refer to one Guava repository (such as `@guava1//`) in the
repository of one binary and another Guava repository (such as `@guava2//`) the
repository of the other.

Alternatively, this can also be used to **join** diamonds. If a repository
depends on `@guava1//`, and another depends on `@guava2//`, repository mapping
allows one to re-map both repositories to use a canonical `@guava//` repository.

The mapping is specified in the WORKSPACE file as the `repo_mapping` attribute
of individual repository definitions. It then appears in Skyframe as a member of
`WorkspaceFileValue`, where it is plumbed to:

*   `Package.Builder.repositoryMapping` which is used to transform label-valued
    attributes of rules in the package by
    `RuleClass.populateRuleAttributeValues()`
*   `Package.repositoryMapping` which is used in the analysis phase (for
    resolving things like `$(location)` which are not parsed in the loading
    phase)
*   `BzlLoadFunction` for resolving labels in load() statements

## JNI bits

The server of Bazel is _mostly_ written in Java. The exception is the parts that
Java cannot do by itself or couldn't do by itself when we implemented it. This
is mostly limited to interaction with the file system, process control and
various other low-level things.

The C++ code lives under src/main/native and the Java classes with native
methods are:

*   `NativePosixFiles` and `NativePosixFileSystem`
*   `ProcessUtils`
*   `WindowsFileOperations` and `WindowsFileProcesses`
*   `com.google.devtools.build.lib.platform`

## Console output

Emitting console output seems like a simple thing, but the confluence of running
multiple processes (sometimes remotely), fine-grained caching, the desire to
have a nice and colorful terminal output and having a long-running server makes
it non-trivial.

Right after the RPC call comes in from the client, two `RpcOutputStream`
instances are created (for stdout and stderr) that forward the data printed into
them to the client. These are then wrapped in an `OutErr` (an (stdout, stderr)
pair). Anything that needs to be printed on the console goes through these
streams. Then these streams are handed over to
`BlazeCommandDispatcher.execExclusively()`.

Output is by default printed with ANSI escape sequences. When these are not
desired (`--color=no`), they are stripped by an `AnsiStrippingOutputStream`. In
addition, `System.out` and `System.err` are redirected to these output streams.
This is so that debugging information can be printed using
`System.err.println()` and still end up in the terminal output of the client
(which is different from that of the server). Care is taken that if a process
produces binary output (such as `bazel query --output=proto`), no munging of stdout
takes place.

Short messages (errors, warnings and the like) are expressed through the
`EventHandler` interface. Notably, these are different from what one posts to
the `EventBus` (this is confusing). Each `Event` has an `EventKind` (error,
warning, info, and a few others) and they may have a `Location` (the place in
the source code that caused the event to happen).

Some `EventHandler` implementations store the events they received. This is used
to replay information to the UI caused by various kinds of cached processing,
for example, the warnings emitted by a cached configured target.

Some `EventHandler`s also allow posting events that eventually find their way to
the event bus (regular `Event`s do _not _appear there). These are
implementations of `ExtendedEventHandler` and their main use is to replay cached
`EventBus` events. These `EventBus` events all implement `Postable`, but not
everything that is posted to `EventBus` necessarily implements this interface;
only those that are cached by an `ExtendedEventHandler` (it would be nice and
most of the things do; it's not enforced, though)

Terminal output is _mostly_ emitted through `UiEventHandler`, which is
responsible for all the fancy output formatting and progress reporting Bazel
does. It has two inputs:

*   The event bus
*   The event stream piped into it through Reporter

The only direct connection the command execution machinery (for example the rest of
Bazel) has to the RPC stream to the client is through `Reporter.getOutErr()`,
which allows direct access to these streams. It's only used when a command needs
to dump large amounts of possible binary data (such as `bazel query`).

## Profiling Bazel

Bazel is fast. Bazel is also slow, because builds tend to grow until just the
edge of what's bearable. For this reason, Bazel includes a profiler which can be
used to profile builds and Bazel itself. It's implemented in a class that's
aptly named `Profiler`. It's turned on by default, although it records only
abridged data so that its overhead is tolerable; The command line
`--record_full_profiler_data` makes it record everything it can.

It emits a profile in the Chrome profiler format; it's best viewed in Chrome.
It's data model is that of task stacks: one can start tasks and end tasks and
they are supposed to be neatly nested within each other. Each Java thread gets
its own task stack. **TODO:** How does this work with actions and
continuation-passing style?

The profiler is started and stopped in `BlazeRuntime.initProfiler()` and
`BlazeRuntime.afterCommand()` respectively and attempts to be live for as long
as possible so that we can profile everything. To add something to the profile,
call `Profiler.instance().profile()`. It returns a `Closeable`, whose closure
represents the end of the task. It's best used with try-with-resources
statements.

We also do rudimentary memory profiling in `MemoryProfiler`. It's also always on
and it mostly records maximum heap sizes and GC behavior.

## Testing Bazel

Bazel has two main kinds of tests: ones that observe Bazel as a "black box" and
ones that only run the analysis phase. We call the former "integration tests"
and the latter "unit tests", although they are more like integration tests that
are, well, less integrated. We also have some actual unit tests, where they are
necessary.

Of integration tests, we have two kinds:

1.  Ones implemented using a very elaborate bash test framework under
    `src/test/shell`
2.  Ones implemented in Java. These are implemented as subclasses of
    `BuildIntegrationTestCase`

`BuildIntegrationTestCase` is the preferred integration testing framework as it
is well-equipped for most testing scenarios. As it is a Java framework, it
provides debuggability and seamless integration with many common development
tools. There are many examples of `BuildIntegrationTestCase` classes in the
Bazel repository.

Analysis tests are implemented as subclasses of `BuildViewTestCase`. There is a
scratch file system you can use to write `BUILD` files, then various helper
methods can request configured targets, change the configuration and assert
various things about the result of the analysis.

---

## Design Documents
- URL: https://bazel.build/contribute/design-documents
- Source: contribute/design-documents.mdx
- Slug: /contribute/design-documents

If you're planning to add, change, or remove a user-facing feature, or make a
*significant architectural change* to Bazel, you **must** write a design
document and have it reviewed before you can submit the change.

Here are some examples of significant changes:

*   Addition or deletion of native build rules
*   Breaking-changes to native rules
*   Changes to a native build rule semantics that affect the behavior of more
    than a single rule
*   Changes to Bazel's rule definition API
*   Changes to the APIs that Bazel uses to connect to other systems
*   Changes to the Starlark language, semantics, or APIs
*   Changes that could have a pervasive effect on Bazel performance or memory
    usage (for better or for worse)
*   Changes to widely used internal APIs
*   Changes to flags and command-line interface.

## Reasons for design reviews

When you write a design document, you can coordinate with other Bazel developers
and seek guidance from Bazel's core team. For example, when a proposal adds,
removes, or modifies any function or object available in BUILD, MODULE.bazel, or
bzl files, add the [Starlark team](maintainers-guide.md) as reviewers.
Design documents are reviewed before submission because:

*   Bazel is a very complex system; seemingly innocuous local changes can have
    significant global consequences.
*   The team gets many feature requests from users; such requests need to be
    evaluated not only for technical feasibility but importance with regards to
    other feature requests.
*   Bazel features are frequently implemented by people outside the core team;
    such contributors have widely varying levels of Bazel expertise.
*   The Bazel team itself has varying levels of expertise; no single team member
    has a complete understanding of every corner of Bazel.
*   Changes to Bazel must account for backward compatibility and avoid breaking
    changes.

Bazel's design review policy helps to maximize the likelihood that:

*   all feature requests get a baseline level of scrutiny.
*   the right people will weigh in on designs before we've invested in an
    implementation that may not work.

To help you get started, take a look at the design documents in the
[Bazel Proposals Repository](https://github.com/bazelbuild/proposals).
Designs are works in progress, so implementation details can change over time
and with feedback. The published design documents capture the initial design,
and *not* the ongoing changes as designs are implemented. Always go to the
documentation for descriptions of current Bazel functionality.

## Contributor Workflow

As a contributor, you can write a design document, send pull requests and
request reviewers for your proposal.

### Write the design document

All design documents must have a header that includes:

*   author
*   date of last major change
*   list of reviewers, including one (and only one)
    [lead reviewer](#lead-reviewer)
*   current status (_draft_, _in review_, _approved_, _rejected_,
    _being implemented_, _implemented_)
*   link to discussion thread (_to be added after the announcement_)

The document can be written either [as a world-readable Google Doc](#gdocs)
or [using Markdown](#markdown). Read below about for a
[Markdown / Google Docs comparison](#markdown-versus-gdocs).

Proposals that have a user-visible impact must have a section documenting the
impact on backward compatibility (and a rollout plan if needed).

### Create a Pull Request

Share your design doc by creating a pull request (PR) to add the document to
[the design index](https://github.com/bazelbuild/proposals). Add
your markdown file or a document link to your PR.

When possible, [choose a lead reviewer](#lead-reviewer).
and cc other reviewers. If you don't choose a lead reviewer, a Bazel
maintainer will assign one to your PR.

After you create your PR, reviewers can make preliminary comments during the
code review. For example, the lead reviewer can suggest extra reviewers, or
point out missing information. The lead reviewer approves the PR when they
believe the review process can start. This doesn't mean the proposal is perfect
or will be approved; it means that the proposal contains enough information to
start the discussion.

### Announce the new proposal

Send an announcement to
[bazel-dev](https://groups.google.com/forum/#!forum/bazel-dev) when
the PR is submitted.

You may copy other groups (for example,
[bazel-discuss](https://groups.google.com/forum/#!forum/bazel-discuss),
to get feedback from Bazel end-users).

### Iterate with reviewers

Anyone interested can comment on your proposal. Try to answer questions,
clarify the proposal, and address concerns.

Discussion should happen on the announcement thread. If the proposal is in a
Google Doc, comments may be used instead (Note that anonymous comments are
allowed).

### Update the status

Create a new PR to update the status of the proposal, when iteration is
complete. Send the PR to the same lead reviewer and cc the other reviewers.

To officially accept the proposal, the lead reviewer approves the PR after
ensuring that the other reviewers agree with the decision.

There must be at least 1 week between the first announcement and the approval of
a proposal. This ensures that users had enough time to read the document and
share their concerns.

Implementation can begin before the proposal is accepted, for example as a
proof-of-concept or an experimentation. However, you cannot submit the change
before the review is complete.

### Choosing a lead reviewer

A lead reviewer should be a domain expert who is:

*   Knowledgeable of the relevant subsystems
*   Objective and capable of providing constructive feedback
*   Available for the entire review period to lead the process

Consider checking the contacts for various [team
labels](/contribute/maintainers-guide#team-labels).

## Markdown vs Google Docs

Decide what works best for you, since both are accepted.

Benefits of using Google Docs:

* Effective for brainstorming, since it is easy to get started with.
* Collaborative editing.
* Quick iteration.
* Easy way to suggest edits.

Benefits of using Markdown files:

*   Clean URLs for linking.
*   Explicit record of revisions.
*   No forgetting to set up access rights before publicizing a link.
*   Easily searchable with search engines.
*   Future-proof: Plain text is not at the mercy of any specific tool
    and doesn't require an Internet connection.
*   It is possible to update them even if the author is not around anymore.
*   They can be processed automatically (update/detect dead links, fetch
    list of authors, etc.).

You can choose to first iterate on a Google Doc, and then convert it to
Markdown for posterity.

### Using Google Docs

For consistency, use the [Bazel design doc template](
https://docs.google.com/document/d/1cE5zrjrR40RXNg64XtRFewSv6FrLV6slGkkqxBumS1w/edit).
It includes the necessary header and creates visual
consistency with other Bazel related documents. To do that, click on **File** >
**Make a copy** or click this link to [make a copy of the design doc
template](https://docs.google.com/document/d/1cE5zrjrR40RXNg64XtRFewSv6FrLV6slGkkqxBumS1w/copy).

To make your document readable to the world, click on
**Share** > **Advanced** > **Change…**, and
choose "On - Anyone with the link".  If you allow comments on the document,
anyone can comment anonymously, even without a Google account.

### Using Markdown

Documents are stored on GitHub and use the
[GitHub flavor of Markdown](https://guides.github.com/features/mastering-markdown/)
([Specification](https://github.github.com/gfm/)).

Create a PR to update an existing document. Significant changes should be
reviewed by the document reviewers. Trivial changes (such as typos, formatting)
can be approved by anyone.

## Reviewer workflow

A reviewer comments, reviews and approves design documents.

### General reviewer responsibilities

You're responsible for reviewing design documents, asking for additional
information if needed, and approving a design that passes the review process.

#### When you receive a new proposal

1.  Take a quick look at the document.
1.  Comment if critical information is missing, or if the design doesn't fit
    with the goals of the project.
1.  Suggest additional reviewers.
1.  Approve the PR when it is ready for review.

#### During the review process

1. Engage in a dialogue with the design author about issues that are problematic
   or require clarification.
1. If appropriate, invite comments from non-reviewers who should be aware of
   the design.
1. Decide which comments must be addressed by the author as a prerequisite to
   approval.
1. Write "LGTM" (_Looks Good To Me_) in the discussion thread when you are
   happy with the current state of the proposal.

Follow this process for all design review requests. Do not approve designs
affecting Bazel if they are not in the
[design index](https://github.com/bazelbuild/proposals).

### Lead reviewer responsibilities

You're responsible for making the go / no-go decision on implementation
of a pending design. If you're not able to do this, you should identify a
suitable delegate (reassign the PR to the delegate), or reassign the bug to a
Bazel manager for further disposition.

#### During the review process

1.  Ensure that the comment and design iteration process moves forward
    constructively.
1.  Prior to approval, ensure that concerns from other reviewers have been
    resolved.

#### After approval by all reviewers

1.  Make sure there has been at least 1 week since the announcement on the
    mailing list.
1.  Make sure the PR updates the status.
1.  Approve the PR sent by the proposal author.

#### Rejecting designs

1.  Make sure the PR author sends a PR; or send them a PR.
1.  The PR updates the status of the document.
1.  Add a comment to the document explaining why the design can't be approved in
    its current state, and outlining next steps, if any (such as "revisit invalid
    assumptions and resubmit").

---

## Contribute to Bazel documentation
- URL: https://bazel.build/contribute/docs
- Source: contribute/docs.mdx
- Slug: /contribute/docs

Thank you for contributing to Bazel's documentation! There are a few ways to
help create better docs for our community.

## Documentation types

This site includes a few types of content.

 - *Narrative documentation*, which is written by technical writers and
   engineers. Most of this site is narrative documentation that covers
   conceptual and task-based guides.
 - *Reference documentation*, which is generated documentation from code comments.
   You can't make changes to the reference doc pages directly, but instead need
   to change their source.

## Documentation infrastructure

Bazel documentation is served from Google and the source files are mirrored in
Bazel's GitHub repository. You can make changes to the source files in GitHub.
If approved, you can merge the changes and a Bazel maintainer will update the
website source to publish your updates.


## Small changes

You can approach small changes, such as fixing errors or typos, in a couple of
ways.

 - **Pull request**. You can create a pull request in GitHub with the
   [web-based editor](https://docs.github.com/repositories/working-with-files/managing-files/editing-files) or on a branch.
 - **Bug**. You can file a bug with details and suggested changes and the Bazel
   documentation owners will make the update.

## Large changes

If you want to make substantial changes to existing documentation or propose
new documentation, you can either create a pull request or start with a Google
doc and contact the Bazel Owners to collaborate.

---

## Guide for Bazel Maintainers
- URL: https://bazel.build/contribute/maintainers-guide
- Source: contribute/maintainers-guide.mdx
- Slug: /contribute/maintainers-guide

This is a guide for the maintainers of the Bazel open source project.

If you are looking to contribute to Bazel, please read [Contributing to
Bazel](/contribute) instead.

The objectives of this page are to:

1. Serve as the maintainers' source of truth for the project’s contribution
   process.
1. Set expectations between the community contributors and the project
   maintainers.

Bazel's [core group of contributors](/contribute/policy) has dedicated
subteams to manage aspects of the open source project. These are:

* **Release Process**: Manage Bazel's release process.
* **Green Team**: Grow a healthy ecosystem of rules and tools.
* **Developer Experience Gardeners**: Encourage external contributions, review
  issues and pull requests, and make our development workflow more open.

## Releases

* [Release Playbook](https://github.com/bazelbuild/continuous-integration/blob/master/docs/release-playbook.md)
* [Testing local changes with downstream projects](https://github.com/bazelbuild/continuous-integration/blob/master/docs/downstream-testing.md)

## Continuous Integration

Read the Green team's guide to Bazel's CI infrastructure on the
[bazelbuild/continuous-integration](https://github.com/bazelbuild/continuous-integration/blob/master/buildkite/README.md)
repository.

## Lifecycle of an Issue

1. A user creates an issue by choosing one of the
[issue templates](https://github.com/bazelbuild/bazel/issues/new/choose)
   and it enters the pool of [unreviewed open
   issues](https://github.com/bazelbuild/bazel/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+-label%3Auntriaged+-label%3Ap2+-label%3Ap1+-label%3Ap3+-label%3Ap4+-label%3Ateam-Starlark+-label%3Ateam-Rules-CPP+-label%3Ateam-Rules-Java+-label%3Ateam-XProduct+-label%3Ateam-Android+-label%3Ateam-Apple+-label%3Ateam-Configurability++-label%3Ateam-Performance+-label%3Ateam-Rules-Server+-label%3Ateam-Core+-label%3Ateam-Rules-Python+-label%3Ateam-Remote-Exec+-label%3Ateam-Local-Exec+-label%3Ateam-Bazel).
1. A member on the Developer Experience (DevEx) subteam rotation reviews the
   issue.
   1. If the issue is **not a bug** or a **feature request**, the DevEx member
      will usually close the issue and redirect the user to
      [StackOverflow](https://stackoverflow.com/questions/tagged/bazel) and
      [bazel-discuss](https://groups.google.com/forum/#!forum/bazel-discuss) for
      higher visibility on the question.
   1. If the issue belongs in one of the rules repositories owned by the
      community, like [rules_apple](https://github.com.bazelbuild/rules_apple),
      the DevEx member will [transfer this issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/transferring-an-issue-to-another-repository)
      to the correct repository.
   1. If the issue is vague or has missing information, the DevEx member will
      assign the issue back to the user to request for more information before
      continuing. This usually occurs when the user does not choose the right
      [issue template](https://github.com/bazelbuild/bazel/issues/new/choose)
       or provides incomplete information.
1. After reviewing the issue, the DevEx member decides if the issue requires
   immediate attention. If it does, they will assign the **P0**
   [priority](#priority) label and an owner from the list of team leads.
1. The DevEx member assigns the `untriaged` label and exactly one [team
   label](#team-labels) for routing.
1. The DevEx member also assigns exactly one `type:` label, such as `type: bug`
   or `type: feature request`, according to the type of the issue.
1. For platform-specific issues, the DevEx member assigns one `platform:` label,
   such as `platform:apple` for Mac-specific issues.
1. If the issue is low priority and can be worked on by a new community
   contributor, the DevEx member assigns the `good first issue` label.
At this stage, the issue enters the pool of [untriaged open
issues](https://github.com/bazelbuild/bazel/issues?q=is%3Aissue+is%3Aopen+label%3Auntriaged).

Each Bazel subteam will triage all issues under labels they own, preferably on a
weekly basis. The subteam will review and evaluate the issue and provide a
resolution, if possible. If you are an owner of a team label, see [this section
](#label-own) for more information.

When an issue is resolved, it can be closed.

## Lifecycle of a Pull Request

1. A user creates a pull request.
1. If you a member of a Bazel team and sending a PR against your own area,
   you are responsible for assigning your team label and finding the best
   reviewer.
1. Otherwise, during daily triage, a DevEx member assigns one
   [team label](#team-labels) and the team's technical lead (TL) for routing.
   1. The TL may optionally assign someone else to review the PR.
1. The assigned reviewer reviews the PR and works with the author until it is
   approved or dropped.
1. If approved, the reviewer **imports** the PR's commit(s) into Google's
   internal version control system for further tests. As Bazel is the same build
   system used internally at Google, we need to test all PR commits against the
   internal test suite. This is the reason why we do not merge PRs directly.
1. If the imported commit passes all internal tests, the commit will be squashed
   and exported back out to GitHub.
1. When the commit merges into master, GitHub automatically closes the PR.


## My team owns a label. What should I do?

Subteams need to triage all issues in the [labels they own](#team-labels),
preferably on a weekly basis.

### Issues

1. Filter the list of issues by your team label **and** the `untriaged` label.
1. Review the issue.
1. Identify a [priority level](#priority) and assign the label.
  1. The issue may have already been prioritized by the DevEx subteam if it's a
     P0. Re-prioritize if needed.
  1. Each issue needs to have exactly one [priority label](#priority). If an
     issue is either P0 or P1 we assume that is actively worked on.
1. Remove the `untriaged` label.

Note that you need to be in the [bazelbuild
organization](https://github.com/bazelbuild) to be able to add or remove labels.

### Pull Requests

1. Filter the list of pull requests by your team label.
1. Review open pull requests.
  1. **Optional**: If you are assigned for the review but is not the right fit
  for it, re-assign the appropriate reviewer to perform a code review.
1. Work with the pull request creator to complete a code review.
1. Approve the PR.
1. Ensure that all tests pass.
1. Import the patch to the internal version control system and run the internal
   presubmits.
1. Submit the internal patch. If the patch submits and exports successfully, the
   PR will be closed automatically by GitHub.

## Priority

The following definitions for priority will be used by the maintainers to triage
issues.

* [**P0**](https://github.com/bazelbuild/bazel/labels/P0) - Major broken
  functionality that causes a Bazel release (minus release candidates) to be
  unusable, or a downed service that severely impacts development of the Bazel
  project. This includes regressions introduced in a new release that blocks a
  significant number of users, or an incompatible breaking change that was not
  compliant to the [Breaking
  Change](https://docs.google.com/document/d/1q5GGRxKrF_mnwtaPKI487P8OdDRh2nN7jX6U-FXnHL0/edit?pli=1#heading=h.ceof6vpkb3ik)
  policy. No practical workaround exists.
* [**P1**](https://github.com/bazelbuild/bazel/labels/P1) - Critical defect or
  feature which should be addressed in the next release, or a serious issue that
  impacts many users (including the development of the Bazel project), but a
  practical workaround exists. Typically does not require immediate action. In
  high demand and planned in the current quarter's roadmap.
* [**P2**](https://github.com/bazelbuild/bazel/labels/P2) - Defect or feature
  that should be addressed but we don't currently work on. Moderate live issue
  in a released Bazel version that is inconvenient for a user that needs to be
  addressed in an future release and/or an easy workaround exists.
* [**P3**](https://github.com/bazelbuild/bazel/labels/P3) - Desirable minor bug
  fix or enhancement with small impact. Not prioritized into Bazel roadmaps or
  any imminent release, however community contributions are encouraged.
* [**P4**](https://github.com/bazelbuild/bazel/labels/P4) - Low priority defect
  or feature request that is unlikely to get closed. Can also be kept open for a
  potential re-prioritization if more users are impacted.

## Team labels

*   [`team-Android`](https://github.com/bazelbuild/bazel/labels/team-Android): Issues for Android team
    *   Contact: [ahumesky](https://github.com/ahumesky)
*   [`team-Bazel`](https://github.com/bazelbuild/bazel/labels/team-Bazel): General Bazel product/strategy issues
    * Contact: [meisterT](https://github.com/meisterT)
*   [`team-CLI`](https://github.com/bazelbuild/bazel/labels/team-CLI): Console UI
    * Contact: [meisterT](https://github.com/meisterT)
*   [`team-Configurability`](https://github.com/bazelbuild/bazel/labels/team-Configurability): Issues for Configurability team. Includes: Core build configuration and transition system. Does *not* include: Changes to new or existing flags
    * Contact: [gregestren](https://github.com/gregestren)
*   [`team-Core`](https://github.com/bazelbuild/bazel/labels/team-Core): Skyframe, bazel query, BEP, options parsing, bazelrc
    * Contact: [haxorz](https://github.com/haxorz)
*   [`team-Documentation`](https://github.com/bazelbuild/bazel/labels/team-Documentation): Issues for Documentation team
*   [`team-ExternalDeps`](https://github.com/bazelbuild/bazel/labels/team-ExternalDeps): External dependency handling, Bzlmod, remote repositories, WORKSPACE file
    * Contact: [meteorcloudy](https://github.com/meteorcloudy)
*   [`team-Loading-API`](https://github.com/bazelbuild/bazel/labels/team-Loading-API): BUILD file and macro processing: labels, package(), visibility, glob
    * Contact: [brandjon](https://github.com/brandjon)
*   [`team-Local-Exec`](https://github.com/bazelbuild/bazel/labels/team-Local-Exec): Issues for Execution (Local) team
    * Contact: [meisterT](https://github.com/meisterT)
*   [`team-OSS`](https://github.com/bazelbuild/bazel/labels/team-OSS): Issues for Bazel OSS team: installation, release process, Bazel packaging, website, docs infrastructure
    * Contact: [meteorcloudy](https://github.com/meteorcloudy)
*   [`team-Performance`](https://github.com/bazelbuild/bazel/labels/team-Performance): Issues for Bazel Performance team
    * Contact: [meisterT](https://github.com/meisterT)
*   [`team-Remote-Exec`](https://github.com/bazelbuild/bazel/labels/team-Remote-Exec): Issues for Execution (Remote) team
    * Contact: [coeuvre](https://github.com/coeuvre)
*   [`team-Rules-API`](https://github.com/bazelbuild/bazel/labels/team-Rules-API): API for writing rules/aspects: providers, runfiles, actions, artifacts
    * Contact: [comius](https://github.com/comius)
*   [`team-Rules-CPP`](https://github.com/bazelbuild/bazel/labels/team-Rules-CPP) / [`team-Rules-ObjC`](https://github.com/bazelbuild/bazel/labels/team-Rules-ObjC): Issues for C++/Objective-C rules, including native Apple rule logic
    * Contact: [pzembrod](https://github.com/pzembrod)
*   [`team-Rules-Java`](https://github.com/bazelbuild/bazel/labels/team-Rules-Java): Issues for Java rules
    * Contact: [hvadehra](https://github.com/hvadehra)
*   [`team-Rules-Python`](https://github.com/bazelbuild/bazel/labels/team-Rules-Python): Issues for the native Python rules
    * Contact: [rickeylev](https://github.com/rickeylev)
*   [`team-Rules-Server`](https://github.com/bazelbuild/bazel/labels/team-Rules-Server): Issues for server-side rules included with Bazel
    * Contact: [comius](https://github.com/comius)
*   [`team-Starlark-Integration`](https://github.com/bazelbuild/bazel/labels/team-Starlark-Integration): Non-API Bazel + Starlark integration. Includes: how Bazel triggers the Starlark interpreter, Stardoc, builtins injection, character encoding.  Does *not* include: BUILD or .bzl language issues.
    * Contact: [brandjon](https://github.com/brandjon)
*   [`team-Starlark-Interpreter`](https://github.com/bazelbuild/bazel/labels/team-Starlark-Interpreter): Issues for the Starlark interpreter (anything in [java.net.starlark](https://github.com/bazelbuild/bazel/tree/master/src/main/java/net/starlark/java)). BUILD and .bzl API issues (which represent Bazel's *integration* with Starlark) go in `team-Build-Language`.
    * Contact: [brandjon](https://github.com/brandjon)

For new issues, we deprecated the `category: *` labels in favor of the team
labels.

See the full list of labels [here](https://github.com/bazelbuild/bazel/labels).

---

## Naming a Bazel related project
- URL: https://bazel.build/contribute/naming
- Source: contribute/naming.mdx
- Slug: /contribute/naming

First, thank you for contributing to the Bazel ecosystem! Please reach out to
the Bazel community on the
[bazel-discuss mailing list](https://groups.google.com/forum/#!forum/bazel-discuss
) to share your project and its suggested name.

If you are building a Bazel related tool or sharing your Skylark rules,
we recommend following these guidelines for the name of your project:

## Naming Starlark rules

See [Deploying new Starlark rules](/rules/deploying)
in the docs.

## Naming other Bazel related tools

This section applies if you are building a tool to enrich the Bazel ecosystem.
For example, a new IDE plugin or a new build system migrator.

Picking a good name for your tool can be hard. If we’re not careful and use too
many codenames, the Bazel ecosystem could become very difficult to understand
for newcomers.

Follow these guidelines for naming Bazel tools:

1. Prefer **not introducing a new brand name**: "*Bazel*" is already a new brand
for our users, we should avoid confusing them with too many new names.

2. Prefer **using a name that includes "Bazel"**: This helps to express that it
is a Bazel related tool, it also helps people find it with a search engine.

3. Prefer **using names that are descriptive about what the tool is doing**:
Ideally, the name should not need a subtitle for users to have a first good
guess at what the tool does. Using english words separated by spaces is a good
way to achieve this.

4. **It is not a requirement to use a floral or food theme**: Bazel evokes
[basil](https://en.wikipedia.org/wiki/Basil), the plant. You do not need to
look for a name that is a plant, food or that relates to "basil."

5. **If your tool relates to another third party brand, use it only as a
descriptor**: For example, use "Bazel migrator for Cmake" instead of
"Cmake Bazel migrator".

These guidelines also apply to the GitHub repository URL. Reading the repository
URL should help people understand what the tool does. Of course, the repository
name can be shorter and must use dashes instead of spaces and lower case letters.


Examples of good names:

* *Bazel for Eclipse*: Users will understand that if they want to use Bazel
  with Eclipse, this is where they should be looking. It uses a third party brand
  as a descriptor.
* *Bazel buildfarm*: A "buildfarm" is a
  [compile farm](https://en.wikipedia.org/wiki/Compile_farm). Users
  will understand that this project relates to building on servers.

Examples of names to avoid:

* *Ocimum*: The [scientific name of basil](https://en.wikipedia.org/wiki/Ocimum)
  does not relate enough to the Bazel project.
* *Bazelizer*: The tool behind this name could do a lot of things, this name is
   not descriptive enough.

Note that these recommendations are aligned with the
[guidelines](https://opensource.google.com/docs/releasing/preparing/#name)
Google uses when open sourcing a project.

---

## Patch Acceptance Process
- URL: https://bazel.build/contribute/patch-acceptance
- Source: contribute/patch-acceptance.mdx
- Slug: /contribute/patch-acceptance

This page outlines how contributors can propose and make changes to the Bazel
code base.

1. Read the [Bazel Contribution policy](/contribute/policy).
1. Create a [GitHub issue](https://github.com/bazelbuild/bazel/) to
   discuss your plan and design. Pull requests that change or add behavior
   need a corresponding issue for tracking.
1. If you're proposing significant changes, write a
   [design document](/contribute/design-documents).
1. Ensure you've signed a [Contributor License
   Agreement](https://cla.developers.google.com).
1. Prepare a git commit that implements the feature. Don't forget to add tests
   and update the documentation. If your change has user-visible effects, please
   [add release notes](/contribute/release-notes). If it is an incompatible change,
   read the [guide for rolling out breaking changes](/contribute/breaking-changes).
1. Create a pull request on
   [GitHub](https://github.com/bazelbuild/bazel/pulls). If you're new to GitHub,
   read [about pull
   requests](https://help.github.com/articles/about-pull-requests/). Note that
   we restrict permissions to create branches on the main Bazel repository, so
   you will need to push your commit to [your own fork of the
   repository](https://help.github.com/articles/working-with-forks/).
1. A Bazel maintainer should assign you a reviewer within two business days
   (excluding holidays in the USA and Germany). If you aren't assigned a
   reviewer in that time, you can request one by emailing
   [bazel-discuss@googlegroups.com]
   (mailto:bazel-discuss@googlegroups.com).
1. Work with the reviewer to complete a code review. For each change, create a
   new commit and push it to make changes to your pull request. If the review
   takes too long (for instance, if the reviewer is unresponsive), send an email to
   [bazel-discuss@googlegroups.com]
   (mailto:bazel-discuss@googlegroups.com).
1. After your review is complete, a Bazel maintainer applies your patch to
   Google's internal version control system.

   This triggers internal presubmit checks
   that may suggest more changes. If you haven't expressed a preference, the
   maintainer submitting your change  adds "trivial" changes (such as
   [linting](https://en.wikipedia.org/wiki/Lint_(software))) that don't affect
   design. If deeper changes are required or you'd prefer to apply
   changes directly, you and the reviewer should communicate preferences
   clearly in review comments.

   After internal submission, the patch is exported as a Git commit,
   at which point the GitHub pull request is closed. All final changes
   are attributed to you.

---

## Policy
- URL: https://bazel.build/contribute/policy
- Source: contribute/policy.mdx
- Slug: /contribute/policy

translation: human
page_type: lcat
---
title: 'Contribution policy'
---



This page covers Bazel's governance model and contribution policy.

## Governance model

The [Bazel project](https://github.com/bazelbuild) is led and managed by Google
and has a large community of contributors outside of Google. Some Bazel
components (such as specific rules repositories under the
[bazelbuild](https://github.com/bazelbuild) organization) are led,
maintained, and managed by members of the community. The Google Bazel team
reviews suggestions to add community-owned repositories (such as rules) to the
[bazelbuild](https://github.com/bazelbuild) GitHub organization.

### Contributor roles

Here are outlines of the roles in the Bazel project, including their
responsibilities:

*   **Owners**: The Google Bazel team. Owners are responsible for:
    *   Strategy, maintenance, and leadership of the Bazel project.
    *   Building and maintaining Bazel's core functionality.
    *   Appointing Maintainers and approving new repositories.
*   **Maintainers**: The Google Bazel team and designated GitHub users.
    Maintainers are responsible for:
    *   Building and maintaining the primary functionality of their repository.
    *   Reviewing and approving contributions to areas of the Bazel code base.
    *   Supporting users and contributors with timely and transparent issue
        management, PR review, and documentation.
    *   Releasing, testing and collaborating with Bazel Owners.
*   **Contributors**: All users who contribute code or documentation to the
    Bazel project.
    *   Creating well-written PRs to contribute to Bazel's codebase and
        documentation.
    *   Using standard channels, such as GitHub Issues, to propose changes and
        report issues.

### Becoming a Maintainer

Bazel Owners may appoint Maintainers to lead well-defined areas of code, such as
rule sets. Contributors with a record of consistent, responsible past
contributions who are planning major contributions in the future could be
considered to become qualified Maintainers.

## Contribution policy

The Bazel project accepts contributions from external contributors. Here are the
contribution policies for Google-managed and Community-managed areas of code.

*   **Licensing**. All Maintainers and Contributors must sign the
    [Google’s Contributor License Agreement](https://cla.developers.google.com/clas).
*   **Contributions**. Owners and Maintainers should make every effort to accept
    worthwhile contributions. All contributions must be:
    *   Well written and well tested
    *   Discussed and approved by the Maintainers of the relevant area of code.
        Discussions and approvals happen on GitHub Issues and in GitHub PRs.
        Larger contributions require a
        [design review](/contribute/design-documents).
    *   Added to Bazel's Continuous Integration system if not already present.
    *   Supportable and aligned with Bazel product direction
*   **Code review**. All changes in all `bazelbuild` repositories require
    review:
    *   All PRs must be approved by an Owner or Maintainer.
    *   Only Owners and Maintainers can merge PRs.
*   **Compatibility**. Owners may need to reject or request modifications to PRs
    in the unlikely event that the change requires substantial modifications to
    internal Google systems.
*   **Documentation**. Where relevant, feature contributions should include
    documentation updates.

For more details on contributing to Bazel, see our
[contribution guidelines](/contribute/).

---

## Writing release notes
- URL: https://bazel.build/contribute/release-notes
- Source: contribute/release-notes.mdx
- Slug: /contribute/release-notes

This document is targeted at Bazel contributors.

Commit descriptions in Bazel include a `RELNOTES:` tag followed by a release
note. This is used by the Bazel team to track changes in each release and write
the release announcement.

## Overview

* Is your change a bugfix? In that case, you don't need a release note. Please
  include a reference to the GitHub issue.

* If the change adds / removes / changes Bazel in a user-visible way, then it
  may be advantageous to mention it.

If the change is significant, follow the [design document
policy](/contribute/design-documents) first.

## Guidelines

The release notes will be read by our users, so it should be short (ideally one
sentence), avoid jargon (Bazel-internal terminology), should focus on what the
change is about.

* Include a link to the relevant documentation. Almost any release note should
  contain a link. If the description mentions a flag, a feature, a command name,
  users will probably want to know more about it.

* Use backquotes around code, symbols, flags, or any word containing an
  underscore.

* Do not just copy and paste bug descriptions. They are often cryptic and only
  make sense to us and leave the user scratching their head. Release notes are
  meant to explain what has changed and why in user-understandable language.

* Always use present tense and the format "Bazel now supports Y" or "X now does
  Z." We don't want our release notes to sound like bug entries. All release
  note entries should be informative and use a consistent style and language.

* If something has been deprecated or removed, use "X has been deprecated" or "X
  has been removed." Not "is removed" or "was removed."

* If Bazel now does something differently, use "X now $newBehavior instead of
  $oldBehavior" in present tense. This lets the user know in detail what to
  expect when they use the new release.

* If Bazel now supports or no longer supports something, use "Bazel now supports
  / no longer supports X".

* Explain why something has been removed / deprecated / changed. One sentence is
  enough but we want the user to be able to evaluate impact on their builds.

* Do NOT make any promises about future functionality. Avoid "this flag will be
  removed" or "this will be changed." It introduces uncertainty. The first thing
  the user will wonder is "when?" and we don't want them to start worrying about
  their current builds breaking at some unknown time.

## Process

As part of the [release
process](https://github.com/bazelbuild/continuous-integration/blob/master/docs/release-playbook.md),
we collect the `RELNOTES` tags of every commit. We copy everything in a [Google
Doc](https://docs.google.com/document/d/1wDvulLlj4NAlPZamdlEVFORks3YXJonCjyuQMUQEmB0/edit)
where we review, edit, and organize the notes.

The release manager sends an email to the
[bazel-dev](https://groups.google.com/forum/#!forum/bazel-dev) mailing-list.
Bazel contributors are invited to contribute to the document and make sure
their changes are correctly reflected in the announcement.

Later, the announcement will be submitted to the [Bazel
blog](https://blog.bazel.build/), using the [bazel-blog
repository](https://github.com/bazelbuild/bazel-blog/tree/master/_posts).

---

## A Guide to Skyframe `StateMachine`s
- URL: https://bazel.build/contribute/statemachine-guide
- Source: contribute/statemachine-guide.mdx
- Slug: /contribute/statemachine-guide

## Overview

A Skyframe `StateMachine` is a *deconstructed* function-object that resides on
the heap. It supports flexible and evaluation without redundancy[^1] when
required values are not immediately available but computed asynchronously. The
`StateMachine` cannot tie up a thread resource while waiting, but instead has to
be suspended and resumed. The deconstruction thus exposes explicit re-entry
points so that prior computations can be skipped.

`StateMachine`s can be used to express sequences, branching, structured logical
concurrency and are tailored specifically for Skyframe interaction.
`StateMachine`s can be composed into larger `StateMachine`s and share
sub-`StateMachine`s. Concurrency is always hierarchical by construction and
purely logical. Every concurrent subtask runs in the single shared parent
SkyFunction thread.

## Introduction

This section briefly motivates and introduces `StateMachine`s, found in the
[`java.com.google.devtools.build.skyframe.state`](https://github.com/bazelbuild/bazel/tree/master/src/main/java/com/google/devtools/build/skyframe/state)
package.

### A brief introduction to Skyframe restarts

Skyframe is a framework that performs parallel evaluation of dependency graphs.
Each node in the graph corresponds with the evaluation of a SkyFunction with a
SkyKey specifying its parameters and SkyValue specifying its result. The
computational model is such that a SkyFunction may lookup SkyValues by SkyKey,
triggering recursive, parallel evaluation of additional SkyFunctions. Instead of
blocking, which would tie up a thread, when a requested SkyValue is not yet
ready because some subgraph of computation is incomplete, the requesting
SkyFunction observes a `null` `getValue` response and should return `null`
instead of a SkyValue, signaling that it is incomplete due to missing inputs.
Skyframe *restarts* the SkyFunctions when all previously requested SkyValues
become available.

Before the introduction of `SkyKeyComputeState`, the traditional way of handling
a restart was to fully rerun the computation. Although this has quadratic
complexity, functions written this way eventually complete because each rerun,
fewer lookups return `null`. With `SkyKeyComputeState` it is possible to
associate hand-specified check-point data with a SkyFunction, saving significant
recomputation.

`StateMachine`s are objects that live inside `SkyKeyComputeState` and eliminate
virtually all recomputation when a SkyFunction restarts (assuming that
`SkyKeyComputeState` does not fall out of cache) by exposing suspend and resume
execution hooks.

### Stateful computations inside `SkyKeyComputeState`

From an object-oriented design standpoint, it makes sense to consider storing
computational objects inside `SkyKeyComputeState` instead of pure data values.
In *Java*, the bare minimum description of a behavior carrying object is a
*functional interface* and it turns out to be sufficient. A `StateMachine` has
the following, curiously recursive, definition[^2].

```
@FunctionalInterface
public interface StateMachine {
  StateMachine step(Tasks tasks) throws InterruptedException;
}
```

The `Tasks` interface is analogous to `SkyFunction.Environment` but it is
designed for asynchrony and adds support for logically concurrent subtasks[^3].

The return value of `step` is another `StateMachine`, allowing the specification
of a sequence of steps, inductively. `step` returns `DONE` when the
`StateMachine` is done. For example:

```
class HelloWorld implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    System.out.println("hello");
    return this::step2;  // The next step is HelloWorld.step2.
  }

  private StateMachine step2(Tasks tasks) {
     System.out.println("world");
     // DONE is special value defined in the `StateMachine` interface signaling
     // that the computation is done.
     return DONE;
  }
}
```

describes a `StateMachine` with the following output.

```
hello
world
```

Note that the method reference `this::step2` is also a `StateMachine` due to
`step2` satisfying `StateMachine`'s functional interface definition. Method
references are the most common way to specify the next state in a
`StateMachine`.

![Suspending and resuming](/contribute/images/suspend-resume.svg)

Intuitively, breaking a computation down into `StateMachine` steps, instead of a
monolithic function, provides the hooks needed to *suspend* and *resume* a
computation. When `StateMachine.step` returns, there is an explicit *suspension*
point. The continuation specified by the returned `StateMachine` value is an
explicit *resume* point. Recomputation can thus be avoided because the
computation can be picked up exactly where it left off.

### Callbacks, continuations and asynchronous computation

In technical terms, a `StateMachine` serves as a *continuation*, determining the
subsequent computation to be executed. Instead of blocking, a `StateMachine` can
voluntarily *suspend* by returning from the `step` function, which transfers
control back to a [`Driver`](#drivers-and-bridging) instance. The `Driver` can
then switch to a ready `StateMachine` or relinquish control back to Skyframe.

Traditionally, *callbacks* and *continuations* are conflated into one concept.
However, `StateMachine`s maintain a distinction between the two.

*   *Callback* - describes where to store the result of an asynchronous
    computation.
*   *Continuation* - specifies the next execution state.

Callbacks are required when invoking an asynchronous operation, which means that
the actual operation doesn't occur immediately upon calling the method, as in
the case of a SkyValue lookup. Callbacks should be kept as simple as possible.

Caution: A common pitfall of callbacks is that the asynchronous computation must
ensure the callback is called by the end of every reachable path. It's possible
to overlook some branches and the compiler doesn't give warnings about this.

*Continuations* are the `StateMachine` return values of `StateMachine`s and
encapsulate the complex execution that follows once all asynchronous
computations resolve. This structured approach helps to keep the complexity of
callbacks manageable.

## Tasks

The `Tasks` interface provides `StateMachine`s with an API to lookup SkyValues
by SkyKey and to schedule concurrent subtasks.

```
interface Tasks {
  void enqueue(StateMachine subtask);

  void lookUp(SkyKey key, Consumer<SkyValue> sink);

  <E extends Exception>
  void lookUp(SkyKey key, Class<E> exceptionClass, ValueOrExceptionSink<E> sink);

  // lookUp overloads for 2 and 3 exception types exist, but are elided here.
}
```

Tip: When any state uses the `Tasks` interface to perform lookups or create
subtasks, those lookups and subtasks will complete before the next state begins.

Tip: (Corollary) If subtasks are complex `StateMachine`s or recursively create
subtasks, they all *transitively* complete before the next state begins.

### SkyValue lookups

`StateMachine`s use `Tasks.lookUp` overloads to look up SkyValues. They are
analogous to `SkyFunction.Environment.getValue` and
`SkyFunction.Environment.getValueOrThrow` and have similar exception handling
semantics. The implementation does not immediately perform the lookup, but
instead, batches[^4] as many lookups as possible before doing so. The value
might not be immediately available, for example, requiring a Skyframe restart,
so the caller specifies what to do with the resulting value using a callback.

The `StateMachine` processor ([`Driver`s and bridging to
SkyFrame](#drivers-and-bridging)) guarantees that the value is available before
the next state begins. An example follows.

```
class DoesLookup implements StateMachine, Consumer<SkyValue> {
  private Value value;

  @Override
  public StateMachine step(Tasks tasks) {
    tasks.lookUp(new Key(), (Consumer<SkyValue>) this);
    return this::processValue;
  }

  // The `lookUp` call in `step` causes this to be called before `processValue`.
  @Override  // Implementation of Consumer<SkyValue>.
  public void accept(SkyValue value) {
    this.value = (Value)value;
  }

  private StateMachine processValue(Tasks tasks) {
    System.out.println(value);  // Prints the string representation of `value`.
    return DONE;
  }
}
```

In the above example, the first step does a lookup for `new Key()`, passing
`this` as the consumer. That is possible because `DoesLookup` implements
`Consumer<SkyValue>`.

Tip: When passing `this` as a value sink, it's helpful to readers to upcast it
to the receiver type to narrow down the purpose of passing `this`. The example
passes `(Consumer<SkyValue>) this`.

By contract, before the next state `DoesLookup.processValue` begins, all the
lookups of `DoesLookup.step` are complete. Therefore `value` is available when
it is accessed in `processValue`.

### Subtasks

`Tasks.enqueue` requests the execution of logically concurrent subtasks.
Subtasks are also `StateMachine`s and can do anything regular `StateMachine`s
can do, including recursively creating more subtasks or looking up SkyValues.
Much like `lookUp`, the state machine driver ensures that all subtasks are
complete before proceeding to the next step. An example follows.

```
class Subtasks implements StateMachine {
  private int i = 0;

  @Override
  public StateMachine step(Tasks tasks) {
    tasks.enqueue(new Subtask1());
    tasks.enqueue(new Subtask2());
    // The next step is Subtasks.processResults. It won't be called until both
    // Subtask1 and Subtask 2 are complete.
    return this::processResults;
  }

  private StateMachine processResults(Tasks tasks) {
    System.out.println(i);  // Prints "3".
    return DONE;  // Subtasks is done.
  }

  private class Subtask1 implements StateMachine {
    @Override
    public StateMachine step(Tasks tasks) {
      i += 1;
      return DONE;  // Subtask1 is done.
    }
  }

  private class Subtask2 implements StateMachine {
    @Override
    public StateMachine step(Tasks tasks) {
      i += 2;
      return DONE;  // Subtask2 is done.
    }
  }
}
```

Though `Subtask1` and `Subtask2` are logically concurrent, everything runs in a
single thread so the "concurrent" update of `i` does not need any
synchronization.

### Structured concurrency

Since every `lookUp` and `enqueue` must resolve before advancing to the next
state, it means that concurrency is naturally limited to tree-structures. It's
possible to create hierarchical[^5] concurrency as shown in the following
example.

![Structured Concurrency](/contribute/images/structured-concurrency.svg)

It's hard to tell from the *UML* that the concurrency structure forms a tree.
There's an [alternate view](#concurrency-tree-diagram) that better shows the
tree structure.

![Unstructured Concurrency](/contribute/images/unstructured-concurrency.svg)

Structured concurrency is much easier to reason about.

## Composition and control flow patterns

This section presents examples for how multiple `StateMachine`s can be composed
and solutions to certain control flow problems.

### Sequential states

This is the most common and straightforward control flow pattern. An example of
this is shown in [Stateful computations inside
`SkyKeyComputeState`](#stateful-computations).

### Branching

Branching states in `StateMachine`s can be achieved by returning different
values using regular *Java* control flow, as shown in the following example.

```
class Branch implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    // Returns different state machines, depending on condition.
    if (shouldUseA()) {
      return this::performA;
    }
    return this::performB;
  }
  …
}
```

It’s very common for certain branches to return `DONE`, for early completion.

### Advanced sequential composition

Since the `StateMachine` control structure is memoryless, sharing `StateMachine`
definitions as subtasks can sometimes be awkward. Let *M<sub>1</sub>* and
*M<sub>2</sub>* be `StateMachine` instances that share a `StateMachine`, *S*,
with *M<sub>1</sub>* and *M<sub>2</sub>* being the sequences *&lt;A, S, B>* and
*&lt;X, S, Y>* respectively. The problem is that *S* doesn’t know whether to
continue to *B* or *Y* after it completes and `StateMachine`s don't quite keep a
call stack. This section reviews some techniques for achieving this.

#### `StateMachine` as terminal sequence element

This doesn’t solve the initial problem posed. It only demonstrates sequential
composition when the shared `StateMachine` is terminal in the sequence.

```
// S is the shared state machine.
class S implements StateMachine { … }

class M1 implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    performA();
    return new S();
  }
}

class M2 implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    performX();
    return new S();
  }
}
```

This works even if *S* is itself a complex state machine.

#### Subtask for sequential composition

Since enqueued subtasks are guaranteed to complete before the next state, it’s
sometimes possible to slightly abuse[^6] the subtask mechanism.

```
class M1 implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    performA();
    // S starts after `step` returns and by contract must complete before `doB`
    // begins. It is effectively sequential, inducing the sequence < A, S, B >.
    tasks.enqueue(new S());
    return this::doB;
  }

  private StateMachine doB(Tasks tasks) {
    performB();
    return DONE;
  }
}

class M2 implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    performX();
    // Similarly, this induces the sequence < X, S, Y>.
    tasks.enqueue(new S());
    return this::doY;
  }

  private StateMachine doY(Tasks tasks) {
    performY();
    return DONE;
  }
}
```

#### `runAfter` injection

Sometimes, abusing `Tasks.enqueue` is impossible because there are other
parallel subtasks or `Tasks.lookUp` calls that must be completed before *S*
executes. In this case, injecting a `runAfter` parameter into *S* can be used to
inform *S* of what to do next.

```
class S implements StateMachine {
  // Specifies what to run after S completes.
  private final StateMachine runAfter;

  @Override
  public StateMachine step(Tasks tasks) {
    … // Performs some computations.
    return this::processResults;
  }

  @Nullable
  private StateMachine processResults(Tasks tasks) {
    … // Does some additional processing.

    // Executes the state machine defined by `runAfter` after S completes.
    return runAfter;
  }
}

class M1 implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    performA();
    // Passes `this::doB` as the `runAfter` parameter of S, resulting in the
    // sequence < A, S, B >.
    return new S(/* runAfter= */ this::doB);
  }

  private StateMachine doB(Tasks tasks) {
    performB();
    return DONE;
  }
}

class M2 implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks) {
    performX();
    // Passes `this::doY` as the `runAfter` parameter of S, resulting in the
    // sequence < X, S, Y >.
    return new S(/* runAfter= */ this::doY);
  }

  private StateMachine doY(Tasks tasks) {
    performY();
    return DONE;
  }
}
```

This approach is cleaner than abusing subtasks. However, applying this too
liberally, for example, by nesting multiple `StateMachine`s with `runAfter`, is
the road to [Callback Hell](#callback-hell). It’s better to break up sequential
`runAfter`s with ordinary sequential states instead.

```
  return new S(/* runAfter= */ new T(/* runAfter= */ this::nextStep))
```

can be replaced with the following.

```
  private StateMachine step1(Tasks tasks) {
     doStep1();
     return new S(/* runAfter= */ this::intermediateStep);
  }

  private StateMachine intermediateStep(Tasks tasks) {
    return new T(/* runAfter= */ this::nextStep);
  }
```

Note: It's possible to pass `DONE` as the `runAfter` parameter when there's
nothing to run afterwards.

Tip: When using `runAfter`, always annotate the parameter with `/* runAfter= */`
to let the reader know the meaning at the callsite.

#### *Forbidden* alternative: `runAfterUnlessError`

In an earlier draft, we had considered a `runAfterUnlessError` that would abort
early on errors. This was motivated by the fact that errors often end up getting
checked twice, once by the `StateMachine` that has a `runAfter` reference and
once by the `runAfter` machine itself.

After some deliberation, we decided that uniformity of the code is more
important than deduplicating the error checking. It would be confusing if the
`runAfter` mechanism did not work in a consistent manner with the
`tasks.enqueue` mechanism, which always requires error checking.

Warning: When using `runAfter`, the machine that has the injected `runAfter`
should invoke it unconditionally at completion, even on error, for consistency.

### Direct delegation

Each time there is a formal state transition, the main `Driver` loop advances.
As per contract, advancing states means that all previously enqueued SkyValue
lookups and subtasks resolve before the next state executes. Sometimes the logic
of a delegate `StateMachine` makes a phase advance unnecessary or
counterproductive. For example, if the first `step` of the delegate performs
SkyKey lookups that could be parallelized with lookups of the delegating state
then a phase advance would make them sequential. It could make more sense to
perform direct delegation, as shown in the example below.

```
class Parent implements StateMachine {
  @Override
  public StateMachine step(Tasks tasks ) {
    tasks.lookUp(new Key1(), this);
    // Directly delegates to `Delegate`.
    //
    // The (valid) alternative:
    //   return new Delegate(this::afterDelegation);
    // would cause `Delegate.step` to execute after `step` completes which would
    // cause lookups of `Key1` and `Key2` to be sequential instead of parallel.
    return new Delegate(this::afterDelegation).step(tasks);
  }

  private StateMachine afterDelegation(Tasks tasks) {
    …
  }
}

class Delegate implements StateMachine {
  private final StateMachine runAfter;

  Delegate(StateMachine runAfter) {
    this.runAfter = runAfter;
  }

  @Override
  public StateMachine step(Tasks tasks) {
    tasks.lookUp(new Key2(), this);
    return …;
  }

  // Rest of implementation.
  …

  private StateMachine complete(Tasks tasks) {
    …
    return runAfter;
  }
}
```

## Data flow

The focus of the previous discussion has been on managing control flow. This
section describes the propagation of data values.

### Implementing `Tasks.lookUp` callbacks

There’s an example of implementing a `Tasks.lookUp` callback in [SkyValue
lookups](#skyvalue-lookups). This section provides rationale and suggests
approaches for handling multiple SkyValues.

#### `Tasks.lookUp` callbacks

The `Tasks.lookUp` method takes a callback, `sink`, as a parameter.

```
  void lookUp(SkyKey key, Consumer<SkyValue> sink);
```

The idiomatic approach would be to use a *Java* lambda to implement this:

```
  tasks.lookUp(key, value -> myValue = (MyValueClass)value);
```

with `myValue` being a member variable of the `StateMachine` instance doing the
lookup. However, the lambda requires an extra memory allocation compared to
implementing the `Consumer<SkyValue>` interface in the `StateMachine`
implementation. The lambda is still useful when there are multiple lookups that
would be ambiguous.

Note: Bikeshed warning. There is a noticeable difference of approximately 1%
end-to-end CPU usage when implementing callbacks systematically in
`StateMachine` implementations compared to using lambdas, which makes this
recommendation debatable. To avoid unnecessary debates, it is advised to leave
the decision up to the individual implementing the solution.

There are also error handling overloads of `Tasks.lookUp`, that are analogous to
`SkyFunction.Environment.getValueOrThrow`.

```
  <E extends Exception> void lookUp(
      SkyKey key, Class<E> exceptionClass, ValueOrExceptionSink<E> sink);

  interface ValueOrExceptionSink<E extends Exception> {
    void acceptValueOrException(@Nullable SkyValue value, @Nullable E exception);
  }
```

An example implementation is shown below.

```
class PerformLookupWithError extends StateMachine, ValueOrExceptionSink<MyException> {
  private MyValue value;
  private MyException error;

  @Override
  public StateMachine step(Tasks tasks) {
    tasks.lookUp(new MyKey(), MyException.class, ValueOrExceptionSink<MyException>) this);
    return this::processResult;
  }

  @Override
  public acceptValueOrException(@Nullable SkyValue value, @Nullable MyException exception) {
    if (value != null) {
      this.value = (MyValue)value;
      return;
    }
    if (exception != null) {
      this.error = exception;
      return;
    }
    throw new IllegalArgumentException("Both parameters were unexpectedly null.");
  }

  private StateMachine processResult(Tasks tasks) {
    if (exception != null) {
      // Handles the error.
      …
      return DONE;
    }
    // Processes `value`, which is non-null.
    …
  }
}
```

As with lookups without error handling, having the `StateMachine` class directly
implement the callback saves a memory allocation for the lamba.

[Error handling](#error-handling) provides a bit more detail, but essentially,
there's not much difference between the propagation of errors and normal values.

#### Consuming multiple SkyValues

Multiple SkyValue lookups are often required. An approach that works much of the
time is to switch on the type of SkyValue. The following is an example that has
been simplified from prototype production code.

```
  @Nullable
  private StateMachine fetchConfigurationAndPackage(Tasks tasks) {
    var configurationKey = configuredTarget.getConfigurationKey();
    if (configurationKey != null) {
      tasks.lookUp(configurationKey, (Consumer<SkyValue>) this);
    }

    var packageId = configuredTarget.getLabel().getPackageIdentifier();
    tasks.lookUp(PackageValue.key(packageId), (Consumer<SkyValue>) this);

    return this::constructResult;
  }

  @Override  // Implementation of `Consumer<SkyValue>`.
  public void accept(SkyValue value) {
    if (value instanceof BuildConfigurationValue) {
      this.configurationValue = (BuildConfigurationValue) value;
      return;
    }
    if (value instanceof PackageValue) {
      this.pkg = ((PackageValue) value).getPackage();
      return;
    }
    throw new IllegalArgumentException("unexpected value: " + value);
  }
```

The `Consumer<SkyValue>` callback implementation can be shared unambiguously
because the value types are different. When that’s not the case, falling back to
lambda-based implementations or full inner-class instances that implement the
appropriate callbacks is viable.

### Propagating values between `StateMachine`s

So far, this document has only explained how to arrange work in a subtask, but
subtasks also need to report a values back to the caller. Since subtasks are
logically asynchronous, their results are communicated back to the caller using
a *callback*. To make this work, the subtask defines a sink interface that is
injected via its constructor.

```
class BarProducer implements StateMachine {
  // Callers of BarProducer implement the following interface to accept its
  // results. Exactly one of the two methods will be called by the time
  // BarProducer completes.
  interface ResultSink {
    void acceptBarValue(Bar value);
    void acceptBarError(BarException exception);
  }

  private final ResultSink sink;

  BarProducer(ResultSink sink) {
     this.sink = sink;
  }

  … // StateMachine steps that end with this::complete.

  private StateMachine complete(Tasks tasks) {
    if (hasError()) {
      sink.acceptBarError(getError());
      return DONE;
    }
    sink.acceptBarValue(getValue());
    return DONE;
  }
}
```

Tip: It would be tempting to use the more concise signature void `accept(Bar
value)` rather than the stuttery `void acceptBarValue(Bar value)` above.
However, `Consumer<SkyValue>` is a common overload of `void accept(Bar value)`,
so doing this often leads to violations of the [Overloads: never
split](https://google.github.io/styleguide/javaguide.html#s3.4.2-ordering-class-contents)
style-guide rule.

Tip: Using a custom `ResultSink` type instead of a generic one from
`java.util.function` makes it easy to find implementations in the code base,
improving readability.

A caller `StateMachine` would then look like the following.

```
class Caller implements StateMachine, BarProducer.ResultSink {
  interface ResultSink {
    void acceptCallerValue(Bar value);
    void acceptCallerError(BarException error);
  }

  private final ResultSink sink;

  private Bar value;

  Caller(ResultSink sink) {
    this.sink = sink;
  }

  @Override
  @Nullable
  public StateMachine step(Tasks tasks) {
    tasks.enqueue(new BarProducer((BarProducer.ResultSink) this));
    return this::processResult;
  }

  @Override
  public void acceptBarValue(Bar value) {
    this.value = value;
  }

  @Override
  public void acceptBarError(BarException error) {
    sink.acceptCallerError(error);
  }

  private StateMachine processResult(Tasks tasks) {
    // Since all enqueued subtasks resolve before `processResult` starts, one of
    // the `BarResultSink` callbacks must have been called by this point.
    if (value == null) {
      return DONE;  // There was a previously reported error.
    }
    var finalResult = computeResult(value);
    sink.acceptCallerValue(finalResult);
    return DONE;
  }
}
```

The preceding example demonstrates a few things. `Caller` has to propagate its
results back and defines its own `Caller.ResultSink`. `Caller` implements the
`BarProducer.ResultSink` callbacks. Upon resumption, `processResult` checks if
`value` is null to determine if an error occurred. This is a common behavior
pattern after accepting output from either a subtask or SkyValue lookup.

Note that the implementation of `acceptBarError` eagerly forwards the result to
the `Caller.ResultSink`, as required by [Error bubbling](#error-bubbling).

Alternatives for top-level `StateMachine`s are described in [`Driver`s and
bridging to SkyFunctions](#drivers-and-bridging).

### Error handling

There's a couple of examples of error handling already in [`Tasks.lookUp`
callbacks](#tasks-lookup-callbacks) and [Propagating values between
`StateMachines`](#propagating-values). Exceptions, other than
`InterruptedException` are not thrown, but instead passed around through
callbacks as values. Such callbacks often have exclusive-or semantics, with
exactly one of a value or error being passed.

The next section describes a a subtle, but important interaction with Skyframe
error handling.

#### Error bubbling (--nokeep\_going)

Warning: Errors need to be eagerly propagated all the way back to the
SkyFunction for error bubbling to function correctly.

During error bubbling, a SkyFunction may be restarted even if not all requested
SkyValues are available. In such cases, the subsequent state will never be
reached due to the `Tasks` API contract. However, the `StateMachine` should
still propagate the exception.

Since propagation must occur regardless of whether the next state is reached,
the error handling callback must perform this task. For an inner `StateMachine`,
this is achieved by invoking the parent callback.

At the top-level `StateMachine`, which interfaces with the SkyFunction, this can
be done by calling the `setException` method of `ValueOrExceptionProducer`.
`ValueOrExceptionProducer.tryProduceValue` will then throw the exception, even
if there are missing SkyValues.

If a `Driver` is being utilized directly, it is essential to check for
propagated errors from the SkyFunction, even if the machine has not finished
processing.

### Event Handling

For SkyFunctions that need to emit events, a `StoredEventHandler` is injected
into SkyKeyComputeState and further injected into `StateMachine`s that require
them. Historically, the `StoredEventHandler` was needed due to Skyframe dropping
certain events unless they are replayed but this was subsequently fixed.
`StoredEventHandler` injection is preserved because it simplifies the
implementation of events emitted from error handling callbacks.

## `Driver`s and bridging to SkyFunctions

A `Driver` is responsible for managing the execution of `StateMachine`s,
beginning with a specified root `StateMachine`. As `StateMachine`s can
recursively enqueue subtask `StateMachine`s, a single `Driver` can manage
numerous subtasks. These subtasks create a tree structure, a result of
[Structured concurrency](#structured-concurrency). The `Driver` batches SkyValue
lookups across subtasks for improved efficiency.

There are a number of classes built around the `Driver`, with the following API.

```
public final class Driver {
  public Driver(StateMachine root);
  public boolean drive(SkyFunction.Environment env) throws InterruptedException;
}
```

`Driver` takes a single root `StateMachine` as a parameter. Calling
`Driver.drive` executes the `StateMachine` as far as it can go without a
Skyframe restart. It returns true when the `StateMachine` completes and false
otherwise, indicating that not all values were available.

`Driver` maintains the concurrent state of the `StateMachine` and it is well
suited for embedding in `SkyKeyComputeState`.

### Directly instantiating `Driver`

`StateMachine` implementations conventionally communicate their results via
callbacks. It's possible to directly instantiate a `Driver` as shown in the
following example.

The `Driver` is embedded in the `SkyKeyComputeState` implementation along with
an implementation of the corresponding `ResultSink` to be defined a bit further
down. At the top level, the `State` object is an appropriate receiver for the
result of the computation as it is guaranteed to outlive `Driver`.

```
class State implements SkyKeyComputeState, ResultProducer.ResultSink {
  // The `Driver` instance, containing the full tree of all `StateMachine`
  // states. Responsible for calling `StateMachine.step` implementations when
  // asynchronous values are available and performing batched SkyFrame lookups.
  //
  // Non-null while `result` is being computed.
  private Driver resultProducer;

  // Variable for storing the result of the `StateMachine`
  //
  // Will be non-null after the computation completes.
  //
  private ResultType result;

  // Implements `ResultProducer.ResultSink`.
  //
  // `ResultProducer` propagates its final value through a callback that is
  // implemented here.
  @Override
  public void acceptResult(ResultType result) {
    this.result = result;
  }
}
```

The code below sketches the `ResultProducer`.

```
class ResultProducer implements StateMachine {
  interface ResultSink {
    void acceptResult(ResultType value);
  }

  private final Parameters parameters;
  private final ResultSink sink;

  … // Other internal state.

  ResultProducer(Parameters parameters, ResultSink sink) {
    this.parameters = parameters;
    this.sink = sink;
  }

  @Override
  public StateMachine step(Tasks tasks) {
    …  // Implementation.
    return this::complete;
  }

  private StateMachine complete(Tasks tasks) {
    sink.acceptResult(getResult());
    return DONE;
  }
}
```

Then the code for lazily computing the result could look like the following.

```
@Nullable
private Result computeResult(State state, Skyfunction.Environment env)
    throws InterruptedException {
  if (state.result != null) {
    return state.result;
  }
  if (state.resultProducer == null) {
    state.resultProducer = new Driver(new ResultProducer(
      new Parameters(), (ResultProducer.ResultSink)state));
  }
  if (state.resultProducer.drive(env)) {
    // Clears the `Driver` instance as it is no longer needed.
    state.resultProducer = null;
  }
  return state.result;
}
```

### Embedding `Driver`

If the `StateMachine` produces a value and raises no exceptions, embedding
`Driver` is another possible implementation, as shown in the following example.

```
class ResultProducer implements StateMachine {
  private final Parameters parameters;
  private final Driver driver;

  private ResultType result;

  ResultProducer(Parameters parameters) {
    this.parameters = parameters;
    this.driver = new Driver(this);
  }

  @Nullable  // Null when a Skyframe restart is needed.
  public ResultType tryProduceValue( SkyFunction.Environment env)
      throws InterruptedException {
    if (!driver.drive(env)) {
      return null;
    }
    return result;
  }

  @Override
  public StateMachine step(Tasks tasks) {
    …  // Implementation.
}
```

The SkyFunction may have code that looks like the following (where `State` is
the function specific type of `SkyKeyComputeState`).

```
@Nullable  // Null when a Skyframe restart is needed.
Result computeResult(SkyFunction.Environment env, State state)
    throws InterruptedException {
  if (state.result != null) {
    return state.result;
  }
  if (state.resultProducer == null) {
    state.resultProducer = new ResultProducer(new Parameters());
  }
  var result = state.resultProducer.tryProduceValue(env);
  if (result == null) {
    return null;
  }
  state.resultProducer = null;
  return state.result = result;
}
```

Embedding `Driver` in the `StateMachine` implementation is a better fit for
Skyframe's synchronous coding style.

### StateMachines that may produce exceptions

Otherwise, there are `SkyKeyComputeState`-embeddable `ValueOrExceptionProducer`
and `ValueOrException2Producer` classes that have synchronous APIs to match
synchronous SkyFunction code.

The `ValueOrExceptionProducer` abstract class includes the following methods.

```
public abstract class ValueOrExceptionProducer<V, E extends Exception>
    implements StateMachine {
  @Nullable
  public final V tryProduceValue(Environment env)
      throws InterruptedException, E {
    …  // Implementation.
  }

  protected final void setValue(V value)  {  … // Implementation. }
  protected final void setException(E exception) {  … // Implementation. }
}
```

It includes an embedded `Driver` instance and closely resembles the
`ResultProducer` class in [Embedding driver](#embedding-driver) and interfaces
with the SkyFunction in a similar manner. Instead of defining a `ResultSink`,
implementations call `setValue` or `setException` when either of those occur.
When both occur, the exception takes priority. The `tryProduceValue` method
bridges the asynchronous callback code to synchronous code and throws an
exception when one is set.

As previously noted, during error bubbling, it's possible for an error to occur
even if the machine is not yet done because not all inputs are available. To
accommodate this, `tryProduceValue` throws any set exceptions, even before the
machine is done.

## Epilogue: Eventually removing callbacks

`StateMachine`s are a highly efficient, but boilerplate intensive way to perform
asynchronous computation. Continuations (particularly in the form of `Runnable`s
passed to `ListenableFuture`) are widespread in certain parts of *Bazel* code,
but aren't prevalent in analysis SkyFunctions. Analysis is mostly CPU bound and
there are no efficient asynchronous APIs for disk I/O. Eventually, it would be
good to optimize away callbacks as they have a learning curve and impede
readability.

One of the most promising alternatives is *Java* virtual threads. Instead of
having to write callbacks, everything is replaced with synchronous, blocking
calls. This is possible because tying up a virtual thread resource, unlike a
platform thread, is supposed to be cheap. However, even with virtual threads,
replacing simple synchronous operations with thread creation and synchronization
primitives is too expensive. We performed a migration from `StateMachine`s to
*Java* virtual threads and they were orders of magnitude slower, leading to
almost a 3x increase in end-to-end analysis latency. Since virtual threads are
still a preview feature, it's possible that this migration can be performed at a
later date when performance improves.

Another approach to consider is waiting for *Loom* coroutines, if they ever
become available. The advantage here is that it might be possible to reduce
synchronization overhead by using cooperative multitasking.

If all else fails, low-level bytecode rewriting could also be a viable
alternative. With enough optimization, it might be possible to achieve
performance that approaches hand-written callback code.

## Appendix

### Callback Hell

Callback hell is an infamous problem in asynchronous code that uses callbacks.
It stems from the fact that the continuation for a subsequent step is nested
within the previous step. If there are many steps, this nesting can be extremely
deep. If coupled with control flow the code becomes unmanageable.

```
class CallbackHell implements StateMachine {
  @Override
  public StateMachine step(Tasks task) {
    doA();
    return (t, l) -> {
      doB();
      return (t1, l2) -> {
        doC();
        return DONE;
      };
    };
  }
}
```

One of the advantages of nested implementations is that the stack frame of the
outer step can be preserved. In *Java*, captured lambda variables must be
effectively final so using such variables can be cumbersome. Deep nesting is
avoided by returning method references as continuations instead of lambdas as
shown as follows.

```
class CallbackHellAvoided implements StateMachine {
  @Override
  public StateMachine step(Tasks task) {
    doA();
    return this::step2;
  }

  private StateMachine step2(Tasks tasks) {
    doB();
    return this::step3;
  }

  private StateMachine step3(Tasks tasks) {
    doC();
    return DONE;
  }
}
```

Callback hell may also occur if the [`runAfter` injection](#runafter-injection)
pattern is used too densely, but this can be avoided by interspersing injections
with sequential steps.

#### Example: Chained SkyValue lookups

It is often the case that the application logic requires dependent chains of
SkyValue lookups, for example, if a second SkyKey depends on the first SkyValue.
Thinking about this naively, this would result in a complex, deeply nested
callback structure.

```
private ValueType1 value1;
private ValueType2 value2;

private StateMachine step1(...) {
  tasks.lookUp(key1, (Consumer<SkyValue>) this);  // key1 has type KeyType1.
  return this::step2;
}

@Override
public void accept(SkyValue value) {
  this.value1 = (ValueType1) value;
}

private StateMachine step2(...) {
  KeyType2 key2 = computeKey(value1);
  tasks.lookup(key2, this::acceptValueType2);
  return this::step3;
}

private void acceptValueType2(SkyValue value) {
  this.value2 = (ValueType2) value;
}
```

However, since continuations are specified as method references, the code looks
procedural across state transitions: `step2` follows `step1`. Note that here, a
lambda is used to assign `value2`. This makes the ordering of the code match the
ordering of the computation from top-to-bottom.

### Miscellaneous Tips

#### Readability: Execution Ordering

To improve readability, strive to keep the `StateMachine.step` implementations
in execution order and callback implementations immediately following where they
are passed in the code. This isn't always possible where the control flow
branches. Additional comments might be helpful in such cases.

In [Example: Chained SkyValue lookups](#chained-skyvalue-lookups), an
intermediate method reference is created to achieve this. This trades a small
amount of performance for readability, which is likely worthwhile here.

#### Generational Hypothesis

Medium-lived *Java* objects break the generational hypothesis of the *Java*
garbage collector, which is designed to handle objects that live for a very
short time or objects that live forever. By definition, objects in
`SkyKeyComputeState` violate this hypothesis. Such objects, containing the
constructed tree of all still-running `StateMachine`s, rooted at `Driver` have
an intermediate lifespan as they suspend, waiting for asynchronous computations
to complete.

It seems less bad in JDK19, but when using `StateMachine`s, it's sometimes
possible to observe an increase in GC time, even with dramatic decreases in
actual garbage generated. Since `StateMachine`s have an intermediate lifespan
they could be promoted to old gen, causing it to fill up more quickly, thus
necessitating more expensive major or full GCs to clean up.

The initial precaution is to minimize the use of `StateMachine` variables, but
it is not always feasible, for example, if a value is needed across multiple
states. Where it is possible, local stack `step` variables are young generation
variables and efficiently GC'd.

For `StateMachine` variables, breaking things down into subtasks and following
the recommended pattern for [Propagating values between
`StateMachine`s](#propagating-values) is also helpful. Observe that when
following the pattern, only child `StateMachine`s have references to parent
`StateMachine`s and not vice versa. This means that as children complete and
update the parents using result callbacks, the children naturally fall out of
scope and become eligible for GC.

Finally, in some cases, a `StateMachine` variable is needed in earlier states
but not in later states. It can be beneficial to null out references of large
objects once it is known that they are no longer needed.

#### Naming states

When naming a method, it's usually possible to name a method for the behavior
that happens within that method. It's less clear how to do this in
`StateMachine`s because there is no stack. For example, suppose method `foo`
calls a sub-method `bar`. In a `StateMachine`, this could be translated into the
state sequence `foo`, followed by `bar`. `foo` no longer includes the behavior
`bar`. As a result, method names for states tend to be narrower in scope,
potentially reflecting local behavior.

### Concurrency tree diagram

The following is an alternative view of the diagram in [Structured
concurrency](#structured-concurrency) that better depicts the tree structure.
The blocks form a small tree.

![Structured Concurrency 3D](/contribute/images/structured-concurrency-3d.svg)

[^1]: In contrast to Skyframe's convention of restarting from the beginning when
 values are not available.
[^2]: Note that `step` is permitted to throw `InterruptedException`, but the
 examples omit this. There are a few low methods in *Bazel* code that throw
 this exception and it propagates up to the `Driver`, to be described later,
 that runs the `StateMachine`. It's fine to not declare it to be thrown when
 unneeded.
[^3]: Concurrent subtasks were motivated by the `ConfiguredTargetFunction` which
 performs *independent* work for each dependency. Instead of manipulating
 complex data structures that process all the dependencies at once,
 introducing inefficiencies, each dependency has its own independent
 `StateMachine`.
[^4]: Multiple `tasks.lookUp` calls within a single step are batched together.
 Additional batching can be created by lookups occurring within concurrent
 subtasks.
[^5]: This is conceptually similar to Java’s structured concurrency
 [jeps/428](https://openjdk.org/jeps/428).
[^6]: Doing this is similar to spawning a thread and joining it to achieve
 sequential composition.

---

## Maintaining Bazel Chocolatey package on Windows
- URL: https://bazel.build/contribute/windows-chocolatey-maintenance
- Source: contribute/windows-chocolatey-maintenance.mdx
- Slug: /contribute/windows-chocolatey-maintenance

Note: The Chocolatey package is experimental; please provide feedback
(`@petemounce` in issue tracker).

## Prerequisites

You need:

*    [chocolatey package manager](https://chocolatey.org) installed
*    (to publish) a chocolatey API key granting you permission to publish the
     `bazel` package
     * [@petemounce](https://github.com/petemounce) currently
     maintains this unofficial package.
*    (to publish) to have set up that API key for the chocolatey source locally
     via `choco apikey -k <your key here> -s https://chocolatey.org/`

## Build

Compile bazel with msys2 shell and `compile.sh`.

```powershell
pushd scripts/packages/chocolatey
  ./build.ps1 -version 0.3.2 -mode local
popd
```

Should result in `scripts/packages/chocolatey/bazel.<version>.nupkg` being
created.

The `build.ps1` script supports `mode` values `local`, `rc` and `release`.

## Test

0. Build the package (with `-mode local`)

    * run a webserver (`python -m SimpleHTTPServer` in
      `scripts/packages/chocolatey` is convenient and starts one on
      `http://localhost:8000`)

0. Test the install

    The `test.ps1` should install the package cleanly (and error if it did not
    install cleanly), then tell you what to do next.

0. Test the uninstall

    ```sh
    choco uninstall bazel
    # should remove bazel from the system
    ```

Chocolatey's moderation process automates checks here as well.

## Release

Modify `tools/parameters.json` for the new release's URI and checksum once the
release has been published to github releases.

```powershell
./build.ps1 -version <version> -isRelease
./test.ps1 -version <version>
# if the test.ps1 passes
choco push bazel.x.y.z.nupkg --source https://chocolatey.org/
```

Chocolatey.org will then run automated checks and respond to the push via email
to the maintainers.

---

## Maintaining Bazel Scoop package on Windows
- URL: https://bazel.build/contribute/windows-scoop-maintenance
- Source: contribute/windows-scoop-maintenance.mdx
- Slug: /contribute/windows-scoop-maintenance

Note: The Scoop package is experimental. To provide feedback, go to
`@excitoon` in issue tracker.

## Prerequisites

You need:

*    [Scoop package manager](https://scoop.sh/) installed
*    GitHub account in order to publish and create pull requests to
     [scoopinstaller/scoop-main](https://github.com/scoopinstaller/scoop-main)
     * [@excitoon](https://github.com/excitoon) currently maintains this
       unofficial package. Feel free to ask questions by
       [e-mail](mailto:vladimir.chebotarev@gmail.com) or
       [Telegram](http://telegram.me/excitoon).

## Release process

Scoop packages are very easy to maintain. Once you have the URL of released
Bazel, you need to make appropriate changes in
[this file](https://github.com/scoopinstaller/scoop-main/blob/master/bucket/bazel.json):

- update version
- update dependencies if needed
- update URL
- update hash (`sha256` by default)

In your filesystem, `bazel.json` is located in the directory
`%UserProfile%/scoop/buckets/main/bucket` by default. This directory belongs to
your clone of a Git repository
[scoopinstaller/scoop-main](https://github.com/scoopinstaller/scoop-main).

Test the result:

```
scoop uninstall bazel
scoop install bazel
bazel version
bazel something_else
```

The first time, make a fork of
[scoopinstaller/scoop-main](https://github.com/scoopinstaller/scoop-main) and
specify it as your own remote for `%UserProfile%/scoop/buckets/main`:

```
git remote add mine FORK_URL
```

Push your changes to your fork and create a pull request.

---

## Android Build Performance
- URL: https://bazel.build/docs/android-build-performance
- Source: docs/android-build-performance.mdx
- Slug: /docs/android-build-performance

This page contains information on optimizing build performance for Android
apps specifically. For general build performance optimization with Bazel, see
[Optimizing Performance](/rules/performance).

## Recommended flags

The flags are in the
[`bazelrc` configuration syntax](/run/bazelrc#bazelrc-syntax-semantics), so
they can be pasted directly into a `bazelrc` file and invoked with
`--config=<configuration_name>` on the command line.

**Profiling performance**

Bazel writes a JSON trace profile by default to a file called
`command.profile.gz` in Bazel's output base.
See the [JSON Profile documentation](/rules/performance#performance-profiling) for
how to read and interact with the profile.

**Persistent workers for Android build actions**.

A subset of Android build actions has support for
[persistent workers](https://blog.bazel.build/2015/12/10/java-workers.html).

These actions' mnemonics are:

*   DexBuilder
*   Javac
*   Desugar
*   AaptPackage
*   AndroidResourceParser
*   AndroidResourceValidator
*   AndroidResourceCompiler
*   RClassGenerator
*   AndroidResourceLink
*   AndroidAapt2
*   AndroidAssetMerger
*   AndroidResourceMerger
*   AndroidCompiledResourceMerger

Enabling workers can result in better build performance by saving on JVM
startup costs from invoking each of these tools, but at the cost of increased
memory usage on the system by persisting them.

To enable workers for these actions, apply these flags with
`--config=android_workers` on the command line:

```
build:android_workers --strategy=DexBuilder=worker
build:android_workers --strategy=Javac=worker
build:android_workers --strategy=Desugar=worker

# A wrapper flag for these resource processing actions:
# - AndroidResourceParser
# - AndroidResourceValidator
# - AndroidResourceCompiler
# - RClassGenerator
# - AndroidResourceLink
# - AndroidAapt2
# - AndroidAssetMerger
# - AndroidResourceMerger
# - AndroidCompiledResourceMerger
build:android_workers --persistent_android_resource_processor
```

The default number of persistent workers created per action is `4`. We have
[measured improved build performance](https://github.com/bazelbuild/bazel/issues/8586#issuecomment-500070549)
by capping the number of instances for each action to `1` or `2`, although this
may vary depending on the system Bazel is running on, and the project being
built.

To cap the number of instances for an action, apply these flags:

```
build:android_workers --worker_max_instances=DexBuilder=2
build:android_workers --worker_max_instances=Javac=2
build:android_workers --worker_max_instances=Desugar=2
build:android_workers --worker_max_instances=AaptPackage=2
# .. and so on for each action you're interested in.
```

**Using AAPT2**

[`aapt2`](https://developer.android.com/studio/command-line/aapt2) has improved
performance over `aapt` and also creates smaller APKs. To use `aapt2`, use the
`--android_aapt=aapt2` flag or set `aapt2` on the `aapt_version` on
`android_binary` and `android_local_test`.

**SSD optimizations**

The `--experimental_multi_threaded_digest` flag is useful for optimizing digest
computation on SSDs.

---

## Android Instrumentation Tests
- URL: https://bazel.build/docs/android-instrumentation-test
- Source: docs/android-instrumentation-test.mdx
- Slug: /docs/android-instrumentation-test

_If you're new to Bazel, start with the [Building Android with
Bazel](/start/android-app ) tutorial._

![Running Android instrumentation tests in parallel](/docs/images/android_test.gif "Android instrumentation test")

**Figure 1.** Running parallel Android instrumentation tests.

[`android_instrumentation_test`](/reference/be/android#android_instrumentation_test)
allows developers to test their apps on Android emulators and devices.
It utilizes real Android framework APIs and the Android Test Library.

For hermeticity and reproducibility, Bazel creates and launches Android
emulators in a sandbox, ensuring that tests always run from a clean state. Each
test gets an isolated emulator instance, allowing tests to run in parallel
without passing states between them.

For more information on Android instrumentation tests, check out the [Android
developer
documentation](https://developer.android.com/training/testing/unit-testing/instrumented-unit-tests.html).

Please file issues in the [GitHub issue tracker](https://github.com/bazelbuild/bazel/issues).

## How it works

When you run `bazel test` on an `android_instrumentation_test` target for the
first time, Bazel performs the following steps:

1. Builds the test APK, APK under test, and their transitive dependencies
2. Creates, boots, and caches clean emulator states
3. Starts the emulator
4. Installs the APKs
5. Runs tests utilizing the [Android Test Orchestrator](https://developer.android.com/training/testing/junit-runner.html#using-android-test-orchestrator)
6. Shuts down the emulator
7. Reports the results

In subsequent test runs, Bazel boots the emulator from the clean, cached state
created in step 2, so there are no leftover states from previous runs. Caching
emulator state also speeds up test runs.

## Prerequisites

Ensure your environment satisfies the following prerequisites:

- **Linux**. Tested on Ubuntu 16.04, and 18.04.

- **Bazel 0.12.0** or later. Verify the version by running `bazel info release`.

```posix-terminal
bazel info release
```
This results in output similar to the following:

```none {:.devsite-disable-click-to-copy}
release 4.1.0
```

- **KVM**. Bazel requires emulators to have [hardware
  acceleration](https://developer.android.com/studio/run/emulator-acceleration.html#accel-check)
  with KVM on Linux. You can follow these
  [installation instructions](https://help.ubuntu.com/community/KVM/Installation)
  for Ubuntu.

To verify that KVM has the correct configuration, run:

```posix-terminal
apt-get install cpu-checker && kvm-ok
```

If it prints the following message, you have the correct configuration:

```none {:.devsite-disable-click-to-copy}
INFO: /dev/kvm exists
KVM acceleration can be used
```

- **Xvfb**. To run headless tests (for example, on CI servers), Bazel requires
  the [X virtual framebuffer](https://www.x.org/archive/X11R7.6/doc/man/man1/Xvfb.1.xhtml).

To install it, run:

```posix-terminal
apt-get install xvfb
```
Verify that `Xvfb` is installed correctly and is installed at `/usr/bin/Xvfb`
by running:

```posix-terminal
which Xvfb
```
The output is the following:

```{:.devsite-disable-click-to-copy}
/usr/bin/Xvfb
```

- **32-bit Libraries**. Some of the binaries used by the test infrastructure are
  32-bit, so on 64-bit machines, ensure that 32-bit binaries can be run. For
  Ubuntu, install these 32-bit libraries:

```posix-terminal
sudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386 lib32z1 libbz2-1.0:i386
```

## Getting started

Here is a typical target dependency graph of an `android_instrumentation_test`:

![The target dependency graph on an Android instrumentation test](/docs/images/android_instrumentation_test.png "Target dependency graph")

**Figure 2.** Target dependency graph of an `android_instrumentation_test`.


### BUILD file

The graph translates into a `BUILD` file like this:

```python
android_instrumentation_test(
    name = "my_test",
    test_app = ":my_test_app",
    target_device = "@android_test_support//tools/android/emulated_devices/generic_phone:android_23_x86",
)

# Test app and library
android_binary(
    name = "my_test_app",
    instruments = ":my_app",
    manifest = "AndroidTestManifest.xml",
    deps = [":my_test_lib"],
    # ...
)

android_library(
    name = "my_test_lib",
    srcs = glob(["javatest/**/*.java"]),
    deps = [
        ":my_app_lib",
        "@maven//:androidx_test_core",
        "@maven//:androidx_test_runner",
        "@maven//:androidx_test_espresso_espresso_core",
    ],
    # ...
)

# Target app and library under test
android_binary(
    name = "my_app",
    manifest = "AndroidManifest.xml",
    deps = [":my_app_lib"],
    # ...
)

android_library(
    name = "my_app_lib",
    srcs = glob(["java/**/*.java"]),
    deps = [
        "@maven//:androidx_appcompat_appcompat",
        "@maven//:androidx_annotation_annotation",
    ]
    # ...
)
```

The main attributes of the rule `android_instrumentation_test` are:

- `test_app`: An `android_binary` target. This target contains test code and
  dependencies like Espresso and UIAutomator. The selected `android_binary`
  target is required to specify an `instruments` attribute pointing to another
  `android_binary`, which is the app under test.

- `target_device`: An `android_device` target. This target describes the
  specifications of the Android emulator which Bazel uses to create, launch and
  run the tests. See the [section on choosing an Android
  device](#android-device-target) for more information.

The test app's `AndroidManifest.xml` must include [an `<instrumentation>`
tag](https://developer.android.com/studio/test/#configure_instrumentation_manifest_settings).
This tag must specify the attributes for the **package of the target app** and
the **fully qualified class name of the instrumentation test runner**,
`androidx.test.runner.AndroidJUnitRunner`.

Here is an example `AndroidTestManifest.xml` for the test app:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
          xmlns:tools="http://schemas.android.com/tools"
          package="com.example.android.app.test"
    android:versionCode="1"
    android:versionName="1.0">

    <instrumentation
        android:name="androidx.test.runner.AndroidJUnitRunner"
        android:targetPackage="com.example.android.app" />

    <uses-sdk
        android:minSdkVersion="16"
        android:targetSdkVersion="27" />

    <application >
       <!-- ... -->
    </application>
</manifest>
```

### WORKSPACE dependencies

In order to use this rule, your project needs to depend on these external
repositories:

- `@androidsdk`: The Android SDK. Download this through Android Studio.

- `@android_test_support`: Hosts the test runner, emulator launcher, and
  `android_device` targets. You can find the [latest release
  here](https://github.com/android/android-test/releases).

Enable these dependencies by adding the following lines to your `WORKSPACE`
file:

```python
# Android SDK
android_sdk_repository(
    name = "androidsdk",
    path = "/path/to/sdk", # or set ANDROID_HOME
)

# Android Test Support
ATS_COMMIT = "$COMMIT_HASH"
http_archive(
    name = "android_test_support",
    strip_prefix = "android-test-%s" % ATS_COMMIT,
    urls = ["https://github.com/android/android-test/archive/%s.tar.gz" % ATS_COMMIT],
)
load("@android_test_support//:repo.bzl", "android_test_repositories")
android_test_repositories()
```

## Maven dependencies

For managing dependencies on Maven artifacts from repositories, such as [Google
Maven](https://maven.google.com) or [Maven Central](https://central.maven.org),
you should use a Maven resolver, such as
[`rules_jvm_external`](https://github.com/bazelbuild/rules_jvm_external).

The rest of this page shows how to use `rules_jvm_external` to
resolve and fetch dependencies from Maven repositories.

## Choosing an android_device target

`android_instrumentation_test.target_device` specifies which Android device to
run the tests on. These `android_device` targets are defined in
[`@android_test_support`](https://github.com/google/android-testing-support-library/tree/master/tools/android/emulated_devices).

For example, you can query for the sources for a particular target by running:

```posix-terminal
bazel query --output=build @android_test_support//tools/android/emulated_devices/generic_phone:android_23_x86
```
Which results in output that looks similar to:

```python
# .../external/android_test_support/tools/android/emulated_devices/generic_phone/BUILD:43:1
android_device(
  name = "android_23_x86",
  visibility = ["//visibility:public"],
  tags = ["requires-kvm"],
  generator_name = "generic_phone",
  generator_function = "make_device",
  generator_location = "tools/android/emulated_devices/generic_phone/BUILD:43",
  vertical_resolution = 800,
  horizontal_resolution = 480,
  ram = 2048,
  screen_density = 240,
  cache = 32,
  vm_heap = 256,
  system_image = "@android_test_support//tools/android/emulated_devices/generic_phone:android_23_x86_images",
  default_properties = "@android_test_support//tools/android/emulated_devices/generic_phone:_android_23_x86_props",
)
```

The device target names use this template:

```
@android_test_support//tools/android/emulated_devices/{{ "<var>" }}device_type{{ "</var>" }}:{{ "<var>" }}system{{ "</var>" }}_{{ "<var>" }}api_level{{ "</var>" }}_x86_qemu2
```

In order to launch an `android_device`, the `system_image` for the selected API
level is required. To download the system image, use Android SDK's
`tools/bin/sdkmanager`. For example, to download the system image for
`generic_phone:android_23_x86`, run `$sdk/tools/bin/sdkmanager
"system-images;android-23;default;x86"`.

To see the full list of supported `android_device` targets in
`@android_test_support`, run the following command:

```posix-terminal
bazel query 'filter("x86_qemu2$", kind(android_device, @android_test_support//tools/android/emulated_devices/...:*))'
```

Bazel currently supports x86-based emulators only. For better performance, use
`QEMU2` `android_device` targets instead of `QEMU` ones.

## Running tests

To run tests, add these lines to your project's
`<var>project root</var>:<var>/.bazelrc` file.

```
# Configurations for testing with Bazel
# Select a configuration by running
# `bazel test //my:target --config={headless, gui, local_device}`

# Headless instrumentation tests (No GUI)
test:headless --test_arg=--enable_display=false

# Graphical instrumentation tests. Ensure that $DISPLAY is set.
test:gui --test_env=DISPLAY
test:gui --test_arg=--enable_display=true

# Testing with a local emulator or device. Ensure that `adb devices` lists the
# device.
# Run tests serially.
test:local_device --test_strategy=exclusive
# Use the local device broker type, as opposed to WRAPPED_EMULATOR.
test:local_device --test_arg=--device_broker_type=LOCAL_ADB_SERVER
# Uncomment and set $device_id if there is more than one connected device.
# test:local_device --test_arg=--device_serial_number=$device_id
```

Then, use one of the configurations to run tests:

- `bazel test //my/test:target --config=gui`
- `bazel test //my/test:target --config=headless`
- `bazel test //my/test:target --config=local_device`

Use __only one configuration__ or tests will fail.

### Headless testing

With `Xvfb`, it is possible to test with emulators without the graphical
interface, also known as headless testing. To disable the graphical interface
when running tests, pass the test argument `--enable_display=false` to Bazel:

```posix-terminal
bazel test //my/test:target --test_arg=--enable_display=false
```

### GUI testing

If the `$DISPLAY` environment variable is set, it's possible to enable the
graphical interface of the emulator while the test is running. To do this, pass
these test arguments to Bazel:

```posix-terminal
bazel test //my/test:target --test_arg=--enable_display=true --test_env=DISPLAY
```

### Testing with a local emulator or device

Bazel also supports testing directly on a locally launched emulator or connected
device. Pass the flags
`--test_strategy=exclusive` and
`--test_arg=--device_broker_type=LOCAL_ADB_SERVER` to enable local testing mode.
If there is more than one connected device, pass the flag
`--test_arg=--device_serial_number=$device_id` where `$device_id` is the id of
the device/emulator listed in `adb devices`.

## Sample projects

If you are looking for canonical project samples, see the [Android testing
samples](https://github.com/googlesamples/android-testing#experimental-bazel-support)
for projects using Espresso and UIAutomator.

## Espresso setup

If you write UI tests with [Espresso](https://developer.android.com/training/testing/espresso/)
(`androidx.test.espresso`), you can use the following snippets to set up your
Bazel workspace with the list of commonly used Espresso artifacts and their
dependencies:

```
androidx.test.espresso:espresso-core
androidx.test:rules
androidx.test:runner
javax.inject:javax.inject
org.hamcrest:java-hamcrest
junit:junit
```

One way to organize these dependencies is to create a `//:test_deps` shared
library in your `<var>project root</var>/BUILD.bazel` file:

```python
java_library(
    name = "test_deps",
    visibility = ["//visibility:public"],
    exports = [
        "@maven//:androidx_test_espresso_espresso_core",
        "@maven//:androidx_test_rules",
        "@maven//:androidx_test_runner",
        "@maven//:javax_inject_javax_inject"
        "@maven//:org_hamcrest_java_hamcrest",
        "@maven//:junit_junit",
    ],
)
```

Then, add the required dependencies in `<var>project root</var>/WORKSPACE`:

```python
load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

RULES_JVM_EXTERNAL_TAG = "2.8"
RULES_JVM_EXTERNAL_SHA = "79c9850690d7614ecdb72d68394f994fef7534b292c4867ce5e7dec0aa7bdfad"

http_archive(
    name = "rules_jvm_external",
    strip_prefix = "rules_jvm_external-%s" % RULES_JVM_EXTERNAL_TAG,
    sha256 = RULES_JVM_EXTERNAL_SHA,
    url = "https://github.com/bazelbuild/rules_jvm_external/archive/%s.zip" % RULES_JVM_EXTERNAL_TAG,
)

load("@rules_jvm_external//:defs.bzl", "maven_install")

maven_install(
    artifacts = [
        "junit:junit:4.12",
        "javax.inject:javax.inject:1",
        "org.hamcrest:java-hamcrest:2.0.0.0"
        "androidx.test.espresso:espresso-core:3.1.1",
        "androidx.test:rules:aar:1.1.1",
        "androidx.test:runner:aar:1.1.1",
    ],
    repositories = [
        "https://maven.google.com",
        "https://repo1.maven.org/maven2",
    ],
)
```

Finally, in your test `android_binary` target, add the `//:test_deps`
dependency:

```python
android_binary(
    name = "my_test_app",
    instruments = "//path/to:app",
    deps = [
        "//:test_deps",
        # ...
    ],
    # ...
)
```

## Tips

### Reading test logs

Use `--test_output=errors` to print logs for failing tests, or
`--test_output=all` to print all test output. If you're looking for an
individual test log, go to
`$PROJECT_ROOT/bazel-testlogs/path/to/InstrumentationTestTargetName`.

For example, the test logs for `BasicSample` canonical project are in
`bazel-testlogs/ui/espresso/BasicSample/BasicSampleInstrumentationTest`, run:

```posix-terminal
tree bazel-testlogs/ui/espresso/BasicSample/BasicSampleInstrumentationTest
```
This results in the following output:

```none

$ tree bazel-testlogs/ui/espresso/BasicSample/BasicSampleInstrumentationTest
.
├── adb.409923.log
├── broker_logs
│   ├── aapt_binary.10.ok.txt
│   ├── aapt_binary.11.ok.txt
│   ├── adb.12.ok.txt
│   ├── adb.13.ok.txt
│   ├── adb.14.ok.txt
│   ├── adb.15.fail.txt
│   ├── adb.16.ok.txt
│   ├── adb.17.fail.txt
│   ├── adb.18.ok.txt
│   ├── adb.19.fail.txt
│   ├── adb.20.ok.txt
│   ├── adb.21.ok.txt
│   ├── adb.22.ok.txt
│   ├── adb.23.ok.txt
│   ├── adb.24.fail.txt
│   ├── adb.25.ok.txt
│   ├── adb.26.fail.txt
│   ├── adb.27.ok.txt
│   ├── adb.28.fail.txt
│   ├── adb.29.ok.txt
│   ├── adb.2.ok.txt
│   ├── adb.30.ok.txt
│   ├── adb.3.ok.txt
│   ├── adb.4.ok.txt
│   ├── adb.5.ok.txt
│   ├── adb.6.ok.txt
│   ├── adb.7.ok.txt
│   ├── adb.8.ok.txt
│   ├── adb.9.ok.txt
│   ├── android_23_x86.1.ok.txt
│   └── exec-1
│       ├── adb-2.txt
│       ├── emulator-2.txt
│       └── mksdcard-1.txt
├── device_logcat
│   └── logcat1635880625641751077.txt
├── emulator_itCqtc.log
├── outputs.zip
├── pipe.log.txt
├── telnet_pipe.log.txt
└── tmpuRh4cy
    ├── watchdog.err
    └── watchdog.out

4 directories, 41 files
```

### Reading emulator logs

The emulator logs for `android_device` targets are stored in the `/tmp/`
directory with the name `emulator_xxxxx.log`, where `xxxxx` is a
randomly-generated sequence of characters.

Use this command to find the latest emulator log:

```posix-terminal
ls -1t /tmp/emulator_*.log | head -n 1
```

### Testing against multiple API levels

If you would like to test against multiple API levels, you can use a list
comprehension to create test targets for each API level. For example:

```python
API_LEVELS = [
    "19",
    "20",
    "21",
    "22",
]

[android_instrumentation_test(
    name = "my_test_%s" % API_LEVEL,
    test_app = ":my_test_app",
    target_device = "@android_test_support//tools/android/emulated_devices/generic_phone:android_%s_x86_qemu2" % API_LEVEL,
) for API_LEVEL in API_LEVELS]
```

## Known issues

- [Forked adb server processes are not terminated after
  tests](https://github.com/bazelbuild/bazel/issues/4853)
- While APK building works on all platforms (Linux, macOS, Windows), testing
  only works on Linux.
- Even with `--config=local_adb`, users still need to specify
  `android_instrumentation_test.target_device`.
- If using a local device or emulator, Bazel does not uninstall the APKs after
  the test. Clean the packages by running this command:

```posix-terminal
adb shell pm list
packages com.example.android.testing | cut -d ':' -f 2 | tr -d '\r' | xargs
-L1 -t adb uninstall
```

---

## Using the Android Native Development Kit with Bazel
- URL: https://bazel.build/docs/android-ndk
- Source: docs/android-ndk.mdx
- Slug: /docs/android-ndk

_If you're new to Bazel, please start with the [Building Android with
Bazel](/start/android-app ) tutorial._

## Overview

Bazel can run in many different build configurations, including several that use
the Android Native Development Kit (NDK) toolchain. This means that normal
`cc_library` and `cc_binary` rules can be compiled for Android directly within
Bazel. Bazel accomplishes this by using the `android_ndk_repository` repository
rule and its related bzlmod extension.

For general Android
compilation, use [`rules_android`](https://github.com/bazelbuild/rules_android).
This tutorial demonstrates how to integrate C++ library dependencies into
Android apps and uses
[`rules_android_ndk`](https://github.com/bazelbuild/rules_android_ndk) for NDK
toolchain discovery and registration.

## Prerequisites

Please ensure that you have installed the Android SDK and NDK.

### NDK and SDK setup

External repository setup varies depending on whether you are using WORKSPACE
or bzlmod (MODULE.bazel). *Bzlmod is the preferred solution for Bazel 7+.*
Note that the MODULE.bazel and WORKSPACE setup stanzas are independent of
each other.
If you are using one dependency management solution, you don't need to add
the boilerplate for the other.

#### Bzlmod MODULE.bazel setup

Add the following snippet to your MODULE.bazel:

```python
# NDK
bazel_dep(name = "rules_android_ndk", version = "0.1.3")
android_ndk_repository_extension = use_extension("@rules_android_ndk//:extension.bzl", "android_ndk_repository_extension")
use_repo(android_ndk_repository_extension, "androidndk")
register_toolchains("@androidndk//:all")

# SDK
bazel_dep(name = "rules_android", version = "0.6.6")
register_toolchains(
    "@rules_android//toolchains/android:android_default_toolchain",
    "@rules_android//toolchains/android_sdk:android_sdk_tools",
)
android_sdk_repository_extension = use_extension("@rules_android//rules/android_sdk_repository:rule.bzl", "android_sdk_repository_extension")
use_repo(android_sdk_repository_extension, "androidsdk")
register_toolchains("@androidsdk//:sdk-toolchain", "@androidsdk//:all")
```

#### Legacy WORKSPACE setup

Add the following snippet to your `WORKSPACE`:

```python
load("@rules_android//rules:rules.bzl", "android_sdk_repository")
android_sdk_repository(
    name = "androidsdk", # Required. Name *must* be "androidsdk".
    path = "/path/to/sdk", # Optional. Can be omitted if `ANDROID_HOME` environment variable is set.
)

load("@rules_android_ndk//:rules.bzl", "android_ndk_repository")
android_ndk_repository(
    name = "androidndk", # Required. Name *must* be "androidndk".
    path = "/path/to/ndk", # Optional. Can be omitted if `ANDROID_NDK_HOME` environment variable is set.
)
```

Compatibility notes for WORKSPACE:

* Both `rules_android` and `rules_android_ndk` rules require extra
  boilerplate not depicted in the WORKSPACE snippet above. For an up-to-date
  and fully-formed instantiation stanza, see the [WORKSPACE](https://github.com/bazelbuild/rules_android_ndk/blob/main/examples/basic/WORKSPACE)
  file of `rules_android_ndk`'s basic example app.

For more information about the `android_ndk_repository` rule, see its
[docstring](https://github.com/bazelbuild/rules_android_ndk/blob/7b4300f6d731139ca097f3332a5aebae5b0d91d0/rules.bzl#L18-L25).

## Quick start

To build C++ for Android, simply add `cc_library` dependencies to your
`android_binary` or `android_library` rules.

For example, given the following `BUILD` file for an Android app:

```python
# In <project>/app/src/main/BUILD.bazel
load("@rules_cc//cc:cc_library.bzl", "cc_library")
load("@rules_android//rules:rules.bzl", "android_binary", "android_library")

cc_library(
    name = "jni_lib",
    srcs = ["cpp/native-lib.cpp"],
)

android_library(
    name = "lib",
    srcs = ["java/com/example/android/bazel/MainActivity.java"],
    resource_files = glob(["res/**/*"]),
    custom_package = "com.example.android.bazel",
    manifest = "LibraryManifest.xml",
    deps = [":jni_lib"],
)

android_binary(
    name = "app",
    deps = [":lib"],
    manifest = "AndroidManifest.xml",
)
```

This `BUILD` file results in the following target graph:

![Example results](/docs/images/android_ndk.png "Build graph results")

**Figure 1.** Build graph of Android project with cc_library dependencies.

To build the app, simply run:

```posix-terminal
bazel build //app/src/main:app --android_platforms=<your platform>
```

Note that if you don't specify `--android_platforms`, your build will fail with
errors about missing JNI headers.

The `bazel build` command compiles the Java files, Android resource files, and
`cc_library` rules, and packages everything into an APK:

```posix-terminal
$ zipinfo -1 bazel-bin/app/src/main/app.apk
nativedeps
lib/armeabi-v7a/libapp.so
classes.dex
AndroidManifest.xml
...
res/...
...
META-INF/CERT.SF
META-INF/CERT.RSA
META-INF/MANIFEST.MF
```

Bazel compiles all of the cc_libraries into a single shared object (`.so`) file,
targeted the architectures specified by `--android_platforms`.
See the section on [configuring the target ABI](#configuring-target-abi) for
more details.

## Example setup

This example is available in the [Bazel examples
repository](https://github.com/bazelbuild/examples/tree/master/android/ndk).

In the `BUILD.bazel` file, three targets are defined with the `android_binary`,
`android_library`, and `cc_library` rules.

The `android_binary` top-level target builds the APK.

The `cc_library` target contains a single C++ source file with a JNI function
implementation:

```c++
#include <jni.h>
#include <string>

extern "C"
JNIEXPORT jstring

JNICALL
Java_com_example_android_bazel_MainActivity_stringFromJNI(
        JNIEnv *env,
        jobject /* this */) {
    std::string hello = "Hello from C++";
    return env->NewStringUTF(hello.c_str());
}
```

The `android_library` target specifies the Java sources, resource files, and the
dependency on a `cc_library` target. For this example, `MainActivity.java` loads
the shared object file `libapp.so`, and defines the method signature for the JNI
function:

```java
public class MainActivity extends AppCompatActivity {

    static {
        System.loadLibrary("app");
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
       // ...
    }

    public native String stringFromJNI();

}
```

Note: The name of the native library is derived from the name of the top
level `android_binary` target. In this example, it is `app`.

## Configuring the target ABI

To configure the target ABI, use the `--android_platforms` flag as follows:

```posix-terminal
bazel build //:app --android_platforms={{ "<var>" }}comma-separated list of platforms{{ "</var>" }}
```

Just like the `--platforms` flag, the values passed to `--android_platforms` are
the labels of [`platform`](https://bazel.build/reference/be/platforms-and-toolchains#platform)
targets, using standard constraint values to describe your device.

For example, for an Android device with a 64-bit ARM processor, you'd define
your platform like this:

```py
platform(
    name = "android_arm64",
    constraint_values = [
        "@platforms//os:android",
        "@platforms//cpu:arm64",
    ],
)
```

Every Android `platform` should use the [`@platforms//os:android`](https://github.com/bazelbuild/platforms/blob/33a3b209f94856193266871b1545054afb90bb28/os/BUILD#L36)
OS constraint. To migrate the CPU constraint, check this chart:

CPU Value     | Platform
------------- | ------------------------------------------
`armeabi-v7a` | `@platforms//cpu:armv7`
`arm64-v8a`   | `@platforms//cpu:arm64`
`x86`         | `@platforms//cpu:x86_32`
`x86_64`      | `@platforms//cpu:x86_64`

And, of course, for a multi-architecture APK, you pass multiple labels, for
example: `--android_platforms=//:arm64,//:x86_64` (assuming you defined those in
your top-level `BUILD.bazel` file).

Bazel is unable to select a default Android platform, so one must be defined and
specified with `--android_platforms`.

Depending on the NDK revision and Android API level, the following ABIs are
available:

| NDK revision | ABIs                                                        |
|--------------|-------------------------------------------------------------|
| 16 and lower | armeabi, armeabi-v7a, arm64-v8a, mips, mips64, x86, x86\_64 |
| 17 and above | armeabi-v7a, arm64-v8a, x86, x86\_64                        |

See [the NDK docs](https://developer.android.com/ndk/guides/abis.html)
for more information on these ABIs.

Multi-ABI Fat APKs are not recommended for release builds since they increase
the size of the APK, but can be useful for development and QA builds.

## Selecting a C++ standard

Use the following flags to build according to a C++ standard:

| C++ Standard | Flag                    |
|--------------|-------------------------|
| C++98        | Default, no flag needed |
| C++11        | `--cxxopt=-std=c++11`   |
| C++14        | `--cxxopt=-std=c++14`   |
| C++17        | `--cxxopt=-std=c++17`   |

For example:

```posix-terminal
bazel build //:app --cxxopt=-std=c++11
```

Read more about passing compiler and linker flags with `--cxxopt`, `--copt`, and
`--linkopt` in the [User Manual](/docs/user-manual#cxxopt).

Compiler and linker flags can also be specified as attributes in `cc_library`
using `copts` and `linkopts`. For example:

```python
cc_library(
    name = "jni_lib",
    srcs = ["cpp/native-lib.cpp"],
    copts = ["-std=c++11"],
    linkopts = ["-ldl"], # link against libdl
)
```

## Building a `cc_library` for Android without using `android_binary`

To build a standalone `cc_binary` or `cc_library` for Android without using an
`android_binary`, use the `--platforms` flag.

For example, assuming you have defined Android platforms in
`my/platforms/BUILD`:

```posix-terminal
bazel build //my/cc/jni:target \
      --platforms=//my/platforms:x86_64
```

With this approach, the entire build tree is affected.

Note: All of the targets on the command line must be compatible with
building for Android when specifying these flags, which may make it difficult to
use [Bazel wild-cards](/run/build#specifying-build-targets) like
`/...` and `:all`.

These flags can be put into a `bazelrc` config (one for each ABI), in
`<var>project</var>/.bazelrc`:

```
common:android_x86 --platforms=//my/platforms:x86

common:android_armeabi-v7a --platforms=//my/platforms:armeabi-v7a

# In general
common:android_<abi> --platforms=//my/platforms:<abi>
```

Then, to build a `cc_library` for `x86` for example, run:

```posix-terminal
bazel build //my/cc/jni:target --config=android_x86
```

In general, use this method for low-level targets (like `cc_library`) or when
you know exactly what you're building; rely on the automatic configuration
transitions from `android_binary` for high-level targets where you're expecting
to build a lot of targets you don't control.

---

## Android and Bazel
- URL: https://bazel.build/docs/bazel-and-android
- Source: docs/bazel-and-android.mdx
- Slug: /docs/bazel-and-android

This page contains resources that help you use Bazel with Android projects. It
links to a tutorial, build rules, and other information specific to building
Android projects with Bazel.

## Getting started

The following resources will help you work with Bazel on Android projects:

*  [Tutorial: Building an Android app](/start/android-app ). This
   tutorial is a good place to start learning about Bazel commands and concepts,
   and how to build Android apps with Bazel.
*  [Codelab: Building Android Apps with Bazel](https://developer.android.com/codelabs/bazel-android-intro#0).
   This codelab explains how to build Android apps with Bazel.

## Features

Bazel has Android rules for building and testing Android apps, integrating with
the SDK/NDK, and creating emulator images. There are also Bazel plugins for
Android Studio and IntelliJ.

*  [Android rules](/reference/be/android). The Build Encyclopedia describes the rules
   for building and testing Android apps with Bazel.
*  [Integration with Android Studio](/install/ide). Bazel is compatible with
   Android Studio using the [Android Studio with Bazel](https://ij.bazel.build/)
   plugin.
*  [`mobile-install` for Android](/docs/mobile-install). Bazel's `mobile-install`
   feature provides automated build-and-deploy functionality for building and
   testing Android apps directly on Android devices and emulators.
*  [Android instrumentation testing](/docs/android-instrumentation-test) on
   emulators and devices.
*  [Android NDK integration](/docs/android-ndk). Bazel supports compiling to
   native code through direct NDK integration and the C++ rules.
*  [Android build performance](/docs/android-build-performance). This page
   provides information on optimizing build performance for Android apps.

## Further reading

*  Integrating with dependencies from Google Maven and Maven Central with [rules_jvm_external](https://github.com/bazelbuild/rules_jvm_external).
*  Learn [How Android Builds Work in Bazel](https://blog.bazel.build/2018/02/14/how-android-builds-work-in-bazel.html).

---

## Apple Apps and Bazel
- URL: https://bazel.build/docs/bazel-and-apple
- Source: docs/bazel-and-apple.mdx
- Slug: /docs/bazel-and-apple

This page contains resources that help you use Bazel to build macOS and iOS
projects. It links to a tutorial, build rules, and other information specific to
using Bazel to build and test for those platforms.

## Working with Bazel

The following resources will help you work with Bazel on macOS and iOS projects:

*  [Tutorial: Building an iOS app](/start/ios-app)
*  [Objective-C build rules](/reference/be/objective-c)
*  [General Apple rules](https://github.com/bazelbuild/rules_apple)
*  [Integration with Xcode](/install/ide)

## Migrating to Bazel

If you currently build your macOS and iOS projects with Xcode, follow the steps
in the migration guide to start building them with Bazel:

*  [Migrating from Xcode to Bazel](/migrate/xcode)

## Apple apps and new rules

**Note**: Creating new rules is for advanced build and test scenarios.
You do not need it when getting started with Bazel.

The following modules, configuration fragments, and providers will help you
[extend Bazel's capabilities](/extending/concepts)
when building your macOS and iOS projects:

*  Modules:

   *  [`apple_bitcode_mode`](/rules/lib/builtins/apple_bitcode_mode)
   *  [`apple_common`](/rules/lib/toplevel/apple_common)
   *  [`apple_platform`](/rules/lib/builtins/apple_platform)
   *  [`apple_platform_type`](/rules/lib/builtins/apple_platform_type)
   *  [`apple_toolchain`](/rules/lib/builtins/apple_toolchain)

*  Configuration fragments:

   *  [`apple`](/rules/lib/fragments/apple)

*  Providers:

   *  [`ObjcProvider`](/rules/lib/providers/ObjcProvider)
   *  [`XcodeVersionConfig`](/rules/lib/providers/XcodeVersionConfig)

## Xcode selection

If your build requires Xcode, Bazel will select an appropriate version based on
the `--xcode_config` and `--xcode_version` flags. The `--xcode_config` consumes
the set of available Xcode versions and sets a default version if
`--xcode_version` is not passed. This default is overridden by the
`--xcode_version` flag, as long as it is set to an Xcode version that is
represented in the `--xcode_config` target.

If you do not pass `--xcode_config`, Bazel will use the autogenerated
[`XcodeVersionConfig`](/rules/lib/providers/XcodeVersionConfig) that represents the
Xcode versions available on your host machine. The default version is
the newest available Xcode version. This is appropriate for local execution.

If you are performing remote builds, you should set `--xcode_config` to an
[`xcode_config`](/reference/be/objective-c#xcode_config)
target whose `versions` attribute is a list of remotely available
[`xcode_version`](/reference/be/objective-c#xcode_version)
targets, and whose `default` attribute is one of these
[`xcode_versions`](/reference/be/objective-c#xcode_version).

If you are using dynamic execution, you should set `--xcode_config` to an
[`xcode_config`](/reference/be/objective-c#xcode_config)
target whose `remote_versions` attribute is an
[`available_xcodes`](/reference/be/workspace#available_xcodes)
target containing the remotely available Xcode versions, and whose
`local_versions` attribute is an
[`available_xcodes`](/reference/be/workspace#available_xcodes)
target containing the locally available Xcode versions. For `local_versions`,
you probably want to use the autogenerated
`@local_config_xcode//:host_available_xcodes`. The default Xcode version is the
newest mutually available version, if there is one, otherwise the default of the
`local_versions` target. If you prefer to use the `local_versions` default
as the default, you can pass `--experimental_prefer_mutual_default=false`.

---

## C++ and Bazel
- URL: https://bazel.build/docs/bazel-and-cpp
- Source: docs/bazel-and-cpp.mdx
- Slug: /docs/bazel-and-cpp

This page contains resources that help you use Bazel with C++ projects. It links
to a tutorial, build rules, and other information specific to building C++
projects with Bazel.

## Working with Bazel

The following resources will help you work with Bazel on C++ projects:

*  [Tutorial: Building a C++ project](/start/cpp)
*  [C++ common use cases](/tutorials/cpp-use-cases)
*  [C/C++ rules](/reference/be/c-cpp)
*  Essential Libraries
   -  [Abseil](https://abseil.io/docs/cpp/quickstart)
   -  [Boost](https://github.com/nelhage/rules_boost)
   -  [HTTPS Requests: CPR and libcurl](https://github.com/hedronvision/bazel-make-cc-https-easy)
*  [C++ toolchain configuration](/docs/cc-toolchain-config-reference)
*  [Tutorial: Configuring C++ toolchains](/tutorials/ccp-toolchain-config)
*  [Integrating with C++ rules](/configure/integrate-cpp)

## Best practices

In addition to [general Bazel best practices](/configure/best-practices), below are
best practices specific to C++ projects.

### BUILD files

Follow the guidelines below when creating your BUILD files:

*  Each `BUILD` file should contain one [`cc_library`](/reference/be/c-cpp#cc_library)
   rule target per compilation unit in the directory.

*  You should granularize your C++ libraries as much as
   possible to maximize incrementality and parallelize the build.

*  If there is a single source file in `srcs`, name the library the same as
   that C++ file's name. This library should contain C++ file(s), any matching
   header file(s), and the library's direct dependencies. For example:

   ```python
   cc_library(
       name = "mylib",
       srcs = ["mylib.cc"],
       hdrs = ["mylib.h"],
       deps = [":lower-level-lib"]
   )
   ```

*  Use one `cc_test` rule target per `cc_library` target in the file. Name the
   target `[library-name]_test` and the source file `[library-name]_test.cc`.
   For example, a test target for the `mylib` library target shown above would
   look like this:

   ```python
   cc_test(
       name = "mylib_test",
       srcs = ["mylib_test.cc"],
       deps = [":mylib"]
   )
   ```

### Include paths

Follow these guidelines for include paths:

*  Make all include paths relative to the workspace directory.

*  Use quoted includes (`#include "foo/bar/baz.h"`) for non-system headers, not
   angle-brackets (`#include &lt;foo/bar/baz.h&gt;`).

*  Avoid using UNIX directory shortcuts, such as `.` (current directory) or `..`
   (parent directory).

*  For legacy or `third_party` code that requires includes pointing outside the
   project repository, such as external repository includes requiring a prefix,
   use the [`include_prefix`](/reference/be/c-cpp#cc_library.include_prefix) and
   [`strip_include_prefix`](/reference/be/c-cpp#cc_library.strip_include_prefix)
   arguments on the `cc_library` rule target.

### Toolchain features

The following optional [features](/docs/cc-toolchain-config-reference#features)
can improve the hygiene of a C++ project. They can be enabled using the
`--features` command-line flag or the `features` attribute of
[`repo`](/external/overview#repo.bazel),
[`package`](/reference/be/functions#package) or `cc_*` rules:

* The `parse_headers` feature makes it so that the C++ compiler is used to parse
  (but not compile) all header files in the built targets and their dependencies
  when using the
  [`--process_headers_in_dependencies`](/reference/command-line-reference#flag--process_headers_in_dependencies)
  flag. This can help catch issues in header-only libraries and ensure that
  headers are self-contained and independent of the order in which they are
  included.
* The `layering_check` feature enforces that targets only include headers
  provided by their direct dependencies. The default toolchain supports this
  feature on Linux with `clang` as the compiler.

---

## Java and Bazel
- URL: https://bazel.build/docs/bazel-and-java
- Source: docs/bazel-and-java.mdx
- Slug: /docs/bazel-and-java

This page contains resources that help you use Bazel with Java projects. It
links to a tutorial, build rules, and other information specific to building
Java projects with Bazel.

## Working with Bazel

The following resources will help you work with Bazel on Java projects:

*   [Tutorial: Building a Java Project](/start/java)
*   [Java rules](/reference/be/java)

## Migrating to Bazel

If you currently build your Java projects with Maven, follow the steps in the
migration guide to start building your Maven projects with Bazel:

*   [Migrating from Maven to Bazel](/migrate/maven)

## Java versions

There are two relevant versions of Java that are set with configuration flags:

*   the version of the source files in the repository
*   the version of the Java runtime that is used to execute the code and to test
    it

### Configuring the version of the source code in your repository

Without an additional configuration, Bazel assumes all Java source files in the
repository are written in a single Java version. To specify the version of the
sources in the repository add `build --java_language_version={ver}` to
`.bazelrc` file, where `{ver}` is for example `11`. Bazel repository owners
should set this flag so that Bazel and its users can reference the source code's
Java version number. For more details, see
[Java language version flag](/docs/user-manual#java-language-version).

### Configuring the JVM used to execute and test the code

Bazel uses one JDK for compilation and another JVM to execute and test the code.

By default Bazel compiles the code using a JDK it downloads and it executes and
tests the code with the JVM installed on the local machine. Bazel searches for
the JVM using `JAVA_HOME` or path.

The resulting binaries are compatible with locally installed JVM in system
libraries, which means the resulting binaries depend on what is installed on the
machine.

To configure the JVM used for execution and testing use `--java_runtime_version`
flag. The default value is `local_jdk`.

### Hermetic testing and compilation

To create a hermetic compile, you can use command line flag
`--java_runtime_version=remotejdk_11`. The code is compiled for, executed, and
tested on the JVM downloaded from a remote repository. For more details, see
[Java runtime version flag](/docs/user-manual#java_runtime_version).

### Configuring compilation and execution of build tools in Java

There is a second pair of JDK and JVM used to build and execute tools, which are
used in the build process, but are not in the build results. That JDK and JVM
are controlled using `--tool_java_language_version` and
`--tool_java_runtime_version`. Default values are `11` and `remotejdk_11`,
respectively.

#### Compiling using locally installed JDK

Bazel by default compiles using remote JDK, because it is overriding JDK's
internals. The compilation toolchains using locally installed JDK are configured,
however not used.

To compile using locally installed JDK, that is use the compilation toolchains
for local JDK, use additional flag `--extra_toolchains=@local_jdk//:all`,
however, mind that this may not work on JDK of arbitrary vendors.

For more details, see
[configuring Java toolchains](#config-java-toolchains).

## Best practices

In addition to [general Bazel best practices](/configure/best-practices), below are
best practices specific to Java projects.

### Directory structure

Prefer Maven's standard directory layout (sources under `src/main/java`, tests
under `src/test/java`).

### BUILD files

Follow these guidelines when creating your `BUILD` files:

*   Use one `BUILD` file per directory containing Java sources, because this
    improves build performance.

*   Every `BUILD` file should contain one `java_library` rule that looks like
    this:

    ```python
    java_library(
        name = "directory-name",
        srcs = glob(["*.java"]),
        deps = [...],
    )
    ```

*   The name of the library should be the name of the directory containing the
    `BUILD` file. This makes the label of the library shorter, that is use
    `"//package"` instead of `"//package:package"`.

*   The sources should be a non-recursive [`glob`](/reference/be/functions#glob) of
    all Java files in the directory.

*   Tests should be in a matching directory under `src/test` and depend on this
    library.

## Creating new rules for advanced Java builds

**Note**: Creating new rules is for advanced build and test scenarios. You do
not need it when getting started with Bazel.

The following modules, configuration fragments, and providers will help you
[extend Bazel's capabilities](/extending/concepts) when building your Java
projects:

*   Main Java module: [`java_common`](/rules/lib/toplevel/java_common)
*   Main Java provider: [`JavaInfo`](/rules/lib/providers/JavaInfo)
*   Configuration fragment: [`java`](/rules/lib/fragments/java)
*   Other modules:

    *   [`java_annotation_processing`](/rules/lib/builtins/java_annotation_processing)
    *   [`java_compilation_info`](/rules/lib/providers/java_compilation_info)
    *   [`java_output_jars`](/rules/lib/providers/java_output_jars)
    *   [`JavaRuntimeInfo`](/rules/lib/providers/JavaRuntimeInfo)
    *   [`JavaToolchainInfo`](/rules/lib/providers/JavaToolchainInfo)

## Configuring the Java toolchains

Bazel uses two types of Java toolchains:

* execution, used to execute and test Java binaries, controlled with the
  `--java_runtime_version` flag
* compilation, used to compile Java sources, controlled with the
  `--java_language_version` flag

### Configuring additional execution toolchains

Execution toolchain is the JVM, either local or from a repository, with some
additional information about its version, operating system, and CPU
architecture.

Java execution toolchains may added using the `local_java_repository` or
`remote_java_repository` repo rules in a module extension. Adding the rule makes
the JVM available using a flag. When multiple definitions for the same operating
system and CPU architecture are given, the first one is used.

Example configuration of local JVM in MODULE.bazel:

```python
custom_jdk = use_extension("@rules_java//java:extensions.bzl", "java_repository")

custom_jdk.local(
  name = "additionaljdk",          # Can be used with --java_runtime_version=additionaljdk, --java_runtime_version=11 or --java_runtime_version=additionaljdk_11
  version = "11",                  # Optional, if not set it is autodetected
  java_home = "/usr/lib/jdk-15/",  # Path to directory containing bin/java
)
use_repo(custom_jdk, "additionaljdk")
register_toolchains("@additionaljdk//:all")
```

Example configuration of remote JVM:

```python
custom_jdk = use_extension("@rules_java//java:extensions.bzl", "java_repository")

custom_jdk.remote(
  name = "openjdk_canary_linux_arm",
  prefix = "openjdk_canary", # Can be used with --java_runtime_version=openjdk_canary_11
  version = "11",            # or --java_runtime_version=11
  target_compatible_with = [ # Specifies constraints this JVM is compatible with
    "@platforms//cpu:arm",
    "@platforms//os:linux",
  ],
  urls = ...,               # Other parameters are from http_repository rule.
  sha256 = ...,
  strip_prefix = ...
)
use_repo(custom_jdk, "openjdk_canary_linux_arm", "openjdk_canary_linux_arm_toolchain_config_repo")

register_toolchains("@openjdk_canary_linux_arm_toolchain_config_repo//:all")
```

### Configuring additional compilation toolchains

Compilation toolchain is composed of JDK and multiple tools that Bazel uses
during the compilation and that provides additional features, such as: Error
Prone, strict Java dependencies, header compilation, Android desugaring,
coverage instrumentation, and genclass handling for IDEs.

JavaBuilder is a Bazel-bundled tool that executes compilation, and provides the
aforementioned features. Actual compilation is executed using the internal
compiler by the JDK. The JDK used for compilation is specified by `java_runtime`
attribute of the toolchain.

Bazel overrides some JDK internals. In case of JDK version > 9,
`java.compiler` and `jdk.compiler` modules are patched using JDK's flag
`--patch_module`. In case of JDK version 8, the Java compiler is patched using
`-Xbootclasspath` flag.

VanillaJavaBuilder is a second implementation of JavaBuilder,
which does not modify JDK's internal compiler and does not have any of the
additional features. VanillaJavaBuilder is not used by any of the built-in
toolchains.

In addition to JavaBuilder, Bazel uses several other tools during compilation.

The `ijar` tool processes `jar` files to remove everything except call
signatures. Resulting jars are called header jars. They are used to improve the
compilation incrementality by only recompiling downstream dependents when the
body of a function changes.

The `singlejar` tool packs together multiple `jar` files into a single one.

The `genclass` tool post-processes the output of a Java compilation, and produces
a `jar` containing only the class files for sources that were generated by
annotation processors.

The `JacocoRunner` tool runs Jacoco over instrumented files and outputs results in
LCOV format.

The `TestRunner` tool executes JUnit 4 tests in a controlled environment.

You can reconfigure the compilation by adding `default_java_toolchain` macro to
a `BUILD` file and registering it either by adding `register_toolchains` rule to
the `MODULE.bazel` file or by using
[`--extra_toolchains`](/docs/user-manual#extra-toolchains) flag.

The toolchain is only used when the `source_version` attribute matches the
value specified by `--java_language_version` flag.

Example toolchain configuration:

```python
load(
  "@rules_java//toolchains:default_java_toolchain.bzl",
  "default_java_toolchain", "DEFAULT_TOOLCHAIN_CONFIGURATION", "BASE_JDK9_JVM_OPTS", "DEFAULT_JAVACOPTS"
)

default_java_toolchain(
  name = "repository_default_toolchain",
  configuration = DEFAULT_TOOLCHAIN_CONFIGURATION,        # One of predefined configurations
                                                          # Other parameters are from java_toolchain rule:
  java_runtime = "@rules_java//toolchains:remote_jdk11", # JDK to use for compilation and toolchain's tools execution
  jvm_opts = BASE_JDK9_JVM_OPTS + ["--enable_preview"],   # Additional JDK options
  javacopts = DEFAULT_JAVACOPTS + ["--enable_preview"],   # Additional javac options
  source_version = "9",
)
```

which can be used using `--extra_toolchains=//:repository_default_toolchain_definition`
or by adding `register_toolchains("//:repository_default_toolchain_definition")`
to the workpace.

Predefined configurations:

-   `DEFAULT_TOOLCHAIN_CONFIGURATION`: all features, supports JDK versions >= 9
-   `VANILLA_TOOLCHAIN_CONFIGURATION`: no additional features, supports JDKs of
    arbitrary vendors.
-   `PREBUILT_TOOLCHAIN_CONFIGURATION`: same as default, but only use prebuilt
    tools (`ijar`, `singlejar`)
-   `NONPREBUILT_TOOLCHAIN_CONFIGURATION`: same as default, but all tools are
    built from sources (this may be useful on operating system with different
    libc)

#### Configuring JVM and Java compiler flags

You may configure JVM and javac flags either with flags or with
 `default_java_toolchain` attributes.

The relevant flags are `--jvmopt`, `--host_jvmopt`, `--javacopt`,  and
`--host_javacopt`.

The relevant `default_java_toolchain` attributes are `javacopts`, `jvm_opts`,
`javabuilder_jvm_opts`, and `turbine_jvm_opts`.

#### Package specific Java compiler flags configuration

You can configure different Java compiler flags for specific source
files using `package_configuration` attribute of `default_java_toolchain`.
Please refer to the example below.

```python
load("@rules_java//toolchains:default_java_toolchain.bzl", "default_java_toolchain")

# This is a convenience macro that inherits values from Bazel's default java_toolchain
default_java_toolchain(
    name = "toolchain",
    package_configuration = [
        ":error_prone",
    ],
    visibility = ["//visibility:public"],
)

# This associates a set of javac flags with a set of packages
java_package_configuration(
    name = "error_prone",
    javacopts = [
        "-Xep:MissingOverride:ERROR",
    ],
    packages = ["error_prone_packages"],
)

# This is a regular package_group, which is used to specify a set of packages to apply flags to
package_group(
    name = "error_prone_packages",
    packages = [
        "//foo/...",
        "-//foo/bar/...", # this is an exclusion
    ],
)
```

#### Multiple versions of Java source code in a single repository

Bazel only supports compiling a single version of Java sources in a build.
build. This means that when building a Java test or an application, all
 dependencies are built against the same Java version.

However, separate builds may be executed using different flags.

To make the task of using different flags easier, sets of flags for a specific
version may be grouped with `.bazelrc` configs":

```python
build:java8 --java_language_version=8
build:java8 --java_runtime_version=local_jdk_8
build:java11 --java_language_version=11
build:java11 --java_runtime_version=remotejdk_11
```

These configs can be used with the `--config` flag, for example
`bazel test --config=java11 //:java11_test`.

---

## JavaScript and Bazel
- URL: https://bazel.build/docs/bazel-and-javascript
- Source: docs/bazel-and-javascript.mdx
- Slug: /docs/bazel-and-javascript

This page contains resources that help you use Bazel with JavaScript projects.
It links to build rules and other information specific to building JavaScript
with Bazel.

The following resources will help you work with Bazel on JavaScript projects:

*  [NodeJS toolchain](https://github.com/bazelbuild/rules_nodejs)
*  [rules_js](https://github.com/aspect-build/rules_js) - Bazel rules for building JavaScript programs
*  [rules_esbuild](https://github.com/aspect-build/rules_esbuild) - Bazel rules for [esbuild](https://esbuild.github.io) JS bundler
*  [rules_terser](https://github.com/aspect-build/rules_terser) - Bazel rules for [Terser](https://terser.org) - a JavaScript minifier
*  [rules_swc](https://github.com/aspect-build/rules_swc) - Bazel rules for [swc](https://swc.rs)
*  [rules_ts](https://github.com/aspect-build/rules_ts) - Bazel rules for [TypeScript](http://typescriptlang.org)
*  [rules_webpack](https://github.com/aspect-build/rules_webpack) - Bazel rules for [Webpack](https://webpack.js.org)
*  [rules_rollup](https://github.com/aspect-build/rules_rollup) - Bazel rules for [Rollup](https://rollupjs.org) - a JavaScript bundler
*  [rules_jest](https://github.com/aspect-build/rules_jest) - Bazel rules to run tests using [Jest](https://jestjs.io)
*  [rules_jasmine](https://github.com/aspect-build/rules_jasmine) - Bazel rules to run tests using [Jasmine](https://jasmine.github.io/)
*  [rules_cypress](https://github.com/aspect-build/rules_cypress) - Bazel rules to run tests using [Cypress](https://cypress.io)
*  [rules_deno](https://github.com/aspect-build/rules_deno) - Bazel rules for [Deno](http://deno.land)

---

## Configurable Build Attributes
- URL: https://bazel.build/docs/configurable-attributes
- Source: docs/configurable-attributes.mdx
- Slug: /docs/configurable-attributes

**_Configurable attributes_**, commonly known as [`select()`](
/reference/be/functions#select), is a Bazel feature that lets users toggle the values
of build rule attributes at the command line.

This can be used, for example, for a multiplatform library that automatically
chooses the appropriate implementation for the architecture, or for a
feature-configurable binary that can be customized at build time.

## Example

```python
# myapp/BUILD

cc_binary(
    name = "mybinary",
    srcs = ["main.cc"],
    deps = select({
        ":arm_build": [":arm_lib"],
        ":x86_debug_build": [":x86_dev_lib"],
        "//conditions:default": [":generic_lib"],
    }),
)

config_setting(
    name = "arm_build",
    values = {"cpu": "arm"},
)

config_setting(
    name = "x86_debug_build",
    values = {
        "cpu": "x86",
        "compilation_mode": "dbg",
    },
)
```

This declares a `cc_binary` that "chooses" its deps based on the flags at the
command line. Specifically, `deps` becomes:

<table>
  <tr style="background: #E9E9E9; font-weight: bold">
    <td>Command</td>
    <td>deps =</td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary --cpu=arm</code></td>
    <td><code>[":arm_lib"]</code></td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary -c dbg --cpu=x86</code></td>
    <td><code>[":x86_dev_lib"]</code></td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary --cpu=ppc</code></td>
    <td><code>[":generic_lib"]</code></td>
  </tr>
  <tr>
    <td><code>bazel build //myapp:mybinary -c dbg --cpu=ppc</code></td>
    <td><code>[":generic_lib"]</code></td>
  </tr>
</table>

`select()` serves as a placeholder for a value that will be chosen based on
*configuration conditions*, which are labels referencing [`config_setting`](/reference/be/general#config_setting)
targets. By using `select()` in a configurable attribute, the attribute
effectively adopts different values when different conditions hold.

Matches must be unambiguous: if multiple conditions match then either:

*  They all resolve to the same value. For example, when running on linux x86, this is unambiguous
   `{"@platforms//os:linux": "Hello", "@platforms//cpu:x86_64": "Hello"}` because both branches resolve to "hello".
*  One's `values` is a strict superset of all others'. For example, `values = {"cpu": "x86", "compilation_mode": "dbg"}`
   is an unambiguous specialization of `values = {"cpu": "x86"}`.

The built-in condition [`//conditions:default`](#default-condition) automatically matches when
nothing else does.

While this example uses `deps`, `select()` works just as well on `srcs`,
`resources`, `cmd`, and most other attributes. Only a small number of attributes
are *non-configurable*, and these are clearly annotated. For example,
`config_setting`'s own
[`values`](/reference/be/general#config_setting.values) attribute is non-configurable.

## `select()` and dependencies

Certain attributes change the build parameters for all transitive dependencies
under a target. For example, `genrule`'s `tools` changes `--cpu` to the CPU of
the machine running Bazel (which, thanks to cross-compilation, may be different
than the CPU the target is built for). This is known as a
[configuration transition](/reference/glossary#transition).

Given

```python
#myapp/BUILD

config_setting(
    name = "arm_cpu",
    values = {"cpu": "arm"},
)

config_setting(
    name = "x86_cpu",
    values = {"cpu": "x86"},
)

genrule(
    name = "my_genrule",
    srcs = select({
        ":arm_cpu": ["g_arm.src"],
        ":x86_cpu": ["g_x86.src"],
    }),
    tools = select({
        ":arm_cpu": [":tool1"],
        ":x86_cpu": [":tool2"],
    }),
)

cc_binary(
    name = "tool1",
    srcs = select({
        ":arm_cpu": ["armtool.cc"],
        ":x86_cpu": ["x86tool.cc"],
    }),
)
```

running

```sh
$ bazel build //myapp:my_genrule --cpu=arm
```

on an `x86` developer machine binds the build to `g_arm.src`, `tool1`, and
`x86tool.cc`. Both of the `select`s attached to `my_genrule` use `my_genrule`'s
build parameters, which include `--cpu=arm`. The `tools` attribute changes
`--cpu` to `x86` for `tool1` and its transitive dependencies. The `select` on
`tool1` uses `tool1`'s build parameters, which include `--cpu=x86`.

## Configuration conditions

Each key in a configurable attribute is a label reference to a
[`config_setting`](/reference/be/general#config_setting) or
[`constraint_value`](/reference/be/platforms-and-toolchains#constraint_value).

`config_setting` is just a collection of
expected command line flag settings. By encapsulating these in a target, it's
easy to maintain "standard" conditions users can reference from multiple places.

`constraint_value` provides support for [multi-platform behavior](#platforms).

### Built-in flags

Flags like `--cpu` are built into Bazel: the build tool natively understands
them for all builds in all projects. These are specified with
[`config_setting`](/reference/be/general#config_setting)'s
[`values`](/reference/be/general#config_setting.values) attribute:

```python
config_setting(
    name = "meaningful_condition_name",
    values = {
        "flag1": "value1",
        "flag2": "value2",
        ...
    },
)
```

`flagN` is a flag name (without `--`, so `"cpu"` instead of `"--cpu"`). `valueN`
is the expected value for that flag. `:meaningful_condition_name` matches if
*every* entry in `values` matches. Order is irrelevant.

`valueN` is parsed as if it was set on the command line. This means:

*  `values = { "compilation_mode": "opt" }` matches `bazel build -c opt`
*  `values = { "force_pic": "true" }` matches `bazel build --force_pic=1`
*  `values = { "force_pic": "0" }` matches `bazel build --noforce_pic`

`config_setting` only supports flags that affect target behavior. For example,
[`--show_progress`](/docs/user-manual#show-progress) isn't allowed because
it only affects how Bazel reports progress to the user. Targets can't use that
flag to construct their results. The exact set of supported flags isn't
documented. In practice, most flags that "make sense" work.

### Custom flags

You can model your own project-specific flags with
[Starlark build settings][BuildSettings]. Unlike built-in flags, these are
defined as build targets, so Bazel references them with target labels.

These are triggered with [`config_setting`](/reference/be/general#config_setting)'s
[`flag_values`](/reference/be/general#config_setting.flag_values)
attribute:

```python
config_setting(
    name = "meaningful_condition_name",
    flag_values = {
        "//myflags:flag1": "value1",
        "//myflags:flag2": "value2",
        ...
    },
)
```

Behavior is the same as for [built-in flags](#built-in-flags). See [here](https://github.com/bazelbuild/examples/tree/HEAD/configurations/select_on_build_setting)
for a working example.

[`--define`](/reference/command-line-reference#flag--define)
is an alternative legacy syntax for custom flags (for example
`--define foo=bar`). This can be expressed either in the
[values](/reference/be/general#config_setting.values) attribute
(`values = {"define": "foo=bar"}`) or the
[define_values](/reference/be/general#config_setting.define_values) attribute
(`define_values = {"foo": "bar"}`). `--define` is only supported for backwards
compatibility. Prefer Starlark build settings whenever possible.

`values`, `flag_values`, and `define_values` evaluate independently. The
`config_setting` matches if all values across all of them match.

## The default condition

The built-in condition `//conditions:default` matches when no other condition
matches.

Because of the "exactly one match" rule, a configurable attribute with no match
and no default condition emits a `"no matching conditions"` error. This can
protect against silent failures from unexpected settings:

```python
# myapp/BUILD

config_setting(
    name = "x86_cpu",
    values = {"cpu": "x86"},
)

cc_library(
    name = "x86_only_lib",
    srcs = select({
        ":x86_cpu": ["lib.cc"],
    }),
)
```

```sh
$ bazel build //myapp:x86_only_lib --cpu=arm
ERROR: Configurable attribute "srcs" doesn't match this configuration (would
a default condition help?).
Conditions checked:
  //myapp:x86_cpu
```

For even clearer errors, you can set custom messages with `select()`'s
[`no_match_error`](#custom-error-messages) attribute.

## Platforms

While the ability to specify multiple flags on the command line provides
flexibility, it can also be burdensome to individually set each one every time
you want to build a target.
   [Platforms](/extending/platforms)
let you consolidate these into simple bundles.

```python
# myapp/BUILD

sh_binary(
    name = "my_rocks",
    srcs = select({
        ":basalt": ["pyroxene.sh"],
        ":marble": ["calcite.sh"],
        "//conditions:default": ["feldspar.sh"],
    }),
)

config_setting(
    name = "basalt",
    constraint_values = [
        ":black",
        ":igneous",
    ],
)

config_setting(
    name = "marble",
    constraint_values = [
        ":white",
        ":metamorphic",
    ],
)

# constraint_setting acts as an enum type, and constraint_value as an enum value.
constraint_setting(name = "color")
constraint_value(name = "black", constraint_setting = "color")
constraint_value(name = "white", constraint_setting = "color")
constraint_setting(name = "texture")
constraint_value(name = "smooth", constraint_setting = "texture")
constraint_setting(name = "type")
constraint_value(name = "igneous", constraint_setting = "type")
constraint_value(name = "metamorphic", constraint_setting = "type")

platform(
    name = "basalt_platform",
    constraint_values = [
        ":black",
        ":igneous",
    ],
)

platform(
    name = "marble_platform",
    constraint_values = [
        ":white",
        ":smooth",
        ":metamorphic",
    ],
)
```

The platform can be specified on the command line. It activates the
`config_setting`s that contain a subset of the platform's `constraint_values`,
allowing those `config_setting`s to match in `select()` expressions.

For example, in order to set the `srcs` attribute of `my_rocks` to `calcite.sh`,
you can simply run

```sh
bazel build //my_app:my_rocks --platforms=//myapp:marble_platform
```

Without platforms, this might look something like

```sh
bazel build //my_app:my_rocks --define color=white --define texture=smooth --define type=metamorphic
```

`select()` can also directly read `constraint_value`s:

```python
constraint_setting(name = "type")
constraint_value(name = "igneous", constraint_setting = "type")
constraint_value(name = "metamorphic", constraint_setting = "type")
sh_binary(
    name = "my_rocks",
    srcs = select({
        ":igneous": ["igneous.sh"],
        ":metamorphic" ["metamorphic.sh"],
    }),
)
```

This saves the need for boilerplate `config_setting`s when you only need to
check against single values.

Platforms are still under development. See the
[documentation](/concepts/platforms) for details.

## Combining `select()`s

`select` can appear multiple times in the same attribute:

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"] +
           select({
               ":armeabi_mode": ["armeabi_src.sh"],
               ":x86_mode": ["x86_src.sh"],
           }) +
           select({
               ":opt_mode": ["opt_extras.sh"],
               ":dbg_mode": ["dbg_extras.sh"],
           }),
)
```

Note: Some restrictions apply on what can be combined in the `select`s values:
 - Duplicate labels can appear in different paths of the same `select`.
 - Duplicate labels can *not* appear within the same path of a `select`.
 - Duplicate labels can *not* appear across multiple combined `select`s (no matter what path)

`select` cannot appear inside another `select`. If you need to nest `selects`
and your attribute takes other targets as values, use an intermediate target:

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":armeabi_mode": [":armeabi_lib"],
        ...
    }),
)

sh_library(
    name = "armeabi_lib",
    srcs = select({
        ":opt_mode": ["armeabi_with_opt.sh"],
        ...
    }),
)
```

If you need a `select` to match when multiple conditions match, consider [AND
chaining](#and-chaining).

## OR chaining

Consider the following:

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1": [":standard_lib"],
        ":config2": [":standard_lib"],
        ":config3": [":standard_lib"],
        ":config4": [":special_lib"],
    }),
)
```

Most conditions evaluate to the same dep. But this syntax is hard to read and
maintain. It would be nice to not have to repeat `[":standard_lib"]` multiple
times.

One option is to predefine the value as a BUILD variable:

```python
STANDARD_DEP = [":standard_lib"]

sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1": STANDARD_DEP,
        ":config2": STANDARD_DEP,
        ":config3": STANDARD_DEP,
        ":config4": [":special_lib"],
    }),
)
```

This makes it easier to manage the dependency. But it still causes unnecessary
duplication.

For more direct support, use one of the following:

### `selects.with_or`

The
[with_or](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md#selectswith_or)
macro in [Skylib](https://github.com/bazelbuild/bazel-skylib)'s
[`selects`](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md)
module supports `OR`ing conditions directly inside a `select`:

```python
load("@bazel_skylib//lib:selects.bzl", "selects")
```

```python
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = selects.with_or({
        (":config1", ":config2", ":config3"): [":standard_lib"],
        ":config4": [":special_lib"],
    }),
)
```

### `selects.config_setting_group`


The
[config_setting_group](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md#selectsconfig_setting_group)
macro in [Skylib](https://github.com/bazelbuild/bazel-skylib)'s
[`selects`](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md)
module supports `OR`ing multiple `config_setting`s:

```python
load("@bazel_skylib//lib:selects.bzl", "selects")
```


```python
config_setting(
    name = "config1",
    values = {"cpu": "arm"},
)
config_setting(
    name = "config2",
    values = {"compilation_mode": "dbg"},
)
selects.config_setting_group(
    name = "config1_or_2",
    match_any = [":config1", ":config2"],
)
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1_or_2": [":standard_lib"],
        "//conditions:default": [":other_lib"],
    }),
)
```

Unlike `selects.with_or`, different targets can share `:config1_or_2` across
different attributes.

It's an error for multiple conditions to match unless one is an unambiguous
"specialization" of the others or they all resolve to the same value. See [here](#configurable-build-example) for details.

## AND chaining

If you need a `select` branch to match when multiple conditions match, use the
[Skylib](https://github.com/bazelbuild/bazel-skylib) macro
[config_setting_group](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md#selectsconfig_setting_group):

```python
config_setting(
    name = "config1",
    values = {"cpu": "arm"},
)
config_setting(
    name = "config2",
    values = {"compilation_mode": "dbg"},
)
selects.config_setting_group(
    name = "config1_and_2",
    match_all = [":config1", ":config2"],
)
sh_binary(
    name = "my_target",
    srcs = ["always_include.sh"],
    deps = select({
        ":config1_and_2": [":standard_lib"],
        "//conditions:default": [":other_lib"],
    }),
)
```

Unlike OR chaining, existing `config_setting`s can't be directly `AND`ed
inside a `select`. You have to explicitly wrap them in a `config_setting_group`.

## Custom error messages

By default, when no condition matches, the target the `select()` is attached to
fails with the error:

```sh
ERROR: Configurable attribute "deps" doesn't match this configuration (would
a default condition help?).
Conditions checked:
  //tools/cc_target_os:darwin
  //tools/cc_target_os:android
```

This can be customized with the [`no_match_error`](/reference/be/functions#select)
attribute:

```python
cc_library(
    name = "my_lib",
    deps = select(
        {
            "//tools/cc_target_os:android": [":android_deps"],
            "//tools/cc_target_os:windows": [":windows_deps"],
        },
        no_match_error = "Please build with an Android or Windows toolchain",
    ),
)
```

```sh
$ bazel build //myapp:my_lib
ERROR: Configurable attribute "deps" doesn't match this configuration: Please
build with an Android or Windows toolchain
```

## Rules compatibility

Rule implementations receive the *resolved values* of configurable
attributes. For example, given:

```python
# myapp/BUILD

some_rule(
    name = "my_target",
    some_attr = select({
        ":foo_mode": [":foo"],
        ":bar_mode": [":bar"],
    }),
)
```

```sh
$ bazel build //myapp/my_target --define mode=foo
```

Rule implementation code sees `ctx.attr.some_attr` as `[":foo"]`.

Macros can accept `select()` clauses and pass them through to native
rules. But *they cannot directly manipulate them*. For example, there's no way
for a macro to convert

```python
select({"foo": "val"}, ...)
```

to

```python
select({"foo": "val_with_suffix"}, ...)
```

This is for two reasons.

First, macros that need to know which path a `select` will choose *cannot work*
because macros are evaluated in Bazel's [loading phase](/run/build#loading),
which occurs before flag values are known.
This is a core Bazel design restriction that's unlikely to change any time soon.

Second, macros that just need to iterate over *all* `select` paths, while
technically feasible, lack a coherent UI. Further design is necessary to change
this.

## Bazel query and cquery

Bazel [`query`](/query/guide) operates over Bazel's
[loading phase](/reference/glossary#loading-phase).
This means it doesn't know what command line flags a target uses since those
flags aren't evaluated until later in the build (in the
[analysis phase](/reference/glossary#analysis-phase)).
So it can't determine which `select()` branches are chosen.

Bazel [`cquery`](/query/cquery) operates after Bazel's analysis phase, so it has
all this information and can accurately resolve `select()`s.

Consider:

```python
load("@bazel_skylib//rules:common_settings.bzl", "string_flag")
```
```python
# myapp/BUILD

string_flag(
    name = "dog_type",
    build_setting_default = "cat"
)

cc_library(
    name = "my_lib",
    deps = select({
        ":long": [":foo_dep"],
        ":short": [":bar_dep"],
    }),
)

config_setting(
    name = "long",
    flag_values = {":dog_type": "dachshund"},
)

config_setting(
    name = "short",
    flag_values = {":dog_type": "pug"},
)
```

`query` overapproximates `:my_lib`'s dependencies:

```sh
$ bazel query 'deps(//myapp:my_lib)'
//myapp:my_lib
//myapp:foo_dep
//myapp:bar_dep
```

while `cquery` shows its exact dependencies:

```sh
$ bazel cquery 'deps(//myapp:my_lib)' --//myapp:dog_type=pug
//myapp:my_lib
//myapp:bar_dep
```

## FAQ

### Why doesn't select() work in macros?

select() *does* work in rules! See [Rules compatibility](#rules-compatibility) for
details.

The key issue this question usually means is that select() doesn't work in
*macros*. These are different than *rules*. See the
documentation on [rules](/extending/rules) and [macros](/extending/macros)
to understand the difference.
Here's an end-to-end example:

Define a rule and macro:

```python
# myapp/defs.bzl

# Rule implementation: when an attribute is read, all select()s have already
# been resolved. So it looks like a plain old attribute just like any other.
def _impl(ctx):
    name = ctx.attr.name
    allcaps = ctx.attr.my_config_string.upper()  # This works fine on all values.
    print("My name is " + name + " with custom message: " + allcaps)

# Rule declaration:
my_custom_bazel_rule = rule(
    implementation = _impl,
    attrs = {"my_config_string": attr.string()},
)

# Macro declaration:
def my_custom_bazel_macro(name, my_config_string):
    allcaps = my_config_string.upper()  # This line won't work with select(s).
    print("My name is " + name + " with custom message: " + allcaps)
```

Instantiate the rule and macro:

```python
# myapp/BUILD

load("//myapp:defs.bzl", "my_custom_bazel_rule")
load("//myapp:defs.bzl", "my_custom_bazel_macro")

my_custom_bazel_rule(
    name = "happy_rule",
    my_config_string = select({
        "//third_party/bazel_platforms/cpu:x86_32": "first string",
        "//third_party/bazel_platforms/cpu:ppc": "second string",
    }),
)

my_custom_bazel_macro(
    name = "happy_macro",
    my_config_string = "fixed string",
)

my_custom_bazel_macro(
    name = "sad_macro",
    my_config_string = select({
        "//third_party/bazel_platforms/cpu:x86_32": "first string",
        "//third_party/bazel_platforms/cpu:ppc": "other string",
    }),
)
```

Building fails because `sad_macro` can't process the `select()`:

```sh
$ bazel build //myapp:all
ERROR: /myworkspace/myapp/BUILD:17:1: Traceback
  (most recent call last):
File "/myworkspace/myapp/BUILD", line 17
my_custom_bazel_macro(name = "sad_macro", my_config_stri..."}))
File "/myworkspace/myapp/defs.bzl", line 4, in
  my_custom_bazel_macro
my_config_string.upper()
type 'select' has no method upper().
ERROR: error loading package 'myapp': Package 'myapp' contains errors.
```

Building succeeds when you comment out `sad_macro`:

```sh
# Comment out sad_macro so it doesn't mess up the build.
$ bazel build //myapp:all
DEBUG: /myworkspace/myapp/defs.bzl:5:3: My name is happy_macro with custom message: FIXED STRING.
DEBUG: /myworkspace/myapp/hi.bzl:15:3: My name is happy_rule with custom message: FIRST STRING.
```

This is impossible to change because *by definition* macros are evaluated before
Bazel reads the build's command line flags. That means there isn't enough
information to evaluate select()s.

Macros can, however, pass `select()`s as opaque blobs to rules:

```python
# myapp/defs.bzl

def my_custom_bazel_macro(name, my_config_string):
    print("Invoking macro " + name)
    my_custom_bazel_rule(
        name = name + "_as_target",
        my_config_string = my_config_string,
    )
```

```sh
$ bazel build //myapp:sad_macro_less_sad
DEBUG: /myworkspace/myapp/defs.bzl:23:3: Invoking macro sad_macro_less_sad.
DEBUG: /myworkspace/myapp/defs.bzl:15:3: My name is sad_macro_less_sad with custom message: FIRST STRING.
```

### Why does select() always return true?

Because *macros* (but not rules) by definition
[can't evaluate `select()`s](#faq-select-macro), any attempt to do so
usually produces an error:

```sh
ERROR: /myworkspace/myapp/BUILD:17:1: Traceback
  (most recent call last):
File "/myworkspace/myapp/BUILD", line 17
my_custom_bazel_macro(name = "sad_macro", my_config_stri..."}))
File "/myworkspace/myapp/defs.bzl", line 4, in
  my_custom_bazel_macro
my_config_string.upper()
type 'select' has no method upper().
```

Booleans are a special case that fail silently, so you should be particularly
vigilant with them:

```sh
$ cat myapp/defs.bzl
def my_boolean_macro(boolval):
  print("TRUE" if boolval else "FALSE")

$ cat myapp/BUILD
load("//myapp:defs.bzl", "my_boolean_macro")
my_boolean_macro(
    boolval = select({
        "//third_party/bazel_platforms/cpu:x86_32": True,
        "//third_party/bazel_platforms/cpu:ppc": False,
    }),
)

$ bazel build //myapp:all --cpu=x86
DEBUG: /myworkspace/myapp/defs.bzl:4:3: TRUE.
$ bazel build //mypro:all --cpu=ppc
DEBUG: /myworkspace/myapp/defs.bzl:4:3: TRUE.
```

This happens because macros don't understand the contents of `select()`.
So what they're really evaluting is the `select()` object itself. According to
[Pythonic](https://docs.python.org/release/2.5.2/lib/truth.html) design
standards, all objects aside from a very small number of exceptions
automatically return true.

### Can I read select() like a dict?

Macros [can't](#faq-select-macro) evaluate select(s) because macros evaluate before
Bazel knows what the build's command line parameters are. Can they at least read
the `select()`'s dictionary to, for example, add a suffix to each value?

Conceptually this is possible, but [it isn't yet a Bazel feature](https://github.com/bazelbuild/bazel/issues/8419).
What you *can* do today is prepare a straight dictionary, then feed it into a
`select()`:

```sh
$ cat myapp/defs.bzl
def selecty_genrule(name, select_cmd):
  for key in select_cmd.keys():
    select_cmd[key] += " WITH SUFFIX"
  native.genrule(
      name = name,
      outs = [name + ".out"],
      srcs = [],
      cmd = "echo " + select(select_cmd + {"//conditions:default": "default"})
        + " > $@"
  )

$ cat myapp/BUILD
selecty_genrule(
    name = "selecty",
    select_cmd = {
        "//third_party/bazel_platforms/cpu:x86_32": "x86 mode",
    },
)

$ bazel build //testapp:selecty --cpu=x86 && cat bazel-genfiles/testapp/selecty.out
x86 mode WITH SUFFIX
```

If you'd like to support both `select()` and native types, you can do this:

```sh
$ cat myapp/defs.bzl
def selecty_genrule(name, select_cmd):
    cmd_suffix = ""
    if type(select_cmd) == "string":
        cmd_suffix = select_cmd + " WITH SUFFIX"
    elif type(select_cmd) == "dict":
        for key in select_cmd.keys():
            select_cmd[key] += " WITH SUFFIX"
        cmd_suffix = select(select_cmd + {"//conditions:default": "default"})

    native.genrule(
        name = name,
        outs = [name + ".out"],
        srcs = [],
        cmd = "echo " + cmd_suffix + "> $@",
    )
```

### Why doesn't select() work with bind()?

First of all, do not use `bind()`. It is deprecated in favor of `alias()`.

The technical answer is that [`bind()`](/reference/be/workspace#bind) is a repo
rule, not a BUILD rule.

Repo rules do not have a specific configuration, and aren't evaluated in
the same way as BUILD rules. Therefore, a `select()` in a `bind()` can't
actually evaluate to any specific branch.

Instead, you should use [`alias()`](/reference/be/general#alias), with a `select()` in
the `actual` attribute, to perform this type of run-time determination. This
works correctly, since `alias()` is a BUILD rule, and is evaluated with a
specific configuration.

You can even have a `bind()` target point to an `alias()`, if needed.

```sh
$ cat WORKSPACE
workspace(name = "myapp")
bind(name = "openssl", actual = "//:ssl")
http_archive(name = "alternative", ...)
http_archive(name = "boringssl", ...)

$ cat BUILD
config_setting(
    name = "alt_ssl",
    define_values = {
        "ssl_library": "alternative",
    },
)

alias(
    name = "ssl",
    actual = select({
        "//:alt_ssl": "@alternative//:ssl",
        "//conditions:default": "@boringssl//:ssl",
    }),
)
```

With this setup, you can pass `--define ssl_library=alternative`, and any target
that depends on either `//:ssl` or `//external:ssl` will see the alternative
located at `@alternative//:ssl`.

But really, stop using `bind()`.

### Why doesn't my select() choose what I expect?

If `//myapp:foo` has a `select()` that doesn't choose the condition you expect,
use [cquery](/query/cquery) and `bazel config` to debug:

If `//myapp:foo` is the top-level target you're building, run:

```sh
$ bazel cquery //myapp:foo <desired build flags>
//myapp:foo (12e23b9a2b534a)
```

If you're building some other target `//bar` that depends on
//myapp:foo somewhere in its subgraph, run:

```sh
$ bazel cquery 'somepath(//bar, //myapp:foo)' <desired build flags>
//bar:bar   (3ag3193fee94a2)
//bar:intermediate_dep (12e23b9a2b534a)
//myapp:foo (12e23b9a2b534a)
```

The `(12e23b9a2b534a)` next to `//myapp:foo` is a *hash* of the
configuration that resolves `//myapp:foo`'s `select()`. You can inspect its
values with `bazel config`:

```sh
$ bazel config 12e23b9a2b534a
BuildConfigurationValue 12e23b9a2b534a
Fragment com.google.devtools.build.lib.analysis.config.CoreOptions {
  cpu: darwin
  compilation_mode: fastbuild
  ...
}
Fragment com.google.devtools.build.lib.rules.cpp.CppOptions {
  linkopt: [-Dfoo=bar]
  ...
}
...
```

Then compare this output against the settings expected by each `config_setting`.

`//myapp:foo` may exist in different configurations in the same build. See the
[cquery docs](/query/cquery) for guidance on using `somepath` to get the right
one.

Caution: To prevent restarting the Bazel server, invoke `bazel config` with the
same command line flags as the `bazel cquery`. The `config` command relies on
the configuration nodes from the still-running server of the previous command.

### Why doesn't `select()` work with platforms?

Bazel doesn't support configurable attributes checking whether a given platform
is the target platform because the semantics are unclear.

For example:

```py
platform(
    name = "x86_linux_platform",
    constraint_values = [
        "@platforms//cpu:x86",
        "@platforms//os:linux",
    ],
)

cc_library(
    name = "lib",
    srcs = [...],
    linkopts = select({
        ":x86_linux_platform": ["--enable_x86_optimizations"],
        "//conditions:default": [],
    }),
)
```

In this `BUILD` file, which `select()` should be used if the target platform has both the
`@platforms//cpu:x86` and `@platforms//os:linux` constraints, but is **not** the
`:x86_linux_platform` defined here? The author of the `BUILD` file and the user
who defined the separate platform may have different ideas.

#### What should I do instead?

Instead, define a `config_setting` that matches **any** platform with
these constraints:

```py
config_setting(
    name = "is_x86_linux",
    constraint_values = [
        "@platforms//cpu:x86",
        "@platforms//os:linux",
    ],
)

cc_library(
    name = "lib",
    srcs = [...],
    linkopts = select({
        ":is_x86_linux": ["--enable_x86_optimizations"],
        "//conditions:default": [],
    }),
)
```

This process defines specific semantics, making it clearer to users what
platforms meet the desired conditions.

#### What if I really, really want to `select` on the platform?

If your build requirements specifically require checking the platform, you
can flip the value of the `--platforms` flag in a `config_setting`:

```py
config_setting(
    name = "is_specific_x86_linux_platform",
    values = {
        "platforms": ["//package:x86_linux_platform"],
    },
)

cc_library(
    name = "lib",
    srcs = [...],
    linkopts = select({
        ":is_specific_x86_linux_platform": ["--enable_x86_optimizations"],
        "//conditions:default": [],
    }),
)
```

The Bazel team doesn't endorse doing this; it overly constrains your build and
confuses users when the expected condition does not match.

[BuildSettings]: /extending/config#user-defined-build-settings

---

## bazel mobile-install
- URL: https://bazel.build/docs/mobile-install
- Source: docs/mobile-install.mdx
- Slug: /docs/mobile-install

<p class="lead">Fast iterative development for Android</p>

This page describes how `bazel mobile-install` makes iterative development
for Android much faster. It describes the benefits of this approach versus the
drawbacks of separate build and install steps.

## Summary

To install small changes to an Android app very quickly, do the following:

 1. Find the `android_binary` rule of the app you want to install.
 2. Connect your device to `adb`.
 3. Run `bazel mobile-install :your_target`. App startup will be a little
    slower than usual.
 4. Edit the code or Android resources.
 5. Run `bazel mobile-install :your_target`.
 6. Enjoy a fast and minimal incremental installation!

Some command line options to Bazel that may be useful:

 - `--adb` tells Bazel which adb binary to use
 - `--adb_arg` can be used to add extra arguments to the command line of `adb`.
   One useful application of this is to select which device you want to install
   to if you have multiple devices connected to your workstation:
   `bazel mobile-install :your_target -- --adb_arg=-s --adb_arg=<SERIAL>`

When in doubt, look at the
[example](https://github.com/bazelbuild/rules_android/tree/main/examples/basicapp),
contact us on [Google Groups](https://groups.google.com/forum/#!forum/bazel-discuss),
or [file a GitHub issue](https://github.com/bazelbuild/rules_android/issues)

## Introduction

One of the most important attributes of a developer's toolchain is speed: there
is a world of difference between changing the code and seeing it run within a
second and having to wait minutes, sometimes hours, before you get any feedback
on whether your changes do what you expect them to.

Unfortunately, the traditional Android toolchain for building an .apk entails
many monolithic, sequential steps and all of these have to be done in order to
build an Android app. At Google, waiting five minutes to build a single-line
change was not unusual on larger projects like Google Maps.

`bazel mobile-install` makes iterative development for Android much faster by
using a combination of change pruning, work sharding, and clever manipulation of
Android internals, all without changing any of your app's code.

## Problems with traditional app installation

Building an Android app has some issues, including:

- Dexing. By default, the Dexer tool (historically `dx`, now `d8` or `r8`)
is invoked exactly once in the build and it does not know how to reuse work from
previous builds: it dexes every method again, even though only one method was
changed.

- Uploading data to the device. adb does not use the full bandwidth of a USB 2.0
connection, and larger apps can take a lot of time to upload. The entire app is
uploaded, even if only small parts have changed, for example, a resource or a
single method, so this can be a major bottleneck.

- Compilation to native code. Android L introduced ART, a new Android runtime,
which compiles apps ahead-of-time rather than compiling them just-in-time like
Dalvik. This makes apps much faster at the cost of longer installation
time. This is a good tradeoff for users because they typically install an app
once and use it many times, but results in slower development where an app is
installed many times and each version is run at most a handful of times.

## The approach of `bazel mobile-install`

`bazel mobile-install `makes the following improvements:

 - Sharded desugaring and dexing. After building the app's Java code, Bazel
   shards the class files into approximately equal-sized parts and invokes `d8`
   separately on them. `d8` is not invoked on shards that did not change since
   the last build. These shards are then compiled into separate sharded APKs.

 - Incremental file transfer. Android resources, .dex files, and native
   libraries are removed from the main .apk and are stored in under a separate
   mobile-install directory. This makes it possible to update code and Android
   resources independently without reinstalling the whole app. Thus,
   transferring the files takes less time and only the .dex files that have
   changed are recompiled on-device.

 - Sharded installation. Mobile-install uses Android Studio's
   [`apkdeployer`](https://maven.google.com/web/index.html?q=deployer#com.android.tools.apkdeployer:apkdeployer)
   tool to combine sharded APKs on the connected device and provide a cohesive
   experience.

### Sharded Dexing

Sharded dexing is reasonably straightforward: once the .jar files are built, a
[tool](https://github.com/bazelbuild/rules_android/blob/main/src/tools/java/com/google/devtools/build/android/ziputils/DexMapper.java)
shards them into separate .jar files of approximately equal size, then invokes
`d8` on those that were changed since the previous build. The logic that
determines which shards to dex is not specific to Android: it just uses the
general change pruning algorithm of Bazel.

The first version of the sharding algorithm simply ordered the .class files
alphabetically, then cut the list up into equal-sized parts, but this proved to
be suboptimal: if a class was added or removed (even a nested or an anonymous
one), it would cause all the classes alphabetically after it to shift by one,
resulting in dexing those shards again. Thus, it was decided to shard Java
packages rather than individual classes. Of course, this still results in
dexing many shards if a new package is added or removed, but that is much less
frequent than adding or removing a single class.

The number of shards is controlled by command-line configuration, using the
`--define=num_dex_shards=N` flag. In an ideal world, Bazel would
automatically determine how many shards are best, but Bazel currently must know
the set of actions (for example, commands to be executed during the build) before
executing any of them, so it cannot determine the optimal number of shards
because it doesn't know how many Java classes there will eventually be in the
app. Generally speaking, the more shards, the faster the build and the
installation will be, but the slower app startup becomes, because the dynamic
linker has to do more work. The sweet spot is usually between 10 and 50 shards.

### Incremental deployment

Incremental APK shard transfer and installation is now handled by the
`apkdeployer` utility described in ["The approach of mobile-install"](#approach-mobile-install).
Whereas earlier (native) versions of mobile-install required manually tracking
first-time installations and selectively apply the `--incremental`
flag on subsequent installation, the most recent version in [`rules_android`](https://github.com/bazelbuild/rules_android/tree/main/mobile_install)
has been greatly simplified. The same mobile-install
invocation can be used regardless of how many times the app has been installed
or reinstalled.

At a high level, the `apkdeployer` tool is a wrapper around various `adb`
sub-commands. The main entrypoint logic can be found in the
[`com.android.tools.deployer.Deployer`](https://cs.android.com/android-studio/platform/tools/base/+/mirror-goog-studio-main:deploy/deployer/src/main/java/com/android/tools/deployer/Deployer.java)
class, with other utility classes colocated in the same package.
The `Deployer` class ingests, among other things, a list of paths to split
APKs and a protobuf with information about the installation, and leverages
deployment features for [Android app bundles](https://developer.android.com/guide/app-bundle)
in order to create an install session and incrementally deploy app splits.
See the [`ApkPreInstaller`](https://cs.android.com/android-studio/platform/tools/base/+/mirror-goog-studio-main:deploy/deployer/src/main/java/com/android/tools/deployer/ApkPreInstaller.java)
and [`ApkInstaller`](https://cs.android.com/android-studio/platform/tools/base/+/mirror-goog-studio-main:deploy/deployer/src/main/java/com/android/tools/deployer/ApkInstaller.java)
classes for implementation details.

## Results

### Performance

In general, `bazel mobile-install` results in a 4x to 10x speedup of building
and installing large apps after a small change.

The following numbers were computed for a few Google products:

<img src="/docs/images/mobile-install-performance.svg"/>

This, of course, depends on the nature of the change: recompilation after
changing a base library takes more time.

### Limitations

The tricks the stub application plays don't work in every case.
The following cases highlight where it does not work as expected:

 - Mobile-install is only supported via the Starlark rules of `rules_android`.
   See the ["brief history of mobile-install"](#mobile-install-history) for
   more detail.

 - Only devices running ART are supported. Mobile-install uses API and runtime features
   that only exist on devices running ART, not Dalvik. Any Android runtime more
   recent than Android L (API 21+) should be compatible.

 - Bazel itself must be run with a tool Java runtime _and_ language version
   of 17 or higher.

 - Bazel versions prior to 8.4.0 must specify some additional flags for
   mobile-install. See [the Bazel Android tutorial](/start/android-app). These
   flags inform Bazel where the Starlark mobile-install aspect is and which
   rules are supported.

### A brief history of mobile-install
Earlier Bazel versions _natively_ included built-in build and test rules for
popular languages and ecosystems such as C++, Java, and Android. These rules
were therefore referred to as _native_ rules. Bazel 8 (released in 2024) removed
support for these rules because many of them had been migrated to the
[Starlark](/rules/language) language. See the ["Bazel 8.0 LTS blog post"](https://blog.bazel.build/2024/12/09/bazel-8-release.html)
for more details.

The legacy native Android rules also supported a legacy _native_ version of
mobile-install functionality. This is referred to as "mobile-install v1" or
"native mobile-install" now. This functionality was deleted in Bazel 8, along
with the built-in Android rules.

Now, all mobile-install functionality, as well as all Android build and test
rules, are implemented in Starlark and reside in the `rules_android` GitHub
repository. The latest version is known as "mobile-install v3" or "MIv3".

_Naming note_: There was a "mobile-install **v2**" available only internally
at Google at one point, but this was never published externally, and only v3
continues to be used for both Google-internal and OSS rules_android deployment.

---

## Sandboxing
- URL: https://bazel.build/docs/sandboxing
- Source: docs/sandboxing.mdx
- Slug: /docs/sandboxing

This article covers sandboxing in Bazel and debugging your sandboxing
environment.

*Sandboxing* is a permission restricting strategy that isolates processes from
each other or from resources in a system. For Bazel, this means restricting file
system access.

Bazel's file system sandbox runs processes in a working directory that only
contains known inputs, such that compilers and other tools don't see source
files they should not access, unless they know the absolute paths to them.

Sandboxing doesn't hide the host environment in any way. Processes can freely
access all files on the file system. However, on platforms that support user
namespaces, processes can't modify any files outside their working directory.
This ensures that the build graph doesn't have hidden dependencies that could
affect the reproducibility of the build.

More specifically, Bazel constructs an `execroot/` directory for each action,
which acts as the action's work directory at execution time. `execroot/`
contains all input files to the action and serves as the container for any
generated outputs. Bazel then uses an operating-system-provided technique,
containers on Linux and `sandbox-exec` on macOS, to constrain the action within
`execroot/`.

## Reasons for sandboxing

-   Without action sandboxing, Bazel doesn't know if a tool uses undeclared
    input files (files that are not explicitly listed in the dependencies of an
    action). When one of the undeclared input files changes, Bazel still
    believes that the build is up-to-date and won’t rebuild the action. This can
    result in an incorrect incremental build.

-   Incorrect reuse of cache entries creates problems during remote caching. A
    bad cache entry in a shared cache affects every developer on the project,
    and wiping the entire remote cache is not a feasible solution.

-   Sandboxing mimics the behavior of remote execution — if a build works well
    with sandboxing, it will likely also work with remote execution. By making
    remote execution upload all necessary files (including local tools), you can
    significantly reduce maintenance costs for compile clusters compared to
    having to install the tools on every machine in the cluster every time you
    want to try out a new compiler or make a change to an existing tool.

## What sandbox strategy to use

You can choose which kind of sandboxing to use, if any, with the
[strategy flags](user-manual.html#strategy-options). Using the `sandboxed`
strategy makes Bazel pick one of the sandbox implementations listed below,
preferring an OS-specific sandbox to the less hermetic generic one.
[Persistent workers](/remote/persistent) run in a generic sandbox if you pass
the `--worker_sandboxing` flag.

The `local` (a.k.a. `standalone`) strategy does not do any kind of sandboxing.
It simply executes the action's command line with the working directory set to
the execroot of your workspace.

`processwrapper-sandbox` is a sandboxing strategy that does not require any
"advanced" features - it should work on any POSIX system out of the box. It
builds a sandbox directory consisting of symlinks that point to the original
source files, executes the action's command line with the working directory set
to this directory instead of the execroot, then moves the known output artifacts
out of the sandbox into the execroot and deletes the sandbox. This prevents the
action from accidentally using any input files that are not declared and from
littering the execroot with unknown output files.

`linux-sandbox` goes one step further and builds on top of the
`processwrapper-sandbox`. Similar to what Docker does under the hood, it uses
Linux Namespaces (User, Mount, PID, Network and IPC namespaces) to isolate the
action from the host. That is, it makes the entire filesystem read-only except
for the sandbox directory, so the action cannot accidentally modify anything on
the host filesystem. This prevents situations like a buggy test accidentally rm
-rf'ing your $HOME directory. Optionally, you can also prevent the action from
accessing the network. `linux-sandbox` uses PID namespaces to prevent the action
from seeing any other processes and to reliably kill all processes (even daemons
spawned by the action) at the end.

`darwin-sandbox` is similar, but for macOS. It uses Apple's `sandbox-exec` tool
to achieve roughly the same as the Linux sandbox.

Both the `linux-sandbox` and the `darwin-sandbox` do not work in a "nested"
scenario due to restrictions in the mechanisms provided by the operating
systems. Because Docker also uses Linux namespaces for its container magic, you
cannot easily run `linux-sandbox` inside a Docker container, unless you use
`docker run --privileged`. On macOS, you cannot run `sandbox-exec` inside a
process that's already being sandboxed. Thus, in these cases, Bazel
automatically falls back to using `processwrapper-sandbox`.

If you would rather get a build error — such as to not accidentally build with a
less strict execution strategy — explicitly modify the list of execution
strategies that Bazel tries to use (for example, `bazel build
--spawn_strategy=worker,linux-sandbox`).

Dynamic execution usually requires sandboxing for local execution. To opt out,
pass the `--experimental_local_lockfree_output` flag. Dynamic execution silently
sandboxes [persistent workers](/remote/persistent).

## Downsides to sandboxing

-   Sandboxing incurs extra setup and teardown cost. How big this cost is
    depends on many factors, including the shape of the build and the
    performance of the host OS. For Linux, sandboxed builds are rarely more than
    a few percent slower. Setting `--reuse_sandbox_directories` can
    mitigate the setup and teardown cost.

-   Sandboxing effectively disables any cache the tool may have. You can
    mitigate this by using [persistent workers](/remote/persistent), at
    the cost of weaker sandbox guarantees.

-   [Multiplex workers](/remote/multiplex) require explicit worker support
    to be sandboxed. Workers that do not support multiplex sandboxing run as
    singleplex workers under dynamic execution, which can cost extra memory.

## Debugging

Follow the strategies below to debug issues with sandboxing.

### Deactivated namespaces

On some platforms, such as
[Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/)
cluster nodes or Debian, user namespaces are deactivated by default due to
security concerns. If the `/proc/sys/kernel/unprivileged_userns_clone` file
exists and contains a 0, you can activate user namespaces by running:

```posix-terminal
   sudo sysctl kernel.unprivileged_userns_clone=1
```

### Rule execution failures

The sandbox may fail to execute rules because of the system setup. If you see a
message like `namespace-sandbox.c:633: execvp(argv[0], argv): No such file or
directory`, try to deactivate the sandbox with `--strategy=Genrule=local` for
genrules, and `--spawn_strategy=local` for other rules.

### Detailed debugging for build failures

If your build failed, use `--verbose_failures` and `--sandbox_debug` to make
Bazel show the exact command it ran when your build failed, including the part
that sets up the sandbox.

Example error message:

```
ERROR: path/to/your/project/BUILD:1:1: compilation of rule
'//path/to/your/project:all' failed:

Sandboxed execution failed, which may be legitimate (such as a compiler error),
or due to missing dependencies. To enter the sandbox environment for easier
debugging, run the following command in parentheses. On command failure, a bash
shell running inside the sandbox will then automatically be spawned

namespace-sandbox failed: error executing command
  (cd /some/path && \
  exec env - \
    LANG=en_US \
    PATH=/some/path/bin:/bin:/usr/bin \
    PYTHONPATH=/usr/local/some/path \
  /some/path/namespace-sandbox @/sandbox/root/path/this-sandbox-name.params --
  /some/path/to/your/some-compiler --some-params some-target)
```

You can now inspect the generated sandbox directory and see which files Bazel
created and run the command again to see how it behaves.

Note that Bazel does not delete the sandbox directory when you use
`--sandbox_debug`. Unless you are actively debugging, you should disable
`--sandbox_debug` because it fills up your disk over time.

---

## Aspects
- URL: https://bazel.build/extending/aspects
- Source: extending/aspects.mdx
- Slug: /extending/aspects

This page explains the basics and benefits of using
[aspects](/rules/lib/globals/bzl#aspect) and provides simple and advanced
examples.

Aspects allow augmenting build dependency graphs with additional information
and actions. Some typical scenarios when aspects can be useful:

*   IDEs that integrate Bazel can use aspects to collect information about the
    project.
*   Code generation tools can leverage aspects to execute on their inputs in
    *target-agnostic* manner. As an example, `BUILD` files can specify a hierarchy
    of [protobuf](https://developers.google.com/protocol-buffers/) library
    definitions, and language-specific rules can use aspects to attach
    actions generating protobuf support code for a particular language.

## Aspect basics

`BUILD` files provide a description of a project’s source code: what source
files are part of the project, what artifacts (_targets_) should be built from
those files, what the dependencies between those files are, etc. Bazel uses
this information to perform a build, that is, it figures out the set of actions
needed to produce the artifacts (such as running compiler or linker) and
executes those actions. Bazel accomplishes this by constructing a _dependency
graph_ between targets and visiting this graph to collect those actions.

Consider the following `BUILD` file:

```python
java_library(name = 'W', ...)
java_library(name = 'Y', deps = [':W'], ...)
java_library(name = 'Z', deps = [':W'], ...)
java_library(name = 'Q', ...)
java_library(name = 'T', deps = [':Q'], ...)
java_library(name = 'X', deps = [':Y',':Z'], runtime_deps = [':T'], ...)
```

This `BUILD` file defines a dependency graph shown in the following figure:

![Build graph](/rules/build-graph.png "Build graph")

**Figure 1.** `BUILD` file dependency graph.

Bazel analyzes this dependency graph by calling an implementation function of
the corresponding [rule](/extending/rules) (in this case "java_library") for every
target in the above example. Rule implementation functions generate actions that
build artifacts, such as `.jar` files, and pass information, such as locations
and names of those artifacts, to the reverse dependencies of those targets in
[providers](/extending/rules#providers).

Aspects are similar to rules in that they have an implementation function that
generates actions and returns providers. However, their power comes from
the way the dependency graph is built for them. An aspect has an implementation
and a list of all attributes it propagates along. Consider an aspect A that
propagates along attributes named "deps". This aspect can be applied to
a target X, yielding an aspect application node A(X). During its application,
aspect A is applied recursively to all targets that X refers to in its "deps"
attribute (all attributes in A's propagation list).

Thus a single act of applying aspect A to a target X yields a "shadow graph" of
the original dependency graph of targets shown in the following figure:

![Build Graph with Aspect](/rules/build-graph-aspects.png "Build graph with aspects")

**Figure 2.** Build graph with aspects.

The only edges that are shadowed are the edges along the attributes in
the propagation set, thus the `runtime_deps` edge is not shadowed in this
example. An aspect implementation function is then invoked on all nodes in
the shadow graph similar to how rule implementations are invoked on the nodes
of the original graph.

## Simple example

This example demonstrates how to recursively print the source files for a
rule and all of its dependencies that have a `deps` attribute. It shows
an aspect implementation, an aspect definition, and how to invoke the aspect
from the Bazel command line.

```python
def _print_aspect_impl(target, ctx):
    # Make sure the rule has a srcs attribute.
    if hasattr(ctx.rule.attr, 'srcs'):
        # Iterate through the files that make up the sources and
        # print their paths.
        for src in ctx.rule.attr.srcs:
            for f in src.files.to_list():
                print(f.path)
    return []

print_aspect = aspect(
    implementation = _print_aspect_impl,
    attr_aspects = ['deps'],
    required_providers = [CcInfo],
)
```

Let's break the example up into its parts and examine each one individually.

### Aspect definition

```python
print_aspect = aspect(
    implementation = _print_aspect_impl,
    attr_aspects = ['deps'],
    required_providers = [CcInfo],
)
```
Aspect definitions are similar to rule definitions, and defined using
the [`aspect`](/rules/lib/globals/bzl#aspect) function.

Just like a rule, an aspect has an implementation function which in this case is
``_print_aspect_impl``.

``attr_aspects`` is a list of rule attributes along which the aspect propagates.
In this case, the aspect will propagate along the ``deps`` attribute of the
rules that it is applied to.

Another common argument for `attr_aspects` is `['*']` which would propagate the
aspect to all attributes of a rule.

``required_providers`` is a list of providers that allows the aspect to limit
its propagation to only the targets whose rules advertise its required
providers. For more details consult
[the documentation of the aspect function](/rules/lib/globals/bzl#aspect).
In this case, the aspect will only apply on targets that declare `CcInfo`
provider.

### Aspect implementation

```python
def _print_aspect_impl(target, ctx):
    # Make sure the rule has a srcs attribute.
    if hasattr(ctx.rule.attr, 'srcs'):
        # Iterate through the files that make up the sources and
        # print their paths.
        for src in ctx.rule.attr.srcs:
            for f in src.files.to_list():
                print(f.path)
    return []
```

Aspect implementation functions are similar to the rule implementation
functions. They return [providers](/extending/rules#providers), can generate
[actions](/extending/rules#actions), and take two arguments:

*  `target`: the [target](/rules/lib/builtins/Target) the aspect is being applied to.
*   `ctx`: [`ctx`](/rules/lib/builtins/ctx) object that can be used to access attributes
    and generate outputs and actions.

The implementation function can access the attributes of the target rule via
[`ctx.rule.attr`](/rules/lib/builtins/ctx#rule). It can examine providers that are
provided by the target to which it is applied (via the `target` argument).

Aspects are required to return a list of providers. In this example, the aspect
does not provide anything, so it returns an empty list.

### Invoking the aspect using the command line

The simplest way to apply an aspect is from the command line using the
[`--aspects`](/reference/command-line-reference#flag--aspects)
argument. Assuming the aspect above were defined in a file named `print.bzl`
this:

```bash
bazel build //MyExample:example --aspects print.bzl%print_aspect
```

would apply the `print_aspect` to the target `example` and all of the
target rules that are accessible recursively via the `deps` attribute.

The `--aspects` flag takes one argument, which is a specification of the aspect
in the format `<extension file label>%<aspect top-level name>`.

## Advanced example

The following example demonstrates using an aspect from a target rule
that counts files in targets, potentially filtering them by extension.
It shows how to use a provider to return values, how to use parameters to pass
an argument into an aspect implementation, and how to invoke an aspect from a rule.

Note: Aspects added in rules' attributes are called *rule-propagated aspects* as
opposed to *command-line aspects* that are specified using the ``--aspects``
flag.

`file_count.bzl` file:

```python
FileCountInfo = provider(
    fields = {
        'count' : 'number of files'
    }
)

def _file_count_aspect_impl(target, ctx):
    count = 0
    # Make sure the rule has a srcs attribute.
    if hasattr(ctx.rule.attr, 'srcs'):
        # Iterate through the sources counting files
        for src in ctx.rule.attr.srcs:
            for f in src.files.to_list():
                if ctx.attr.extension == '*' or ctx.attr.extension == f.extension:
                    count = count + 1
    # Get the counts from our dependencies.
    for dep in ctx.rule.attr.deps:
        count = count + dep[FileCountInfo].count
    return [FileCountInfo(count = count)]

file_count_aspect = aspect(
    implementation = _file_count_aspect_impl,
    attr_aspects = ['deps'],
    attrs = {
        'extension' : attr.string(values = ['*', 'h', 'cc']),
    }
)

def _file_count_rule_impl(ctx):
    for dep in ctx.attr.deps:
        print(dep[FileCountInfo].count)

file_count_rule = rule(
    implementation = _file_count_rule_impl,
    attrs = {
        'deps' : attr.label_list(aspects = [file_count_aspect]),
        'extension' : attr.string(default = '*'),
    },
)
```

`BUILD.bazel` file:

```python
load('//:file_count.bzl', 'file_count_rule')

cc_library(
    name = 'lib',
    srcs = [
        'lib.h',
        'lib.cc',
    ],
)

cc_binary(
    name = 'app',
    srcs = [
        'app.h',
        'app.cc',
        'main.cc',
    ],
    deps = ['lib'],
)

file_count_rule(
    name = 'file_count',
    deps = ['app'],
    extension = 'h',
)
```

### Aspect definition

```python
file_count_aspect = aspect(
    implementation = _file_count_aspect_impl,
    attr_aspects = ['deps'],
    attrs = {
        'extension' : attr.string(values = ['*', 'h', 'cc']),
    }
)
```

This example shows how the aspect propagates through the ``deps`` attribute.

``attrs`` defines a set of attributes for an aspect. Public aspect attributes
define parameters and can only be of types ``bool``, ``int`` or ``string``.
For rule-propagated aspects, ``int`` and ``string`` parameters must have
``values`` specified on them. This example has a parameter called ``extension``
that is allowed to have '``*``', '``h``', or '``cc``' as a value.

For rule-propagated aspects, parameter values are taken from the rule requesting
the aspect, using the attribute of the rule that has the same name and type.
(see the definition of ``file_count_rule``).

For command-line aspects, the parameters values can be passed using
[``--aspects_parameters``](/reference/command-line-reference#flag--aspects_parameters)
flag. The ``values`` restriction of ``int`` and ``string`` parameters may be
omitted.

Aspects are also allowed to have private attributes of types ``label`` or
``label_list``. Private label attributes can be used to specify dependencies on
tools or libraries that are needed for actions generated by aspects. There is not
a private attribute defined in this example, but the following code snippet
demonstrates how you could pass in a tool to an aspect:

```python
...
    attrs = {
        '_protoc' : attr.label(
            default = Label('//tools:protoc'),
            executable = True,
            cfg = "exec"
        )
    }
...
```

### Aspect implementation

```python
FileCountInfo = provider(
    fields = {
        'count' : 'number of files'
    }
)

def _file_count_aspect_impl(target, ctx):
    count = 0
    # Make sure the rule has a srcs attribute.
    if hasattr(ctx.rule.attr, 'srcs'):
        # Iterate through the sources counting files
        for src in ctx.rule.attr.srcs:
            for f in src.files.to_list():
                if ctx.attr.extension == '*' or ctx.attr.extension == f.extension:
                    count = count + 1
    # Get the counts from our dependencies.
    for dep in ctx.rule.attr.deps:
        count = count + dep[FileCountInfo].count
    return [FileCountInfo(count = count)]
```

Just like a rule implementation function, an aspect implementation function
returns a struct of providers that are accessible to its dependencies.

In this example, the ``FileCountInfo`` is defined as a provider that has one
field ``count``. It is best practice to explicitly define the fields of a
provider using the ``fields`` attribute.

The set of providers for an aspect application A(X) is the union of providers
that come from the implementation of a rule for target X and from the
implementation of aspect A. The providers that a rule implementation propagates
are created and frozen before aspects are applied and cannot be modified from an
aspect. It is an error if a target and an aspect that is applied to it each
provide a provider with the same type, with the exceptions of
[`OutputGroupInfo`](/rules/lib/providers/OutputGroupInfo)
(which is merged, so long as the
rule and aspect specify different output groups) and
[`InstrumentedFilesInfo`](/rules/lib/providers/InstrumentedFilesInfo)
(which is taken from the aspect). This means that aspect implementations may
never return [`DefaultInfo`](/rules/lib/providers/DefaultInfo).

The parameters and private attributes are passed in the attributes of the
``ctx``. This example references the ``extension`` parameter and determines
what files to count.

For returning providers, the values of attributes along which
the aspect is propagated (from the `attr_aspects` list) are replaced with
the results of an application of the aspect to them. For example, if target
X has Y and Z in its deps, `ctx.rule.attr.deps` for A(X) will be [A(Y), A(Z)].
In this example, ``ctx.rule.attr.deps`` are Target objects that are the
results of applying the aspect to the 'deps' of the original target to which
the aspect has been applied.

In the example, the aspect accesses the ``FileCountInfo`` provider from the
target's dependencies to accumulate the total transitive number of files.

### Invoking the aspect from a rule

```python
def _file_count_rule_impl(ctx):
    for dep in ctx.attr.deps:
        print(dep[FileCountInfo].count)

file_count_rule = rule(
    implementation = _file_count_rule_impl,
    attrs = {
        'deps' : attr.label_list(aspects = [file_count_aspect]),
        'extension' : attr.string(default = '*'),
    },
)
```

The rule implementation demonstrates how to access the ``FileCountInfo``
via the ``ctx.attr.deps``.

The rule definition demonstrates how to define a parameter (``extension``)
and give it a default value (``*``). Note that having a default value that
was not one of '``cc``', '``h``', or '``*``' would be an error due to the
restrictions placed on the parameter in the aspect definition.

### Invoking an aspect through a target rule

```python
load('//:file_count.bzl', 'file_count_rule')

cc_binary(
    name = 'app',
...
)

file_count_rule(
    name = 'file_count',
    deps = ['app'],
    extension = 'h',
)
```

This demonstrates how to pass the ``extension`` parameter into the aspect
via the rule. Since the ``extension`` parameter has a default value in the
rule implementation, ``extension`` would be considered an optional parameter.

When the ``file_count`` target is built, our aspect will be evaluated for
itself, and all of the targets accessible recursively via ``deps``.

## References

* [`aspect` API reference](/rules/lib/globals/bzl#aspect)

---

## Automatic Execution Groups (AEGs)
- URL: https://bazel.build/extending/auto-exec-groups
- Source: extending/auto-exec-groups.mdx
- Slug: /extending/auto-exec-groups

Automatic execution groups select an [execution platform][exec_platform]
for each toolchain type. In other words, one target can have multiple
execution platforms without defining execution groups.

## Quick summary

Automatic execution groups are closely connected to toolchains. If you are using
toolchains, you need to set them on the affected actions (actions which use an
executable or a tool from a toolchain) by adding `toolchain` parameter. For
example:

```python
ctx.actions.run(
    ...,
    executable = ctx.toolchain['@bazel_tools//tools/jdk:toolchain_type'].tool,
    ...,
    toolchain = '@bazel_tools//tools/jdk:toolchain_type',
)
```
If the action does not use a tool or executable from a toolchain, and Blaze
doesn't detect that ([the error](#first-error-message) is raised), you can set
`toolchain = None`.

If you need to use multiple toolchains on a single execution platform (an action
uses executable or tools from two or more toolchains), you need to manually
define [exec_groups][exec_groups] (check
[When should I use a custom exec_group?][multiple_toolchains_exec_groups]
section).

## History

Before AEGs, the execution platform was selected on a rule level. For example:

```python
my_rule = rule(
    _impl,
    toolchains = ['//tools:toolchain_type_1', '//tools:toolchain_type_2'],
)
```

Rule `my_rule` registers two toolchain types. This means that the [Toolchain
Resolution](https://bazel.build/extending/toolchains#toolchain-resolution) used
to find an execution platform which supports both toolchain types. The selected
execution platform was used for each registered action inside the rule, unless
specified differently with [exec_groups][exec_groups].
In other words, all actions inside the rule used to have a single execution
platform even if they used tools from different toolchains (execution platform
is selected for each target). This resulted in failures when there was no
execution platform supporting all toolchains.

## Current state

With AEGs, the execution platform is selected for each toolchain type. The
implementation function of the earlier example, `my_rule`, would look like:

```python
def _impl(ctx):
    ctx.actions.run(
      mnemonic = "First action",
      executable = ctx.toolchain['//tools:toolchain_type_1'].tool,
      toolchain = '//tools:toolchain_type_1',
    )

    ctx.actions.run(
      mnemonic = "Second action",
      executable = ctx.toolchain['//tools:toolchain_type_2'].tool,
      toolchain = '//tools:toolchain_type_2',
    )
```

This rule creates two actions, the `First action` which uses executable from a
`//tools:toolchain_type_1` and the `Second action` which uses executable from a
`//tools:toolchain_type_2`. Before AEGs, both of these actions would be executed
on a single execution platform which supports both toolchain types. With AEGs,
by adding the `toolchain` parameter inside the actions, each action executes on
the execution platform that provides the toolchain. The actions may be executed
on different execution platforms.

The same is effective with [ctx.actions.run_shell][run_shell] where `toolchain`
parameter should be added when `tools` are from a toolchain.

## Difference between custom exec groups and automatic exec groups

As the name suggests, AEGs are exec groups created automatically for each
toolchain type registered on a rule. There is no need to manually specify them,
unlike the "classic" exec groups. Moreover, name of AEG is automatically set to
its toolchain type (e.g. `//tools:toolchain_type_1`).

### When should I use a custom exec_group?

Custom exec_groups are needed only in case where multiple toolchains need to
execute on a single execution platform. In all other cases there's no need to
define custom exec_groups. For example:

```python
def _impl(ctx):
    ctx.actions.run(
      ...,
      executable = ctx.toolchain['//tools:toolchain_type_1'].tool,
      tools = [ctx.toolchain['//tools:toolchain_type_2'].tool],
      exec_group = 'two_toolchains',
    )
```

```python
my_rule = rule(
    _impl,
    exec_groups = {
        "two_toolchains": exec_group(
            toolchains = ['//tools:toolchain_type_1', '//tools:toolchain_type_2'],
        ),
    }
)
```

## Migration of AEGs

Internally in google3, Blaze is already using AEGs.
Externally for Bazel, migration is in the process. Some rules are already using
this feature (e.g. Java and C++ rules).

### Which Bazel versions support this migration?

AEGs are fully supported from Bazel 7.

### How to enable AEGs?

Set `--incompatible_auto_exec_groups` to true. More information about the flag
on [the GitHub issue][github_flag].

### How to enable AEGs inside a particular rule?

Set the `_use_auto_exec_groups` attribute on a rule.

```python
my_rule = rule(
    _impl,
    attrs = {
      "_use_auto_exec_groups": attr.bool(default = True),
    }
)
```
This enables AEGs only in `my_rule` and its actions start using the new logic
when selecting the execution platform. Incompatible flag is overridden with this
attribute.

### How to disable AEGs in case of an error?

Set `--incompatible_auto_exec_groups` to false to completely disable AEGs in
your project ([flag's GitHub issue][github_flag]), or disable a particular rule
by setting `_use_auto_exec_groups` attribute to `False`
([more details about the attribute](#how-enable-particular-rule)).

### Error messages while migrating to AEGs

#### Couldn't identify if tools are from implicit dependencies or a toolchain. Please set the toolchain parameter. If you're not using a toolchain, set it to 'None'.
  * In this case you get a stack of calls before the error happened and you can
    clearly see which exact action needs the toolchain parameter. Check which
    toolchain is used for the action and set it with the toolchain param. If no
    toolchain is used inside the action for tools or executable, set it to
    `None`.

#### Action declared for non-existent toolchain '[toolchain_type]'.
  * This means that you've set the toolchain parameter on the action but didn't
register it on the rule. Register the toolchain or set `None` inside the action.

## Additional material

For more information, check design document:
[Automatic exec groups for toolchains][aegs_design_doc].

[exec_platform]: https://bazel.build/extending/platforms#:~:text=Execution%20%2D%20a%20platform%20on%20which%20build%20tools%20execute%20build%20actions%20to%20produce%20intermediate%20and%20final%20outputs.
[exec_groups]: https://bazel.build/extending/exec-groups
[github_flag]: https://github.com/bazelbuild/bazel/issues/17134
[aegs_design_doc]: https://docs.google.com/document/d/1-rbP_hmKs9D639YWw5F_JyxPxL2bi6dSmmvj_WXak9M/edit#heading=h.5mcn15i0e1ch
[run_shell]: https://bazel.build/rules/lib/builtins/actions#run_shell
[multiple_toolchains_exec_groups]: /extending/auto-exec-groups#when-should-use-exec-groups

---

## Extension Overview
- URL: https://bazel.build/extending/concepts
- Source: extending/concepts.mdx
- Slug: /extending/concepts

{/* [TOC] */}

This page describes how to extend the BUILD language using macros
and rules.

Bazel extensions are files ending in `.bzl`. Use a
[load statement](/concepts/build-files#load) to import a symbol from an extension.

Before learning the more advanced concepts, first:

* Read about the [Starlark language](/rules/language), used in both the
  `BUILD` and `.bzl` files.

* Learn how you can [share variables](/build/share-variables)
  between two `BUILD` files.

## Macros and rules

A macro is a function that instantiates rules. Macros come in two flavors:
[symbolic macros](/extending/macros) (new in Bazel 8) and [legacy
macros](/extending/legacy-macros). The two flavors of macros are defined
differently, but behave almost the same from the point of view of a user. A
macro is useful when a `BUILD` file is getting too repetitive or too complex, as
it lets you reuse some code. The function is evaluated as soon as the `BUILD`
file is read. After the evaluation of the `BUILD` file, Bazel has little
information about macros. If your macro generates a `genrule`, Bazel will
behave *almost* as if you declared that `genrule` in the `BUILD` file. (The one
exception is that targets declared in a symbolic macro have [special visibility
semantics](/extending/macros#visibility): a symbolic macro can hide its internal
targets from the rest of the package.)

A [rule](/extending/rules) is more powerful than a macro. It can access Bazel
internals and have full control over what is going on. It may for example pass
information to other rules.

If you want to reuse simple logic, start with a macro; we recommend a symbolic
macro, unless you need to support older Bazel versions. If a macro becomes
complex, it is often a good idea to make it a rule. Support for a new language
is typically done with a rule. Rules are for advanced users, and most users will
never have to write one; they will only load and call existing rules.

## Evaluation model

A build consists of three phases.

* **Loading phase**. First, load and evaluate all extensions and all `BUILD`
  files that are needed for the build. The execution of the `BUILD` files simply
  instantiates rules (each time a rule is called, it gets added to a graph).
  This is where macros are evaluated.

* **Analysis phase**. The code of the rules is executed (their `implementation`
  function), and actions are instantiated. An action describes how to generate
  a set of outputs from a set of inputs, such as "run gcc on hello.c and get
  hello.o". You must list explicitly which files will be generated before
  executing the actual commands. In other words, the analysis phase takes
  the graph generated by the loading phase and generates an action graph.

* **Execution phase**. Actions are executed, when at least one of their outputs is
  required. If a file is missing or if a command fails to generate one output,
  the build fails. Tests are also run during this phase.

Bazel uses parallelism to read, parse and evaluate the `.bzl` files and `BUILD`
files. A file is read at most once per build and the result of the evaluation is
cached and reused. A file is evaluated only once all its dependencies (`load()`
statements) have been resolved. By design, loading a `.bzl` file has no visible
side-effect, it only defines values and functions.

Bazel tries to be clever: it uses dependency analysis to know which files must
be loaded, which rules must be analyzed, and which actions must be executed. For
example, if a rule generates actions that you don't need for the current build,
they will not be executed.

## Creating extensions

* [Create your first macro](/rules/macro-tutorial) in order to reuse some code.
  Then [learn more about macros](/extending/macros) and [using them to create
  "custom verbs"](/rules/verbs-tutorial).

* [Follow the rules tutorial](/rules/rules-tutorial) to get started with rules.
  Next, you can read more about the [rules concepts](/extending/rules).

The two links below will be very useful when writing your own extensions. Keep
them within reach:

* The [API reference](/rules/lib)

* [Examples](https://github.com/bazelbuild/examples/tree/master/rules)

## Going further

In addition to [macros](/extending/macros) and [rules](/extending/rules), you
may want to write [aspects](/extending/aspects) and [repository
rules](/external/repo).

* Use [Buildifier](https://github.com/bazelbuild/buildtools)
  consistently to format and lint your code.

* Follow the [`.bzl` style guide](/rules/bzl-style).

* [Test](/rules/testing) your code.

* [Generate documentation](https://skydoc.bazel.build/) to help your users.

* [Optimize the performance](/rules/performance) of your code.

* [Deploy](/rules/deploying) your extensions to other people.

---

## Depsets
- URL: https://bazel.build/extending/depsets
- Source: extending/depsets.mdx
- Slug: /extending/depsets

[Depsets](/rules/lib/builtins/depset) are a specialized data structure for efficiently
collecting data across a target’s transitive dependencies. They are an essential
element of rule processing.

The defining feature of depset is its time- and space-efficient union operation.
The depset constructor accepts a list of elements ("direct") and a list of other
depsets ("transitive"), and returns a depset representing a set containing all the
direct elements and the union of all the transitive sets. Conceptually, the
constructor creates a new graph node that has the direct and transitive nodes
as its successors. Depsets have a well-defined ordering semantics, based on
traversal of this graph.

Example uses of depsets include:

*   Storing the paths of all object files for a program’s libraries, which can
    then be passed to a linker action through a provider.

*   For an interpreted language, storing the transitive source files that are
    included in an executable's runfiles.

## Description and operations

Conceptually, a depset is a directed acyclic graph (DAG) that typically looks
similar to the target graph. It is constructed from the leaves up to the root.
Each target in a dependency chain can add its own contents on top of the
previous without having to read or copy them.

Each node in the DAG holds a list of direct elements and a list of child nodes.
The contents of the depset are the transitive elements, such as the direct elements
of all the nodes. A new depset can be created using the
[depset](/rules/lib/globals/bzl#depset) constructor: it accepts a list of direct
elements and another list of child nodes.

```python
s = depset(["a", "b", "c"])
t = depset(["d", "e"], transitive = [s])

print(s)    # depset(["a", "b", "c"])
print(t)    # depset(["d", "e", "a", "b", "c"])
```

To retrieve the contents of a depset, use the
[to_list()](/rules/lib/builtins/depset#to_list) method. It returns a list of all transitive
elements, not including duplicates. There is no way to directly inspect the
precise structure of the DAG, although this structure does affect the order in
which the elements are returned.

```python
s = depset(["a", "b", "c"])

print("c" in s.to_list())              # True
print(s.to_list() == ["a", "b", "c"])  # True
```

The allowed items in a depset are restricted, just as the allowed keys in
dictionaries are restricted. In particular, depset contents may not be mutable.

Depsets use reference equality: a depset is equal to itself, but unequal to any
other depset, even if they have the same contents and same internal structure.

```python
s = depset(["a", "b", "c"])
t = s
print(s == t)  # True

t = depset(["a", "b", "c"])
print(s == t)  # False

d = {}
d[s] = None
d[t] = None
print(len(d))  # 2
```

To compare depsets by their contents, convert them to sorted lists.

```python
s = depset(["a", "b", "c"])
t = depset(["c", "b", "a"])
print(sorted(s.to_list()) == sorted(t.to_list()))  # True
```

There is no ability to remove elements from a depset. If this is needed, you
must read out the entire contents of the depset, filter the elements you want to
remove, and reconstruct a new depset. This is not particularly efficient.

```python
s = depset(["a", "b", "c"])
t = depset(["b", "c"])

# Compute set difference s - t. Precompute t.to_list() so it's not done
# in a loop, and convert it to a dictionary for fast membership tests.
t_items = {e: None for e in t.to_list()}
diff_items = [x for x in s.to_list() if x not in t_items]
# Convert back to depset if it's still going to be used for union operations.
s = depset(diff_items)
print(s)  # depset(["a"])
```

### Order

The `to_list` operation performs a traversal over the DAG. The kind of traversal
depends on the *order* that was specified at the time the depset was
constructed. It is useful for Bazel to support multiple orders because sometimes
tools care about the order of their inputs. For example, a linker action may
need to ensure that if `B` depends on `A`, then `A.o` comes before `B.o` on the
linker’s command line. Other tools might have the opposite requirement.

Three traversal orders are supported: `postorder`, `preorder`, and
`topological`. The first two work exactly like [tree
traversals](https://en.wikipedia.org/wiki/Tree_traversal#Depth-first_search)
except that they operate on DAGs and skip already visited nodes. The third order
works as a topological sort from root to leaves, essentially the same as
preorder except that shared children are listed only after all of their parents.
Preorder and postorder operate as left-to-right traversals, but note that within
each node direct elements have no order relative to children. For topological
order, there is no left-to-right guarantee, and even the
all-parents-before-child guarantee does not apply in the case that there are
duplicate elements in different nodes of the DAG.

```python
# This demonstrates different traversal orders.

def create(order):
  cd = depset(["c", "d"], order = order)
  gh = depset(["g", "h"], order = order)
  return depset(["a", "b", "e", "f"], transitive = [cd, gh], order = order)

print(create("postorder").to_list())  # ["c", "d", "g", "h", "a", "b", "e", "f"]
print(create("preorder").to_list())   # ["a", "b", "e", "f", "c", "d", "g", "h"]
```

```python
# This demonstrates different orders on a diamond graph.

def create(order):
  a = depset(["a"], order=order)
  b = depset(["b"], transitive = [a], order = order)
  c = depset(["c"], transitive = [a], order = order)
  d = depset(["d"], transitive = [b, c], order = order)
  return d

print(create("postorder").to_list())    # ["a", "b", "c", "d"]
print(create("preorder").to_list())     # ["d", "b", "a", "c"]
print(create("topological").to_list())  # ["d", "b", "c", "a"]
```

Due to how traversals are implemented, the order must be specified at the time
the depset is created with the constructor’s `order` keyword argument. If this
argument is omitted, the depset has the special `default` order, in which case
there are no guarantees about the order of any of its elements (except that it
is deterministic).

## Full example

This example is available at
[https://github.com/bazelbuild/examples/tree/main/rules/depsets](https://github.com/bazelbuild/examples/tree/main/rules/depsets).

Suppose there is a hypothetical interpreted language Foo. In order to build
each `foo_binary` you need to know all the `*.foo` files that it directly or
indirectly depends on.

```python
# //depsets:BUILD

load(":foo.bzl", "foo_library", "foo_binary")

# Our hypothetical Foo compiler.
py_binary(
    name = "foocc",
    srcs = ["foocc.py"],
)

foo_library(
    name = "a",
    srcs = ["a.foo", "a_impl.foo"],
)

foo_library(
    name = "b",
    srcs = ["b.foo", "b_impl.foo"],
    deps = [":a"],
)

foo_library(
    name = "c",
    srcs = ["c.foo", "c_impl.foo"],
    deps = [":a"],
)

foo_binary(
    name = "d",
    srcs = ["d.foo"],
    deps = [":b", ":c"],
)
```

```python
# //depsets:foocc.py

# "Foo compiler" that just concatenates its inputs to form its output.
import sys

if __name__ == "__main__":
  assert len(sys.argv) >= 1
  output = open(sys.argv[1], "wt")
  for path in sys.argv[2:]:
    input = open(path, "rt")
    output.write(input.read())
```

Here, the transitive sources of the binary `d` are all of the `*.foo` files in
the `srcs` fields of `a`, `b`, `c`, and `d`. In order for the `foo_binary`
target to know about any file besides `d.foo`, the `foo_library` targets need to
pass them along in a provider. Each library receives the providers from its own
dependencies, adds its own immediate sources, and passes on a new provider with
the augmented contents. The `foo_binary` rule does the same, except that instead
of returning a provider, it uses the complete list of sources to construct a
command line for an action.

Here’s a complete implementation of the `foo_library` and `foo_binary` rules.

```python
# //depsets/foo.bzl

# A provider with one field, transitive_sources.
foo_files = provider(fields = ["transitive_sources"])

def get_transitive_srcs(srcs, deps):
  """Obtain the source files for a target and its transitive dependencies.

  Args:
    srcs: a list of source files
    deps: a list of targets that are direct dependencies
  Returns:
    a collection of the transitive sources
  """
  return depset(
        srcs,
        transitive = [dep[foo_files].transitive_sources for dep in deps])

def _foo_library_impl(ctx):
  trans_srcs = get_transitive_srcs(ctx.files.srcs, ctx.attr.deps)
  return [foo_files(transitive_sources=trans_srcs)]

foo_library = rule(
    implementation = _foo_library_impl,
    attrs = {
        "srcs": attr.label_list(allow_files=True),
        "deps": attr.label_list(),
    },
)

def _foo_binary_impl(ctx):
  foocc = ctx.executable._foocc
  out = ctx.outputs.out
  trans_srcs = get_transitive_srcs(ctx.files.srcs, ctx.attr.deps)
  srcs_list = trans_srcs.to_list()
  ctx.actions.run(executable = foocc,
                  arguments = [out.path] + [src.path for src in srcs_list],
                  inputs = srcs_list + [foocc],
                  outputs = [out])

foo_binary = rule(
    implementation = _foo_binary_impl,
    attrs = {
        "srcs": attr.label_list(allow_files=True),
        "deps": attr.label_list(),
        "_foocc": attr.label(default=Label("//depsets:foocc"),
                             allow_files=True, executable=True, cfg="host")
    },
    outputs = {"out": "%{name}.out"},
)
```

You can test this by copying these files into a fresh package, renaming the
labels appropriately, creating the source `*.foo` files with dummy content, and
building the `d` target.


## Performance

To see the motivation for using depsets, consider what would happen if
`get_transitive_srcs()` collected its sources in a list.

```python
def get_transitive_srcs(srcs, deps):
  trans_srcs = []
  for dep in deps:
    trans_srcs += dep[foo_files].transitive_sources
  trans_srcs += srcs
  return trans_srcs
```

This does not take into account duplicates, so the source files for `a`
will appear twice on the command line and twice in the contents of the output
file.

An alternative is using a general set, which can be simulated by a
dictionary where the keys are the elements and all the keys map to `True`.

```python
def get_transitive_srcs(srcs, deps):
  trans_srcs = {}
  for dep in deps:
    for file in dep[foo_files].transitive_sources:
      trans_srcs[file] = True
  for file in srcs:
    trans_srcs[file] = True
  return trans_srcs
```

This gets rid of the duplicates, but it makes the order of the command line
arguments (and therefore the contents of the files) unspecified, although still
deterministic.

Moreover, both approaches are asymptotically worse than the depset-based
approach. Consider the case where there is a long chain of dependencies on
Foo libraries. Processing every rule requires copying all of the transitive
sources that came before it into a new data structure. This means that the
time and space cost for analyzing an individual library or binary target
is proportional to its own height in the chain. For a chain of length n,
foolib_1 ← foolib_2 ← … ← foolib_n, the overall cost is effectively O(n^2).

Generally speaking, depsets should be used whenever you are accumulating
information through your transitive dependencies. This helps ensure that
your build scales well as your target graph grows deeper.

Finally, it’s important to not retrieve the contents of the depset
unnecessarily in rule implementations. One call to `to_list()`
at the end in a binary rule is fine, since the overall cost is just O(n). It’s
when many non-terminal targets try to call `to_list()` that quadratic behavior
occurs.

For more information about using depsets efficiently, see the [performance](/rules/performance) page.

## API Reference

Please see [here](/rules/lib/builtins/depset) for more details.

---

## Execution Groups
- URL: https://bazel.build/extending/exec-groups
- Source: extending/exec-groups.mdx
- Slug: /extending/exec-groups

Execution groups allow for multiple execution platforms within a single target.
Each execution group has its own [toolchain](/extending/toolchains) dependencies and
performs its own [toolchain resolution](/extending/toolchains#toolchain-resolution).

## Current status

Execution groups for certain natively declared actions, like `CppLink`, can be
used inside `exec_properties` to set per-action, per-target execution
requirements. For more details, see the
[Default execution groups](#exec-groups-for-native-rules) section.

## Background

Execution groups allow the rule author to define sets of actions, each with a
potentially different execution platform. Multiple execution platforms can allow
actions to execution differently, for example compiling an iOS app on a remote
(linux) worker and then linking/code signing on a local mac worker.

Being able to define groups of actions also helps alleviate the usage of action
mnemonics as a proxy for specifying actions. Mnemonics are not guaranteed to be
unique and can only reference a single action. This is especially helpful in
allocating extra resources to specific memory and processing intensive actions
like linking in C++ builds without over-allocating to less demanding tasks.

## Defining execution groups

During rule definition, rule authors can
[declare](/rules/lib/globals/bzl#exec_group)
a set of execution groups. On each execution group, the rule author can specify
everything needed to select an execution platform for that execution group,
namely any constraints via `exec_compatible_with` and toolchain types via
`toolchain`.

```python
# foo.bzl
my_rule = rule(
    _impl,
    exec_groups = {
        "link": exec_group(
            exec_compatible_with = ["@platforms//os:linux"],
            toolchains = ["//foo:toolchain_type"],
        ),
        "test": exec_group(
            toolchains = ["//foo_tools:toolchain_type"],
        ),
    },
    attrs = {
        "_compiler": attr.label(cfg = config.exec("link"))
    },
)
```

In the code snippet above, you can see that tool dependencies can also specify
transition for an exec group using the
[`cfg`](/rules/lib/toplevel/attr#label)
attribute param and the
[`config`](/rules/lib/toplevel/config)
module. The module exposes an `exec` function which takes a single string
parameter which is the name of the exec group for which the dependency should be
built.

As on native rules, the `test` execution group is present by default on Starlark
test rules.

## Accessing execution groups

In the rule implementation, you can declare that actions should be run on the
execution platform of an execution group. You can do this by using the `exec_group`
param of action generating methods, specifically [`ctx.actions.run`]
(/rules/lib/builtins/actions#run) and
[`ctx.actions.run_shell`](/rules/lib/builtins/actions#run_shell).

```python
# foo.bzl
def _impl(ctx):
  ctx.actions.run(
     inputs = [ctx.attr._some_tool, ctx.srcs[0]]
     exec_group = "compile",
     # ...
  )
```

Rule authors will also be able to access the [resolved toolchains](/extending/toolchains#toolchain-resolution)
of execution groups, similarly to how you
can access the resolved toolchain of a target:

```python
# foo.bzl
def _impl(ctx):
  foo_info = ctx.exec_groups["link"].toolchains["//foo:toolchain_type"].fooinfo
  ctx.actions.run(
     inputs = [foo_info, ctx.srcs[0]]
     exec_group = "link",
     # ...
  )
```

Note: If an action uses a toolchain from an execution group, but doesn't specify
that execution group in the action declaration, that may potentially cause
issues. A mismatch like this may not immediately cause failures, but is a latent
problem.

### Default execution groups

The following execution groups are predefined:

* `test`: Test runner actions (for more details, see
  the [execution platform section of the Test Encylopedia](/reference/test-encyclopedia#execution-platform)).
* `cpp_link`: C++ linking actions.

## Using execution groups to set execution properties

Execution groups are integrated with the
[`exec_properties`](/reference/be/common-definitions#common-attributes)
attribute that exists on every rule and allows the target writer to specify a
string dict of properties that is then passed to the execution machinery. For
example, if you wanted to set some property, say memory, for the target and give
certain actions a higher memory allocation, you would write an `exec_properties`
entry with an execution-group-augmented key, such as:

```python
# BUILD
my_rule(
    name = 'my_target',
    exec_properties = {
        'mem': '12g',
        'link.mem': '16g'
    }
    …
)
```

All actions with `exec_group = "link"` would see the exec properties
dictionary as `{"mem": "16g"}`. As you see here, execution-group-level
settings override target-level settings.

## Using execution groups to set platform constraints

Execution groups are also integrated with the
[`exec_compatible_with`](/reference/be/common-definitions#common-attributes) and
[`exec_group_compatible_with`](/reference/be/common-definitions#common-attributes)
attributes that exist on every rule and allow the target writer to specify
additional constraints that must be satisfied by the execution platforms
selected for the target's actions.

For example, if the rule `my_test` defines the `link` execution group in
addition to the default and the `test` execution group, then the following
usage of these attributes would run actions in the default execution group on
a platform with a high number of CPUs, the test action on Linux, and the link
action on the default execution platform:

```python
# BUILD
constraint_setting(name = "cpu")
constraint_value(name = "high_cpu", constraint_setting = ":cpu")

platform(
  name = "high_cpu_platform",
  constraint_values = [":high_cpu"],
  exec_properties = {
    "cpu": "256",
  },
)

my_test(
  name = "my_test",
  exec_compatible_with = ["//constraints:high_cpu"],
  exec_group_compatible_with = {
    "test": ["@platforms//os:linux"],
  },
  ...
)
```

### Execution groups for native rules

The following execution groups are available for actions defined by native
rules:

* `test`: Test runner actions.
* `cpp_link`: C++ linking actions.

### Execution groups and platform execution properties

It is possible to define `exec_properties` for arbitrary execution groups on
platform targets (unlike `exec_properties` set directly on a target, where
properties for unknown execution groups are rejected). Targets then inherit the
execution platform's `exec_properties` that affect the default execution group
and any other relevant execution groups.

For example, suppose running tests on the exec platform requires some resource
to be available, but it isn't required for compiling and linking; this can be
modelled as follows:

```python
constraint_setting(name = "resource")
constraint_value(name = "has_resource", constraint_setting = ":resource")

platform(
    name = "platform_with_resource",
    constraint_values = [":has_resource"],
    exec_properties = {
        "test.resource": "...",
    },
)

cc_test(
    name = "my_test",
    srcs = ["my_test.cc"],
    exec_compatible_with = [":has_resource"],
)
```

`exec_properties` defined directly on targets take precedence over those that
are inherited from the execution platform.

---

## Legacy Macros
- URL: https://bazel.build/extending/legacy-macros
- Source: extending/legacy-macros.mdx
- Slug: /extending/legacy-macros

Legacy macros are unstructured functions called from `BUILD` files that can
create targets. By the end of the
[loading phase](/extending/concepts#evaluation-model), legacy macros don't exist
anymore, and Bazel sees only the concrete set of instantiated rules.

## Why you shouldn't use legacy macros (and should use Symbolic macros instead)

Where possible you should use [symbolic macros](macros.md#macros).

Symbolic macros

*   Prevent action at a distance
*   Make it possible to hide implementation details through granular visibility
*   Take typed attributes, which in turn means automatic label and select
    conversion.
*   Are more readable
*   Will soon have [lazy evaluation](macros.md#laziness)

## Usage

The typical use case for a macro is when you want to reuse a rule.

For example, genrule in a `BUILD` file generates a file using `//:generator`
with a `some_arg` argument hardcoded in the command:

```python
genrule(
    name = "file",
    outs = ["file.txt"],
    cmd = "$(location //:generator) some_arg > $@",
    tools = ["//:generator"],
)
```

Note: `$@` is a
[Make variable](/reference/be/make-variables#predefined_genrule_variables) that
refers to the execution-time locations of the files in the `outs` attribute
list. It is equivalent to `$(locations :file.txt)`.

If you want to generate more files with different arguments, you may want to
extract this code to a macro function. To create a macro called
`file_generator`, which has `name` and `arg` parameters, we can replace the
genrule with the following:

```python
load("//path:generator.bzl", "file_generator")

file_generator(
    name = "file",
    arg = "some_arg",
)

file_generator(
    name = "file-two",
    arg = "some_arg_two",
)

file_generator(
    name = "file-three",
    arg = "some_arg_three",
)
```

Here, you load the `file_generator` symbol from a `.bzl` file located in the
`//path` package. By putting macro function definitions in a separate `.bzl`
file, you keep your `BUILD` files clean and declarative, The `.bzl` file can be
loaded from any package in the workspace.

Finally, in `path/generator.bzl`, write the definition of the macro to
encapsulate and parameterize the original genrule definition:

```python
def file_generator(name, arg, visibility=None):
  native.genrule(
    name = name,
    outs = [name + ".txt"],
    cmd = "$(location //:generator) %s > $@" % arg,
    tools = ["//:generator"],
    visibility = visibility,
  )
```

You can also use macros to chain rules together. This example shows chained
genrules, where a genrule uses the outputs of a previous genrule as inputs:

```python
def chained_genrules(name, visibility=None):
  native.genrule(
    name = name + "-one",
    outs = [name + ".one"],
    cmd = "$(location :tool-one) $@",
    tools = [":tool-one"],
    visibility = ["//visibility:private"],
  )

  native.genrule(
    name = name + "-two",
    srcs = [name + ".one"],
    outs = [name + ".two"],
    cmd = "$(location :tool-two) $< $@",
    tools = [":tool-two"],
    visibility = visibility,
  )
```

The example only assigns a visibility value to the second genrule. This allows
macro authors to hide the outputs of intermediate rules from being depended upon
by other targets in the workspace.

Note: Similar to `$@` for outputs, `$<` expands to the locations of files in the
`srcs` attribute list.

## Expanding macros

When you want to investigate what a macro does, use the `query` command with
`--output=build` to see the expanded form:

```none
$ bazel query --output=build :file
# /absolute/path/test/ext.bzl:42:3
genrule(
  name = "file",
  tools = ["//:generator"],
  outs = ["//test:file.txt"],
  cmd = "$(location //:generator) some_arg > $@",
)
```

## Instantiating native rules

Native rules (rules that don't need a `load()` statement) can be instantiated
from the [native](/rules/lib/toplevel/native) module:

```python
def my_macro(name, visibility=None):
  native.cc_library(
    name = name,
    srcs = ["main.cc"],
    visibility = visibility,
  )
```

If you need to know the package name (for example, which `BUILD` file is calling
the macro), use the function
[native.package_name()](/rules/lib/toplevel/native#package_name). Note that
`native` can only be used in `.bzl` files, and not in `BUILD` files.

## Label resolution in macros

Since legacy macros are evaluated in the
[loading phase](concepts.md#evaluation-model), label strings such as
`"//foo:bar"` that occur in a legacy macro are interpreted relative to the
`BUILD` file in which the macro is used rather than relative to the `.bzl` file
in which it is defined. This behavior is generally undesirable for macros that
are meant to be used in other repositories, such as because they are part of a
published Starlark ruleset.

To get the same behavior as for Starlark rules, wrap the label strings with the
[`Label`](/rules/lib/builtins/Label#Label) constructor:

```python
# @my_ruleset//rules:defs.bzl
def my_cc_wrapper(name, deps = [], **kwargs):
  native.cc_library(
    name = name,
    deps = deps + select({
      # Due to the use of Label, this label is resolved within @my_ruleset,
      # regardless of its site of use.
      Label("//config:needs_foo"): [
        # Due to the use of Label, this label will resolve to the correct target
        # even if the canonical name of @dep_of_my_ruleset should be different
        # in the main repo, such as due to repo mappings.
        Label("@dep_of_my_ruleset//tools:foo"),
      ],
      "//conditions:default": [],
    }),
    **kwargs,
  )
```

With the `--incompatible_resolve_select_keys_eagerly` flag enabled, all keys
that are label strings will be automatically resolved to `Label` objects
relative to the package of the file that contains the `select` call. If this is
not chosen, wrap the label string with
[native.package_relative_label()](/rules/lib/toplevel/native#package_relative_label).

## Debugging

*   `bazel query --output=build //my/path:all` will show you how the `BUILD`
    file looks after evaluation. All legacy macros, globs, loops are expanded.
    Known limitation: `select` expressions are not shown in the output.

*   You may filter the output based on `generator_function` (which function
    generated the rules) or `generator_name` (the name attribute of the macro):
    `bash $ bazel query --output=build 'attr(generator_function, my_macro,
    //my/path:all)'`

*   To find out where exactly the rule `foo` is generated in a `BUILD` file, you
    can try the following trick. Insert this line near the top of the `BUILD`
    file: `cc_library(name = "foo")`. Run Bazel. You will get an exception when
    the rule `foo` is created (due to a name conflict), which will show you the
    full stack trace.

*   You can also use [print](/rules/lib/globals/all#print) for debugging. It
    displays the message as a `DEBUG` log line during the loading phase. Except
    in rare cases, either remove `print` calls, or make them conditional under a
    `debugging` parameter that defaults to `False` before submitting the code to
    the depot.

## Errors

If you want to throw an error, use the [fail](/rules/lib/globals/all#fail)
function. Explain clearly to the user what went wrong and how to fix their
`BUILD` file. It is not possible to catch an error.

```python
def my_macro(name, deps, visibility=None):
  if len(deps) < 2:
    fail("Expected at least two values in deps")
  # ...
```

## Conventions

*   All public functions (functions that don't start with underscore) that
    instantiate rules must have a `name` argument. This argument should not be
    optional (don't give a default value).

*   Public functions should use a docstring following
    [Python conventions](https://www.python.org/dev/peps/pep-0257/#one-line-docstrings).

*   In `BUILD` files, the `name` argument of the macros must be a keyword
    argument (not a positional argument).

*   The `name` attribute of rules generated by a macro should include the name
    argument as a prefix. For example, `macro(name = "foo")` can generate a
    `cc_library` `foo` and a genrule `foo_gen`.

*   In most cases, optional parameters should have a default value of `None`.
    `None` can be passed directly to native rules, which treat it the same as if
    you had not passed in any argument. Thus, there is no need to replace it
    with `0`, `False`, or `[]` for this purpose. Instead, the macro should defer
    to the rules it creates, as their defaults may be complex or may change over
    time. Additionally, a parameter that is explicitly set to its default value
    looks different than one that is never set (or set to `None`) when accessed
    through the query language or build-system internals.

*   Macros should have an optional `visibility` argument.

---

## Macros
- URL: https://bazel.build/extending/macros
- Source: extending/macros.mdx
- Slug: /extending/macros

This page covers the basics of using macros and includes typical use cases,
debugging, and conventions.

A macro is a function called from the `BUILD` file that can instantiate rules.
Macros are mainly used for encapsulation and code reuse of existing rules and
other macros.

Macros come in two flavors: symbolic macros, which are described on this page,
and [legacy macros](legacy-macros.md). Where possible, we recommend using
symbolic macros for code clarity.

Symbolic macros offer typed arguments (string to label conversion, relative to
where the macro was called) and the ability to restrict and specify the
visibility of targets created. They are designed to be amenable to lazy
evaluation (which will be added in a future Bazel release). Symbolic macros are
available by default in Bazel 8. Where this document mentions `macros`, it's
referring to **symbolic macros**.

An executable example of symbolic macros can be found in the
[examples repository](https://github.com/bazelbuild/examples/tree/main/macros).

## Usage

Macros are defined in `.bzl` files by calling the
[`macro()`](https://bazel.build/rules/lib/globals/bzl.html#macro) function with
two required parameters: `attrs` and `implementation`.

### Attributes

`attrs` accepts a dictionary of attribute name to [attribute
types](https://bazel.build/rules/lib/toplevel/attr#members), which represents
the arguments to the macro. Two common attributes – `name` and `visibility` –
are implicitly added to all macros and are not included in the dictionary passed
to `attrs`.

```starlark
# macro/macro.bzl
my_macro = macro(
    attrs = {
        "deps": attr.label_list(mandatory = True, doc = "The dependencies passed to the inner cc_binary and cc_test targets"),
        "create_test": attr.bool(default = False, configurable = False, doc = "If true, creates a test target"),
    },
    implementation = _my_macro_impl,
)
```

Attribute type declarations accept the
[parameters](https://bazel.build/rules/lib/toplevel/attr#parameters),
`mandatory`, `default`, and `doc`. Most attribute types also accept the
`configurable` parameter, which determines whether the attribute accepts
`select`s. If an attribute is `configurable`, it will parse non-`select` values
as an unconfigurable `select` – `"foo"` will become
`select({"//conditions:default": "foo"})`. Learn more in [selects](#selects).

#### Attribute inheritance

Macros are often intended to wrap a rule (or another macro), and the macro's
author often wants to forward the bulk of the wrapped symbol's attributes
unchanged, using `**kwargs`, to the macro's main target (or main inner macro).

To support this pattern, a macro can *inherit attributes* from a rule or another
macro by passing the [rule](https://bazel.build/rules/lib/builtins/rule) or
[macro symbol](https://bazel.build/rules/lib/builtins/macro) to `macro()`'s
`inherit_attrs` argument. (You can also use the special string `"common"`
instead of a rule or macro symbol to inherit the [common attributes defined for
all Starlark build
rules](https://bazel.build/reference/be/common-definitions#common-attributes).)
Only public attributes get inherited, and the attributes in the macro's own
`attrs` dictionary override inherited attributes with the same name. You can
also *remove* inherited attributes by using `None` as a value in the `attrs`
dictionary:

```starlark
# macro/macro.bzl
my_macro = macro(
    inherit_attrs = native.cc_library,
    attrs = {
        # override native.cc_library's `local_defines` attribute
        "local_defines": attr.string_list(default = ["FOO"]),
        # do not inherit native.cc_library's `defines` attribute
        "defines": None,
    },
    ...
)
```

The default value of non-mandatory inherited attributes is always overridden to
be `None`, regardless of the original attribute definition's default value. If
you need to examine or modify an inherited non-mandatory attribute – for
example, if you want to add a tag to an inherited `tags` attribute – you must
make sure to handle the `None` case in your macro's implementation function:

```starlark
# macro/macro.bzl
def _my_macro_impl(name, visibility, tags, **kwargs):
    # Append a tag; tags attr is an inherited non-mandatory attribute, and
    # therefore is None unless explicitly set by the caller of our macro.
    my_tags = (tags or []) + ["another_tag"]
    native.cc_library(
        ...
        tags = my_tags,
        **kwargs,
    )
    ...
```

### Implementation

`implementation` accepts a function which contains the logic of the macro.
Implementation functions often create targets by calling one or more rules, and
they are usually private (named with a leading underscore). Conventionally,
they are named the same as their macro, but prefixed with `_` and suffixed with
`_impl`.

Unlike rule implementation functions, which take a single argument (`ctx`) that
contains a reference to the attributes, macro implementation functions accept a
parameter for each argument.

```starlark
# macro/macro.bzl
def _my_macro_impl(name, visibility, deps, create_test):
    cc_library(
        name = name + "_cc_lib",
        deps = deps,
    )

    if create_test:
        cc_test(
            name = name + "_test",
            srcs = ["my_test.cc"],
            deps = deps,
        )
```

If a macro inherits attributes, its implementation function *must* have a
`**kwargs` residual keyword parameter, which can be forwarded to the call that
invokes the inherited rule or submacro. (This helps ensure that your macro won't
be broken if the rule or macro which from which you are inheriting adds a new
attribute.)

### Declaration

Macros are declared by loading and calling their definition in a `BUILD` file.

```starlark

# pkg/BUILD

my_macro(
    name = "macro_instance",
    deps = ["src.cc"] + select(
        {
            "//config_setting:special": ["special_source.cc"],
            "//conditions:default": [],
        },
    ),
    create_tests = True,
)
```

This would create targets
`//pkg:macro_instance_cc_lib` and`//pkg:macro_instance_test`.

Just like in rule calls, if an attribute value in a macro call is set to `None`,
that attribute is treated as if it was omitted by the macro's caller. For
example, the following two macro calls are equivalent:

```starlark
# pkg/BUILD
my_macro(name = "abc", srcs = ["src.cc"], deps = None)
my_macro(name = "abc", srcs = ["src.cc"])
```

This is generally not useful in `BUILD` files, but is helpful when
programmatically wrapping a macro inside another macro.

## Details

### Naming conventions for targets created

The names of any targets or submacros created by a symbolic macro must
either match the macro's `name` parameter or must be prefixed by `name` followed
by `_` (preferred), `.` or `-`. For example, `my_macro(name = "foo")` may only
create files or targets named `foo`, or prefixed by `foo_`, `foo-` or `foo.`,
for example, `foo_bar`.

Targets or files that violate macro naming convention can be declared, but
cannot be built and cannot be used as dependencies.

Non-macro files and targets within the same package as a macro instance should
*not* have names that conflict with potential macro target names, though this
exclusivity is not enforced. We are in the progress of implementing
[lazy evaluation](#laziness) as a performance improvement for Symbolic macros,
which will be impaired in packages that violate the naming schema.

### Restrictions

Symbolic macros have some additional restrictions compared to legacy macros.

Symbolic macros

*   must take a `name` argument and a `visibility` argument
*   must have an `implementation` function
*   may not return values
*   may not mutate their arguments
*   may not call `native.existing_rules()` unless they are special `finalizer`
    macros
*   may not call `native.package()`
*   may not call `glob()`
*   may not call `native.environment_group()`
*   must create targets whose names adhere to the [naming schema](#naming)
*   can't refer to input files that weren't declared or passed in as an argument
*   can't refer to private targets of their callers (see
    [visibility and macros](#visibility) for more details).

### Visibility and macros

The [visibility](/concepts/visibility) system helps protect the implementation
details of both (symbolic) macros and their callers.

By default, targets created in a symbolic macro are visible within the macro
itself, but not necessarily to the macro's caller. The macro can "export" a
target as a public API by forwarding the value of its own `visibility`
attribute, as in `some_rule(..., visibility = visibility)`.

The key ideas of macro visibility are:

1. Visibility is checked based on what macro declared the target, not what
   package called the macro.

   * In other words, being in the same package does not by itself make one
     target visible to another. This protects the macro's internal targets
     from becoming dependencies of other macros or top-level targets in the
     package.

1. All `visibility` attributes, on both rules and macros, automatically
   include the place where the rule or macro was called.

   * Thus, a target is unconditionally visible to other targets declared in the
     same macro (or the `BUILD` file, if not in a macro).

In practice, this means that when a macro declares a target without setting its
`visibility`, the target defaults to being internal to the macro. (The package's
[default visibility](/reference/be/functions#package.default_visibility) does
not apply within a macro.) Exporting the target means that the target is visible
to whatever the macro's caller specified in the macro's `visibility` attribute,
plus the package of the macro's caller itself, as well as the macro's own code.
Another way of thinking of it is that the visibility of a macro determines who
(aside from the macro itself) can see the macro's exported targets.

```starlark
# tool/BUILD
...
some_rule(
    name = "some_tool",
    visibility = ["//macro:__pkg__"],
)
```

```starlark
# macro/macro.bzl

def _impl(name, visibility):
    cc_library(
        name = name + "_helper",
        ...
        # No visibility passed in. Same as passing `visibility = None` or
        # `visibility = ["//visibility:private"]`. Visible to the //macro
        # package only.
    )
    cc_binary(
        name = name + "_exported",
        deps = [
            # Allowed because we're also in //macro. (Targets in any other
            # instance of this macro, or any other macro in //macro, can see it
            # too.)
            name + "_helper",
            # Allowed by some_tool's visibility, regardless of what BUILD file
            # we're called from.
            "//tool:some_tool",
        ],
        ...
        visibility = visibility,
    )

my_macro = macro(implementation = _impl, ...)
```

```starlark
# pkg/BUILD
load("//macro:macro.bzl", "my_macro")
...

my_macro(
    name = "foo",
    ...
)

some_rule(
    ...
    deps = [
        # Allowed, its visibility is ["//pkg:__pkg__", "//macro:__pkg__"].
        ":foo_exported",
        # Disallowed, its visibility is ["//macro:__pkg__"] and
        # we are not in //macro.
        ":foo_helper",
    ]
)
```

If `my_macro` were called with `visibility = ["//other_pkg:__pkg__"]`, or if
the `//pkg` package had set its `default_visibility` to that value, then
`//pkg:foo_exported` could also be used within `//other_pkg/BUILD` or within a
macro defined in `//other_pkg:defs.bzl`, but `//pkg:foo_helper` would remain
protected.

A macro can declare that a target is visible to a friend package by passing
`visibility = ["//some_friend:__pkg__"]` (for an internal target) or
`visibility = visibility + ["//some_friend:__pkg__"]` (for an exported one).
Note that it is an antipattern for a macro to declare a target with public
visibility (`visibility = ["//visibility:public"]`). This is because it makes
the target unconditionally visible to every package, even if the caller
specified a more restricted visibility.

All visibility checking is done with respect to the innermost currently running
symbolic macro. However, there is a visibility delegation mechanism: If a macro
passes a label as an attribute value to an inner macro, any usages of the label
in the inner macro are checked with respect to the outer macro. See the
[visibility page](/concepts/visibility#symbolic-macros) for more details.

Remember that legacy macros are entirely transparent to the visibility system,
and behave as though their location is whatever BUILD file or symbolic macro
they were called from.

#### Finalizers and visibility

Targets declared in a rule finalizer, in addition to seeing targets following
the usual symbolic macro visibility rules, can *also* see all targets which are
visible to the finalizer target's package.

This means that if you migrate a `native.existing_rules()`-based legacy macro to
a finalizer, the targets declared by the finalizer will still be able to see
their old dependencies.

However, note that it's possible to declare a target in a symbolic macro such
that a finalizer's targets cannot see it under the visibility system – even
though the finalizer can *introspect* its attributes using
`native.existing_rules()`.

### Selects

If an attribute is `configurable` (the default) and its value is not `None`,
then the macro implementation function will see the attribute value as wrapped
in a trivial `select`. This makes it easier for the macro author to catch bugs
where they did not anticipate that the attribute value could be a `select`.

For example, consider the following macro:

```starlark
my_macro = macro(
    attrs = {"deps": attr.label_list()},  # configurable unless specified otherwise
    implementation = _my_macro_impl,
)
```

If `my_macro` is invoked with `deps = ["//a"]`, that will cause `_my_macro_impl`
to be invoked with its `deps` parameter set to `select({"//conditions:default":
["//a"]})`. If this causes the implementation function to fail (say, because the
code tried to index into the value as in `deps[0]`, which is not allowed for
`select`s), the macro author can then make a choice: either they can rewrite
their macro to only use operations compatible with `select`, or they can mark
the attribute as nonconfigurable (`attr.label_list(configurable = False)`). The
latter ensures that users are not permitted to pass a `select` value in.

Rule targets reverse this transformation, and store trivial `select`s as their
unconditional values; in the above example, if `_my_macro_impl` declares a rule
target `my_rule(..., deps = deps)`, that rule target's `deps` will be stored as
`["//a"]`. This ensures that `select`-wrapping does not cause trivial `select`
values to be stored in all targets instantiated by macros.

If the value of a configurable attribute is `None`, it does not get wrapped in a
`select`. This ensures that tests like `my_attr == None` still work, and that
when the attribute is forwarded to a rule with a computed default, the rule
behaves properly (that is, as if the attribute were not passed in at all). It is
not always possible for an attribute to take on a `None` value, but it can
happen for the `attr.label()` type, and for any inherited non-mandatory
attribute.

## Finalizers

A rule finalizer is a special symbolic macro which – regardless of its lexical
position in a BUILD file – is evaluated in the final stage of loading a package,
after all non-finalizer targets have been defined. Unlike ordinary symbolic
macros, a finalizer can call `native.existing_rules()`, where it behaves
slightly differently than in legacy macros: it only returns the set of
non-finalizer rule targets. The finalizer may assert on the state of that set or
define new targets.

To declare a finalizer, call `macro()` with `finalizer = True`:

```starlark
def _my_finalizer_impl(name, visibility, tags_filter):
    for r in native.existing_rules().values():
        for tag in r.get("tags", []):
            if tag in tags_filter:
                my_test(
                    name = name + "_" + r["name"] + "_finalizer_test",
                    deps = [r["name"]],
                    data = r["srcs"],
                    ...
                )
                continue

my_finalizer = macro(
    attrs = {"tags_filter": attr.string_list(configurable = False)},
    implementation = _impl,
    finalizer = True,
)
```

## Laziness

IMPORTANT: We are in the process of implementing lazy macro expansion and
evaluation. This feature is not available yet.

Currently, all macros are evaluated as soon as the BUILD file is loaded, which
can negatively impact performance for targets in packages that also have costly
unrelated macros. In the future, non-finalizer symbolic macros will only be
evaluated if they're required for the build. The prefix naming schema helps
Bazel determine which macro to expand given a requested target.

## Migration troubleshooting

Here are some common migration headaches and how to fix them.

*   Legacy macro calls `glob()`

Move the `glob()` call to your BUILD file (or to a legacy macro called from the
BUILD file), and pass the `glob()` value to the symbolic macro using a
label-list attribute:

```starlark
# BUILD file
my_macro(
    ...,
    deps = glob(...),
)
```

*   Legacy macro has a parameter that isn't a valid starlark `attr` type.

Pull as much logic as possible into a nested symbolic macro, but keep the
top level macro a legacy macro.

*  Legacy macro calls a rule that creates a target that breaks the naming schema

That's okay, just don't depend on the "offending" target. The naming check will
be quietly ignored.

---

## Platforms
- URL: https://bazel.build/extending/platforms
- Source: extending/platforms.mdx
- Slug: /extending/platforms

Bazel can build and test code on a variety of hardware, operating systems, and
system configurations, using many different versions of build tools such as
linkers and compilers. To help manage this complexity, Bazel has a concept of
*constraints* and *platforms*. A constraint is a dimension in which build or
production environments may differ, such as CPU architecture, the presence or
absence of a GPU, or the version of a system-installed compiler. A platform is a
named collection of choices for these constraints, representing the particular
resources that are available in some environment.

Modeling the environment as a platform helps Bazel to automatically select the
appropriate
[toolchains](/extending/toolchains)
for build actions. Platforms can also be used in combination with the
[config_setting](/reference/be/general#config_setting)
rule to write [configurable attributes](/docs/configurable-attributes).

Bazel recognizes three roles that a platform may serve:

*  **Host** - the platform on which Bazel itself runs.
*  **Execution** - a platform on which build tools execute build actions to
   produce intermediate and final outputs.
*  **Target** - a platform on which a final output resides and executes.

Bazel supports the following build scenarios regarding platforms:

*  **Single-platform builds** (default) - host, execution, and target platforms
   are the same. For example, building a Linux executable on Ubuntu running on
   an Intel x64 CPU.

*  **Cross-compilation builds** - host and execution platforms are the same, but
   the target platform is different. For example, building an iOS app on macOS
   running on a MacBook Pro.

*  **Multi-platform builds** - host, execution, and target platforms are all
   different.

Tip: for detailed instructions on migrating your project to platforms, see
[Migrating to Platforms](/concepts/platforms).

## Defining constraints and platforms

The space of possible choices for platforms is defined by using the
[`constraint_setting`][constraint_setting] and
[`constraint_value`][constraint_value] rules within `BUILD` files.
`constraint_setting` creates a new dimension, while
`constraint_value` creates a new value for a given dimension; together they
effectively define an enum and its possible values. For example, the following
snippet of a `BUILD` file introduces a constraint for the system's glibc version
with two possible values.

[constraint_setting]: /reference/be/platforms-and-toolchains#constraint_setting
[constraint_value]: /reference/be/platforms-and-toolchains#constraint_value

```python
constraint_setting(name = "glibc_version")

constraint_value(
    name = "glibc_2_25",
    constraint_setting = ":glibc_version",
)

constraint_value(
    name = "glibc_2_26",
    constraint_setting = ":glibc_version",
)
```

Constraints and their values may be defined across different packages in the
workspace. They are referenced by label and subject to the usual visibility
controls. If visibility allows, you can extend an existing constraint setting by
defining your own value for it.

The [`platform`](/reference/be/platforms-and-toolchains#platform) rule introduces a new platform with
certain choices of constraint values. The
following creates a platform named `linux_x86`, and says that it describes any
environment that runs a Linux operating system on an x86_64 architecture with a
glibc version of 2.25. (See below for more on Bazel's built-in constraints.)

```python
platform(
    name = "linux_x86",
    constraint_values = [
        "@platforms//os:linux",
        "@platforms//cpu:x86_64",
        ":glibc_2_25",
    ],
)
```

Note: It is an error for a platform to specify more than one value of the
same constraint setting, such as `@platforms//cpu:x86_64` and
`@platforms//cpu:arm` for `@platforms//cpu:cpu`.

## Generally useful constraints and platforms

To keep the ecosystem consistent, Bazel team maintains a repository with
constraint definitions for the most popular CPU architectures and operating
systems. These are all located in
[https://github.com/bazelbuild/platforms](https://github.com/bazelbuild/platforms).

Bazel ships with the following special platform definition:
`@platforms//host` (aliased as `@bazel_tools//tools:host_platform`). This is the
autodetected host platform value -
represents autodetected platform for the system Bazel is running on.

## Specifying a platform for a build

You can specify the host and target platforms for a build using the following
command-line flags:

*  `--host_platform` - defaults to `@bazel_tools//tools:host_platform`
   *  This target is aliased to `@platforms//host`, which is backed by a repo
      rule that detects the host OS and CPU and writes the platform target.
   *  There's also `@platforms//host:constraints.bzl`, which exposes
      an array called `HOST_CONSTRAINTS`, which can be used in other BUILD and
      Starlark files.
*  `--platforms` - defaults to the host platform
   *  This means that when no other flags are set,
      `@platforms//host` is the target platform.
   *  If `--host_platform` is set and not `--platforms`, the value of
      `--host_platform` is both the host and target platform.

## Skipping incompatible targets

When building for a specific target platform it is often desirable to skip
targets that will never work on that platform. For example, your Windows device
driver is likely going to generate lots of compiler errors when building on a
Linux machine with `//...`. Use the
[`target_compatible_with`](/reference/be/common-definitions#common.target_compatible_with)
attribute to tell Bazel what target platform constraints your code has.

The simplest use of this attribute restricts a target to a single platform.
The target will not be built for any platform that doesn't satisfy all of the
constraints. The following example restricts `win_driver_lib.cc` to 64-bit
Windows.

```python
cc_library(
    name = "win_driver_lib",
    srcs = ["win_driver_lib.cc"],
    target_compatible_with = [
        "@platforms//cpu:x86_64",
        "@platforms//os:windows",
    ],
)
```

`:win_driver_lib` is *only* compatible for building with 64-bit Windows and
incompatible with all else. Incompatibility is transitive. Any targets
that transitively depend on an incompatible target are themselves considered
incompatible.

### When are targets skipped?

Targets are skipped when they are considered incompatible and included in the
build as part of a target pattern expansion. For example, the following two
invocations skip any incompatible targets found in a target pattern expansion.

```console
$ bazel build --platforms=//:myplatform //...
```

```console
$ bazel build --platforms=//:myplatform //:all
```

Incompatible tests in a [`test_suite`](/reference/be/general#test_suite) are
similarly skipped if the `test_suite` is specified on the command line with
[`--expand_test_suites`](/reference/command-line-reference#flag--expand_test_suites).
In other words, `test_suite` targets on the command line behave like `:all` and
`...`. Using `--noexpand_test_suites` prevents expansion and causes
`test_suite` targets with incompatible tests to also be incompatible.

Explicitly specifying an incompatible target on the command line results in an
error message and a failed build.

```console
$ bazel build --platforms=//:myplatform //:target_incompatible_with_myplatform
...
ERROR: Target //:target_incompatible_with_myplatform is incompatible and cannot be built, but was explicitly requested.
...
FAILED: Build did NOT complete successfully
```

Incompatible explicit targets are silently skipped if
`--skip_incompatible_explicit_targets` is enabled.

### More expressive constraints

For more flexibility in expressing constraints, use the
`@platforms//:incompatible`
[`constraint_value`](/reference/be/platforms-and-toolchains#constraint_value)
that no platform satisfies.

Use [`select()`](/reference/be/functions#select) in combination with
`@platforms//:incompatible` to express more complicated restrictions. For
example, use it to implement basic OR logic. The following marks a library
compatible with macOS and Linux, but no other platforms.

Note: An empty constraints list is equivalent to "compatible with everything".

```python
cc_library(
    name = "unixish_lib",
    srcs = ["unixish_lib.cc"],
    target_compatible_with = select({
        "@platforms//os:osx": [],
        "@platforms//os:linux": [],
        "//conditions:default": ["@platforms//:incompatible"],
    }),
)
```

The above can be interpreted as follows:

1. When targeting macOS, the target has no constraints.
2. When targeting Linux, the target has no constraints.
3. Otherwise, the target has the `@platforms//:incompatible` constraint. Because
   `@platforms//:incompatible` is not part of any platform, the target is
   deemed incompatible.

To make your constraints more readable, use
[skylib](https://github.com/bazelbuild/bazel-skylib)'s
[`selects.with_or()`](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/selects_doc.md#selectswith_or).

You can express inverse compatibility in a similar way. The following example
describes a library that is compatible with everything _except_ for ARM.

```python
cc_library(
    name = "non_arm_lib",
    srcs = ["non_arm_lib.cc"],
    target_compatible_with = select({
        "@platforms//cpu:arm": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
)
```

### Detecting incompatible targets using `bazel cquery`

You can use the
[`IncompatiblePlatformProvider`](/rules/lib/providers/IncompatiblePlatformProvider)
in `bazel cquery`'s [Starlark output
format](/query/cquery#output-format-definition) to distinguish
incompatible targets from compatible ones.

This can be used to filter out incompatible targets. The example below will
only print the labels for targets that are compatible. Incompatible targets are
not printed.

```console
$ cat example.cquery

def format(target):
  if "IncompatiblePlatformProvider" not in providers(target):
    return target.label
  return ""


$ bazel cquery //... --output=starlark --starlark:file=example.cquery
```

### Known Issues

Incompatible targets [ignore visibility
restrictions](https://github.com/bazelbuild/bazel/issues/16044).

---

## Rules
- URL: https://bazel.build/extending/rules
- Source: extending/rules.mdx
- Slug: /extending/rules

A **rule** defines a series of [**actions**](#actions) that Bazel performs on
inputs to produce a set of outputs, which are referenced in
[**providers**](#providers) returned by the rule's
[**implementation function**](#implementation_function). For example, a C++
binary rule might:

1.  Take a set of `.cpp` source files (inputs).
2.  Run `g++` on the source files (action).
3.  Return the `DefaultInfo` provider with the executable output and other files
    to make available at runtime.
4.  Return the `CcInfo` provider with C++-specific information gathered from the
    target and its dependencies.

From Bazel's perspective, `g++` and the standard C++ libraries are also inputs
to this rule. As a rule writer, you must consider not only the user-provided
inputs to a rule, but also all of the tools and libraries required to execute
the actions.

Before creating or modifying any rule, ensure you are familiar with Bazel's
[build phases](/extending/concepts). It is important to understand the three
phases of a build (loading, analysis, and execution). It is also useful to
learn about [macros](/extending/macros) to understand the difference between rules and
macros. To get started, first review the [Rules Tutorial](/rules/rules-tutorial).
Then, use this page as a reference.

A few rules are built into Bazel itself. These *native rules*, such as
`genrule` and `filegroup`, provide some core support.
By defining your own rules, you can add support for languages and tools
that Bazel doesn't support natively.

Bazel provides an extensibility model for writing rules using the
[Starlark](/rules/language) language. These rules are written in `.bzl` files, which
can be loaded directly from `BUILD` files.

When defining your own rule, you get to decide what attributes it supports and
how it generates its outputs.

The rule's `implementation` function defines its exact behavior during the
[analysis phase](/extending/concepts#evaluation-model). This function doesn't run any
external commands. Rather, it registers [actions](#actions) that will be used
later during the execution phase to build the rule's outputs, if they are
needed.

## Rule creation

In a `.bzl` file, use the [rule](/rules/lib/globals/bzl#rule) function to define a new
rule, and store the result in a global variable. The call to `rule` specifies
[attributes](#attributes) and an
[implementation function](#implementation_function):

```python
example_library = rule(
    implementation = _example_library_impl,
    attrs = {
        "deps": attr.label_list(),
        ...
    },
)
```

This defines a [rule kind](/query/language#kind) named `example_library`.

The call to `rule` also must specify if the rule creates an
[executable](#executable-rules) output (with `executable = True`), or specifically
a test executable (with `test = True`). If the latter, the rule is a *test rule*,
and the name of the rule must end in `_test`.

## Target instantiation

Rules can be [loaded](/concepts/build-files#load) and called in `BUILD` files:

```python
load('//some/pkg:rules.bzl', 'example_library')

example_library(
    name = "example_target",
    deps = [":another_target"],
    ...
)
```

Each call to a build rule returns no value, but has the side effect of defining
a target. This is called *instantiating* the rule. This specifies a name for the
new target and values for the target's [attributes](#attributes).

Rules can also be called from Starlark functions and loaded in `.bzl` files.
Starlark functions that call rules are called [Starlark macros](/extending/macros).
Starlark macros must ultimately be called from `BUILD` files, and can only be
called during the [loading phase](/extending/concepts#evaluation-model), when `BUILD`
files are evaluated to instantiate targets.

## Attributes

An *attribute* is a rule argument. Attributes can provide specific values to a
target's [implementation](#implementation_function), or they can refer to other
targets, creating a graph of dependencies.

Rule-specific attributes, such as `srcs` or `deps`, are defined by passing a map
from attribute names to schemas (created using the [`attr`](/rules/lib/toplevel/attr)
module) to the `attrs` parameter of `rule`.
[Common attributes](/reference/be/common-definitions#common-attributes), such as
`name` and `visibility`, are implicitly added to all rules. Additional
attributes are implicitly added to
[executable and test rules](#executable-rules) specifically. Attributes which
are implicitly added to a rule can't be included in the dictionary passed to
`attrs`.

### Dependency attributes

Rules that process source code usually define the following attributes to handle
various [types of dependencies](/concepts/dependencies#types_of_dependencies):

*   `srcs` specifies source files processed by a target's actions. Often, the
    attribute schema specifies which file extensions are expected for the sort
    of source file the rule processes. Rules for languages with header files
    generally specify a separate `hdrs` attribute for headers processed by a
    target and its consumers.
*   `deps` specifies code dependencies for a target. The attribute schema should
    specify which [providers](#providers) those dependencies must provide. (For
    example, `cc_library` provides `CcInfo`.)
*   `data` specifies files to be made available at runtime to any executable
    which depends on a target. That should allow arbitrary files to be
    specified.

```python
example_library = rule(
    implementation = _example_library_impl,
    attrs = {
        "srcs": attr.label_list(allow_files = [".example"]),
        "hdrs": attr.label_list(allow_files = [".header"]),
        "deps": attr.label_list(providers = [ExampleInfo]),
        "data": attr.label_list(allow_files = True),
        ...
    },
)
```

These are examples of *dependency attributes*. Any attribute that specifies
an input label (those defined with
[`attr.label_list`](/rules/lib/toplevel/attr#label_list),
[`attr.label`](/rules/lib/toplevel/attr#label), or
[`attr.label_keyed_string_dict`](/rules/lib/toplevel/attr#label_keyed_string_dict))
specifies dependencies of a certain type
between a target and the targets whose labels (or the corresponding
[`Label`](/rules/lib/builtins/Label) objects) are listed in that attribute when the target
is defined. The repository, and possibly the path, for these labels is resolved
relative to the defined target.

```python
example_library(
    name = "my_target",
    deps = [":other_target"],
)

example_library(
    name = "other_target",
    ...
)
```

In this example, `other_target` is a dependency of `my_target`, and therefore
`other_target` is analyzed first. It is an error if there is a cycle in the
dependency graph of targets.

<a name="private-attributes"></a>

### Private attributes and implicit dependencies

A dependency attribute with a default value creates an *implicit dependency*. It
is implicit because it's a part of the target graph that the user doesn't
specify it in a `BUILD` file. Implicit dependencies are useful for hard-coding a
relationship between a rule and a *tool* (a build-time dependency, such as a
compiler), since most of the time a user is not interested in specifying what
tool the rule uses. Inside the rule's implementation function, this is treated
the same as other dependencies.

If you want to provide an implicit dependency without allowing the user to
override that value, you can make the attribute *private* by giving it a name
that begins with an underscore (`_`). Private attributes must have default
values. It generally only makes sense to use private attributes for implicit
dependencies.

```python
example_library = rule(
    implementation = _example_library_impl,
    attrs = {
        ...
        "_compiler": attr.label(
            default = Label("//tools:example_compiler"),
            allow_single_file = True,
            executable = True,
            cfg = "exec",
        ),
    },
)
```

In this example, every target of type `example_library` has an implicit
dependency on the compiler `//tools:example_compiler`. This allows
`example_library`'s implementation function to generate actions that invoke the
compiler, even though the user did not pass its label as an input. Since
`_compiler` is a private attribute, it follows that `ctx.attr._compiler`
will always point to `//tools:example_compiler` in all targets of this rule
type. Alternatively, you can name the attribute `compiler` without the
underscore and keep the default value. This allows users to substitute a
different compiler if necessary, but it requires no awareness of the compiler's
label.

Implicit dependencies are generally used for tools that reside in the same
repository as the rule implementation. If the tool comes from the
[execution platform](/extending/platforms) or a different repository instead, the
rule should obtain that tool from a [toolchain](/extending/toolchains).

### Output attributes

*Output attributes*, such as [`attr.output`](/rules/lib/toplevel/attr#output) and
[`attr.output_list`](/rules/lib/toplevel/attr#output_list), declare an output file that the
target generates. These differ from dependency attributes in two ways:

*   They define output file targets instead of referring to targets defined
    elsewhere.
*   The output file targets depend on the instantiated rule target, instead of
    the other way around.

Typically, output attributes are only used when a rule needs to create outputs
with user-defined names which can't be based on the target name. If a rule has
one output attribute, it is typically named `out` or `outs`.

Output attributes are the preferred way of creating *predeclared outputs*, which
can be specifically depended upon or
[requested at the command line](#requesting_output_files).

## Implementation function

Every rule requires an `implementation` function. These functions are executed
strictly in the [analysis phase](/extending/concepts#evaluation-model) and transform the
graph of targets generated in the loading phase into a graph of
[actions](#actions) to be performed during the execution phase. As such,
implementation functions can't actually read or write files.

Rule implementation functions are usually private (named with a leading
underscore). Conventionally, they are named the same as their rule, but suffixed
with `_impl`.

Implementation functions take exactly one parameter: a
[rule context](/rules/lib/builtins/ctx), conventionally named `ctx`. They return a list of
[providers](#providers).

### Targets

Dependencies are represented at analysis time as [`Target`](/rules/lib/builtins/Target)
objects. These objects contain the [providers](#providers) generated when the
target's implementation function was executed.

[`ctx.attr`](/rules/lib/builtins/ctx#attr) has fields corresponding to the names of each
dependency attribute, containing `Target` objects representing each direct
dependency using that attribute. For `label_list` attributes, this is a list of
`Targets`. For `label` attributes, this is a single `Target` or `None`.

A list of provider objects are returned by a target's implementation function:

```python
return [ExampleInfo(headers = depset(...))]
```

Those can be accessed using index notation (`[]`), with the type of provider as
a key. These can be [custom providers](#custom_providers) defined in Starlark or
[providers for native rules](/rules/lib/providers) available as Starlark
global variables.

For example, if a rule takes header files using a `hdrs` attribute and provides
them to the compilation actions of the target and its consumers, it could
collect them like so:

```python
def _example_library_impl(ctx):
    ...
    transitive_headers = [hdr[ExampleInfo].headers for hdr in ctx.attr.hdrs]
```

There's a legacy struct style, which is strongly discouraged and rules should be
[migrated away from it](#migrating_from_legacy_providers).

### Files

Files are represented by [`File`](/rules/lib/builtins/File) objects. Since Bazel doesn't
perform file I/O during the analysis phase, these objects can't be used to
directly read or write file content. Rather, they are passed to action-emitting
functions (see [`ctx.actions`](/rules/lib/builtins/actions)) to construct pieces of the
action graph.

A `File` can either be a source file or a generated file. Each generated file
must be an output of exactly one action. Source files can't be the output of
any action.

For each dependency attribute, the corresponding field of
[`ctx.files`](/rules/lib/builtins/ctx#files) contains a list of the default outputs of all
dependencies using that attribute:

```python
def _example_library_impl(ctx):
    ...
    headers = depset(ctx.files.hdrs, transitive = transitive_headers)
    srcs = ctx.files.srcs
    ...
```

[`ctx.file`](/rules/lib/builtins/ctx#file) contains a single `File` or `None` for
dependency attributes whose specs set `allow_single_file = True`.
[`ctx.executable`](/rules/lib/builtins/ctx#executable) behaves the same as `ctx.file`, but only
contains fields for dependency attributes whose specs set `executable = True`.

### Declaring outputs

During the analysis phase, a rule's implementation function can create outputs.
Since all labels have to be known during the loading phase, these additional
outputs have no labels. `File` objects for outputs can be created using
[`ctx.actions.declare_file`](/rules/lib/builtins/actions#declare_file) and
[`ctx.actions.declare_directory`](/rules/lib/builtins/actions#declare_directory).
Often, the names of outputs are based on the target's name,
[`ctx.label.name`](/rules/lib/builtins/ctx#label):

```python
def _example_library_impl(ctx):
  ...
  output_file = ctx.actions.declare_file(ctx.label.name + ".output")
  ...
```

For *predeclared outputs*, like those created for
[output attributes](#output_attributes), `File` objects instead can be retrieved
from the corresponding fields of [`ctx.outputs`](/rules/lib/builtins/ctx#outputs).

### Actions

An action describes how to generate a set of outputs from a set of inputs, for
example "run gcc on hello.c and get hello.o". When an action is created, Bazel
doesn't run the command immediately. It registers it in a graph of dependencies,
because an action can depend on the output of another action. For example, in C,
the linker must be called after the compiler.

General-purpose functions that create actions are defined in
[`ctx.actions`](/rules/lib/builtins/actions):

*   [`ctx.actions.run`](/rules/lib/builtins/actions#run), to run an executable.
*   [`ctx.actions.run_shell`](/rules/lib/builtins/actions#run_shell), to run a shell
    command.
*   [`ctx.actions.write`](/rules/lib/builtins/actions#write), to write a string to a file.
*   [`ctx.actions.expand_template`](/rules/lib/builtins/actions#expand_template), to
    generate a file from a template.

[`ctx.actions.args`](/rules/lib/builtins/actions#args) can be used to efficiently
accumulate the arguments for actions. It avoids flattening depsets until
execution time:

```python
def _example_library_impl(ctx):
    ...

    transitive_headers = [dep[ExampleInfo].headers for dep in ctx.attr.deps]
    headers = depset(ctx.files.hdrs, transitive = transitive_headers)
    srcs = ctx.files.srcs
    inputs = depset(srcs, transitive = [headers])
    output_file = ctx.actions.declare_file(ctx.label.name + ".output")

    args = ctx.actions.args()
    args.add_joined("-h", headers, join_with = ",")
    args.add_joined("-s", srcs, join_with = ",")
    args.add("-o", output_file)

    ctx.actions.run(
        mnemonic = "ExampleCompile",
        executable = ctx.executable._compiler,
        arguments = [args],
        inputs = inputs,
        outputs = [output_file],
    )
    ...
```

Actions take a list or depset of input files and generate a (non-empty) list of
output files. The set of input and output files must be known during the
[analysis phase](/extending/concepts#evaluation-model). It might depend on the value of
attributes, including providers from dependencies, but it can't depend on the
result of the execution. For example, if your action runs the unzip command, you
must specify which files you expect to be inflated (before running unzip).
Actions which create a variable number of files internally can wrap those in a
single file (such as a zip, tar, or other archive format).

Actions must list all of their inputs. Listing inputs that are not used is
permitted, but inefficient.

Actions must create all of their outputs. They may write other files, but
anything not in outputs won't be available to consumers. All declared outputs
must be written by some action.

Actions are comparable to pure functions: They should depend only on the
provided inputs, and avoid accessing computer information, username, clock,
network, or I/O devices (except for reading inputs and writing outputs). This is
important because the output will be cached and reused.

Dependencies are resolved by Bazel, which decides which actions to
execute. It is an error if there is a cycle in the dependency graph. Creating
an action doesn't guarantee that it will be executed, that depends on whether
its outputs are needed for the build.

### Providers

Providers are pieces of information that a rule exposes to other rules that
depend on it. This data can include output files, libraries, parameters to pass
on a tool's command line, or anything else a target's consumers should know
about.

Since a rule's implementation function can only read providers from the
instantiated target's immediate dependencies, rules need to forward any
information from a target's dependencies that needs to be known by a target's
consumers, generally by accumulating that into a [`depset`](/rules/lib/builtins/depset).

A target's providers are specified by a list of provider objects returned by
the implementation function.

Old implementation functions can also be written in a legacy style where the
implementation function returns a [`struct`](/rules/lib/builtins/struct) instead of list of
provider objects. This style is strongly discouraged and rules should be
[migrated away from it](#migrating_from_legacy_providers).

#### Default outputs

A target's *default outputs* are the outputs that are requested by default when
the target is requested for build at the command line. For example, a
`java_library` target `//pkg:foo` has `foo.jar` as a default output, so that
will be built by the command `bazel build //pkg:foo`.

Default outputs are specified by the `files` parameter of
[`DefaultInfo`](/rules/lib/providers/DefaultInfo):

```python
def _example_library_impl(ctx):
    ...
    return [
        DefaultInfo(files = depset([output_file]), ...),
        ...
    ]
```

If `DefaultInfo` is not returned by a rule implementation or the `files`
parameter is not specified, `DefaultInfo.files` defaults to all
*predeclared outputs* (generally, those created by [output
attributes](#output_attributes)).

Rules that perform actions should provide default outputs, even if those outputs
are not expected to be directly used. Actions that are not in the graph of the
requested outputs are pruned. If an output is only used by a target's consumers,
those actions won't be performed when the target is built in isolation. This
makes debugging more difficult because rebuilding just the failing target won't
reproduce the failure.

#### Runfiles

Runfiles are a set of files used by a target at runtime (as opposed to build
time). During the [execution phase](/extending/concepts#evaluation-model), Bazel creates
a directory tree containing symlinks pointing to the runfiles. This stages the
environment for the binary so it can access the runfiles during runtime.

Runfiles can be added manually during rule creation.
[`runfiles`](/rules/lib/builtins/runfiles) objects can be created by the `runfiles` method
on the rule context, [`ctx.runfiles`](/rules/lib/builtins/ctx#runfiles) and passed to the
`runfiles` parameter on `DefaultInfo`. The executable output of
[executable rules](#executable-rules) is implicitly added to the runfiles.

Some rules specify attributes, generally named
[`data`](/reference/be/common-definitions#common.data), whose outputs are added to
a targets' runfiles. Runfiles should also be merged in from `data`, as well as
from any attributes which might provide code for eventual execution, generally
`srcs` (which might contain `filegroup` targets with associated `data`) and
`deps`.

```python
def _example_library_impl(ctx):
    ...
    runfiles = ctx.runfiles(files = ctx.files.data)
    transitive_runfiles = []
    for runfiles_attr in (
        ctx.attr.srcs,
        ctx.attr.hdrs,
        ctx.attr.deps,
        ctx.attr.data,
    ):
        for target in runfiles_attr:
            transitive_runfiles.append(target[DefaultInfo].default_runfiles)
    runfiles = runfiles.merge_all(transitive_runfiles)
    return [
        DefaultInfo(..., runfiles = runfiles),
        ...
    ]
```

#### Custom providers

Providers can be defined using the [`provider`](/rules/lib/globals/bzl#provider)
function to convey rule-specific information:

```python
ExampleInfo = provider(
    "Info needed to compile/link Example code.",
    fields = {
        "headers": "depset of header Files from transitive dependencies.",
        "files_to_link": "depset of Files from compilation.",
    },
)
```

Rule implementation functions can then construct and return provider instances:

```python
def _example_library_impl(ctx):
  ...
  return [
      ...
      ExampleInfo(
          headers = headers,
          files_to_link = depset(
              [output_file],
              transitive = [
                  dep[ExampleInfo].files_to_link for dep in ctx.attr.deps
              ],
          ),
      )
  ]
```

##### Custom initialization of providers

It's possible to guard the instantiation of a provider with custom
preprocessing and validation logic. This can be used to ensure that all
provider instances satisfy certain invariants, or to give users a cleaner API for
obtaining an instance.

This is done by passing an `init` callback to the
[`provider`](/rules/lib/globals/bzl.html#provider) function. If this callback is given, the
return type of `provider()` changes to be a tuple of two values: the provider
symbol that is the ordinary return value when `init` is not used, and a "raw
constructor".

In this case, when the provider symbol is called, instead of directly returning
a new instance, it will forward the arguments along to the `init` callback. The
callback's return value must be a dict mapping field names (strings) to values;
this is used to initialize the fields of the new instance. Note that the
callback may have any signature, and if the arguments don't match the signature
an error is reported as if the callback were invoked directly.

The raw constructor, by contrast, will bypass the `init` callback.

The following example uses `init` to preprocess and validate its arguments:

```python
# //pkg:exampleinfo.bzl

_core_headers = [...]  # private constant representing standard library files

# Keyword-only arguments are preferred.
def _exampleinfo_init(*, files_to_link, headers = None, allow_empty_files_to_link = False):
    if not files_to_link and not allow_empty_files_to_link:
        fail("files_to_link may not be empty")
    all_headers = depset(_core_headers, transitive = headers)
    return {"files_to_link": files_to_link, "headers": all_headers}

ExampleInfo, _new_exampleinfo = provider(
    fields = ["files_to_link", "headers"],
    init = _exampleinfo_init,
)
```

A rule implementation may then instantiate the provider as follows:

```python
ExampleInfo(
    files_to_link = my_files_to_link,  # may not be empty
    headers = my_headers,  # will automatically include the core headers
)
```

The raw constructor can be used to define alternative public factory functions
that don't go through the `init` logic. For example, exampleinfo.bzl
could define:

```python
def make_barebones_exampleinfo(headers):
    """Returns an ExampleInfo with no files_to_link and only the specified headers."""
    return _new_exampleinfo(files_to_link = depset(), headers = all_headers)
```

Typically, the raw constructor is bound to a variable whose name begins with an
underscore (`_new_exampleinfo` above), so that user code can't load it and
generate arbitrary provider instances.

Another use for `init` is to prevent the user from calling the provider
symbol altogether, and force them to use a factory function instead:

```python
def _exampleinfo_init_banned(*args, **kwargs):
    fail("Do not call ExampleInfo(). Use make_exampleinfo() instead.")

ExampleInfo, _new_exampleinfo = provider(
    ...
    init = _exampleinfo_init_banned)

def make_exampleinfo(...):
    ...
    return _new_exampleinfo(...)
```

<a name="executable-rules"></a>

## Executable rules and test rules

Executable rules define targets that can be invoked by a `bazel run` command.
Test rules are a special kind of executable rule whose targets can also be
invoked by a `bazel test` command. Executable and test rules are created by
setting the respective [`executable`](/rules/lib/globals/bzl#rule.executable) or
[`test`](/rules/lib/globals/bzl#rule.test) argument to `True` in the call to `rule`:

```python
example_binary = rule(
   implementation = _example_binary_impl,
   executable = True,
   ...
)

example_test = rule(
   implementation = _example_binary_impl,
   test = True,
   ...
)
```

Test rules must have names that end in `_test`. (Test *target* names also often
end in `_test` by convention, but this is not required.) Non-test rules must not
have this suffix.

Both kinds of rules must produce an executable output file (which may or may not
be predeclared) that will be invoked by the `run` or `test` commands. To tell
Bazel which of a rule's outputs to use as this executable, pass it as the
`executable` argument of a returned [`DefaultInfo`](/rules/lib/providers/DefaultInfo)
provider. That `executable` is added to the default outputs of the rule (so you
don't need to pass that to both `executable` and `files`). It's also implicitly
added to the [runfiles](#runfiles):

```python
def _example_binary_impl(ctx):
    executable = ctx.actions.declare_file(ctx.label.name)
    ...
    return [
        DefaultInfo(executable = executable, ...),
        ...
    ]
```

The action that generates this file must set the executable bit on the file. For
a [`ctx.actions.run`](/rules/lib/builtins/actions#run) or
[`ctx.actions.run_shell`](/rules/lib/builtins/actions#run_shell) action this should be done
by the underlying tool that is invoked by the action. For a
[`ctx.actions.write`](/rules/lib/builtins/actions#write) action, pass `is_executable = True`.

As [legacy behavior](#deprecated_predeclared_outputs), executable rules have a
special `ctx.outputs.executable` predeclared output. This file serves as the
default executable if you don't specify one using `DefaultInfo`; it must not be
used otherwise. This output mechanism is deprecated because it doesn't support
customizing the executable file's name at analysis time.

See examples of an
[executable rule](https://github.com/bazelbuild/examples/blob/main/rules/executable/fortune.bzl)
and a
[test rule](https://github.com/bazelbuild/examples/blob/main/rules/test_rule/line_length.bzl).

[Executable rules](/reference/be/common-definitions#common-attributes-binaries) and
[test rules](/reference/be/common-definitions#common-attributes-tests) have additional
attributes implicitly defined, in addition to those added for
[all rules](/reference/be/common-definitions#common-attributes). The defaults of
implicitly-added attributes can't be changed, though this can be worked around
by wrapping a private rule in a [Starlark macro](/extending/macros) which alters the
default:

```python
def example_test(size = "small", **kwargs):
  _example_test(size = size, **kwargs)

_example_test = rule(
 ...
)
```

### Runfiles location

When an executable target is run with `bazel run` (or `test`), the root of the
runfiles directory is adjacent to the executable. The paths relate as follows:

```python
# Given launcher_path and runfile_file:
runfiles_root = launcher_path.path + ".runfiles"
workspace_name = ctx.workspace_name
runfile_path = runfile_file.short_path
execution_root_relative_path = "%s/%s/%s" % (
    runfiles_root, workspace_name, runfile_path)
```

The path to a `File` under the runfiles directory corresponds to
[`File.short_path`](/rules/lib/builtins/File#short_path).

The binary executed directly by `bazel` is adjacent to the root of the
`runfiles` directory. However, binaries called *from* the runfiles can't make
the same assumption. To mitigate this, each binary should provide a way to
accept its runfiles root as a parameter using an environment, or command line
argument or flag. This allows binaries to pass the correct canonical runfiles root
to the binaries it calls. If that's not set, a binary can guess that it was the
first binary called and look for an adjacent runfiles directory.

## Advanced topics

### Requesting output files

A single target can have several output files. When a `bazel build` command is
run, some of the outputs of the targets given to the command are considered to
be *requested*. Bazel only builds these requested files and the files that they
directly or indirectly depend on. (In terms of the action graph, Bazel only
executes the actions that are reachable as transitive dependencies of the
requested files.)

In addition to [default outputs](#default_outputs), any *predeclared output* can
be explicitly requested on the command line. Rules can specify predeclared
outputs using [output attributes](#output_attributes). In that case, the user
explicitly chooses labels for outputs when they instantiate the rule. To obtain
[`File`](/rules/lib/builtins/File) objects for output attributes, use the corresponding
attribute of [`ctx.outputs`](/rules/lib/builtins/ctx#outputs). Rules can
[implicitly define predeclared outputs](#deprecated_predeclared_outputs) based
on the target name as well, but this feature is deprecated.

In addition to default outputs, there are *output groups*, which are collections
of output files that may be requested together. These can be requested with
[`--output_groups`](/reference/command-line-reference#flag--output_groups). For
example, if a target `//pkg:mytarget` is of a rule type that has a `debug_files`
output group, these files can be built by running `bazel build //pkg:mytarget
--output_groups=debug_files`. Since non-predeclared outputs don't have labels,
they can only be requested by appearing in the default outputs or an output
group.

Output groups can be specified with the
[`OutputGroupInfo`](/rules/lib/providers/OutputGroupInfo) provider. Note that unlike many
built-in providers, `OutputGroupInfo` can take parameters with arbitrary names
to define output groups with that name:

```python
def _example_library_impl(ctx):
    ...
    debug_file = ctx.actions.declare_file(name + ".pdb")
    ...
    return [
        DefaultInfo(files = depset([output_file]), ...),
        OutputGroupInfo(
            debug_files = depset([debug_file]),
            all_files = depset([output_file, debug_file]),
        ),
        ...
    ]
```

Also unlike most providers, `OutputGroupInfo` can be returned by both an
[aspect](/extending/aspects) and the rule target to which that aspect is applied, as
long as they don't define the same output groups. In that case, the resulting
providers are merged.

Note that `OutputGroupInfo` generally shouldn't be used to convey specific sorts
of files from a target to the actions of its consumers. Define
[rule-specific providers](#custom_providers) for that instead.

### Configurations

Imagine that you want to build a C++ binary for a different architecture. The
build can be complex and involve multiple steps. Some of the intermediate
binaries, like compilers and code generators, have to run on
[the execution platform](/extending/platforms#overview) (which could be your host,
or a remote executor). Some binaries like the final output must be built for the
target architecture.

For this reason, Bazel has a concept of "configurations" and transitions. The
topmost targets (the ones requested on the command line) are built-in the
"target" configuration, while tools that should run on the execution platform
are built-in an "exec" configuration. Rules may generate different actions based
on the configuration, for instance to change the cpu architecture that is passed
to the compiler. In some cases, the same library may be needed for different
configurations. If this happens, it will be analyzed and potentially built
multiple times.

By default, Bazel builds a target's dependencies in the same configuration as
the target itself, in other words without transitions. When a dependency is a
tool that's needed to help build the target, the corresponding attribute should
specify a transition to an exec configuration. This causes the tool and all its
dependencies to build for the execution platform.

For each dependency attribute, you can use `cfg` to decide if dependencies
should build in the same configuration or transition to an exec configuration.
If a dependency attribute has the flag `executable = True`, `cfg` must be set
explicitly. This is to guard against accidentally building a tool for the wrong
configuration.
[See example](https://github.com/bazelbuild/examples/blob/main/rules/actions_run/execute.bzl)

In general, sources, dependent libraries, and executables that will be needed at
runtime can use the same configuration.

Tools that are executed as part of the build (such as compilers or code generators)
should be built for an exec configuration. In this case, specify `cfg = "exec"` in
the attribute.

Otherwise, executables that are used at runtime (such as as part of a test) should
be built for the target configuration. In this case, specify `cfg = "target"` in
the attribute.

`cfg = "target"` doesn't actually do anything: it's purely a convenience value to
help rule designers be explicit about their intentions. When `executable = False`,
which means `cfg` is optional, only set this when it truly helps readability.

You can also use `cfg = my_transition` to use
[user-defined transitions](/extending/config#user-defined-transitions), which allow
rule authors a great deal of flexibility in changing configurations, with the
drawback of
[making the build graph larger and less comprehensible](/extending/config#memory-and-performance-considerations).

**Note**: Historically, Bazel didn't have the concept of execution platforms,
and instead all build actions were considered to run on the host machine. Bazel
versions before 6.0 created a distinct "host" configuration to represent this.
If you see references to "host" in code or old documentation, that's what this
refers to. We recommend using Bazel 6.0 or newer to avoid this extra conceptual
overhead.

<a name="fragments"></a>

### Configuration fragments

Rules may access
[configuration fragments](/rules/lib/fragments) such as
`cpp` and `java`. However, all required fragments must be declared in
order to avoid access errors:

```python
def _impl(ctx):
    # Using ctx.fragments.cpp leads to an error since it was not declared.
    x = ctx.fragments.java
    ...

my_rule = rule(
    implementation = _impl,
    fragments = ["java"],      # Required fragments of the target configuration
    ...
)
```

### Runfiles symlinks

Normally, the relative path of a file in the runfiles tree is the same as the
relative path of that file in the source tree or generated output tree. If these
need to be different for some reason, you can specify the `root_symlinks` or
`symlinks` arguments. The `root_symlinks` is a dictionary mapping paths to
files, where the paths are relative to the root of the runfiles directory. The
`symlinks` dictionary is the same, but paths are implicitly prefixed with the
name of the main workspace (*not* the name of the repository containing the
current target).

```python
    ...
    runfiles = ctx.runfiles(
        root_symlinks = {"some/path/here.foo": ctx.file.some_data_file2}
        symlinks = {"some/path/here.bar": ctx.file.some_data_file3}
    )
    # Creates something like:
    # sometarget.runfiles/
    #     some/
    #         path/
    #             here.foo -> some_data_file2
    #     <workspace_name>/
    #         some/
    #             path/
    #                 here.bar -> some_data_file3
```

If `symlinks` or `root_symlinks` is used, be careful not to map two different
files to the same path in the runfiles tree. This will cause the build to fail
with an error describing the conflict. To fix, you will need to modify your
`ctx.runfiles` arguments to remove the collision. This checking will be done for
any targets using your rule, as well as targets of any kind that depend on those
targets. This is especially risky if your tool is likely to be used transitively
by another tool; symlink names must be unique across the runfiles of a tool and
all of its dependencies.

### Code coverage

When the [`coverage`](/reference/command-line-reference#coverage) command is run,
the build may need to add coverage instrumentation for certain targets. The
build also gathers the list of source files that are instrumented. The subset of
targets that are considered is controlled by the flag
[`--instrumentation_filter`](/reference/command-line-reference#flag--instrumentation_filter).
Test targets are excluded, unless
[`--instrument_test_targets`](/reference/command-line-reference#flag--instrument_test_targets)
is specified.

If a rule implementation adds coverage instrumentation at build time, it needs
to account for that in its implementation function.
[ctx.coverage_instrumented](/rules/lib/builtins/ctx#coverage_instrumented) returns
`True` in coverage mode if a target's sources should be instrumented:

```python
# Are this rule's sources instrumented?
if ctx.coverage_instrumented():
  # Do something to turn on coverage for this compile action
```

Logic that always needs to be on in coverage mode (whether a target's sources
specifically are instrumented or not) can be conditioned on
[ctx.configuration.coverage_enabled](/rules/lib/builtins/configuration#coverage_enabled).

If the rule directly includes sources from its dependencies before compilation
(such as header files), it may also need to turn on compile-time instrumentation if
the dependencies' sources should be instrumented:

```python
# Are this rule's sources or any of the sources for its direct dependencies
# in deps instrumented?
if ctx.coverage_instrumented() or any([ctx.coverage_instrumented(dep) for dep in ctx.attr.deps]):
    # Do something to turn on coverage for this compile action
```

Rules also should provide information about which attributes are relevant for
coverage with the `InstrumentedFilesInfo` provider, constructed using
[`coverage_common.instrumented_files_info`](/rules/lib/toplevel/coverage_common#instrumented_files_info).
The `dependency_attributes` parameter of `instrumented_files_info` should list
all runtime dependency attributes, including code dependencies like `deps` and
data dependencies like `data`. The `source_attributes` parameter should list the
rule's source files attributes if coverage instrumentation might be added:

```python
def _example_library_impl(ctx):
    ...
    return [
        ...
        coverage_common.instrumented_files_info(
            ctx,
            dependency_attributes = ["deps", "data"],
            # Omitted if coverage is not supported for this rule:
            source_attributes = ["srcs", "hdrs"],
        )
        ...
    ]
```

If `InstrumentedFilesInfo` is not returned, a default one is created with each
non-tool [dependency attribute](#dependency_attributes) that doesn't set
[`cfg`](#configuration) to `"exec"` in the attribute schema. in
`dependency_attributes`. (This isn't ideal behavior, since it puts attributes
like `srcs` in `dependency_attributes` instead of `source_attributes`, but it
avoids the need for explicit coverage configuration for all rules in the
dependency chain.)

#### Test rules

Test rules require additional setup to generate coverage reports. The rule
itself has to add the following implicit attributes:

```python
my_test = rule(
    ...,
    attrs = {
        ...,
        # Implicit dependencies used by Bazel to generate coverage reports.
        "_lcov_merger": attr.label(
            default = configuration_field(fragment = "coverage", name = "output_generator"),
            executable = True,
            cfg = config.exec(exec_group = "test"),
        ),
        "_collect_cc_coverage": attr.label(
            default = "@bazel_tools//tools/test:collect_cc_coverage",
            executable = True,
            cfg = config.exec(exec_group = "test"),
        )
    },
    test = True,
)
```

By using `configuration_field`, the dependency on the Java LCOV merger tool can
be avoided as long as coverage is not requested.

When the test is run, it should emit coverage information in the form of one or
more [LCOV files]
(https://manpages.debian.org/unstable/lcov/geninfo.1.en.html#TRACEFILE_FORMAT)
with unique names into the directory specified by the `COVERAGE_DIR` environment
variable. Bazel will then merge these files into a single LCOV file using the
`_lcov_merger` tool. If present, it will also collect C/C++ coverage using the
`_collect_cc_coverage` tool.

### Baseline coverage

Since coverage is only collected for code that ends up in the dependency tree of
a test, coverage reports can be misleading as they don't necessarily cover all
the code matched by the `--instrumentation_filter` flag.

For this reason, Bazel allows rules to specify baseline coverage files using the
`baseline_coverage_files` attribute of `ctx.instrumented_files_info`). These
files must be generated in LCOV format by a user-defined action and are supposed
to list all lines, branches, functions and/or blocks in the target's source
files (according to the `sources_attributes` and `extensions` parameters). For
source files in targets that are instrumented for coverage, Bazel merges their
baseline coverage into the combined coverage report generated with
`--combined_report` and thus ensures that untested files still show up as
uncovered.

If a rule doesn't provide any baseline coverage files, Bazel generates synthetic
coverage information that only mentions the source file paths, but doesn't
contain any information about their contents.

### Validation Actions

Sometimes you need to validate something about the build, and the
information required to do that validation is available only in artifacts
(source files or generated files). Because this information is in artifacts,
rules can't do this validation at analysis time because rules can't read
files. Instead, actions must do this validation at execution time. When
validation fails, the action will fail, and hence so will the build.

Examples of validations that might be run are static analysis, linting,
dependency and consistency checks, and style checks.

Validation actions can also help to improve build performance by moving parts
of actions that are not required for building artifacts into separate actions.
For example, if a single action that does compilation and linting can be
separated into a compilation action and a linting action, then the linting
action can be run as a validation action and run in parallel with other actions.

These "validation actions" often don't produce anything that is used elsewhere
in the build, since they only need to assert things about their inputs. This
presents a problem though: If a validation action doesn't produce anything that
is used elsewhere in the build, how does a rule get the action to run?
Historically, the approach was to have the validation action output an empty
file, and artificially add that output to the inputs of some other important
action in the build:

<img src="/rules/validation_action_historical.svg" width="35%" />

This works, because Bazel will always run the validation action when the compile
action is run, but this has significant drawbacks:

1. The validation action is in the critical path of the build. Because Bazel
thinks the empty output is required to run the compile action, it will run the
validation action first, even though the compile action will ignore the input.
This reduces parallelism and slows down builds.

2. If other actions in the build might run instead of the
compile action, then the empty outputs of validation actions need to be added to
those actions as well (`java_library`'s source jar output, for example). This is
also a problem if new actions that might run instead of the compile action are
added later, and the empty validation output is accidentally left off.

The solution to these problems is to use the Validations Output Group.

#### Validations Output Group

The Validations Output Group is an output group designed to hold the otherwise
unused outputs of validation actions, so that they don't need to be artificially
added to the inputs of other actions.

This group is special in that its outputs are always requested, regardless of
the value of the `--output_groups` flag, and regardless of how the target is
depended upon (for example, on the command line, as a dependency, or through
implicit outputs of the target). Note that normal caching and incrementality
still apply: if the inputs to the validation action have not changed and the
validation action previously succeeded, then the validation action won't be
run.

<img src="/rules/validation_action.svg" width="35%" />

Using this output group still requires that validation actions output some file,
even an empty one. This might require wrapping some tools that normally don't
create outputs so that a file is created.

A target's validation actions are not run in three cases:

*    When the target is depended upon as a tool
*    When the target is depended upon as an implicit dependency (for example, an
     attribute that starts with "_")
*    When the target is built in the exec configuration.

It is assumed that these targets have their own
separate builds and tests that would uncover any validation failures.

#### Using the Validations Output Group

The Validations Output Group is named `_validation` and is used like any other
output group:

```python
def _rule_with_validation_impl(ctx):

  ctx.actions.write(ctx.outputs.main, "main output\n")
  ctx.actions.write(ctx.outputs.implicit, "implicit output\n")

  validation_output = ctx.actions.declare_file(ctx.attr.name + ".validation")
  ctx.actions.run(
    outputs = [validation_output],
    executable = ctx.executable._validation_tool,
    arguments = [validation_output.path],
  )

  return [
    DefaultInfo(files = depset([ctx.outputs.main])),
    OutputGroupInfo(_validation = depset([validation_output])),
  ]


rule_with_validation = rule(
  implementation = _rule_with_validation_impl,
  outputs = {
    "main": "%{name}.main",
    "implicit": "%{name}.implicit",
  },
  attrs = {
    "_validation_tool": attr.label(
        default = Label("//validation_actions:validation_tool"),
        executable = True,
        cfg = "exec"
    ),
  }
)
```

Notice that the validation output file is not added to the `DefaultInfo` or the
inputs to any other action. The validation action for a target of this rule kind
will still run if the target is depended upon by label, or any of the target's
implicit outputs are directly or indirectly depended upon.

It is usually important that the outputs of validation actions only go into the
validation output group, and are not added to the inputs of other actions, as
this could defeat parallelism gains. Note however that Bazel doesn't
have any special checking to enforce this. Therefore, you should test
that validation action outputs are not added to the inputs of any actions in the
tests for Starlark rules. For example:

```python
load("@bazel_skylib//lib:unittest.bzl", "analysistest")

def _validation_outputs_test_impl(ctx):
  env = analysistest.begin(ctx)

  actions = analysistest.target_actions(env)
  target = analysistest.target_under_test(env)
  validation_outputs = target.output_groups._validation.to_list()
  for action in actions:
    for validation_output in validation_outputs:
      if validation_output in action.inputs.to_list():
        analysistest.fail(env,
            "%s is a validation action output, but is an input to action %s" % (
                validation_output, action))

  return analysistest.end(env)

validation_outputs_test = analysistest.make(_validation_outputs_test_impl)
```

#### Validation Actions Flag

Running validation actions is controlled by the `--run_validations` command line
flag, which defaults to true.

## Deprecated features

### Deprecated predeclared outputs

There are two **deprecated** ways of using predeclared outputs:

*   The [`outputs`](/rules/lib/globals/bzl#rule.outputs) parameter of `rule` specifies
    a mapping between output attribute names and string templates for generating
    predeclared output labels. Prefer using non-predeclared outputs and
    explicitly adding outputs to `DefaultInfo.files`. Use the rule target's
    label as input for rules which consume the output instead of a predeclared
    output's label.

*   For [executable rules](#executable-rules), `ctx.outputs.executable` refers
    to a predeclared executable output with the same name as the rule target.
    Prefer declaring the output explicitly, for example with
    `ctx.actions.declare_file(ctx.label.name)`, and ensure that the command that
    generates the executable sets its permissions to allow execution. Explicitly
    pass the executable output to the `executable` parameter of `DefaultInfo`.

### Runfiles features to avoid

[`ctx.runfiles`](/rules/lib/builtins/ctx#runfiles) and the [`runfiles`](/rules/lib/builtins/runfiles)
type have a complex set of features, many of which are kept for legacy reasons.
The following recommendations help reduce complexity:

*   **Avoid** use of the `collect_data` and `collect_default` modes of
    [`ctx.runfiles`](/rules/lib/builtins/ctx#runfiles). These modes implicitly collect
    runfiles across certain hardcoded dependency edges in confusing ways.
    Instead, add files using the `files` or `transitive_files` parameters of
    `ctx.runfiles`, or by merging in runfiles from dependencies with
    `runfiles = runfiles.merge(dep[DefaultInfo].default_runfiles)`.

*   **Avoid** use of the `data_runfiles` and `default_runfiles` of the
    `DefaultInfo` constructor. Specify `DefaultInfo(runfiles = ...)` instead.
    The distinction between "default" and "data" runfiles is maintained for
    legacy reasons. For example, some rules put their default outputs in
    `data_runfiles`, but not `default_runfiles`. Instead of using
    `data_runfiles`, rules should *both* include default outputs and merge in
    `default_runfiles` from attributes which provide runfiles (often
    [`data`](/reference/be/common-definitions#common-attributes.data)).

*   When retrieving `runfiles` from `DefaultInfo` (generally only for merging
    runfiles between the current rule and its dependencies), use
    `DefaultInfo.default_runfiles`, **not** `DefaultInfo.data_runfiles`.

### Migrating from legacy providers

Historically, Bazel providers were simple fields on the `Target` object. They
were accessed using the dot operator, and they were created by putting the field
in a [`struct`](/rules/lib/builtins/struct) returned by the rule's
implementation function instead of a list of provider objects:

```python
return struct(example_info = struct(headers = depset(...)))
```

Such providers can be retrieved from the corresponding field of the `Target` object:

```python
transitive_headers = [hdr.example_info.headers for hdr in ctx.attr.hdrs]
```

*This style is deprecated and should not be used in new code;* see following for
information that may help you migrate. The new provider mechanism avoids name
clashes. It also supports data hiding, by requiring any code accessing a
provider instance to retrieve it using the provider symbol.

For the moment, legacy providers are still supported. A rule can return both
legacy and modern providers as follows:

```python
def _old_rule_impl(ctx):
  ...
  legacy_data = struct(x = "foo", ...)
  modern_data = MyInfo(y = "bar", ...)
  # When any legacy providers are returned, the top-level returned value is a
  # struct.
  return struct(
      # One key = value entry for each legacy provider.
      legacy_info = legacy_data,
      ...
      # Additional modern providers:
      providers = [modern_data, ...])
```

If `dep` is the resulting `Target` object for an instance of this rule, the
providers and their contents can be retrieved as `dep.legacy_info.x` and
`dep[MyInfo].y`.

In addition to `providers`, the returned struct can also take several other
fields that have special meaning (and thus don't create a corresponding legacy
provider):

*   The fields `files`, `runfiles`, `data_runfiles`, `default_runfiles`, and
    `executable` correspond to the same-named fields of
    [`DefaultInfo`](/rules/lib/providers/DefaultInfo). It is not allowed to specify any of
    these fields while also returning a `DefaultInfo` provider.

*   The field `output_groups` takes a struct value and corresponds to an
    [`OutputGroupInfo`](/rules/lib/providers/OutputGroupInfo).

In [`provides`](/rules/lib/globals/bzl#rule.provides) declarations of rules, and in
[`providers`](/rules/lib/toplevel/attr#label_list.providers) declarations of dependency
attributes, legacy providers are passed in as strings and modern providers are
passed in by their `Info` symbol. Be sure to change from strings to symbols
when migrating. For complex or large rule sets where it is difficult to update
all rules atomically, you may have an easier time if you follow this sequence of
steps:

1.  Modify the rules that produce the legacy provider to produce both the legacy
    and modern providers, using the preceding syntax. For rules that declare they
    return the legacy provider, update that declaration to include both the
    legacy and modern providers.

2.  Modify the rules that consume the legacy provider to instead consume the
    modern provider. If any attribute declarations require the legacy provider,
    also update them to instead require the modern provider. Optionally, you can
    interleave this work with step 1 by having consumers accept or require either
    provider: Test for the presence of the legacy provider using
    `hasattr(target, 'foo')`, or the new provider using `FooInfo in target`.

3.  Fully remove the legacy provider from all rules.

---

## Toolchains
- URL: https://bazel.build/extending/toolchains
- Source: extending/toolchains.mdx
- Slug: /extending/toolchains

This page describes the toolchain framework, which is a way for rule authors to
decouple their rule logic from platform-based selection of tools. It is
recommended to read the [rules](/extending/rules) and [platforms](/extending/platforms)
pages before continuing. This page covers why toolchains are needed, how to
define and use them, and how Bazel selects an appropriate toolchain based on
platform constraints.

## Motivation

Let's first look at the problem toolchains are designed to solve. Suppose you
are writing rules to support the "bar" programming language. Your `bar_binary`
rule would compile `*.bar` files using the `barc` compiler, a tool that itself
is built as another target in your workspace. Since users who write `bar_binary`
targets shouldn't have to specify a dependency on the compiler, you make it an
implicit dependency by adding it to the rule definition as a private attribute.

```python
bar_binary = rule(
    implementation = _bar_binary_impl,
    attrs = {
        "srcs": attr.label_list(allow_files = True),
        ...
        "_compiler": attr.label(
            default = "//bar_tools:barc_linux",  # the compiler running on linux
            providers = [BarcInfo],
        ),
    },
)
```

`//bar_tools:barc_linux` is now a dependency of every `bar_binary` target, so
it'll be built before any `bar_binary` target. It can be accessed by the rule's
implementation function just like any other attribute:

```python
BarcInfo = provider(
    doc = "Information about how to invoke the barc compiler.",
    # In the real world, compiler_path and system_lib might hold File objects,
    # but for simplicity they are strings for this example. arch_flags is a list
    # of strings.
    fields = ["compiler_path", "system_lib", "arch_flags"],
)

def _bar_binary_impl(ctx):
    ...
    info = ctx.attr._compiler[BarcInfo]
    command = "%s -l %s %s" % (
        info.compiler_path,
        info.system_lib,
        " ".join(info.arch_flags),
    )
    ...
```

The issue here is that the compiler's label is hardcoded into `bar_binary`, yet
different targets may need different compilers depending on what platform they
are being built for and what platform they are being built on -- called the
*target platform* and *execution platform*, respectively. Furthermore, the rule
author does not necessarily even know all the available tools and platforms, so
it is not feasible to hardcode them in the rule's definition.

A less-than-ideal solution would be to shift the burden onto users, by making
the `_compiler` attribute non-private. Then individual targets could be
hardcoded to build for one platform or another.

```python
bar_binary(
    name = "myprog_on_linux",
    srcs = ["mysrc.bar"],
    compiler = "//bar_tools:barc_linux",
)

bar_binary(
    name = "myprog_on_windows",
    srcs = ["mysrc.bar"],
    compiler = "//bar_tools:barc_windows",
)
```

You can improve on this solution by using `select` to choose the `compiler`
[based on the platform](/docs/configurable-attributes):

```python
config_setting(
    name = "on_linux",
    constraint_values = [
        "@platforms//os:linux",
    ],
)

config_setting(
    name = "on_windows",
    constraint_values = [
        "@platforms//os:windows",
    ],
)

bar_binary(
    name = "myprog",
    srcs = ["mysrc.bar"],
    compiler = select({
        ":on_linux": "//bar_tools:barc_linux",
        ":on_windows": "//bar_tools:barc_windows",
    }),
)
```

But this is tedious and a bit much to ask of every single `bar_binary` user.
If this style is not used consistently throughout the workspace, it leads to
builds that work fine on a single platform but fail when extended to
multi-platform scenarios. It also does not address the problem of adding support
for new platforms and compilers without modifying existing rules or targets.

The toolchain framework solves this problem by adding an extra level of
indirection. Essentially, you declare that your rule has an abstract dependency
on *some* member of a family of targets (a toolchain type), and Bazel
automatically resolves this to a particular target (a toolchain) based on the
applicable platform constraints. Neither the rule author nor the target author
need know the complete set of available platforms and toolchains.

## Writing rules that use toolchains

Under the toolchain framework, instead of having rules depend directly on tools,
they instead depend on *toolchain types*. A toolchain type is a simple target
that represents a class of tools that serve the same role for different
platforms. For instance, you can declare a type that represents the bar
compiler:

```python
# By convention, toolchain_type targets are named "toolchain_type" and
# distinguished by their package path. So the full path for this would be
# //bar_tools:toolchain_type.
toolchain_type(name = "toolchain_type")
```

The rule definition in the previous section is modified so that instead of
taking in the compiler as an attribute, it declares that it consumes a
`//bar_tools:toolchain_type` toolchain.

```python
bar_binary = rule(
    implementation = _bar_binary_impl,
    attrs = {
        "srcs": attr.label_list(allow_files = True),
        ...
        # No `_compiler` attribute anymore.
    },
    toolchains = ["//bar_tools:toolchain_type"],
)
```

The implementation function now accesses this dependency under `ctx.toolchains`
instead of `ctx.attr`, using the toolchain type as the key.

```python
def _bar_binary_impl(ctx):
    ...
    info = ctx.toolchains["//bar_tools:toolchain_type"].barcinfo
    # The rest is unchanged.
    command = "%s -l %s %s" % (
        info.compiler_path,
        info.system_lib,
        " ".join(info.arch_flags),
    )
    ...
```

`ctx.toolchains["//bar_tools:toolchain_type"]` returns the
[`ToolchainInfo` provider](/rules/lib/toplevel/platform_common#ToolchainInfo)
of whatever target Bazel resolved the toolchain dependency to. The fields of the
`ToolchainInfo` object are set by the underlying tool's rule; in the next
section, this rule is defined such that there is a `barcinfo` field that wraps
a `BarcInfo` object.

Bazel's procedure for resolving toolchains to targets is described
[below](#toolchain-resolution). Only the resolved toolchain target is actually
made a dependency of the `bar_binary` target, not the whole space of candidate
toolchains.

### Mandatory and Optional Toolchains

By default, when a rule expresses a toolchain type dependency using a bare label
(as shown above), the toolchain type is considered to be **mandatory**. If Bazel
is unable to find a matching toolchain (see
[Toolchain resolution](#toolchain-resolution) below) for a mandatory toolchain
type, this is an error and analysis halts.

It is possible instead to declare an **optional** toolchain type dependency, as
follows:

```python
bar_binary = rule(
    ...
    toolchains = [
        config_common.toolchain_type("//bar_tools:toolchain_type", mandatory = False),
    ],
)
```

When an optional toolchain type cannot be resolved, analysis continues, and the
result of `ctx.toolchains["//bar_tools:toolchain_type"]` is `None`.

The [`config_common.toolchain_type`](/rules/lib/toplevel/config_common#toolchain_type)
function defaults to mandatory.

The following forms can be used:

-  Mandatory toolchain types:
   -  `toolchains = ["//bar_tools:toolchain_type"]`
   -  `toolchains = [config_common.toolchain_type("//bar_tools:toolchain_type")]`
   -  `toolchains = [config_common.toolchain_type("//bar_tools:toolchain_type", mandatory = True)]`
- Optional toolchain types:
   -  `toolchains = [config_common.toolchain_type("//bar_tools:toolchain_type", mandatory = False)]`

```python
bar_binary = rule(
    ...
    toolchains = [
        "//foo_tools:toolchain_type",
        config_common.toolchain_type("//bar_tools:toolchain_type", mandatory = False),
    ],
)
```

You can mix and match forms in the same rule, also. However, if the same
toolchain type is listed multiple times, it will take the most strict version,
where mandatory is more strict than optional.

### Writing aspects that use toolchains

Aspects have access to the same toolchain API as rules: you can define required
toolchain types, access toolchains via the context, and use them to generate new
actions using the toolchain.

```py
bar_aspect = aspect(
    implementation = _bar_aspect_impl,
    attrs = {},
    toolchains = ['//bar_tools:toolchain_type'],
)

def _bar_aspect_impl(target, ctx):
  toolchain = ctx.toolchains['//bar_tools:toolchain_type']
  # Use the toolchain provider like in a rule.
  return []
```

## Defining toolchains

To define some toolchains for a given toolchain type, you need three things:

1. A language-specific rule representing the kind of tool or tool suite. By
   convention this rule's name is suffixed with "\_toolchain".

    1.  **Note:** The `\_toolchain` rule cannot create any build actions.
        Rather, it collects artifacts from other rules and forwards them to the
        rule that uses the toolchain. That rule is responsible for creating all
        build actions.

2. Several targets of this rule type, representing versions of the tool or tool
   suite for different platforms.

3. For each such target, an associated target of the generic
  [`toolchain`](/reference/be/platforms-and-toolchains#toolchain)
   rule, to provide metadata used by the toolchain framework. This `toolchain`
   target also refers to the `toolchain_type` associated with this toolchain.
   This means that a given `_toolchain` rule could be associated with any
   `toolchain_type`, and that only in a `toolchain` instance that uses
   this `_toolchain` rule that the rule is associated with a `toolchain_type`.

For our running example, here's a definition for a `bar_toolchain` rule. Our
example has only a compiler, but other tools such as a linker could also be
grouped underneath it.

```python
def _bar_toolchain_impl(ctx):
    toolchain_info = platform_common.ToolchainInfo(
        barcinfo = BarcInfo(
            compiler_path = ctx.attr.compiler_path,
            system_lib = ctx.attr.system_lib,
            arch_flags = ctx.attr.arch_flags,
        ),
    )
    return [toolchain_info]

bar_toolchain = rule(
    implementation = _bar_toolchain_impl,
    attrs = {
        "compiler_path": attr.string(),
        "system_lib": attr.string(),
        "arch_flags": attr.string_list(),
    },
)
```

The rule must return a `ToolchainInfo` provider, which becomes the object that
the consuming rule retrieves using `ctx.toolchains` and the label of the
toolchain type. `ToolchainInfo`, like `struct`, can hold arbitrary field-value
pairs. The specification of exactly what fields are added to the `ToolchainInfo`
should be clearly documented at the toolchain type. In this example, the values
return wrapped in a `BarcInfo` object to reuse the schema defined above; this
style may be useful for validation and code reuse.

Now you can define targets for specific `barc` compilers.

```python
bar_toolchain(
    name = "barc_linux",
    arch_flags = [
        "--arch=Linux",
        "--debug_everything",
    ],
    compiler_path = "/path/to/barc/on/linux",
    system_lib = "/usr/lib/libbarc.so",
)

bar_toolchain(
    name = "barc_windows",
    arch_flags = [
        "--arch=Windows",
        # Different flags, no debug support on windows.
    ],
    compiler_path = "C:\\path\\on\\windows\\barc.exe",
    system_lib = "C:\\path\\on\\windows\\barclib.dll",
)
```

Finally, you create `toolchain` definitions for the two `bar_toolchain` targets.
These definitions link the language-specific targets to the toolchain type and
provide the constraint information that tells Bazel when the toolchain is
appropriate for a given platform.

```python
toolchain(
    name = "barc_linux_toolchain",
    exec_compatible_with = [
        "@platforms//os:linux",
        "@platforms//cpu:x86_64",
    ],
    target_compatible_with = [
        "@platforms//os:linux",
        "@platforms//cpu:x86_64",
    ],
    toolchain = ":barc_linux",
    toolchain_type = ":toolchain_type",
)

toolchain(
    name = "barc_windows_toolchain",
    exec_compatible_with = [
        "@platforms//os:windows",
        "@platforms//cpu:x86_64",
    ],
    target_compatible_with = [
        "@platforms//os:windows",
        "@platforms//cpu:x86_64",
    ],
    toolchain = ":barc_windows",
    toolchain_type = ":toolchain_type",
)
```

The use of relative path syntax above suggests these definitions are all in the
same package, but there's no reason the toolchain type, language-specific
toolchain targets, and `toolchain` definition targets can't all be in separate
packages.

See the [`go_toolchain`](https://github.com/bazelbuild/rules_go/blob/master/go/private/go_toolchain.bzl)
for a real-world example.

### Toolchains and configurations

An important question for rule authors is, when a `bar_toolchain` target is
analyzed, what [configuration](/reference/glossary#configuration) does it see, and what transitions
should be used for dependencies? The example above uses string attributes, but
what would happen for a more complicated toolchain that depends on other targets
in the Bazel repository?

Let's see a more complex version of `bar_toolchain`:

```python
def _bar_toolchain_impl(ctx):
    # The implementation is mostly the same as above, so skipping.
    pass

bar_toolchain = rule(
    implementation = _bar_toolchain_impl,
    attrs = {
        "compiler": attr.label(
            executable = True,
            mandatory = True,
            cfg = "exec",
        ),
        "system_lib": attr.label(
            mandatory = True,
            cfg = "target",
        ),
        "arch_flags": attr.string_list(),
    },
)
```

The use of [`attr.label`](/rules/lib/toplevel/attr#label) is the same as for a standard rule,
but the meaning of the `cfg` parameter is slightly different.

The dependency from a target (called the "parent") to a toolchain via toolchain
resolution uses a special configuration transition called the "toolchain
transition". The toolchain transition keeps the configuration the same, except
that it forces the execution platform to be the same for the toolchain as for
the parent (otherwise, toolchain resolution for the toolchain could pick any
execution platform, and wouldn't necessarily be the same as for parent). This
allows any `exec` dependencies of the toolchain to also be executable for the
parent's build actions. Any of the toolchain's dependencies which use `cfg =
"target"` (or which don't specify `cfg`, since "target" is the default) are
built for the same target platform as the parent. This allows toolchain rules to
contribute both libraries (the `system_lib` attribute above) and tools (the
`compiler` attribute) to the build rules which need them. The system libraries
are linked into the final artifact, and so need to be built for the same
platform, whereas the compiler is a tool invoked during the build, and needs to
be able to run on the execution platform.

## Registering and building with toolchains

At this point all the building blocks are assembled, and you just need to make
the toolchains available to Bazel's resolution procedure. This is done by
registering the toolchain, either in a `MODULE.bazel` file using
`register_toolchains()`, or by passing the toolchains' labels on the command
line using the `--extra_toolchains` flag.

```python
register_toolchains(
    "//bar_tools:barc_linux_toolchain",
    "//bar_tools:barc_windows_toolchain",
    # Target patterns are also permitted, so you could have also written:
    # "//bar_tools:all",
    # or even
    # "//bar_tools/...",
)
```

When using target patterns to register toolchains, the order in which the
individual toolchains are registered is determined by the following rules:

* The toolchains defined in a subpackage of a package are registered before the
  toolchains defined in the package itself.
* Within a package, toolchains are registered in the lexicographical order of
  their names.

Now when you build a target that depends on a toolchain type, an appropriate
toolchain will be selected based on the target and execution platforms.

```python
# my_pkg/BUILD

platform(
    name = "my_target_platform",
    constraint_values = [
        "@platforms//os:linux",
    ],
)

bar_binary(
    name = "my_bar_binary",
    ...
)
```

```sh
bazel build //my_pkg:my_bar_binary --platforms=//my_pkg:my_target_platform
```

Bazel will see that `//my_pkg:my_bar_binary` is being built with a platform that
has `@platforms//os:linux` and therefore resolve the
`//bar_tools:toolchain_type` reference to `//bar_tools:barc_linux_toolchain`.
This will end up building `//bar_tools:barc_linux` but not
`//bar_tools:barc_windows`.

## Toolchain resolution

Note: [Some Bazel rules](/concepts/platforms#status) do not yet support
toolchain resolution.

For each target that uses toolchains, Bazel's toolchain resolution procedure
determines the target's concrete toolchain dependencies. The procedure takes as
input a set of required toolchain types, the target platform, the list of
available execution platforms, and the list of available toolchains. Its outputs
are a selected toolchain for each toolchain type as well as a selected execution
platform for the current target.

The available execution platforms and toolchains are gathered from the
external dependency graph via
[`register_execution_platforms`](/rules/lib/globals/module#register_execution_platforms)
and
[`register_toolchains`](/rules/lib/globals/module#register_toolchains) calls in
`MODULE.bazel` files.
Additional execution platforms and toolchains may also be specified on the
command line via
[`--extra_execution_platforms`](/reference/command-line-reference#flag--extra_execution_platforms)
and
[`--extra_toolchains`](/reference/command-line-reference#flag--extra_toolchains).
The host platform is automatically included as an available execution platform.
Available platforms and toolchains are tracked as ordered lists for determinism,
with preference given to earlier items in the list.

The set of available toolchains, in priority order, is created from
`--extra_toolchains` and `register_toolchains`:

1. Toolchains registered using `--extra_toolchains` are added first. (Within
   these, the **last** toolchain has highest priority.)
2. Toolchains registered using `register_toolchains` in the transitive external
   dependency graph, in the following order: (Within these, the **first**
   mentioned toolchain has highest priority.)
  1. Toolchains registered by the root module (as in, the `MODULE.bazel` at the
     workspace root);
  2. Toolchains registered in the user's `WORKSPACE` file, including in any
     macros invoked from there;
  3. Toolchains registered by non-root modules (as in, dependencies specified by
     the root module, and their dependencies, and so forth);
  4. Toolchains registered in the "WORKSPACE suffix"; this is only used by
     certain native rules bundled with the Bazel installation.

**NOTE:** [Pseudo-targets like `:all`, `:*`, and
`/...`](/run/build#specifying-build-targets) are ordered by Bazel's package
loading mechanism, which uses a lexicographic ordering.

The resolution steps are as follows.

1. A `target_compatible_with` or `exec_compatible_with` clause *matches* a
   platform if, for each `constraint_value` in its list, the platform also has
   that `constraint_value` (either explicitly or as a default).

   If the platform has `constraint_value`s from `constraint_setting`s not
   referenced by the clause, these do not affect matching.

1. If the target being built specifies the
   [`exec_compatible_with` attribute](/reference/be/common-definitions#common.exec_compatible_with)
   (or its rule definition specifies the
   [`exec_compatible_with` argument](/rules/lib/globals/bzl#rule.exec_compatible_with)),
   the list of available execution platforms is filtered to remove
   any that do not match the execution constraints.

1. The list of available toolchains is filtered to remove any toolchains
   specifying `target_settings` that don't match the current configuration.

1. For each available execution platform, you associate each toolchain type with
   the first available toolchain, if any, that is compatible with this execution
   platform and the target platform.

1. Any execution platform that failed to find a compatible mandatory toolchain
   for one of its toolchain types is ruled out. Of the remaining platforms, the
   first one becomes the current target's execution platform, and its associated
   toolchains (if any) become dependencies of the target.

The chosen execution platform is used to run all actions that the target
generates.

In cases where the same target can be built in multiple configurations (such as
for different CPUs) within the same build, the resolution procedure is applied
independently to each version of the target.

If the rule uses [execution groups](/extending/exec-groups), each execution
group performs toolchain resolution separately, and each has its own execution
platform and toolchains.

## Debugging toolchains

If you are adding toolchain support to an existing rule, use the
`--toolchain_resolution_debug=regex` flag. During toolchain resolution, the flag
provides verbose output for toolchain types or target names that match the regex variable. You
can use `.*` to output all information. Bazel will output names of toolchains it
checks and skips during the resolution process.

For example, to debug toolchain selection for all actions created directly by
`//my:target`:

```sh
$ bazel build //my:all --toolchain_resolution_debug=//my:target
```

To debug toolchain selection for all actions over all build targets:

```sh
$ bazel build //my:all --toolchain_resolution_debug=.*
```

If you'd like to see which [`cquery`](/query/cquery) dependencies are from toolchain
resolution, use `cquery`'s [`--transitions`](/query/cquery#transitions) flag:

```
# Find all direct dependencies of //cc:my_cc_lib. This includes explicitly
# declared dependencies, implicit dependencies, and toolchain dependencies.
$ bazel cquery 'deps(//cc:my_cc_lib, 1)'
//cc:my_cc_lib (96d6638)
@bazel_tools//tools/cpp:toolchain (96d6638)
@bazel_tools//tools/def_parser:def_parser (HOST)
//cc:my_cc_dep (96d6638)
@local_config_platform//:host (96d6638)
@bazel_tools//tools/cpp:toolchain_type (96d6638)
//:default_host_platform (96d6638)
@local_config_cc//:cc-compiler-k8 (HOST)
//cc:my_cc_lib.cc (null)
@bazel_tools//tools/cpp:grep-includes (HOST)

# Which of these are from toolchain resolution?
$ bazel cquery 'deps(//cc:my_cc_lib, 1)' --transitions=lite | grep "toolchain dependency"
  [toolchain dependency]#@local_config_cc//:cc-compiler-k8#HostTransition -> b6df211
```

---

## Module extensions
- URL: https://bazel.build/external/extension
- Source: external/extension.mdx
- Slug: /external/extension

Module extensions allow users to extend the module system by reading input data
from modules across the dependency graph, performing necessary logic to resolve
dependencies, and finally creating repos by calling [repo
rules](/external/repo). These extensions have capabilities similar to repo
rules, which enables them to perform file I/O, send network requests, and so on.
Among other things, they allow Bazel to interact with other package management
systems while also respecting the dependency graph built out of Bazel modules.

You can define module extensions in `.bzl` files, just like repo rules. They're
not invoked directly; rather, each module specifies pieces of data called *tags*
for extensions to read. Bazel runs module resolution before evaluating any
extensions. The extension reads all the tags belonging to it across the entire
dependency graph.

## Extension usage

Extensions are hosted in Bazel modules themselves. To use an extension in a
module, first add a `bazel_dep` on the module hosting the extension, and then
call the [`use_extension`](/rules/lib/globals/module#use_extension) built-in function
to bring it into scope. Consider the following example — a snippet from a
`MODULE.bazel` file to use the "maven" extension defined in the
[`rules_jvm_external`](https://github.com/bazelbuild/rules_jvm_external)
module:

```python
bazel_dep(name = "rules_jvm_external", version = "4.5")
maven = use_extension("@rules_jvm_external//:extensions.bzl", "maven")
```

This binds the return value of `use_extension` to a variable, which allows the
user to use dot-syntax to specify tags for the extension. The tags must follow
the schema defined by the corresponding *tag classes* specified in the
[extension definition](#extension_definition). For an example specifying some
`maven.install` and `maven.artifact` tags:

```python
maven.install(artifacts = ["org.junit:junit:4.13.2"])
maven.artifact(group = "com.google.guava",
               artifact = "guava",
               version = "27.0-jre",
               exclusions = ["com.google.j2objc:j2objc-annotations"])
```

Use the [`use_repo`](/rules/lib/globals/module#use_repo) directive to bring repos
generated by the extension into the scope of the current module.

```python
use_repo(maven, "maven")
```

Repos generated by an extension are part of its API. In this example, the
"maven" module extension promises to generate a repo called `maven`. With the
declaration above, the extension properly resolves labels such as
`@maven//:org_junit_junit` to point to the repo generated by the "maven"
extension.

Note: Module extensions are evaluated lazily. This means that an extension will
typically not be evaluated unless some module brings one of its repositories
into scope using `use_repo` and that repository is referenced in a build. While
testing a module extension, `bazel mod deps` can be useful as it
unconditionally evaluates all module extensions.

## Extension definition

You can define module extensions similarly to [repo rules](/external/repo),
using the [`module_extension`](/rules/lib/globals/bzl#module_extension)
function. However, while repo rules have a number of attributes, module
extensions have [`tag_class`es](/rules/lib/globals/bzl#tag_class), each of which
has a number of attributes. The tag classes define schemas for tags used by this
extension. For example, the "maven" extension above might be defined like this:

```python
# @rules_jvm_external//:extensions.bzl

_install = tag_class(attrs = {"artifacts": attr.string_list(), ...})
_artifact = tag_class(attrs = {"group": attr.string(), "artifact": attr.string(), ...})
maven = module_extension(
  implementation = _maven_impl,
  tag_classes = {"install": _install, "artifact": _artifact},
)
```

These declarations show that `maven.install` and `maven.artifact` tags can be
specified using the specified attribute schema.

The implementation function of module extensions are similar to those of repo
rules, except that they get a [`module_ctx`](/rules/lib/builtins/module_ctx) object,
which grants access to all modules using the extension and all pertinent tags.
The implementation function then calls repo rules to generate repos.

```python
# @rules_jvm_external//:extensions.bzl

load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")  # a repo rule
def _maven_impl(ctx):
  # This is a fake implementation for demonstration purposes only

  # collect artifacts from across the dependency graph
  artifacts = []
  for mod in ctx.modules:
    for install in mod.tags.install:
      artifacts += install.artifacts
    artifacts += [_to_artifact(artifact) for artifact in mod.tags.artifact]

  # call out to the coursier CLI tool to resolve dependencies
  output = ctx.execute(["coursier", "resolve", artifacts])
  repo_attrs = _process_coursier_output(output)

  # call repo rules to generate repos
  for attrs in repo_attrs:
    http_file(**attrs)
  _generate_hub_repo(name = "maven", repo_attrs)
```

### Extension identity

Module extensions are identified by the name and the `.bzl` file that appears
in the call to `use_extension`. In the following example, the extension `maven`
is identified by the `.bzl` file `@rules_jvm_external//:extension.bzl` and the
name `maven`:

```python
maven = use_extension("@rules_jvm_external//:extensions.bzl", "maven")
```

Re-exporting an extension from a different `.bzl` file gives it a new identity
and if both versions of the extension are used in the transitive module graph,
then they will be evaluated separately and will only see the tags associated
with that particular identity.

As an extension author you should make sure that users will only use your
module extension from one single `.bzl` file.

## Repository names and visibility

Repos generated by extensions have canonical names in the form of `{{ "<var>"
}}module_repo_canonical_name</var>+<var>extension_name{{
"</var>" }}+<var>repo_name</var>`. Note that the canonical name
format is not an API you should depend on — it's subject to change at any time.

This naming policy means that each extension has its own "repo namespace"; two
distinct extensions can each define a repo with the same name without risking
any clashes. It also means that `repository_ctx.name` reports the canonical name
of the repo, which is *not* the same as the name specified in the repo rule
call.

Taking repos generated by module extensions into consideration, there are
several repo visibility rules:

*   A Bazel module repo can see all repos introduced in its `MODULE.bazel` file
    via [`bazel_dep`](/rules/lib/globals/module#bazel_dep) and
    [`use_repo`](/rules/lib/globals/module#use_repo).
*   A repo generated by a module extension can see all repos visible to the
    module that hosts the extension, *plus* all other repos generated by the
    same module extension (using the names specified in the repo rule calls as
    their apparent names).
    *   This might result in a conflict. If the module repo can see a repo with
        the apparent name `foo`, and the extension generates a repo with the
        specified name `foo`, then for all repos generated by that extension
        `foo` refers to the former.
*   Similarly, in a module extension's implementation function, repos created
    by the extension can refer to each other by their apparent names in
    attributes, regardless of the order in which they are created.
    *   In case of a conflict with a repository visible to the module, labels
        passed to repository rule attributes can be wrapped in a call to
        [`Label`](/rules/lib/toplevel/attr#label) to ensure that they refer to
        the repo visible to the module instead of the extension-generated repo
        of the same name.

### Overriding and injecting module extension repos

The root module can use
[`override_repo`](/rules/lib/globals/module#override_repo) and
[`inject_repo`](/rules/lib/globals/module#inject_repo) to override or inject
module extension repos.

#### Example: Replacing `rules_java`'s `java_tools` with a vendored copy

```python
# MODULE.bazel
local_repository = use_repo_rule("@bazel_tools//tools/build_defs/repo:local.bzl", "local_repository")
local_repository(
  name = "my_java_tools",
  path = "vendor/java_tools",
)

bazel_dep(name = "rules_java", version = "7.11.1")
java_toolchains = use_extension("@rules_java//java:extension.bzl", "toolchains")

override_repo(java_toolchains, remote_java_tools = "my_java_tools")
```

#### Example: Patch a Go dependency to depend on `@zlib` instead of the system zlib

```python
# MODULE.bazel
bazel_dep(name = "gazelle", version = "0.38.0")
bazel_dep(name = "zlib", version = "1.3.1.bcr.3")

go_deps = use_extension("@gazelle//:extensions.bzl", "go_deps")
go_deps.from_file(go_mod = "//:go.mod")
go_deps.module_override(
  patches = [
    "//patches:my_module_zlib.patch",
  ],
  path = "example.com/my_module",
)
use_repo(go_deps, ...)

inject_repo(go_deps, "zlib")
```

```diff
# patches/my_module_zlib.patch
--- a/BUILD.bazel
+++ b/BUILD.bazel
@@ -1,6 +1,6 @@
 go_binary(
     name = "my_module",
     importpath = "example.com/my_module",
     srcs = ["my_module.go"],
-    copts = ["-lz"],
+    cdeps = ["@zlib"],
 )
```

## Best practices

This section describes best practices when writing extensions so they are
straightforward to use, maintainable, and adapt well to changes over time.

### Put each extension in a separate file

When extensions are in a different files, it allows one extension to load
repositories generated by another extension. Even if you don't use this
functionality, it's best to put them in separate files in case you need it
later. This is because the extension's identify is based on its file, so moving
the extension into another file later changes your public API and is a backwards
incompatible change for your users.

### Specify reproducibility and use facts

If your extension always defines the same repositories given the same inputs
(extension tags, files it reads, etc.) and in particular doesn't rely on
any [downloads](/rules/lib/builtins/module_ctx#download) that aren't guarded by
a checksum, consider returning
[`extension_metadata`](/rules/lib/builtins/module_ctx#extension_metadata) with
`reproducible = True`. This allows Bazel to skip this extension when writing to
the `MODULE.bazel` lockfile, which helps keep the lockfile small and reduces
the chance of merge conflicts. Note that Bazel still caches the results of
reproducible extensions in a way that persists across server restarts, so even
a long-running extension can be marked as reproducible without a performance
penalty.

If your extension relies on effectively immutable data obtained from outside
the build, most commonly from the network, but you don't have a checksum
available to guard the download, consider using the `facts` parameter of
[`extension_metadata`](/rules/lib/builtins/module_ctx#extension_metadata) to
persistently record such data and thus allow your extension to become
reproducible. `facts` is expected to be a dictionary with string keys and
arbitrary JSON-like Starlark values that is always persisted in the lockfile and
available to future evaluations of the extension via the
[`facts`](/rules/lib/builtins/module_ctx#facts) field of `module_ctx`.

`facts` are not invalidated even when the code of your module extension changes,
so be prepared to handle the case where the structure of `facts` changes.
Bazel also assumes that two different `facts` dicts produced by two different
evaluations of the same extension can be shallowly merged (i.e., as if by using
the `|` operator on two dicts). This is partially enforced by `module_ctx.facts`
not supporting enumeration of its entries, just lookups by key.

An example of using `facts` would be to record a mapping from version numbers of
some SDK to the an object containing the download URL and checksum of that
version. The first time the extension is evaluated, it can fetch this mapping
from the network, but on later evaluations it can use the mapping from `facts`
to avoid the network requests.

### Specify dependence on operating system and architecture

If your extension relies on the operating system or its architecture type,
ensure to indicate this in the extension definition using the `os_dependent`
and `arch_dependent` boolean attributes. This ensures that Bazel recognizes the
need for re-evaluation if there are changes to either of them.

Since this kind of dependence on the host makes it more difficult to maintain
the lockfile entry for this extension, consider
[marking the extension reproducible](#specify_reproducibility) if possible.

### Only the root module should directly affect repository names

Remember that when an extension creates repositories, they are created within
the namespace of the extension. This means collisions can occur if different
modules use the same extension and end up creating a repository with the same
name. This often manifests as a module extension's `tag_class` having a `name`
argument that is passed as a repository rule's `name` value.

For example, say the root module, `A`, depends on module `B`. Both modules
depend on module `mylang`. If both `A` and `B` call
`mylang.toolchain(name="foo")`, they will both try to create a repository named
`foo` within the `mylang` module and an error will occur.

To avoid this, either remove the ability to set the repository name directly,
or only allow the root module to do so. It's OK to allow the root module this
ability because nothing will depend on it, so it doesn't have to worry about
another module creating a conflicting name.

---

## Frequently asked questions
- URL: https://bazel.build/external/faq
- Source: external/faq.mdx
- Slug: /external/faq

This page answers some frequently asked questions about external dependencies in
Bazel.

## MODULE.bazel

### How should I version a Bazel module?

Setting `version` with the [`module`] directive in the source archive
`MODULE.bazel` can have several downsides and unintended side effects if not
managed carefully:

*   Duplication: releasing a new version of a module typically involves both
    incrementing the version in `MODULE.bazel` and tagging the release, two
    separate steps that can fall out of sync. While automation can
    reduce this risk, it's simpler and safer to avoid it altogether.

*   Inconsistency: users overriding a module with a specific commit using a
    [non-registry override] will see an incorrect version. for example, if the
    `MODULE.bazel` in the source archive sets `version = "0.3.0"` but
    additional commits have been made since that release, a user overriding
    with one of those commits would still see `0.3.0`. In reality, the version
    should reflect that it's ahead of the release, for example `0.3.1-rc1`.

*   Non-registry override issues: using placeholder values can cause issues
    when users override a module with a non-registry override. For example,
    `0.0.0` doesn't sort as the highest version, which is usually the expected
    behavior users want when doing a non-registry override.

Thus, it's best to avoid setting the version in the source archive
`MODULE.bazel`. Instead, set it in the `MODULE.bazel` stored in the registry
(e.g., the [Bazel Central Registry]), which is the actual source of truth for
the module version during Bazel's external dependency resolution (see [Bazel
registries]).

This is usually automated, for example the [`rules-template`] example rule
repository uses a [bazel-contrib/publish-to-bcr publish.yaml GitHub Action] to
publish the release to the BCR. The action [generates a patch for the source
archive `MODULE.bazel`] with the release version. This patch is stored in the
registry and is applied when the module is fetched during Bazel's external
dependency resolution.

This way, the version in the releases in the registry will be correctly set to
the released version and thus, `bazel_dep`, `single_version_override` and
`multiple_version_override` will work as expected, while avoiding potential
issues when doing a non-registry override because the version in the source
archive will be the default value (`''`), which will always be handled
correctly (it's the default version value after all) and will behave as
expected when sorting (the empty string is treated as the highest version).

[Bazel Central Registry]: https://registry.bazel.build/
[Bazel registries]: https://bazel.build/external/registry
[bazel-contrib/publish-to-bcr publish.yaml GitHub Action]: https://github.com/bazel-contrib/publish-to-bcr/blob/v0.2.2/.github/workflows/publish.yaml
[generates a patch for the source archive `MODULE.bazel`]: https://github.com/bazel-contrib/publish-to-bcr/blob/v0.2.2/src/domain/create-entry.ts#L176-L216
[`module`]: /rules/lib/globals/module#module
[non-registry override]: module.md#non-registry_overrides
[`rules-template`]: https://github.com/bazel-contrib/rules-template

### When should I increment the compatibility level?

The [`compatibility_level`](module.md#compatibility_level) of a Bazel module
should be incremented _in the same commit_ that introduces a backwards
incompatible ("breaking") change.

However, Bazel can throw an error if it detects that versions of the _same
module_ with _different compatibility levels_ exist in the resolved dependency
graph. This can happen when for example' two modules depend on versions of a
third module with different compatibility levels.

Thus, incrementing `compatibility_level` too frequently can be very disruptive
and is discouraged. To avoid this situation, the `compatibility_level` should be
incremented _only_ when the breaking change affects most use cases and isn't
easy to migrate and/or work-around.

### Why does MODULE.bazel not support `load`s?

During dependency resolution, the MODULE.bazel file of all referenced external
dependencies are fetched from registries. At this stage, the source archives of
the dependencies are not fetched yet; so if the MODULE.bazel file `load`s
another file, there is no way for Bazel to actually fetch that file without
fetching the entire source archive. Note the MODULE.bazel file itself is
special, as it's directly hosted on the registry.

There are a few use cases that people asking for `load`s in MODULE.bazel are
generally interested in, and they can be solved without `load`s:

*   Ensuring that the version listed in MODULE.bazel is consistent with build
    metadata stored elsewhere, for example in a .bzl file: This can be achieved
    by using the
    [`native.module_version`](/rules/lib/toplevel/native#module_version) method
    in a .bzl file loaded from a BUILD file.
*   Splitting up a very large MODULE.bazel file into manageable sections,
    particularly for monorepos: The root module can use the
    [`include`](/rules/lib/globals/module#include) directive to split its
    MODULE.bazel file into multiple segments. For the same reason we don't allow
    `load`s in MODULE.bazel files, `include` cannot be used in non-root modules.
*   Users of the old WORKSPACE system might remember declaring a repo, and then
    immediately `load`ing from that repo to perform complex logic. This
    capability has been replaced by [module extensions](extension).

### Can I specify a SemVer range for a `bazel_dep`?

No. Some other package managers like [npm][npm-semver] and [Cargo][cargo-semver]
support version ranges (implicitly or explicitly), and this often requires a
constraint solver (making the output harder to predict for users) and makes
version resolution nonreproducible without a lockfile.

Bazel instead uses [Minimal Version Selection](module#version-selection) like
Go, which in contrast makes the output easy to predict and guarantees
reproducibility. This is a tradeoff that matches Bazel's design goals.

Furthermore, Bazel module versions are [a superset of
SemVer](module#version-format), so what makes sense in a strict SemVer
environment doesn't always carry over to Bazel module versions.

### Can I automatically get the latest version for a `bazel_dep`?

Some users occasionally ask for the ability to specify `bazel_dep(name = "foo",
version = "latest")` to automatically get the latest version of a dep. This is
similar to [the question about SemVer
ranges](#can-i-specify-a-semver-range-for-a-bazel-dep), and the answer is also
no.

The recommended solution here is to have automation take care of this. For
example, [Renovate](https://docs.renovatebot.com/modules/manager/) supports
Bazel modules.

Sometimes, users asking this question are really looking for a way to quickly
iterate during local development. This can be achieved by using a
[`local_path_override`](/rules/lib/globals/module#local_path_override).

### Why all these `use_repo`s?

Module extension usages in MODULE.bazel files sometimes come with a big
`use_repo` directive. For example, a typical usage of the
[`go_deps` extension][go_deps] from `gazelle` might look like:

```python
go_deps = use_extension("@gazelle//:extensions.bzl", "go_deps")
go_deps.from_file(go_mod = "//:go.mod")
use_repo(
    go_deps,
    "com_github_gogo_protobuf",
    "com_github_golang_mock",
    "com_github_golang_protobuf",
    "org_golang_x_net",
    ...  # potentially dozens of lines...
)
```

The long `use_repo` directive may seem redundant, since the information is
arguably already in the referenced `go.mod` file.

The reason Bazel needs this `use_repo` directive is that it runs module
extensions lazily. That is, a module extension is only run if its result is
observed. Since a module extension's "output" is repo definitions, this means
that we only run a module extension if a repo it defines is requested (for
instance, if the target `@org_golang_x_net//:foo` is built, in the example
above). However, we don't know which repos a module extension would define until
after we run it. This is where the `use_repo` directive comes in; the user can
tell Bazel which repos they expect the extension to generate, and Bazel would
then only run the extension when these specific repos are used.

To help the maintain this `use_repo` directive, a module extension can return
an [`extension_metadata`](/rules/lib/builtins/module_ctx#extension_metadata)
object from its implementation function. The user can run the `bazel mod tidy`
command to update the `use_repo` directives for these module extensions.

## Bzlmod migration

### Which is evaluated first, MODULE.bazel or WORKSPACE?

When both `--enable_bzlmod` and `--enable_workspace` are set, it's natural to
wonder which system is consulted first. The short answer is that MODULE.bazel
(Bzlmod) is evaluated first.

The long answer is that "which evaluates first" is not the right question to
ask; rather, the right question to ask is: in the context of the repo with
[canonical name](overview#canonical-repo-name) `@@foo`, what does the [apparent
repo name](overview#apparent-repo-name) `@bar` resolve to? Alternatively, what
is the repo mapping of `@@base`?

Labels with apparent repo names (a single leading `@`) can refer to different
things based on the context they're resolved from. When you see a label
`@bar//:baz` and wonder what it actually points to, you need to first find out
what the context repo is: for example, if the label is in a BUILD file located
in the repo `@@foo`, then the context repo is `@@foo`.

Then, depending on what the context repo is, the ["repository
visibility" table](migration#repository-visibility) in the migration guide can
be used to find out which repo an apparent name actually resolves to.

*   If the context repo is the main repo (`@@`):
    1.  If `bar` is an apparent repo name introduced by the root module's
        MODULE.bazel file (through any of
        [`bazel_dep`](/rules/lib/globals/module#bazel_dep.repo_name),
        [`use_repo`](/rules/lib/globals/module#use_repo),
        [`module`](/rules/lib/globals/module#module.repo_name),
        [`use_repo_rule`](/rules/lib/globals/module#use_repo_rule)), then `@bar`
        resolves to what that MODULE.bazel file claims.
    2.  Otherwise, if `bar` is a repo defined in WORKSPACE (which means that its
        canonical name is `@@bar`), then `@bar` resolves to `@@bar`.
    3.  Otherwise, `@bar` resolves to something like
        `@@[unknown repo 'bar' requested from @@]`, and this will ultimately
        result in an error.
*   If the context repo is a Bzlmod-world repo (that is, it corresponds to a
    non-root Bazel module, or is generated by a module extension), then it
    will only ever see other Bzlmod-world repos, and no WORKSPACE-world repos.
    *   Notably, this includes any repos introduced in a `non_module_deps`-like
        module extension in the root module, or `use_repo_rule` instantiations
        in the root module.
*   If the context repo is defined in WORKSPACE:
    1.  First, check if the context repo definition has the magical
        `repo_mapping` attribute. If so, go through the mapping first (so for a
        repo defined with `repo_mapping = {"@bar": "@baz"}`, we would be looking
        at `@baz` below).
    2.  If `bar` is an apparent repo name introduced by the root module's
        MODULE.bazel file, then `@bar` resolves to what that MODULE.bazel file
        claims. (This is the same as item 1 in the main repo case.)
    3.  Otherwise, `@bar` resolves to `@@bar`. This most likely will point to a
        repo `bar` defined in WORKSPACE; if such a repo is not defined, Bazel
        will throw an error.

For a more succinct version:

*   Bzlmod-world repos (excluding the main repo) will only see Bzlmod-world
    repos.
*   WORKSPACE-world repos (including the main repo) will first see what the root
    module in the Bzlmod world defines, then fall back to seeing WORKSPACE-world
    repos.

Of note, labels in the Bazel command line (including Starlark flags, label-typed
flag values, and build/test target patterns) are treated as having the main repo
as the context repo.

## Other

### How do I prepare and run an offline build?

Use the `bazel fetch` command to prefetch repos. You can use the `--repo` flag
(like `bazel fetch --repo @foo`) to fetch only the repo `@foo` (resolved in the
context of the main repo, see [question
above](#which-is-evaluated-first-module-bazel-or-workspace)), or use a target
pattern (like `bazel fetch @foo//:bar`) to fetch all transitive dependencies of
`@foo//:bar` (this is equivalent to `bazel build --nobuild @foo//:bar`).

The make sure no fetches happen during a build, use `--nofetch`. More precisely,
this makes any attempt to run a non-local repository rule fail.

If you want to fetch repos _and_ modify them to test locally, consider using
the [`bazel vendor`](vendor) command.

### How do I use HTTP proxies?

Bazel respects the `http_proxy` and `HTTPS_PROXY` environment variables commonly
accepted by other programs, such as
[curl](https://everything.curl.dev/usingcurl/proxies/env.html).

### How do I make Bazel prefer IPv6 in dual-stack IPv4/IPv6 setups?

On IPv6-only machines, Bazel can download dependencies with no changes. However,
on dual-stack IPv4/IPv6 machines Bazel follows the same convention as Java,
preferring IPv4 if enabled. In some situations, for example when the IPv4
network cannot resolve/reach external addresses, this can cause `Network
unreachable` exceptions and build failures. In these cases, you can override
Bazel's behavior to prefer IPv6 by using the
[`java.net.preferIPv6Addresses=true` system
property](https://docs.oracle.com/javase/8/docs/api/java/net/doc-files/net-properties.html).
Specifically:

*   Use `--host_jvm_args=-Djava.net.preferIPv6Addresses=true` [startup
    option](/docs/user-manual#startup-options), for example by adding the
    following line in your [`.bazelrc` file](/run/bazelrc):

    `startup --host_jvm_args=-Djava.net.preferIPv6Addresses=true`

*   When running Java build targets that need to connect to the internet (such
    as for integration tests), use the
    `--jvmopt=-Djava.net.preferIPv6Addresses=true` [tool
    flag](/docs/user-manual#jvmopt). For example, include in your [`.bazelrc`
    file](/run/bazelrc):

    `build --jvmopt=-Djava.net.preferIPv6Addresses`

*   If you are using
    [`rules_jvm_external`](https://github.com/bazelbuild/rules_jvm_external) for
    dependency version resolution, also add
    `-Djava.net.preferIPv6Addresses=true` to the `COURSIER_OPTS` environment
    variable to [provide JVM options for
    Coursier](https://github.com/bazelbuild/rules_jvm_external#provide-jvm-options-for-coursier-with-coursier_opts).

### Can repo rules be run remotely with remote execution?

No; or at least, not yet. Users employing remote execution services to speed up
their builds may notice that repo rules are still run locally. For example, an
`http_archive` would be first downloaded onto the local machine (using any local
download cache if applicable), extracted, and then each source file would be
uploaded to the remote execution service as an input file. It's natural to ask
why the remote execution service doesn't just download and extract that archive,
saving a useless roundtrip.

Part of the reason is that repo rules (and module extensions) are akin to
"scripts" that are run by Bazel itself. A remote executor doesn't necessarily
even have a Bazel installed.

Another reason is that Bazel often needs the BUILD files in the downloaded and
extracted archives to perform loading and analysis, which _are_ performed
locally.

There are preliminary ideas to solve this problem by re-imagining repo rules as
build rules, which would naturally allow them to be run remotely, but conversely
raise new architectural concerns (for example, the `query` commands would
potentially need to run actions, complicating their design).

For more previous discussion on this topic, see [A way to support repositories
that need Bazel for being
fetched](https://github.com/bazelbuild/bazel/discussions/20464).

[npm-semver]: https://docs.npmjs.com/about-semantic-versioning
[cargo-semver]: https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html#version-requirement-syntax
[go_deps]: https://github.com/bazel-contrib/rules_go/blob/master/docs/go/core/bzlmod.md#specifying-external-dependencies

---

## Lockfile
- URL: https://bazel.build/external/lockfile
- Source: external/lockfile.mdx
- Slug: /external/lockfile

keywords: product:Bazel,lockfile,Bzlmod
---
title: 'Bazel Lockfile'
---



The lockfile feature in Bazel enables the recording of specific versions or
dependencies of software libraries or packages required by a project. It
achieves this by storing the result of module resolution and extension
evaluation. The lockfile promotes reproducible builds, ensuring consistent
development environments. Additionally, it enhances build efficiency by allowing
Bazel to skip the parts of the resolution process that are unaffected by changes
in project dependencies. Furthermore, the lockfile improves stability by
preventing unexpected updates or breaking changes in external libraries, thereby
reducing the risk of introducing bugs.

## Lockfile Generation

The lockfile is generated under the workspace root with the name
`MODULE.bazel.lock`. It is created or updated during the build process,
specifically after module resolution and extension evaluation. Importantly, it
only includes dependencies that are included in the current invocation of the
build.

When changes occur in the project that affect its dependencies, the lockfile is
automatically updated to reflect the new state. This ensures that the lockfile
remains focused on the specific set of dependencies required for the current
build, providing an accurate representation of the project's resolved
dependencies.

## Lockfile Usage

The lockfile can be controlled by the flag
[`--lockfile_mode`](/reference/command-line-reference#flag--lockfile_mode) to
customize the behavior of Bazel when the project state differs from the
lockfile. The available modes are:

*   `update` (Default): Use the information that is present in the lockfile to
    skip downloads of known registry files and to avoid re-evaluating extensions
    whose results are still up-to-date. If information is missing, it will
    be added to the lockfile. In this mode, Bazel also avoids refreshing
    mutable information, such as yanked versions, for dependencies that haven't
    changed.
*   `refresh`: Like `update`, but mutable information is always refreshed when
    switching to this mode and roughly every hour while in this mode.
*   `error`: Like `update`, but if any information is missing or out-of-date,
    Bazel will fail with an error. This mode never changes the lockfile or
    performs network requests during resolution. Module extensions that marked
    themselves as `reproducible` may still perform network requests, but are
    expected to always produce the same result.
*   `off`: The lockfile is neither checked nor updated.

## Lockfile Benefits

The lockfile offers several benefits and can be utilized in various ways:

-   **Reproducible builds.** By capturing the specific versions or dependencies
    of software libraries, the lockfile ensures that builds are reproducible
    across different environments and over time. Developers can rely on
    consistent and predictable results when building their projects.

-   **Fast incremental resolutions.** The lockfile enables Bazel to avoid
    downloading registry files that were already used in a previous build.
    This significantly improves build efficiency, especially in scenarios where
    resolution can be time-consuming.

-   **Stability and risk reduction.** The lockfile helps maintain stability by
    preventing unexpected updates or breaking changes in external libraries. By
    locking the dependencies to specific versions, the risk of introducing bugs
    due to incompatible or untested updates is reduced.

### Hidden lockfile

Bazel also maintains another lockfile at
`"$(bazel info output_base)"/MODULE.bazel.lock`. The format and contents of this
lockfile are explicitly unspecified. It is only used as a performance
optimization. While it can be deleted together with the output base via
`bazel clean --expunge`, any need to do so is a bug in either Bazel itself or a
module extension.

## Lockfile Contents

The lockfile contains all the necessary information to determine whether the
project state has changed. It also includes the result of building the project
in the current state. The lockfile consists of two main parts:

1.   Hashes of all remote files that are inputs to module resolution.
2.   For each module extension, the lockfile includes inputs that affect it,
     represented by `bzlTransitiveDigest`, `usagesDigest` and other fields, as
     well as the output of running that extension, referred to as
     `generatedRepoSpecs`

Here is an example that demonstrates the structure of the lockfile, along with
explanations for each section:

```json
{
  "lockFileVersion": 10,
  "registryFileHashes": {
    "https://bcr.bazel.build/bazel_registry.json": "8a28e4af...5d5b3497",
    "https://bcr.bazel.build/modules/foo/1.0/MODULE.bazel": "7cd0312e...5c96ace2",
    "https://bcr.bazel.build/modules/foo/2.0/MODULE.bazel": "70390338... 9fc57589",
    "https://bcr.bazel.build/modules/foo/2.0/source.json": "7e3a9adf...170d94ad",
    "https://registry.mycorp.com/modules/foo/1.0/MODULE.bazel": "not found",
    ...
  },
  "selectedYankedVersions": {
    "foo@2.0": "Yanked for demo purposes"
  },
  "moduleExtensions": {
    "//:extension.bzl%lockfile_ext": {
      "general": {
        "bzlTransitiveDigest": "oWDzxG/aLnyY6Ubrfy....+Jp6maQvEPxn0pBM=",
        "usagesDigest": "aLmqbvowmHkkBPve05yyDNGN7oh7QE9kBADr3QIZTZs=",
        ...,
        "generatedRepoSpecs": {
          "hello": {
            "bzlFile": "@@//:extension.bzl",
            ...
          }
        }
      }
    },
    "//:extension.bzl%lockfile_ext2": {
      "os:macos": {
        "bzlTransitiveDigest": "oWDzxG/aLnyY6Ubrfy....+Jp6maQvEPxn0pBM=",
        "usagesDigest": "aLmqbvowmHkkBPve05y....yDNGN7oh7r3QIZTZs=",
        ...,
        "generatedRepoSpecs": {
          "hello": {
            "bzlFile": "@@//:extension.bzl",
            ...
          }
        }
      },
      "os:linux": {
        "bzlTransitiveDigest": "eWDzxG/aLsyY3Ubrto....+Jp4maQvEPxn0pLK=",
        "usagesDigest": "aLmqbvowmHkkBPve05y....yDNGN7oh7r3QIZTZs=",
        ...,
        "generatedRepoSpecs": {
          "hello": {
            "bzlFile": "@@//:extension.bzl",
            ...
          }
        }
      }
    }
  }
}
```

### Registry File Hashes

The `registryFileHashes` section contains the hashes of all files from
remote registries accessed during module resolution. Since the resolution
algorithm is fully deterministic when given the same inputs and all remote
inputs are hashed, this ensures a fully reproducible resolution result while
avoiding excessive duplication of remote information in the lockfile. Note that
this also requires recording when a particular registry didn't contain a certain
module, but a registry with lower precedence did (see the "not found" entry in
the example). This inherently mutable information can be updated via
`bazel mod deps --lockfile_mode=refresh`.

Bazel uses the hashes from the lockfile to look up registry files in the
repository cache before downloading them, which speeds up subsequent
resolutions.

### Selected Yanked Versions

The `selectedYankedVersions` section contains the yanked versions of modules
that were selected by module resolution. Since this usually result in an error
when trying to build, this section is only non-empty when yanked versions are
explicitly allowed via `--allow_yanked_versions` or
`BZLMOD_ALLOW_YANKED_VERSIONS`.

This field is needed since, compared to module files, yanked version information
is inherently mutable and thus can't be referenced by a hash. This information
can be updated via `bazel mod deps --lockfile_mode=refresh`.

### Module Extensions

The `moduleExtensions` section is a map that includes only the extensions used
in the current invocation or previously invoked, while excluding any extensions
that are no longer utilized. In other words, if an extension is not being used
anymore across the dependency graph, it is removed from the `moduleExtensions`
map.

If an extension is independent of the operating system or architecture type,
this section features only a single "general" entry. Otherwise, multiple
entries are included, named after the OS, architecture, or both, with each
corresponding to the result of evaluating the extension on those specifics.

Each entry in the extension map corresponds to a used extension and is
identified by its containing file and name. The corresponding value for each
entry contains the relevant information associated with that extension:

1. The `bzlTransitiveDigest` is the digest of the extension implementation
   and the .bzl files transitively loaded by it.
2. The `usagesDigest` is the digest of the _usages_ of the extension in the
   dependency graph, which includes all tags.
3. Further unspecified fields that track other inputs to the extension,
   such as contents of files or directories it reads or environment
   variables it uses.
4. The `generatedRepoSpecs` encode the repositories created by the
   extension with the current input.
5. The optional `moduleExtensionMetadata` field contains metadata provided by
   the extension such as whether certain repositories it created should be
   imported via `use_repo` by the root module. This information powers the
   `bazel mod tidy` command.

Module extensions can opt out of being included in the lockfile by setting the
returning metadata with `reproducible = True`. By doing so, they promise that
they will always create the same repositories when given the same inputs.

## Best Practices

To maximize the benefits of the lockfile feature, consider the following best
practices:

*   Regularly update the lockfile to reflect changes in project dependencies or
    configuration. This ensures that subsequent builds are based on the most
    up-to-date and accurate set of dependencies. To lock down all extensions
    at once, run `bazel mod deps --lockfile_mode=update`.

*   Include the lockfile in version control to facilitate collaboration and
    ensure that all team members have access to the same lockfile, promoting
    consistent development environments across the project.

*   Use [`bazelisk`](/install/bazelisk) to run Bazel, and include a
    `.bazelversion` file in version control that specifies the Bazel version
    corresponding to the lockfile. Because Bazel itself is a dependency of
    your build, the lockfile is specific to the Bazel version, and will
    change even between [backwards compatible](/release/backward-compatibility)
    Bazel releases. Using `bazelisk` ensures that all developers are using
    a Bazel version that matches the lockfile.

By following these best practices, you can effectively utilize the lockfile
feature in Bazel, leading to more efficient, reliable, and collaborative
software development workflows.

## Merge Conflicts

The lockfile format is designed to minimize merge conflicts, but they can still
happen.

### Automatic Resolution

Bazel provides a custom
[git merge driver](https://git-scm.com/docs/gitattributes#_defining_a_custom_merge_driver)
to help resolve these conflicts automatically.

Set up the driver by adding this line to a `.gitattributes` file in the root of
your git repository:

```gitattributes
# A custom merge driver for the Bazel lockfile.
# https://bazel.build/external/lockfile#automatic-resolution
MODULE.bazel.lock merge=bazel-lockfile-merge
```

Then each developer who wants to use the driver has to register it once by
following these steps:

1. Install [jq](https://jqlang.github.io/jq/download/) (1.5 or higher).
2. Run the following commands:

```bash
jq_script=$(curl https://raw.githubusercontent.com/bazelbuild/bazel/master/scripts/bazel-lockfile-merge.jq)
printf '%s\n' "${jq_script}" | less # to optionally inspect the jq script
git config --global merge.bazel-lockfile-merge.name   "Merge driver for the Bazel lockfile (MODULE.bazel.lock)"
git config --global merge.bazel-lockfile-merge.driver "jq -s '${jq_script}' -- %O %A %B > %A.jq_tmp && mv %A.jq_tmp %A"
```

### Manual Resolution

Simple merge conflicts in the `registryFileHashes` and `selectedYankedVersions`
fields can be safely resolved by keeping all the entries from both sides of the
conflict.

Other types of merge conflicts should not be resolved manually. Instead:

1. Restore the previous state of the lockfile
   via `git reset MODULE.bazel.lock && git checkout MODULE.bazel.lock`.
2. Resolve any conflicts in the `MODULE.bazel` file.
3. Run `bazel mod deps` to update the lockfile.

---

## Migration
- URL: https://bazel.build/external/migration
- Source: external/migration.mdx
- Slug: /external/migration

keywords: bzlmod
---
title: 'Bzlmod Migration Guide'
---



Due to the [shortcomings of
WORKSPACE](/external/overview#workspace-shortcomings), Bzlmod is replacing the
legacy WORKSPACE system. The WORKSPACE file is already disabled in Bazel 8 (late
2024) and will be removed in Bazel 9 (late 2025). This guide helps you migrate
your project to Bzlmod and drop WORKSPACE for managing external dependencies.

## Why migrate to Bzlmod?

*   There are many [advantages](overview#benefits) compared to the legacy
    WORKSPACE system, which helps to ensure a healthy growth of the Bazel
    ecosystem.

*   If your project is a dependency of other projects, migrating to Bzlmod will
    unblock their migration and make it easier for them to depend on your
    project.

*   Migration to Bzlmod is a necessary step in order to use future Bazel
    versions (mandatory in Bazel 9).

## How to migrate to Bzlmod?

Recommended migration process:

1.  Use [migration tool](/external/migration_tool) as a helper tool to
    streamline the migration process as much as possible.
2.  If there are errors left after using the migration tool, resolve them
    manually. For understanding the main differences between concepts inside
    `WORKSPACE` and `MODULE.bazel` files, check [WORKSPACE versus
    Bzlmod](#workspace-vs-bzlmod) section.

## WORKSPACE vs Bzlmod

Bazel's WORKSPACE and Bzlmod offer similar features with different syntax. This
section explains how to migrate from specific WORKSPACE functionalities to
Bzlmod.

### Define the root of a Bazel workspace

The WORKSPACE file marks the source root of a Bazel project, this responsibility
is replaced by MODULE.bazel in Bazel version 6.3 and later. With Bazel versions
prior to 6.3, there should still be a `WORKSPACE` or `WORKSPACE.bazel` file at
your workspace root, maybe with comments like:

*   **WORKSPACE**

    ```python
    # This file marks the root of the Bazel workspace.
    # See MODULE.bazel for external dependencies setup.
    ```

### Enable Bzlmod in your bazelrc

`.bazelrc` lets you set flags that apply every time your run Bazel. To enable
Bzlmod, use the `--enable_bzlmod` flag, and apply it to the `common` command so
it applies to every command:

* **.bazelrc**

    ```
    # Enable Bzlmod for every Bazel command
    common --enable_bzlmod
    ```

### Specify repository name for your workspace

*   **WORKSPACE**

    The [`workspace`](/rules/lib/globals/workspace#workspace) function is used
    to specify a repository name for your workspace. This allows a target
    `//foo:bar` in the workspace to be referenced as `@<workspace
    name>//foo:bar`. If not specified, the default repository name for your
    workspace is `__main__`.

    ```python
    ## WORKSPACE
    workspace(name = "com_foo_bar")
    ```

*   **Bzlmod**

    It's recommended to reference targets in the same workspace with the
    `//foo:bar` syntax without `@<repo name>`. But if you do need the old syntax
    , you can use the module name specified by the
    [`module`](/rules/lib/globals/module#module) function as the repository
    name. If the module name is different from the needed repository name, you
    can use `repo_name` attribute of the
    [`module`](/rules/lib/globals/module#module) function to override the
    repository name.

    ```python
    ## MODULE.bazel
    module(
        name = "bar",
        repo_name = "com_foo_bar",
    )
    ```

### Fetch external dependencies as Bazel modules

If your dependency is a Bazel project, you should be able to depend on it as a
Bazel module when it also adopts Bzlmod.

*   **WORKSPACE**

    With WORKSPACE, it's common to use the
    [`http_archive`](/rules/lib/repo/http#http_archive) or
    [`git_repository`](/rules/lib/repo/git#git_repository) repository rules to
    download the sources of the Bazel project.

    ```python
    ## WORKSPACE
    load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

    http_archive(
        name = "bazel_skylib",
        urls = ["https://github.com/bazelbuild/bazel-skylib/releases/download/1.4.2/bazel-skylib-1.4.2.tar.gz"],
        sha256 = "66ffd9315665bfaafc96b52278f57c7e2dd09f5ede279ea6d39b2be471e7e3aa",
    )
    load("@bazel_skylib//:workspace.bzl", "bazel_skylib_workspace")
    bazel_skylib_workspace()

    http_archive(
        name = "rules_java",
        urls = ["https://github.com/bazelbuild/rules_java/releases/download/6.1.1/rules_java-6.1.1.tar.gz"],
        sha256 = "76402a50ae6859d50bd7aed8c1b8ef09dae5c1035bb3ca7d276f7f3ce659818a",
    )
    load("@rules_java//java:repositories.bzl", "rules_java_dependencies", "rules_java_toolchains")
    rules_java_dependencies()
    rules_java_toolchains()
    ```

    As you can see, it's a common pattern that users need to load transitive
    dependencies from a macro of the dependency. Assume both `bazel_skylib` and
    `rules_java` depends on `platform`, the exact version of the `platform`
    dependency is determined by the order of the macros.

*   **Bzlmod**

    With Bzlmod, as long as your dependency is available in [Bazel Central
    Registry](https://registry.bazel.build) or your custom [Bazel
    registry](/external/registry), you can simply depend on it with a
    [`bazel_dep`](/rules/lib/globals/module#bazel_dep) directive.

    ```python
    ## MODULE.bazel
    bazel_dep(name = "bazel_skylib", version = "1.4.2")
    bazel_dep(name = "rules_java", version = "6.1.1")
    ```

    Bzlmod resolves Bazel module dependencies transitively using the
    [MVS](https://research.swtch.com/vgo-mvs) algorithm. Therefore, the maximal
    required version of `platform` is selected automatically.

### Override a dependency as a Bazel module

As the root module, you can override Bazel module dependencies in different
ways.

Please read the [overrides](/external/module#overrides) section for more
information.

You can find some example usages in the
[examples][override-examples]
repository.

[override-examples]: https://github.com/bazelbuild/examples/blob/main/bzlmod/02-override_bazel_module

### Fetch external dependencies with module extensions

If your dependency is not a Bazel project or not yet available in any Bazel
registry, you can introduce it using
[`use_repo_rule`](/external/module#use_repo_rule) or [module
extensions](/external/extension).

*   **WORKSPACE**

    Download a file using the [`http_file`](/rules/lib/repo/http#http_file)
    repository rule.

    ```python
    ## WORKSPACE
    load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")

    http_file(
        name = "data_file",
        url = "http://example.com/file",
        sha256 = "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
    )
    ```

*   **Bzlmod**

    With Bzlmod, you can use the `use_repo_rule` directive in your MODULE.bazel
    file to directly instantiate repos:

    ```python
    ## MODULE.bazel
    http_file = use_repo_rule("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")
    http_file(
        name = "data_file",
        url = "http://example.com/file",
        sha256 = "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
    )
    ```

    Under the hood, this is implemented using a module extension. If you need to
    perform more complex logic than simply invoking a repo rule, you could also
    implement a module extension yourself. You'll need to move the definition
    into a `.bzl` file, which also lets you share the definition between
    WORKSPACE and Bzlmod during the migration period.

    ```python
    ## repositories.bzl
    load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")
    def my_data_dependency():
        http_file(
            name = "data_file",
            url = "http://example.com/file",
            sha256 = "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
        )
    ```

    Implement a module extension to load the dependencies macro. You can define
    it in the same `.bzl` file of the macro, but to keep compatibility with
    older Bazel versions, it's better to define it in a separate `.bzl` file.

    ```python
    ## extensions.bzl
    load("//:repositories.bzl", "my_data_dependency")
    def _non_module_dependencies_impl(_ctx):
        my_data_dependency()

    non_module_dependencies = module_extension(
        implementation = _non_module_dependencies_impl,
    )
    ```

    To make the repository visible to the root project, you should declare the
    usages of the module extension and the repository in the MODULE.bazel file.

    ```python
    ## MODULE.bazel
    non_module_dependencies = use_extension("//:extensions.bzl", "non_module_dependencies")
    use_repo(non_module_dependencies, "data_file")
    ```

### Resolve conflict external dependencies with module extension

A project can provide a macro that introduces external repositories based on
inputs from its callers. But what if there are multiple callers in the
dependency graph and they cause a conflict?

Assume the project `foo` provides the following macro which takes `version` as
an argument.

```python
## repositories.bzl in foo {:#repositories.bzl-foo}
load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")
def data_deps(version = "1.0"):
    http_file(
        name = "data_file",
        url = "http://example.com/file-%s" % version,
        # Omitting the "sha256" attribute for simplicity
    )
```

*   **WORKSPACE**

    With WORKSPACE, you can load the macro from `@foo` and specify the version
    of the data dependency you need. Assume you have another dependency `@bar`,
    which also depends on `@foo` but requires a different version of the data
    dependency.

    ```python
    ## WORKSPACE

    # Introduce @foo and @bar.
    ...

    load("@foo//:repositories.bzl", "data_deps")
    data_deps(version = "2.0")

    load("@bar//:repositories.bzl", "bar_deps")
    bar_deps() # -> which calls data_deps(version = "3.0")
    ```

    In this case, the end user must carefully adjust the order of macros in the
    WORKSPACE to get the version they need. This is one of the biggest pain
    points with WORKSPACE since it doesn't really provide a sensible way to
    resolve dependencies.

*   **Bzlmod**

    With Bzlmod, the author of project `foo` can use module extension to resolve
    conflicts. For example, let's assume it makes sense to always select the
    maximal required version of the data dependency among all Bazel modules.

    ```python
    ## extensions.bzl in foo
    load("//:repositories.bzl", "data_deps")

    data = tag_class(attrs={"version": attr.string()})

    def _data_deps_extension_impl(module_ctx):
        # Select the maximal required version in the dependency graph.
        version = "1.0"
        for mod in module_ctx.modules:
            for data in mod.tags.data:
                version = max(version, data.version)
        data_deps(version)

    data_deps_extension = module_extension(
        implementation = _data_deps_extension_impl,
        tag_classes = {"data": data},
    )
    ```

    ```python
    ## MODULE.bazel in bar
    bazel_dep(name = "foo", version = "1.0")

    foo_data_deps = use_extension("@foo//:extensions.bzl", "data_deps_extension")
    foo_data_deps.data(version = "3.0")
    use_repo(foo_data_deps, "data_file")
    ```

    ```python
    ## MODULE.bazel in root module
    bazel_dep(name = "foo", version = "1.0")
    bazel_dep(name = "bar", version = "1.0")

    foo_data_deps = use_extension("@foo//:extensions.bzl", "data_deps_extension")
    foo_data_deps.data(version = "2.0")
    use_repo(foo_data_deps, "data_file")
    ```

    In this case, the root module requires data version `2.0`, while its
    dependency `bar` requires `3.0`. The module extension in `foo` can correctly
    resolve this conflict and automatically select version `3.0` for the data
    dependency.

### Integrate third party package manager

Following the last section, since module extension provides a way to collect
information from the dependency graph, perform custom logic to resolve
dependencies and call repository rules to introduce external repositories, this
provides a great way for rules authors to enhance the rulesets that integrate
package managers for specific languages.

Please read the [module extensions](/external/extension) page to learn more
about how to use module extensions.

Here is a list of the rulesets that already adopted Bzlmod to fetch dependencies
from different package managers:

- [rules_jvm_external](https://github.com/bazelbuild/rules_jvm_external/blob/master/docs/bzlmod.md)
- [rules_go](https://github.com/bazelbuild/rules_go/blob/master/docs/go/core/bzlmod.md)
- [rules_python](https://github.com/bazelbuild/rules_python/blob/main/BZLMOD_SUPPORT.md)

A minimal example that integrates a pseudo package manager is available at the
[examples][pkg-mgr-example]
repository.

[pkg-mgr-example]: https://github.com/bazelbuild/examples/tree/main/bzlmod/05-integrate_third_party_package_manager

### Detect toolchains on the host machine

When Bazel build rules need to detect what toolchains are available on your host
machine, they use repository rules to inspect the host machine and generate
toolchain info as external repositories.

*   **WORKSPACE**

    Given the following repository rule to detect a shell toolchain.

    ```python
    ## local_config_sh.bzl
    def _sh_config_rule_impl(repository_ctx):
        sh_path = get_sh_path_from_env("SH_BIN_PATH")

        if not sh_path:
            sh_path = detect_sh_from_path()

        if not sh_path:
            sh_path = "/shell/binary/not/found"

        repository_ctx.file("BUILD", """
    load("@bazel_tools//tools/sh:sh_toolchain.bzl", "sh_toolchain")
    sh_toolchain(
        name = "local_sh",
        path = "{sh_path}",
        visibility = ["//visibility:public"],
    )
    toolchain(
        name = "local_sh_toolchain",
        toolchain = ":local_sh",
        toolchain_type = "@bazel_tools//tools/sh:toolchain_type",
    )
    """.format(sh_path = sh_path))

    sh_config_rule = repository_rule(
        environ = ["SH_BIN_PATH"],
        local = True,
        implementation = _sh_config_rule_impl,
    )
    ```

    You can load the repository rule in WORKSPACE.

    ```python
    ## WORKSPACE
    load("//:local_config_sh.bzl", "sh_config_rule")
    sh_config_rule(name = "local_config_sh")
    ```

*   **Bzlmod**

    With Bzlmod, you can introduce the same repository using a module extension,
    which is similar to introducing the `@data_file` repository in the last
    section.

    ```
    ## local_config_sh_extension.bzl
    load("//:local_config_sh.bzl", "sh_config_rule")

    sh_config_extension = module_extension(
        implementation = lambda ctx: sh_config_rule(name = "local_config_sh"),
    )
    ```

    Then use the extension in the MODULE.bazel file.

    ```python
    ## MODULE.bazel
    sh_config_ext = use_extension("//:local_config_sh_extension.bzl", "sh_config_extension")
    use_repo(sh_config_ext, "local_config_sh")
    ```

### Register toolchains & execution platforms

Following the last section, after introducing a repository hosting toolchain
information (e.g. `local_config_sh`), you probably want to register the
toolchain.

*   **WORKSPACE**

    With WORKSPACE, you can register the toolchain in the following ways.

    1.  You can register the toolchain the `.bzl` file and load the macro in the
    WORKSPACE file.

        ```python
        ## local_config_sh.bzl
        def sh_configure():
            sh_config_rule(name = "local_config_sh")
            native.register_toolchains("@local_config_sh//:local_sh_toolchain")
        ```

        ```python
        ## WORKSPACE
        load("//:local_config_sh.bzl", "sh_configure")
        sh_configure()
        ```

    2.  Or register the toolchain in the WORKSPACE file directly.

        ```python
        ## WORKSPACE
        load("//:local_config_sh.bzl", "sh_config_rule")
        sh_config_rule(name = "local_config_sh")
        register_toolchains("@local_config_sh//:local_sh_toolchain")
        ```

*   **Bzlmod**

    With Bzlmod, the
    [`register_toolchains`](/rules/lib/globals/module#register_toolchains) and
    [`register_execution_platforms`][register_execution_platforms]
    APIs are only available in the MODULE.bazel file. You cannot call
    `native.register_toolchains` in a module extension.

    ```python
    ## MODULE.bazel
    sh_config_ext = use_extension("//:local_config_sh_extension.bzl", "sh_config_extension")
    use_repo(sh_config_ext, "local_config_sh")
    register_toolchains("@local_config_sh//:local_sh_toolchain")
    ```

The toolchains and execution platforms registered in `WORKSPACE`,
`WORKSPACE.bzlmod` and each Bazel module's `MODULE.bazel` file follow this
order of precedence during toolchain selection (from highest to lowest):

1. toolchains and execution platforms registered in the root module's
   `MODULE.bazel` file.
2. toolchains and execution platforms registered in the `WORKSPACE` or
   `WORKSPACE.bzlmod` file.
3. toolchains and execution platforms registered by modules that are
   (transitive) dependencies of the root module.
4. when not using `WORKSPACE.bzlmod`: toolchains registered in the `WORKSPACE`
   [suffix](/external/migration#builtin-default-deps).

[register_execution_platforms]: /rules/lib/globals/module#register_execution_platforms

### Introduce local repositories

You may need to introduce a dependency as a local repository when you need a
local version of the dependency for debugging or you want to incorporate a
directory in your workspace as external repository.

*   **WORKSPACE**

    With WORKSPACE, this is achieved by two native repository rules,
    [`local_repository`](/reference/be/workspace#local_repository) and
    [`new_local_repository`](/reference/be/workspace#new_local_repository).

    ```python
    ## WORKSPACE
    local_repository(
        name = "rules_java",
        path = "/Users/bazel_user/workspace/rules_java",
    )
    ```

*   **Bzlmod**

    With Bzlmod, you can use
    [`local_path_override`](/rules/lib/globals/module#local_path_override) to
    override a module with a local path.

    ```python
    ## MODULE.bazel
    bazel_dep(name = "rules_java")
    local_path_override(
        module_name = "rules_java",
        path = "/Users/bazel_user/workspace/rules_java",
    )
    ```

    Note: With `local_path_override`, you can only introduce a local directory
    as a Bazel module, which means it should have a MODULE.bazel file and its
    transitive dependencies are taken into consideration during dependency
    resolution. In addition, all module override directives can only be used by
    the root module.

    It is also possible to introduce a local repository with module extension.
    However, you cannot call `native.local_repository` in module extension,
    there is ongoing effort on starlarkifying all native repository rules (check
    [#18285](https://github.com/bazelbuild/bazel/issues/18285) for progress).
    Then you can call the corresponding starlark `local_repository` in a module
    extension. It's also trivial to implement a custom version of
    `local_repository` repository rule if this is a blocking issue for you.

### Bind targets

The [`bind`](/reference/be/workspace#bind) rule in WORKSPACE is deprecated and
not supported in Bzlmod. It was introduced to give a target an alias in the
special `//external` package. All users depending on this should migrate away.

For example, if you have

```python
## WORKSPACE
bind(
    name = "openssl",
    actual = "@my-ssl//src:openssl-lib",
)
```

This allows other targets to depend on `//external:openssl`. You can migrate
away from this by:

*   Replace all usages of `//external:openssl` with `@my-ssl//src:openssl-lib`.
    *   Tip: Use `bazel query --output=build --noenable_bzlmod
        --enable_workspace [target]` command to find relevant info
        about the target.

*   Or use the [`alias`](/reference/be/general#alias) build rule
    *   Define the following target in a package (e.g. `//third_party`)

        ```python
        ## third_party/BUILD
        alias(
            name = "openssl",
            actual = "@my-ssl//src:openssl-lib",
        )
        ```

    *   Replace all usages of `//external:openssl` with `//third_party:openssl`.

### Fetch versus Sync

Fetch and sync commands are used to download external repos locally and keep
them updated. Sometimes also to allow building offline using the `--nofetch`
flag after fetching all repos needed for a build.

*   **WORKSPACE**

    Sync performs a force fetch for all repositories, or for a specific
    configured set of repos, while fetch is _only_ used to fetch for a specific
    target.

*   **Bzlmod**

    The sync command is no longer applicable, but fetch offers
    [various options](/reference/command-line-reference#fetch-options).
    You can fetch a target, a repository, a set of configured repos or all
    repositories involved in your dependency resolution and module extensions.
    The fetch result is cached and to force a fetch you must include the
 `--force` option during the fetch process.

## Manual migration

This section provides useful information and guidance for your **manual** Bzlmod
migration process. For more automatized migration process, check [recommended
migration process](#how-migrate-to-bzlmod) section.

### Know your dependencies in WORKSPACE

The first step of migration is to understand what dependencies you have. It
could be hard to figure out what exact dependencies are introduced in the
WORKSPACE file because transitive dependencies are often loaded with `*_deps`
macros.

#### Inspect external dependency with workspace resolved file

Fortunately, the flag
[`--experimental_repository_resolved_file`][resolved_file_flag]
can help. This flag essentially generates a "lock file" of all fetched external
dependencies in your last Bazel command. You can find more details in this [blog
post](https://blog.bazel.build/2018/07/09/bazel-sync-and-resolved-file.html).

[resolved_file_flag]: /reference/command-line-reference#flag--experimental_repository_resolved_file

It can be used in two ways:

1.  To fetch info of external dependencies needed for building certain targets.

    ```shell
    bazel clean --expunge
    bazel build --nobuild --experimental_repository_resolved_file=resolved.bzl //foo:bar
    ```

2.  To fetch info of all external dependencies defined in the WORKSPACE file.

    ```shell
    bazel clean --expunge
    bazel sync --experimental_repository_resolved_file=resolved.bzl
    ```

    With the `bazel sync` command, you can fetch all dependencies defined in the
    WORKSPACE file, which include:

    *   `bind` usages
    *   `register_toolchains` & `register_execution_platforms` usages

    However, if your project is cross platforms, bazel sync may break on certain
    platforms because some repository rules may only run correctly on supported
    platforms.

After running the command, you should have information of your external
dependencies in the `resolved.bzl` file.

#### Inspect external dependency with `bazel query`

You may also know `bazel query` can be used for inspecting repository rules with

```shell
bazel query --output=build //external:<repo name>
```

While it is more convenient and much faster, [bazel query can lie about
external dependency version](https://github.com/bazelbuild/bazel/issues/12947),
so be careful using it! Querying and inspecting external
dependencies with Bzlmod is going to achieved by a [new
subcommand](https://github.com/bazelbuild/bazel/issues/15365).

#### Built-in default dependencies

If you check the file generated by `--experimental_repository_resolved_file`,
you are going to find many dependencies that are not defined in your WORKSPACE.
This is because Bazel in fact adds prefixes and suffixes to the user's WORKSPACE
file content to inject some default dependencies, which are usually required by
native rules (e.g. `@bazel_tools`, `@platforms` and `@remote_java_tools`). With
Bzlmod, those dependencies are introduced with a built-in module
[`bazel_tools`][bazel_tools] , which is a default dependency for every other
Bazel module.

[bazel_tools]: https://github.com/bazelbuild/bazel/blob/master/src/MODULE.tools

### Hybrid mode for gradual migration

Bzlmod and WORKSPACE can work side by side, which allows migrating dependencies
from the WORKSPACE file to Bzlmod to be a gradual process.

Note: In practice, loading "*_deps" macros in WORKSPACE often causes confusions
with Bzlmod dependencies, therefore we recommend starting with a
WORKSPACE.bzlmod file and avoid loading transitive dependencies with macros.

#### WORKSPACE.bzlmod

During the migration, Bazel users may need to switch between builds with and
without Bzlmod enabled. WORKSPACE.bzlmod support is implemented to make the
process smoother.

WORKSPACE.bzlmod has the exact same syntax as WORKSPACE. When Bzlmod is enabled,
if a WORKSPACE.bzlmod file also exists at the workspace root:

*   `WORKSPACE.bzlmod` takes effect and the content of `WORKSPACE` is ignored.
*   No [prefixes or suffixes](/external/migration#builtin-default-deps) are
    added to the WORKSPACE.bzlmod file.

Using the WORKSPACE.bzlmod file can make the migration easier because:

*   When Bzlmod is disabled, you fall back to fetching dependencies from the
    original WORKSPACE file.
*   When Bzlmod is enabled, you can better track what dependencies are left to
    migrate with WORKSPACE.bzlmod.

#### Repository visibility

Bzlmod is able to control which other repositories are visible from a given
repository, check [repository names and strict
deps](/external/module#repository_names_and_strict_deps) for more details.

Here is a summary of repository visibilities from different types of
repositories when also taking WORKSPACE into consideration.

| | From the main repo | From Bazel module repos | From module extension repos | From WORKSPACE repos |
|----------------|--------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------|----------------------|
| The main repo  | Visible | If the root module is a direct dependency | If the root module is a direct dependency of the module hosting the module extension | Visible              |
| Bazel module repos | Direct deps | Direct deps | Direct deps of the module hosting the module extension | Direct deps of the root module |
| Module extension repos | Direct deps | Direct deps | Direct deps of the module hosting the module extension + all repos generated by the same module extension | Direct deps of the root module |
| WORKSPACE Repos | All visible | Not visible | Not visible | All visible |

Note: For the root module, if a repository `@foo` is defined in WORKSPACE and
`@foo` is also used as an [apparent repository
name](/external/overview#apparent-repo-name) in MODULE.bazel, then `@foo`
refers to the one introduced in MODULE.bazel.

Note: For a module extension generated repository `@bar`, if `@foo` is used as
an [apparent repository name](/external/overview#apparent-repo-name) of
another repository generated by the same module extension and direct
dependencies of the module hosting the module extension, then for repository
`@bar`, `@foo` refers to the latter.

### Manual migration process

A typical Bzlmod migration process can look like this:

1.  Understand what dependencies you have in WORKSPACE.
1.  Add an empty MODULE.bazel file at your project root.
1.  Add an empty WORKSPACE.bzlmod file to override the WORKSPACE file content.
1.  Build your targets with Bzlmod enabled and check which repository is
    missing.
1.  Check the definition of the missing repository in the resolved dependency
    file.
1.  Introduce the missing dependency as a Bazel module, through a module
    extension, or leave it in the WORKSPACE.bzlmod for later migration.
1.  Go back to 4 and repeat until all dependencies are available.

## Publish Bazel modules

If your Bazel project is a dependency for other projects, you can publish your
project in the [Bazel Central Registry](https://registry.bazel.build/).

To be able to check in your project in the BCR, you need a source archive URL of
the project. Take note of a few things when creating the source archive:

*   **Make sure the archive is pointing to a specific version.**

    The BCR can only accept versioned source archives because Bzlmod needs to
    conduct version comparison during dependency resolution.

*   **Make sure the archive URL is stable.**

    Bazel verifies the content of the archive by a hash value, so you should
    make sure the checksum of the downloaded file never changes. If the URL is
    from GitHub, please create and upload a release archive in the release page.
    GitHub isn't going to guarantee the checksum of source archives generated on
    demand. In short, URLs in the form of
    `https://github.com/<org>/<repo>/releases/download/...` is considered stable
    while `https://github.com/<org>/<repo>/archive/...` is not. Check [GitHub
    Archive Checksum
    Outage](https://blog.bazel.build/2023/02/15/github-archive-checksum.html)
    for more context.

*   **Make sure the source tree follows the layout of the original repository.**

    In case your repository is very large and you want to create a distribution
    archive with reduced size by stripping out unnecessary sources, please make
    sure the stripped source tree is a subset of the original source tree. This
    makes it easier for end users to override the module to a non-release
    version by [`archive_override`](/rules/lib/globals/module#archive_override)
    and [`git_override`](/rules/lib/globals/module#git_override).

*   **Include a test module in a subdirectory that tests your most common
    APIs.**

    A test module is a Bazel project with its own WORKSPACE and MODULE.bazel
    file located in a subdirectory of the source archive which depends on the
    actual module to be published. It should contain examples or some
    integration tests that cover your most common APIs. Check
    [test module][test_module] to learn how to set it up.

[test_module]: https://github.com/bazelbuild/bazel-central-registry/tree/main/docs#test-module

When you have your source archive URL ready, follow the [BCR contribution
guidelines][bcr_contrib_guide] to submit your module to the BCR with a GitHub
Pull Request.

[bcr_contrib_guide]: https://github.com/bazelbuild/bazel-central-registry/tree/main/docs#contribute-a-bazel-module

It is **highly recommended** to set up the [Publish to
BCR](https://github.com/bazel-contrib/publish-to-bcr) GitHub App for your
repository to automate the process of submitting your module to the BCR.

## Best practices

This section documents a few best practices you should follow for better
managing your external dependencies.

#### Split targets into different packages to avoid fetching unnecessary dependencies.

Check [#12835](https://github.com/bazelbuild/bazel/issues/12835), where dev
dependencies for tests are forced to be fetched unnecessarily for building
targets that don't need them. This is actually not Bzlmod specific, but
following this practices makes it easier to specify dev dependencies correctly.

#### Specify dev dependencies

You can set the `dev_dependency` attribute to true for
[`bazel_dep`](/rules/lib/globals/module#bazel_dep) and
[`use_extension`](/rules/lib/globals/module#use_extension) directives so that
they don't propagate to dependent projects. As the root module, you can use the
[`--ignore_dev_dependency`][ignore_dev_dep_flag] flag to verify if your targets
still build without dev dependencies and overrides.

[ignore_dev_dep_flag]: /reference/command-line-reference#flag--ignore_dev_dependency



## Community migration progress

You can check the [Bazel Central Registry](https://registry.bazel.build) to find
out if your dependencies are already available. Otherwise feel free to join this
[GitHub discussion](https://github.com/bazelbuild/bazel/discussions/18329) to
upvote or post the dependencies that are blocking your migration.

## Report issues

Please check the [Bazel GitHub issue list][bzlmod_github_issue] for known Bzlmod
issues. Feel free to file new issues or feature requests that can help unblock
your migration!

[bzlmod_github_issue]: https://github.com/bazelbuild/bazel/issues?q=is%3Aopen+is%3Aissue+label%3Aarea-Bzlmod

---

## Bazel modules
- URL: https://bazel.build/external/module
- Source: external/module.mdx
- Slug: /external/module

A Bazel **module** is a Bazel project that can have multiple versions, each of
which publishes metadata about other modules that it depends on. This is
analogous to familiar concepts in other dependency management systems, such as a
Maven *artifact*, an npm *package*, a Go *module*, or a Cargo *crate*.

A module must have a `MODULE.bazel` file at its repo root. This file is the
module's manifest, declaring its name, version, list of direct dependencies, and
other information. For a basic example:

```python
module(name = "my-module", version = "1.0")

bazel_dep(name = "rules_cc", version = "0.0.1")
bazel_dep(name = "protobuf", version = "3.19.0")
```

See the [full list](/rules/lib/globals/module) of directives available in
`MODULE.bazel` files.

To perform module resolution, Bazel starts by reading the root module's
`MODULE.bazel` file, and then repeatedly requests any dependency's
`MODULE.bazel` file from a [Bazel registry](/external/registry) until it
discovers the entire dependency graph.

By default, Bazel then [selects](#version-selection) one version of each module
to use. Bazel represents each module with a repo, and consults the registry
again to learn how to define each of the repos.

## Version format

Bazel has a diverse ecosystem and projects use various versioning schemes. The
most popular by far is [SemVer](https://semver.org), but there are
also prominent projects using different schemes such as
[Abseil](https://github.com/abseil/abseil-cpp/releases), whose
versions are date-based, for example `20210324.2`).

For this reason, Bazel adopts a more relaxed version of the SemVer spec. The
differences include:

*   SemVer prescribes that the "release" part of the version must consist of 3
    segments: `MAJOR.MINOR.PATCH`. In Bazel, this requirement is loosened so
    that any number of segments is allowed.
*   In SemVer, each of the segments in the "release" part must be digits only.
    In Bazel, this is loosened to allow letters too, and the comparison
    semantics match the "identifiers" in the "prerelease" part.
*   Additionally, the semantics of major, minor, and patch version increases are
    not enforced. However, see [compatibility level](#compatibility_level) for
    details on how we denote backwards compatibility.

Any valid SemVer version is a valid Bazel module version. Additionally, two
SemVer versions `a` and `b` compare `a < b` if and only if the same holds when
they're compared as Bazel module versions.

Finally, to learn more about module versioning, [see the `MODULE.bazel`
FAQ](faq#module-versioning-best-practices).

## Version selection

Consider the diamond dependency problem, a staple in the versioned dependency
management space. Suppose you have the dependency graph:

```
       A 1.0
      /     \
   B 1.0    C 1.1
     |        |
   D 1.0    D 1.1
```

Which version of `D` should be used? To resolve this question, Bazel uses the
[Minimal Version Selection](https://research.swtch.com/vgo-mvs)
(MVS) algorithm introduced in the Go module system. MVS assumes that all new
versions of a module are backwards compatible, and so picks the highest version
specified by any dependent (`D 1.1` in our example). It's called "minimal"
because `D 1.1` is the earliest version that could satisfy our requirements —
even if `D 1.2` or newer exists, we don't select them. Using MVS creates a
version selection process that is *high-fidelity* and *reproducible*.

### Yanked versions

The registry can declare certain versions as *yanked* if they should be avoided
(such as for security vulnerabilities). Bazel throws an error when selecting a
yanked version of a module. To fix this error, either upgrade to a newer,
non-yanked version, or use the
[`--allow_yanked_versions`](/reference/command-line-reference#flag--allow_yanked_versions)
flag to explicitly allow the yanked version.

## Compatibility level

In Go, MVS's assumption about backwards compatibility works because it treats
backwards incompatible versions of a module as a separate module. In terms of
SemVer, that means `A 1.x` and `A 2.x` are considered distinct modules, and can
coexist in the resolved dependency graph. This is, in turn, made possible by
encoding the major version in the package path in Go, so there aren't any
compile-time or linking-time conflicts. Bazel, however, cannot provide such
guarantees because it follows [a relaxed version of SemVer](#version-format).

Thus, Bazel needs the equivalent of the SemVer major version number to detect
backwards incompatible ("breaking") versions. This number is called the
*compatibility level*, and is specified by each module version in its
[`module()`](/rule/lib/globals/module#module) directive. With this information,
Bazel can throw an error if it detects that versions of the _same module_ with
_different compatibility levels_ exist in the resolved dependency graph.

Finally, incrementing the compatibility level can be disruptive to the users.
To learn more about when and how to increment it, [check the `MODULE.bazel`
FAQ](faq#incrementing-compatibility-level).

## Overrides

Specify overrides in the `MODULE.bazel` file to alter the behavior of Bazel
module resolution. Only the root module's overrides take effect — if a module is
used as a dependency, its overrides are ignored.

Each override is specified for a certain module name, affecting all of its
versions in the dependency graph. Although only the root module's overrides take
effect, they can be for transitive dependencies that the root module does not
directly depend on.

### Single-version override

The [`single_version_override`](/rules/lib/globals/module#single_version_override)
serves multiple purposes:

*   With the `version` attribute, you can pin a dependency to a specific
    version, regardless of which versions of the dependency are requested in the
    dependency graph.
*   With the `registry` attribute, you can force this dependency to come from a
    specific registry, instead of following the normal [registry
    selection](/external/registry#selecting_registries) process.
*   With the `patch*` attributes, you can specify a set of patches to apply to
    the downloaded module.

These attributes are all optional and can be mixed and matched with each other.

### Multiple-version override

A [`multiple_version_override`](/rules/lib/globals/module#multiple_version_override)
can be specified to allow multiple versions of the same module to coexist in the
resolved dependency graph.

You can specify an explicit list of allowed versions for the module, which must
all be present in the dependency graph before resolution — there must exist
*some* transitive dependency depending on each allowed version. After
resolution, only the allowed versions of the module remain, while Bazel upgrades
other versions of the module to the nearest higher allowed version at the same
compatibility level. If no higher allowed version at the same compatibility
level exists, Bazel throws an error.

For example, if versions `1.1`, `1.3`, `1.5`, `1.7`, and `2.0` exist in the
dependency graph before resolution and the major version is the compatibility
level:

*   A multiple-version override allowing `1.3`, `1.7`, and `2.0` results in
    `1.1` being upgraded to `1.3`, `1.5` being upgraded to `1.7`, and other
    versions remaining the same.
*   A multiple-version override allowing `1.5` and `2.0` results in an error, as
    `1.7` has no higher version at the same compatibility level to upgrade to.
*   A multiple-version override allowing `1.9` and `2.0` results in an error, as
    `1.9` is not present in the dependency graph before resolution.

Additionally, users can also override the registry using the `registry`
attribute, similarly to single-version overrides.

### Non-registry overrides

Non-registry overrides completely remove a module from version resolution. Bazel
does not request these `MODULE.bazel` files from a registry, but instead from
the repo itself.

Bazel supports the following non-registry overrides:

*   [`archive_override`](/rules/lib/globals/module#archive_override)
*   [`git_override`](/rules/lib/globals/module#git_override)
*   [`local_path_override`](/rules/lib/globals/module#local_path_override)

Note that setting a version value in the source archive `MODULE.bazel` can have
downsides when the module is being overridden with a non-registry override. To
learn more about this [see the `MODULE.bazel`
FAQ](faq#module-versioning-best-practices).

## Define repos that don't represent Bazel modules

With `bazel_dep`, you can define repos that represent other Bazel modules.
Sometimes there is a need to define a repo that does _not_ represent a Bazel
module; for example, one that contains a plain JSON file to be read as data.

In this case, you could use the [`use_repo_rule`
directive](/rules/lib/globals/module#use_repo_rule) to directly define a repo
by invoking a repo rule. This repo will only be visible to the module it's
defined in.

Under the hood, this is implemented using the same mechanism as [module
extensions](/external/extension), which lets you define repos with more
flexibility.

## Repository names and strict deps

The [apparent name](/external/overview#apparent-repo-name) of a repo backing a
module to its direct dependents defaults to its module name, unless the
`repo_name` attribute of the [`bazel_dep`](/rules/lib/globals/module#bazel_dep)
directive says otherwise. Note that this means a module can only find its direct
dependencies. This helps prevent accidental breakages due to changes in
transitive dependencies.

The [canonical name](/external/overview#canonical-repo-name) of a repo backing a
module is either `<var>module_name</var>+<var>version{{
"</var>" }}` (for example, `bazel_skylib+1.0.3`) or `<var>module_name{{
"</var>" }}+` (for example, `bazel_features+`), depending on whether there are
multiple versions of the module in the entire dependency graph (see
[`multiple_version_override`](/rules/lib/globals/module#multiple_version_override)).
Note that **the canonical name format** is not an API you should depend on and
**is subject to change at any time**. Instead of hard-coding the canonical name,
use a supported way to get it directly from Bazel:

*    In BUILD and `.bzl` files, use
     [`Label.repo_name`](/rules/lib/builtins/Label#repo_name) on a `Label` instance
     constructed from a label string given by the apparent name of the repo, e.g.,
     `Label("@bazel_skylib").repo_name`.
*    When looking up runfiles, use
     [`$(rlocationpath ...)`](https://bazel.build/reference/be/make-variables#predefined_label_variables)
     or one of the runfiles libraries in
     `@bazel_tools//tools/{bash,cpp,java}/runfiles` or, for a ruleset `rules_foo`,
     in `@rules_foo//foo/runfiles`.
*    When interacting with Bazel from an external tool such as an IDE or language
     server, use the `bazel mod dump_repo_mapping` command to get the mapping from
     apparent names to canonical names for a given set of repositories.

[Module extensions](/external/extension) can also introduce additional repos
into the visible scope of a module.

---

## External dependencies overview
- URL: https://bazel.build/external/overview
- Source: external/overview.mdx
- Slug: /external/overview

Bazel supports *external dependencies*, source files (both text and binary) used
in your build that are not from your workspace. For example, they could be a
ruleset hosted in a GitHub repo, a Maven artifact, or a directory on your local
machine outside your current workspace.

This document gives an overview of the system before examining some of the
concepts in more detail.

## Overview of the system

Bazel's external dependency system works on the basis of [*Bazel
modules*](#module), each of which is a versioned Bazel project, and
[*repositories*](#repository) (or repos), which are directory trees containing
source files.

Bazel starts from the root module -- that is, the project you're working on.
Like all modules, it needs to have a `MODULE.bazel` file at its directory root,
declaring its basic metadata and direct dependencies. The following is a basic
example:

```python
module(name = "my-module", version = "1.0")

bazel_dep(name = "rules_cc", version = "0.1.1")
bazel_dep(name = "platforms", version = "0.0.11")
```

From there, Bazel looks up all transitive dependency modules in a
[*Bazel registry*](registry) — by default, the [Bazel Central
Registry](https://bcr.bazel.build/). The registry provides the
dependencies' `MODULE.bazel` files, which allows Bazel to discover the entire
transitive dependency graph before performing version resolution.

After version resolution, in which one version is selected for each module,
Bazel consults the registry again to learn how to define a repo for each module
-- that is, how the sources for each dependency module should be fetched. Most
of the time, these are just archives downloaded from the internet and extracted.

Modules can also specify customized pieces of data called *tags*, which are
consumed by [*module extensions*](extension) after module resolution
to define additional repos. These extensions can perform actions like file I/O
and sending network requests. Among other things, they allow Bazel to interact
with other package management systems while also respecting the dependency graph
built out of Bazel modules.

The three kinds of repos -- the main repo (which is the source tree you're
working in), the repos representing transitive dependency modules, and the repos
created by module extensions -- form the [*workspace*](#workspace) together.
External repos (non-main repos) are fetched on demand, for example when they're
referred to by labels (like `@repo//pkg:target`) in BUILD files.

## Benefits

Bazel's external dependency system offers a wide range of benefits.

### Automatic Dependency Resolution

-   **Deterministic Version Resolution**: Bazel adopts the deterministic
    [MVS](module#version-selection) version resolution algorithm,
    minimizing conflicts and addressing diamond dependency issues.
-   **Simplified Dependency Management**: `MODULE.bazel` declares only direct
    dependencies, while transitive dependencies are automatically resolved,
    providing a clearer overview of the project's dependencies.
-   **[Strict Dependency visibility](module#repository_names_and_strict_deps)**:
    Only direct dependencies are visible, ensuring correctness and
    predictability.

### Ecosystem Integration

-   **[Bazel Central Registry](https://registry.bazel.build/)**: A centralized
    repository for discovering and managing common dependencies as Bazel
    modules.
-   **Adoption of Non-Bazel Projects**: When a non-Bazel project (usually a C++
    library) is adapted for Bazel and made available in BCR, it streamlines its
    integration for the whole community and eliminates duplicated effort and
    conflicts of custom BUILD files.
-   **Unified Integration with Language-Specific Package Managers**: Rulesets
    streamline integration with external package managers for non-Bazel
    dependencies, including:
    *   [rules_jvm_external](https://github.com/bazel-contrib/rules_jvm_external/blob/master/docs/bzlmod.md)
        for Maven,
    *   [rules_python](https://rules-python.readthedocs.io/en/latest/pypi-dependencies.html#using-bzlmod)
        for PyPi,
    *   [bazel-gazelle](https://github.com/bazel-contrib/rules_go/blob/master/docs/go/core/bzlmod.md#external-dependencies)
        for Go Modules,
    *   [rules_rust](https://bazelbuild.github.io/rules_rust/crate_universe_bzlmod.html)
        for Cargo.

### Advanced Features

-   **[Module Extensions](extension)**: The
    [`use_repo_rule`](/rules/lib/globals/module#use_repo_rule) and module
    extension features allow flexible use of custom repository rules and
    resolution logic to introduce any non-Bazel dependencies.
-   **[`bazel mod` Command](mod-command)**: The sub-command offers
    powerful ways to inspect external dependencies. You know exactly how an
    external dependency is defined and where it comes from.
-   **[Vendor Mode](vendor)**: Pre-fetch the exact external dependencies you
    need to facilitate offline builds.
-   **[Lockfile](lockfile)**: The lockfile improves build reproducibility and
    accelerates dependency resolution.
-   **(Upcoming) [BCR Provenance
    Attestations](https://github.com/bazelbuild/bazel-central-registry/discussions/2721)**:
    Strengthen supply chain security by ensuring verified provenance of
    dependencies.

## Concepts

This section gives more detail on concepts related to external dependencies.

### Module

A Bazel project that can have multiple versions, each of which can have
dependencies on other modules.

In a local Bazel workspace, a module is represented by a repository.

For more details, see [Bazel modules](module).

### Repository

A directory tree with a boundary marker file at its root, containing source
files that can be used in a Bazel build. Often shortened to just **repo**.

A repo boundary marker file can be `MODULE.bazel` (signaling that this repo
represents a Bazel module), `REPO.bazel` (see [below](#repo.bazel)), or in
legacy contexts, `WORKSPACE` or `WORKSPACE.bazel`. Any repo boundary marker file
will signify the boundary of a repo; multiple such files can coexist in a
directory.

### Main repository

The repository in which the current Bazel command is being run.

The root of the main repository is also known as the
<span id="#workspace-root">**workspace root**</span>.

### Workspace

The environment shared by all Bazel commands run in the same main repository. It
encompasses the main repo and the set of all defined external repos.

Note that historically the concepts of "repository" and "workspace" have been
conflated; the term "workspace" has often been used to refer to the main
repository, and sometimes even used as a synonym of "repository".

### Canonical repository name

The name by which a repository is always addressable. Within the context of a
workspace, each repository has a single canonical name. A target inside a repo
whose canonical name is `canonical_name` can be addressed by the label
`@@canonical_name//package:target` (note the double `@`).

The main repository always has the empty string as the canonical name.

### Apparent repository name

The name by which a repository is addressable in the context of a certain other
repo. This can be thought of as a repo's "nickname": The repo with the canonical
name `michael` might have the apparent name `mike` in the context of the repo
`alice`, but might have the apparent name `mickey` in the context of the repo
`bob`. In this case, a target inside `michael` can be addressed by the label
`@mike//package:target` in the context of `alice` (note the single `@`).

Conversely, this can be understood as a **repository mapping**: each repo
maintains a mapping from "apparent repo name" to a "canonical repo name".

### Repository rule

A schema for repository definitions that tells Bazel how to materialize a
repository. For example, it could be "download a zip archive from a certain URL
and extract it", or "fetch a certain Maven artifact and make it available as a
`java_import` target", or simply "symlink a local directory". Every repo is
**defined** by calling a repo rule with an appropriate number of arguments.

See [Repository rules](repo) for more information about how to write
your own repository rules.

The most common repo rules by far are
[`http_archive`](/rules/lib/repo/http#http_archive), which downloads an archive
from a URL and extracts it, and
[`local_repository`](/reference/be/workspace#local_repository), which symlinks a
local directory that is already a Bazel repository.

### Fetch a repository

The action of making a repo available on local disk by running its associated
repo rule. The repos defined in a workspace are not available on local disk
before they are fetched.

Normally, Bazel only fetches a repo when it needs something from the repo,
and the repo hasn't already been fetched. If the repo has already been fetched
before, Bazel only re-fetches it if its definition has changed.

The `fetch` command can be used to initiate a pre-fetch for a repository,
target, or all necessary repositories to perform any build. This capability
enables offline builds using the `--nofetch` option.

The `--fetch` option serves to manage network access. Its default value is true.
However, when set to false (`--nofetch`), the command will utilize any cached
version of the dependency, and if none exists, the command will result in
failure.

See [fetch options](/reference/command-line-reference#fetch-options) for more
information about controlling fetch.

### Directory layout

After being fetched, the repo can be found in the subdirectory `external` in the
[output base](/remote/output-directories), under its canonical name.

You can run the following command to see the contents of the repo with the
canonical name `canonical_name`:

```posix-terminal
ls $(bazel info output_base)/external/{{ '<var>' }} canonical_name {{ '</var>' }}
```

### REPO.bazel file

The [`REPO.bazel`](/rules/lib/globals/repo) file is used to mark the topmost
boundary of the directory tree that constitutes a repo. It doesn't need to
contain anything to serve as a repo boundary file; however, it can also be used
to specify some common attributes for all build targets inside the repo.

The syntax of a `REPO.bazel` file is similar to `BUILD` files, except that no
`load` statements are supported. The `repo()` function takes the same arguments as the [`package()`
function](/reference/be/functions#package) in `BUILD` files; whereas `package()`
specifies common attributes for all build targets inside the package, `repo()`
analogously does so for all build targets inside the repo.

For example, you can specify a common license for all targets in your repo by
having the following `REPO.bazel` file:

```python
repo(
    default_package_metadata = ["//:my_license"],
)
```

## The legacy WORKSPACE system

In older Bazel versions (before 9.0), external dependencies were introduced by
defining repos in the `WORKSPACE` (or `WORKSPACE.bazel`) file. This file has a
similar syntax to `BUILD` files, employing repo rules instead of build rules.

The following snippet is an example to use the `http_archive` repo rule in the
`WORKSPACE` file:

```python
load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")
http_archive(
    name = "foo",
    urls = ["https://example.com/foo.zip"],
    sha256 = "c9526390a7cd420fdcec2988b4f3626fe9c5b51e2959f685e8f4d170d1a9bd96",
)
```

The snippet defines a repo whose canonical name is `foo`. In the `WORKSPACE`
system, by default, the canonical name of a repo is also its apparent name to
all other repos.

See the [full list](/rules/lib/globals/workspace) of functions available in
`WORKSPACE` files.

### Shortcomings of the `WORKSPACE` system

In the years after the `WORKSPACE` system was introduced, users reported many
pain points, including:

*   Bazel does not evaluate the `WORKSPACE` files of any dependencies, so all
    transitive dependencies must be defined in the `WORKSPACE` file of the main
    repo, in addition to direct dependencies.
*   To work around this, projects have adopted the "deps.bzl" pattern, in which
    they define a macro which in turn defines multiple repos, and ask users to
    call this macro in their `WORKSPACE` files.
    *   This has its own problems: macros cannot `load` other `.bzl` files, so
        these projects have to define their transitive dependencies in this
        "deps" macro, or work around this issue by having the user call multiple
        layered "deps" macros.
    *   Bazel evaluates the `WORKSPACE` file sequentially. Additionally,
        dependencies are specified using `http_archive` with URLs, without any
        version information. This means that there is no reliable way to perform
        version resolution in the case of diamond dependencies (`A` depends on
        `B` and `C`; `B` and `C` both depend on different versions of `D`).

Due to the shortcomings of WORKSPACE, the new module-based system (codenamed
"Bzlmod") gradually replaced the legacy WORKSPACE system between Bazel 6 and 9.
Read the [Bzlmod migration guide](migration) on how to migrate
to Bzlmod.

### External links on Bzlmod

*   [Bzlmod usage examples in bazelbuild/examples](https://github.com/bazelbuild/examples/tree/main/bzlmod)
*   [Bazel External Dependencies Overhaul](https://docs.google.com/document/d/1moQfNcEIttsk6vYanNKIy3ZuK53hQUFq1b1r0rmsYVg/edit)
    (original Bzlmod design doc)
*   [BazelCon 2021 talk on Bzlmod](https://www.youtube.com/watch?v=TxOCKtU39Fs)
*   [Bazel Community Day talk on Bzlmod](https://www.youtube.com/watch?v=MB6xxis9gWI)

---

## Repository Rules
- URL: https://bazel.build/external/repo
- Source: external/repo.mdx
- Slug: /external/repo

This page covers how to define repository rules and provides examples for more
details.

An [external repository](/external/overview#repository) is a directory tree,
containing source files usable in a Bazel build, which is generated on demand by
running its corresponding **repo rule**. Repos can be defined in a multitude of
ways, but ultimately, each repo is defined by invoking a repo rule, just as
build targets are defined by invoking build rules. They can be used to depend on
third-party libraries (such as Maven packaged libraries) but also to generate
`BUILD` files specific to the host Bazel is running on.

## Repository rule definition

In a `.bzl` file, use the
[repository_rule](/rules/lib/globals/bzl#repository_rule) function to define a
new repo rule and store it in a global variable. After a repo rule is defined,
it can be invoked as a function to define repos. This invocation is usually
performed from inside a [module extension](/external/extension) implementation
function.

The two major components of a repo rule definition are its attribute schema and
implementation function. The attribute schema determines the names and types of
attributes passed to a repo rule invocation, and the implementation function is
run when the repo needs to be fetched.

## Attributes

Attributes are arguments passed to the repo rule invocation. The schema of
attributes accepted by a repo rule is specified using the `attrs` argument when
the repo rule is defined with a call to `repository_rule`. An example defining
`url` and `sha256` attributes as strings:

```python
http_archive = repository_rule(
    implementation=_impl,
    attrs={
        "url": attr.string(mandatory=True),
        "sha256": attr.string(mandatory=True),
    }
)
```

To access an attribute within the implementation function, use
`repository_ctx.attr.<attribute_name>`:

```python
def _impl(repository_ctx):
    url = repository_ctx.attr.url
    checksum = repository_ctx.attr.sha256
```

All `repository_rule`s have the implicitly defined attribute `name`. This is a
string attribute that behaves somewhat magically: when specified as an input to
a repo rule invocation, it takes an apparent repo name; but when read from the
repo rule's implementation function using `repository_ctx.attr.name`, it returns
the canonical repo name.

## Implementation function

Every repo rule requires an `implementation` function. It contains the actual
logic of the rule and is executed strictly in the Loading Phase.

The function has exactly one input parameter, `repository_ctx`. The function
returns either `None` to signify that the rule is reproducible given the
specified parameters, or a dict with a set of parameters for that rule that
would turn that rule into a reproducible one generating the same repo. For
example, for a rule tracking a git repository that would mean returning a
specific commit identifier instead of a floating branch that was originally
specified.

The input parameter `repository_ctx` can be used to access attribute values, and
non-hermetic functions (finding a binary, executing a binary, creating a file in
the repository or downloading a file from the Internet). See [the API
docs](/rules/lib/builtins/repository_ctx) for more context. Example:

```python
def _impl(repository_ctx):
  repository_ctx.symlink(repository_ctx.attr.path, "")

local_repository = repository_rule(
    implementation=_impl,
    ...)
```

## When is the implementation function executed?

The implementation function of a repo rule is executed when Bazel needs a target
from that repository, for example when another target (in another repo) depends
on it or if it is mentioned on the command line. The implementation function is
then expected to create the repo in the file system. This is called "fetching"
the repo.

In contrast to regular targets, repos are not necessarily re-fetched when
something changes that would cause the repo to be different. This is because
there are things that Bazel either cannot detect changes to or it would cause
too much overhead on every build (for example, things that are fetched from the
network). Therefore, repos are re-fetched only if one of the following things
changes:

*   The attributes passed to the repo rule invocation.
*   The Starlark code comprising the implementation of the repo rule.
*   The value of any environment variable passed to `repository_ctx`'s
    `getenv()` method or declared with the `environ` attribute of the
    [`repository_rule`](/rules/lib/globals/bzl#repository_rule). The values of
    these environment variables can be hard-wired on the command line with the
    [`--repo_env`](/reference/command-line-reference#flag--repo_env) flag.
*   The existence, contents, and type of any paths being
    [`watch`ed](/rules/lib/builtins/repository_ctx#watch) in the implementation
    function of the repo rule.
    *   Certain other methods of `repository_ctx` with a `watch` parameter, such
        as `read()`, `execute()`, and `extract()`, can also cause paths to be
        watched.
    *   Similarly, [`repository_ctx.watch_tree`](/rules/lib/builtins/repository_ctx#watch_tree)
        and [`path.readdir`](/rules/lib/builtins/path#readdir) can cause paths
        to be watched in other ways.
*   When `bazel fetch --force` is executed.

There are two parameters of `repository_rule` that control when the repositories
are re-fetched:

*   If the `configure` flag is set, the repository is re-fetched on `bazel
    fetch --force --configure` (non-`configure` repositories are not
    re-fetched).
*   If the `local` flag is set, in addition to the above cases, the repo is also
    re-fetched when the Bazel server restarts.

## Forcing refetch of external repos

Sometimes, an external repo can become outdated without any change to its
definition or dependencies. For example, a repo fetching sources might follow a
particular branch of a third-party repository, and new commits are available on
that branch. In this case, you can ask bazel to refetch all external repos
unconditionally by calling `bazel fetch --force --all`.

Moreover, some repo rules inspect the local machine and might become outdated if
the local machine was upgraded. Here you can ask Bazel to only refetch those
external repos where the [`repository_rule`](/rules/lib/globals#repository_rule)
definition has the `configure` attribute set, use `bazel fetch --force
--configure`.

## Examples

-   [C++ auto-configured
    toolchain](https://cs.opensource.google/bazel/bazel/+/master:tools/cpp/cc_configure.bzl;drc=644b7d41748e09eff9e47cbab2be2263bb71f29a;l=176):
    it uses a repo rule to automatically create the C++ configuration files for
    Bazel by looking for the local C++ compiler, the environment and the flags
    the C++ compiler supports.

-   [Go repositories](https://github.com/bazelbuild/rules_go/blob/67bc217b6210a0922d76d252472b87e9a6118fdf/go/private/go_repositories.bzl#L195)
    uses several `repository_rule` to defines the list of dependencies needed to
    use the Go rules.

-   [rules_jvm_external](https://github.com/bazelbuild/rules_jvm_external)
    creates an external repository called `@maven` by default that generates
    build targets for every Maven artifact in the transitive dependency tree.

---

## Vendor
- URL: https://bazel.build/external/vendor
- Source: external/vendor.mdx
- Slug: /external/vendor

keywords: product:Bazel,Bzlmod,vendor
---
title: 'Vendor Mode'
---



Vendor mode is a feature that lets you create a local copy of
external dependencies. This is useful for offline builds, or when you want to
control the source of an external dependency.

## Enable vendor mode

You can enable vendor mode by specifying `--vendor_dir` flag.

For example, by adding it to your `.bazelrc` file:

```none
# Enable vendor mode with vendor directory under <workspace>/vendor_src
common --vendor_dir=vendor_src
```

The vendor directory can be either a relative path to your workspace root or an
absolute path.

## Vendor a specific external repository

You can use the `vendor` command with the `--repo` flag to specify which repo
to vendor, it accepts both [canonical repo
name](/external/overview#canonical-repo-name) and [apparent repo
name](/external/overview#apparent-repo-name).

For example, running:

```none
bazel vendor --vendor_dir=vendor_src --repo=@rules_cc
```

or

```none
bazel vendor --vendor_dir=vendor_src --repo=@@rules_cc+
```

will both get rules_cc to be vendored under
`<workspace root>/vendor_src/rules_cc+`.

## Vendor external dependencies for given targets

To vendor all external dependencies required for building given target patterns,
you can run `bazel vendor <target patterns>`.

For example

```none
bazel vendor --vendor_dir=vendor_src //src/main:hello-world //src/test/...
```

will vendor all repos required for building the `//src/main:hello-world` target
and all targets under `//src/test/...` with the current configuration.

Under the hood, it's doing a `bazel build --nobuild` command to analyze the
target patterns, therefore build flags could be applied to this command and
affect the result.

### Build the target offline

With the external dependencies vendored, you can build the target offline by

```none
bazel build --vendor_dir=vendor_src //src/main:hello-world //src/test/...
```

The build should work in a clean build environment without network access and
repository cache.

Therefore, you should be able to check in the vendored source and build the same
targets offline on another machine.

Note: If you make changes to the targets to build, the external dependencies,
the build configuration, or the Bazel version, you may need to re-vendor to make
sure offline build still works.

## Vendor all external dependencies

To vendor all repos in your transitive external dependencies graph, you can
run:

```none
bazel vendor --vendor_dir=vendor_src
```

Note that vendoring all dependencies has a few **disadvantages**:

-   Fetching all repos, including those introduced transitively, can be time-consuming.
-   The vendor directory can become very large.
-   Some repos may fail to fetch if they are not compatible with the current platform or environment.

Therefore, consider vendoring for specific targets first.

## Configure vendor mode with VENDOR.bazel

You can control how given repos are handled with the VENDOR.bazel file located
under the vendor directory.

There are two directives available, both accepting a list of
[canonical repo names](/external/overview#canonical-repo-name) as arguments:

- `ignore()`: to completely ignore a repository from vendor mode.
- `pin()`: to pin a repository to its current vendored source as if there is a
  `--override_repository` flag for this repo. Bazel will NOT update the vendored
  source for this repo while running the vendor command unless it's unpinned.
  The user can modify and maintain the vendored source for this repo manually.

For example

```python
ignore("@@rules_cc+")
pin("@@bazel_skylib+")
```

With this configuration

-   Both repos will be excluded from subsequent vendor commands.
-   Repo `bazel_skylib` will be overridden to the source located under the
    vendor directory.
-   The user can safely modify the vendored source of `bazel_skylib`.
-   To re-vendor `bazel_skylib`, the user has to disable the pin statement
    first.

Note: Repository rules with
[`local`](/rules/lib/globals/bzl#repository_rule.local) or
[`configure`](/rules/lib/globals/bzl#repository_rule.configure) set to true are
always excluded from vendoring.

## Understand how vendor mode works

Bazel fetches external dependencies of a project under `$(bazel info
output_base)/external`. Vendoring external dependencies means moving out
relevant files and directories to the given vendor directory and use the
vendored source for later builds.

The content being vendored includes:

-   The repo directory
-   The repo marker file

During a build, if the vendored marker file is up-to-date or the repo is
pinned in the VENDOR.bazel file, then Bazel uses the vendored source by creating
a symlink to it under `$(bazel info output_base)/external` instead of actually
running the repository rule. Otherwise, a warning is printed and Bazel will
fallback to fetching the latest version of the repo.

Note: Bazel assumes the vendored source is not changed by users unless the repo
is pinned in the VENDOR.bazel file. If a user does change the vendored source
without pinning the repo, the changed vendored source will be used, but it will
be overwritten if its existing marker file is
outdated and the repo is vendored again.

### Vendor registry files

Bazel has to perform the Bazel module resolution in order to fetch external
dependencies, which may require accessing registry files through internet. To
achieve offline build, Bazel vendors all registry files fetched from
network under the `<vendor_dir>/_registries` directory.

### Vendor symlinks

External repositories may contain symlinks pointing to other files or
directories. To make sure symlinks work correctly, Bazel uses the following
strategy to rewrite symlinks in the vendored source:

-   Create a symlink `<vendor_dir>/bazel-external` that points to `$(bazel info
    output_base)/external`. It is refreshed by every Bazel command
    automatically.
-   For the vendored source, rewrite all symlinks that originally point to a
    path under `$(bazel info output_base)/external` to a relative path under
    `<vendor_dir>/bazel-external`.

For example, if the original symlink is

```none
<vendor_dir>/repo_foo+/link  =>  $(bazel info output_base)/external/repo_bar+/file
```

It will be rewritten to

```none
<vendor_dir>/repo_foo+/link  =>  ../../bazel-external/repo_bar+/file
```

where

```none
<vendor_dir>/bazel-external  =>  $(bazel info output_base)/external  # This might be new if output base is changed
```

Since `<vendor_dir>/bazel-external` is generated by Bazel automatically, it's
recommended to add it to `.gitignore` or equivalent to avoid checking it in.

With this strategy, symlinks in the vendored source should work correctly even
after the vendored source is moved to another location or the bazel output base
is changed.

Note: symlinks that point to an absolute path outside of $(bazel info
output_base)/external are not rewritten. Therefore, it could still break
cross-machine compatibility.

Note: On Windows, vendoring symlinks only works with
[`--windows_enable_symlinks`][windows_enable_symlinks]
flag enabled.

[windows_enable_symlinks]: /reference/command-line-reference#flag--windows_enable_symlinks

---

## Getting Help
- URL: https://bazel.build/help
- Source: help.mdx
- Slug: /help

This page lists Bazel resources beyond the documentation and covers how to get
support from the Bazel team and community.

## Search existing material

In addition to the documentation, you can find helpful information by searching:

* [Bazel user group](https://groups.google.com/g/bazel-discuss)
* [Bazel GitHub Discussions](https://github.com/bazelbuild/bazel/discussions)
* [Bazel blog](https://blog.bazel.build/)
* [Stack Overflow](https://stackoverflow.com/questions/tagged/bazel)
* [`awesome-bazel` resources](https://github.com/jin/awesome-bazel)

## Watch videos

There are recordings of Bazel talks at various conferences, such as:

* Bazel’s annual conference, BazelCon:
  * [BazelCon 2024](https://www.youtube.com/playlist?list=PLbzoR-pLrL6ptKfAQNZ5RS4HMdmeilBcw)
  * [BazelCon 2023](https://www.youtube.com/playlist?list=PLbzoR-pLrL6rUiqylH-kumoZCWntG1vjp)
  * [BazelCon 2022](https://www.youtube.com/playlist?list=PLbzoR-pLrL6rABfcAJO1VWeOUYL1kIn-p)
  * [BazelCon 2021](https://www.youtube.com/playlist?list=PLbzoR-pLrL6pO6BaaQ1Ndos53gfRVLEoU)
  * [BazelCon 2020](https://www.youtube.com/playlist?list=PLbzoR-pLrL6qZ5JRMtn20_s2uPz9vFYgU)
  * [BazelCon 2019](https://www.youtube.com/playlist?list=PLbzoR-pLrL6ogKgytQXqUxJQ6nZlIWoTH)
  * [BazelCon 2018](https://www.youtube.com/playlist?list=PLbzoR-pLrL6rBDwC0NMRPS8EJ0VRAW0QR)
  * [BazelCon 2017](https://www.youtube.com/playlist?list=PLbzoR-pLrL6qvwchdtlSopLgUrz4J4zKP)
* Bazel day on [Google Open Source Live](https://opensourcelive.withgoogle.com/events/bazel)


## Ask the Bazel community

If there are no existing answers, you can ask the community by:

* Emailing the [Bazel user group](https://groups.google.com/g/bazel-discuss)
* Starting a discussion on [GitHub](https://github.com/bazelbuild/bazel/discussions)
* Asking a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/bazel)
* Chatting with other Bazel contributors on [Slack](https://slack.bazel.build/)
* Consulting a [Bazel community expert](/community/experts)

## Understand Bazel's support level

Please read the [release page](/release) to understand Bazel's release model and
what level of support Bazel provides.

## File a bug

If you encounter a bug or want to request a feature, file a [GitHub
Issue](https://github.com/bazelbuild/bazel/issues).

---

## Installing Bazel
- URL: https://bazel.build/install
- Source: install/index.mdx
- Slug: /install

This page describes the various platforms supported by Bazel and links
to the packages for more details.

[Bazelisk](/install/bazelisk) is the recommended way to install Bazel on [Ubuntu Linux](/install/ubuntu), [macOS](/install/os-x), and [Windows](/install/windows).

You can find available Bazel releases on our [release page](/release).

## Community-supported packages

Bazel community members maintain these packages. The Bazel team doesn't
officially support them. Contact the package maintainers for support.

*   [Alpine Linux](https://pkgs.alpinelinux.org/packages?name=bazel*&branch=edge&repo=&arch=&origin=&flagged=&maintainer=)
*   [Arch Linux][arch]
*   [Debian](https://qa.debian.org/developer.php?email=team%2Bbazel%40tracker.debian.org)
*   [Fedora](https://copr.fedorainfracloud.org/coprs/lihaohong/bazel)
*   [FreeBSD](https://www.freshports.org/devel/bazel)
*   [Homebrew](https://formulae.brew.sh/formula/bazel)
*   [Nixpkgs](https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/tools/build-managers/bazel)
*   [openSUSE](/install/suse)
*   [Scoop](https://github.com/scoopinstaller/scoop-main/blob/master/bucket/bazel.json)
*   [Raspberry Pi](https://github.com/koenvervloesem/bazel-on-arm/blob/master/README.md)

## Community-supported architectures

*   [ppc64el](https://ftp2.osuosl.org/pub/ppc64el/bazel/)

For other platforms, you can try to [compile from source](/install/compile-source).

[arch]: https://archlinux.org/packages/extra/x86_64/bazel/

---

## Installing / Updating Bazel using Bazelisk
- URL: https://bazel.build/install/bazelisk
- Source: install/bazelisk.mdx
- Slug: /install/bazelisk

## Installing Bazel

[Bazelisk](https://github.com/bazelbuild/bazelisk) is the
recommended way to install Bazel on Ubuntu, Windows, and macOS. It automatically
downloads and installs the appropriate version of Bazel. Use Bazelisk if you
need to switch between different versions of Bazel depending on the current
working directory, or to always keep Bazel updated to the latest release.

For more details, see
[the official README](https://github.com/bazelbuild/bazelisk/blob/master/README.md).

## Updating Bazel

Bazel has a [backward compatibility policy](/release/backward-compatibility)
(see [guidance for rolling out incompatible
changes](/contribute/breaking-changes) if you
are the author of one). That page summarizes best practices on how to test and
migrate your project with upcoming incompatible changes and how to provide
feedback to the incompatible change authors.

### Managing Bazel versions with Bazelisk

[Bazelisk](https://github.com/bazelbuild/bazelisk) helps you manage
Bazel versions.

Bazelisk can:

*   Auto-update Bazel to the latest LTS or rolling release.
*   Build the project with a Bazel version specified in the .bazelversion
    file. Check in that file into your version control to ensure reproducibility
    of your builds.
*   Help migrate your project for incompatible changes (see above)
*   Easily try release candidates

### Recommended migration process

Within minor updates to any LTS release, any
project can be prepared for the next release without breaking
compatibility with the current release. However, there may be
backward-incompatible changes between major LTS versions.

Follow this process to migrate from one major version to another:

1. Read the release notes to get advice on how to migrate to the next version.
1. Major incompatible changes should have an associated `--incompatible_*` flag
   and a corresponding GitHub issue:
    *   Migration guidance is available in the associated GitHub issue.
    *   Tooling is available for some of incompatible changes migration. For
        example, [buildifier](https://github.com/bazelbuild/buildtools/releases).
    *   Report migration problems by commenting on the associated GitHub issue.

After migration, you can continue to build your projects without worrying about
backward-compatibility until the next major release.

---

## Compiling Bazel from Source
- URL: https://bazel.build/install/compile-source
- Source: install/compile-source.mdx
- Slug: /install/compile-source

This page describes how to install Bazel from source and provides
troubleshooting tips for common issues.

To build Bazel from source, you can do one of the following:

*   Build it [using an existing Bazel binary](#build-bazel-using-bazel)

*   Build it [without an existing Bazel binary](#bootstrap-bazel) which is known
    as _bootstrapping_.

## Build Bazel using Bazel

### Summary

1.  Get the latest Bazel release from the
    [GitHub release page](https://github.com/bazelbuild/bazel/releases) or with
    [Bazelisk](https://github.com/bazelbuild/bazelisk).

2.  [Download Bazel's sources from GitHub](https://github.com/bazelbuild/bazel/archive/master.zip)
    and extract somewhere.
    Alternatively you can git clone the source tree from https://github.com/bazelbuild/bazel

3.  Install the same prerequisites as for bootstrapping (see
    [for Unix-like systems](#bootstrap-unix-prereq) or
    [for Windows](#bootstrap-windows-prereq))

4.  Build a development build of Bazel using Bazel:
    `bazel build //src:bazel-dev` (or `bazel build //src:bazel-dev.exe` on
    Windows)

5.  The resulting binary is at `bazel-bin/src/bazel-dev`
    (or `bazel-bin\src\bazel-dev.exe` on Windows). You can copy it wherever you
    like and use immediately without further installation.

Detailed instructions follow below.

### Step 1: Get the latest Bazel release

**Goal**: Install or download a release version of Bazel. Make sure you can run
it by typing `bazel` in a terminal.

**Reason**: To build Bazel from a GitHub source tree, you need a pre-existing
Bazel binary. You can install one from a package manager or download one from
GitHub. See [Installing Bazel](/install). (Or you can [build from
scratch (bootstrap)](#bootstrap-bazel).)

**Troubleshooting**:

*   If you cannot run Bazel by typing `bazel` in a terminal:

    *   Maybe your Bazel binary's directory is not on the PATH.

        This is not a big problem. Instead of typing `bazel`, you will need to
        type the full path.

    *   Maybe the Bazel binary itself is not called `bazel` (on Unixes) or
        `bazel.exe` (on Windows).

        This is not a big problem. You can either rename the binary, or type the
        binary's name instead of `bazel`.

    *   Maybe the binary is not executable (on Unixes).

        You must make the binary executable by running `chmod +x /path/to/bazel`.

### Step 2: Download Bazel's sources from GitHub

If you are familiar with Git, then just git clone https://github.com/bazelbuild/bazel

Otherwise:

1.  Download the
    [latest sources as a zip file](https://github.com/bazelbuild/bazel/archive/master.zip).

2.  Extract the contents somewhere.

    For example create a `bazel-src` directory under your home directory and
    extract there.

### Step 3: Install prerequisites

Install the same prerequisites as for bootstrapping (see below) -- JDK, C++
compiler, MSYS2 (if you are building on Windows), etc.

### Step 4a: Build Bazel on Ubuntu Linux, macOS, and other Unix-like systems

For instructions for Windows, see [Build Bazel on Windows](#build-bazel-on-windows).

**Goal**: Run Bazel to build a custom Bazel binary (`bazel-bin/src/bazel-dev`).

**Instructions**:

1.  Start a Bash terminal

2.  `cd` into the directory where you extracted (or cloned) Bazel's sources.

    For example if you extracted the sources under your home directory, run:

        cd ~/bazel-src

3.  Build Bazel from source:

        bazel build //src:bazel-dev

    Alternatively you can run `bazel build //src:bazel --compilation_mode=opt`
    to yield a smaller binary but it's slower to build.

    You can build with `--stamp --embed_label=X.Y.Z` flag to embed a Bazel
    version for the binary so that `bazel --version` outputs the given version.

4.  The output will be at `bazel-bin/src/bazel-dev` (or `bazel-bin/src/bazel`).

### Step 4b: Build Bazel on Windows

For instructions for Unix-like systems, see
[Ubuntu Linux, macOS, and other Unix-like systems](#build-bazel-on-unixes).

**Goal**: Run Bazel to build a custom Bazel binary
(`bazel-bin\src\bazel-dev.exe`).

**Instructions**:

1.  Start Command Prompt (Start Menu &gt; Run &gt; "cmd.exe")

2.  `cd` into the directory where you extracted (or cloned) Bazel's sources.

    For example if you extracted the sources under your home directory, run:

        cd %USERPROFILE%\bazel-src

3.  Build Bazel from source:

    bazel build //src:bazel-dev.exe

    Alternatively you can run `bazel build //src:bazel.exe
    --compilation_mode=opt` to yield a smaller binary but it's slower to build.

    You can build with `--stamp --embed_label=X.Y.Z` flag to embed a Bazel
    version for the binary so that `bazel --version` outputs the given version.

4.  The output will be at `bazel-bin\src\bazel-dev.exe` (or
    `bazel-bin\src\bazel.exe`).

### Step 5: Install the built binary

Actually, there's nothing to install.

The output of the previous step is a self-contained Bazel binary. You can copy
it to any directory and use immediately. (It's useful if that directory is on
your PATH so that you can run "bazel" everywhere.)

---

## Build Bazel from scratch (bootstrapping)

You can also build Bazel from scratch, without using an existing Bazel binary.

### Step 1: Download Bazel's sources (distribution archive)

(This step is the same for all platforms.)

1.  Download `bazel-<version>-dist.zip` from
    [GitHub](https://github.com/bazelbuild/bazel/releases), for example
    `bazel-0.28.1-dist.zip`.

    **Attention**:

    -   There is a **single, architecture-independent** distribution archive.
        There are no architecture-specific or OS-specific distribution archives.
    -   These sources are **not the same as the GitHub source tree**. You
        have to use the distribution archive to bootstrap Bazel. You cannot
        use a source tree cloned from GitHub. (The distribution archive contains
        generated source files that are required for bootstrapping and are not part
        of the normal Git source tree.)

2.  Unpack the distribution archive somewhere on disk.

    You should verify the signature made by Bazel's
    [release key](https://bazel.build/bazel-release.pub.gpg) 3D5919B448457EE0.

### Step 2a: Bootstrap Bazel on Ubuntu Linux, macOS, and other Unix-like systems

For instructions for Windows, see [Bootstrap Bazel on Windows](#bootstrap-windows).

#### 2.1. Install the prerequisites

*   **Bash**

*   **zip, unzip**

*   **C++ build toolchain**

*   **JDK.** Version 21 is required.

*   **Python**. Version 3 is required.

For example on Ubuntu Linux you can install these requirements using the
following command:

```sh
sudo apt-get install build-essential openjdk-21-jdk python3 zip unzip
```

#### 2.2. Bootstrap Bazel on Unix

1.  Open a shell or Terminal window.

3.  `cd` to the directory where you unpacked the distribution archive.

3.  Run the compilation script: `env EXTRA_BAZEL_ARGS="--tool_java_runtime_version=local_jdk" bash ./compile.sh`.

The compiled output is placed into `output/bazel`. This is a self-contained
Bazel binary, without an embedded JDK. You can copy it anywhere or use it
in-place. For convenience, copy this binary to a directory that's on your
`PATH` (such as `/usr/local/bin` on Linux).

To build the `bazel` binary in a reproducible way, also set
[`SOURCE_DATE_EPOCH`](https://reproducible-builds.org/specs/source-date-epoch/)
in the "Run the compilation script" step.

### Step 2b: Bootstrap Bazel on Windows

For instructions for Unix-like systems, see
[Bootstrap Bazel on Ubuntu Linux, macOS, and other Unix-like systems](#bootstrap-unix).

#### 2.1. Install the prerequisites

*   [MSYS2 shell](https://msys2.github.io/)

*   **The MSYS2 packages for zip and unzip.** Run the following command in the MSYS2 shell:

    ```
    pacman -S zip unzip patch
    ```

*   **The Visual C++ compiler.** Install the Visual C++ compiler either as part
    of Visual Studio 2015 or newer, or by installing the latest [Build Tools
    for Visual Studio 2017](https://aka.ms/BuildTools).

*   **JDK.** Version 21 is required.

*   **Python**. Versions 2 and 3 are supported, installing one of them is
    enough. You need the Windows-native version (downloadable from
    [https://www.python.org](https://www.python.org)). Versions installed via
    pacman in MSYS2 will not work.

#### 2.2. Bootstrap Bazel on Windows

1.  Open the MSYS2 shell.

2.  Set the following environment variables:
    *   Either `BAZEL_VS` or `BAZEL_VC` (they are *not* the same): Set to the
        path to the Visual Studio directory (BAZEL\_V<b>S</b>) or to the Visual
        C++ directory (BAZEL\_V<b>C</b>). Setting one of them is enough.
    *   `BAZEL_SH`: Path of the MSYS2 `bash.exe`. See the command in the
        examples below.

        Do not set this to `C:\Windows\System32\bash.exe`. (You have that file
        if you installed Windows Subsystem for Linux.) Bazel does not support
        this version of `bash.exe`.
    *   `PATH`: Add the Python directory.
    *   `JAVA_HOME`: Set to the JDK directory.

    **Example** (using BAZEL\_V<b>S</b>):

        export BAZEL_VS="C:/Program Files (x86)/Microsoft Visual Studio/2017/BuildTools"
        export BAZEL_SH="$(cygpath -m $(realpath $(which bash)))"
        export PATH="/c/python27:$PATH"
        export JAVA_HOME="C:/Program Files/Java/jdk-21"

    or (using BAZEL\_V<b>C</b>):

        export BAZEL_VC="C:/Program Files (x86)/Microsoft Visual Studio/2017/BuildTools/VC"
        export BAZEL_SH="$(cygpath -m $(realpath $(which bash)))"
        export PATH="/c/python27:$PATH"
        export JAVA_HOME="C:/Program Files/Java/jdk-21"

3.  `cd` to the directory where you unpacked the distribution archive.

4.  Run the compilation script: `env EXTRA_BAZEL_ARGS="--tool_java_runtime_version=local_jdk" ./compile.sh`

The compiled output is placed into `output/bazel.exe`. This is a self-contained
Bazel binary, without an embedded JDK. You can copy it anywhere or use it
in-place. For convenience, copy this binary to a directory that's on
your `PATH`.

To build the `bazel.exe` binary in a reproducible way, also set
[`SOURCE_DATE_EPOCH`](https://reproducible-builds.org/specs/source-date-epoch/)
in the "Run the compilation script" step.

You don't need to run Bazel from the MSYS2 shell. You can run Bazel from the
Command Prompt (`cmd.exe`) or PowerShell.

---

## Command-Line Completion
- URL: https://bazel.build/install/completion
- Source: install/completion.mdx
- Slug: /install/completion

You can enable command-line completion (also known as tab-completion) in Bash
and Zsh. This lets you tab-complete command names, flags names and flag values,
and target names.

## Bash

Bazel comes with a Bash completion script.

If you installed Bazel:

*   From the APT repository, then you're done -- the Bash completion script is
    already installed in `/etc/bash_completion.d`.

*   From Homebrew, then you're done -- the Bash completion script is
    already installed in `$(brew --prefix)/etc/bash_completion.d`.

*   From the installer downloaded from GitHub, then:
    1.  Locate the absolute path of the completion file. The installer copied it
        to the `bin` directory.

        Example: if you ran the installer with `--user`, this will be
        `$HOME/.bazel/bin`. If you ran the installer as root, this will be
        `/usr/local/lib/bazel/bin`.
    2.  Do one of the following:
        *   Either copy this file to your completion directory (if you have
            one).

            Example: on Ubuntu this is the `/etc/bash_completion.d` directory.
        *   Or source the completion file from Bash's RC file.

            Add a line similar to the one below to your `~/.bashrc` (on Ubuntu)
            or `~/.bash_profile` (on macOS), using the path to your completion
            file's absolute path:

            ```
            source /path/to/bazel-complete.bash
            ```

*   Via [bootstrapping](/install/compile-source), then:
    1.  Emit the completion script into a file:

        ```
        bazel help completion bash > bazel-complete.bash
        ```
    2.  Do one of the following:
        *   Copy this file to your completion directory, if you have
            one.

            Example: on Ubuntu this is the `/etc/bash_completion.d` directory
        *   Copy it somewhere on your local disk, such as to `$HOME`, and
            source the completion file from Bash's RC file.

            Add a line similar to the one below to your `~/.bashrc` (on Ubuntu)
            or `~/.bash_profile` (on macOS), using the path to your completion
            file's absolute path:

            ```
            source /path/to/bazel-complete.bash
            ```

## Zsh

Bazel comes with a Zsh completion script.

If you installed Bazel:

*   From the APT repository, then you're done -- the Zsh completion script is
    already installed in `/usr/share/zsh/vendor-completions`.

    > If you have a heavily customized `.zshrc` and the autocomplete
    > does not function, try one of the following solutions:
    >
    > Add the following to your `.zshrc`:
    >
    >    ```
    >     zstyle :compinstall filename '/home/tradical/.zshrc'
    >
    >     autoload -Uz compinit
    >     compinit
    >    ```
    >
    > or
    >
    > Follow the instructions
    > [here](https://stackoverflow.com/questions/58331977/bazel-tab-auto-complete-in-zsh-not-working)
    >
    > If you are using `oh-my-zsh`, you may want to install and enable
    > the `zsh-autocomplete` plugin. If you'd prefer not to, use one of the
    > solutions described above.

*   From Homebrew, then you're done -- the Zsh completion script is
    already installed in `$(brew --prefix)/share/zsh/site-functions`.

*   From the installer downloaded from GitHub, then:
    1.  Locate the absolute path of the completion file. The installer copied it
        to the `bin` directory.

        Example: if you ran the installer with `--user`, this will be
        `$HOME/.bazel/bin`. If you ran the installer as root, this will be
        `/usr/local/lib/bazel/bin`.

    2.  Add this script to a directory on your `$fpath`:

        ```
        fpath[1,0]=~/.zsh/completion/
        mkdir -p ~/.zsh/completion/
        cp /path/from/above/step/_bazel ~/.zsh/completion
        ```

        You may have to call `rm -f ~/.zcompdump; compinit`
        the first time to make it work.

    3.  Optionally, add the following to your .zshrc.

        ```
        # This way the completion script does not have to parse Bazel's options
        # repeatedly.  The directory in cache-path must be created manually.
        zstyle ':completion:*' use-cache on
        zstyle ':completion:*' cache-path ~/.zsh/cache
        ```

---

## Getting Started with Bazel Docker Container
- URL: https://bazel.build/install/docker-container
- Source: install/docker-container.mdx
- Slug: /install/docker-container

This page provides details on the contents of the Bazel container, how to build
the [abseil-cpp](https://github.com/abseil/abseil-cpp) project using Bazel
inside the Bazel container, and how to build this project directly
from the host machine using the Bazel container with directory mounting.

## Build Abseil project from your host machine with directory mounting

The instructions in this section allow you to build using the Bazel container
with the sources checked out in your host environment. A container is started up
for each build command you execute. Build results are cached in your host
environment so they can be reused across builds.

Clone the project to a directory in your host machine.

```posix-terminal
git clone --depth 1 --branch 20220623.1 https://github.com/abseil/abseil-cpp.git /src/workspace
```

Create a folder that will have cached results to be shared across builds.

```posix-terminal
mkdir -p /tmp/build_output/
```

Use the Bazel container to build the project and make the build
outputs available in the output folder in your host machine.

```posix-terminal
docker run \
  -e USER="$(id -u)" \
  -u="$(id -u)" \
  -v /src/workspace:/src/workspace \
  -v /tmp/build_output:/tmp/build_output \
  -w /src/workspace \
  gcr.io/bazel-public/bazel:latest \
  --output_user_root=/tmp/build_output \
  build //absl/...
```

Build the project with sanitizers by adding the `--config=<var>asan</var>|<var>tsan</var>|<var>msan</var>` build
flag to select AddressSanitizer (asan), ThreadSanitizer (tsan) or
MemorySanitizer (msan) accordingly.

```posix-terminal
docker run \
  -e USER="$(id -u)" \
  -u="$(id -u)" \
  -v /src/workspace:/src/workspace \
  -v /tmp/build_output:/tmp/build_output \
  -w /src/workspace \
  gcr.io/bazel-public/bazel:latest \
  --output_user_root=/tmp/build_output \
  build --config={asan | tsan | msan} -- //absl/... -//absl/types:variant_test
```

## Build Abseil project from inside the container

The instructions in this section allow you to build using the Bazel container
with the sources inside the container. By starting a container at the beginning
of your development workflow and doing changes in the worskpace within the
container, build results will be cached.

Start a shell in the Bazel container:

```posix-terminal
docker run --interactive --entrypoint=/bin/bash gcr.io/bazel-public/bazel:latest
```

Each container id is unique. In the instructions below, the container was 5a99103747c6.

Clone the project.

```posix-terminal
ubuntu@5a99103747c6:~$ git clone --depth 1 --branch 20220623.1 https://github.com/abseil/abseil-cpp.git && cd abseil-cpp/
```

Do a regular build.

```posix-terminal
ubuntu@5a99103747c6:~/abseil-cpp$ bazel build //absl/...
```

Build the project with sanitizers by adding the `--config=<var>asan</var>|<var>tsan</var>|<var>msan</var>`
build flag to select AddressSanitizer (asan), ThreadSanitizer (tsan) or
MemorySanitizer (msan) accordingly.

```posix-terminal
ubuntu@5a99103747c6:~/abseil-cpp$ bazel build --config={asan | tsan | msan} -- //absl/... -//absl/types:variant_test
```

## Explore the Bazel container

If you haven't already, start an interactive shell inside the Bazel container.

```posix-terminal
docker run -it --entrypoint=/bin/bash gcr.io/bazel-public/bazel:latest
ubuntu@5a99103747c6:~$
```

Explore the container contents.

```posix-terminal
ubuntu@5a99103747c6:~$ gcc --version
gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

ubuntu@5a99103747c6:~$ java -version
openjdk version "1.8.0_362"
OpenJDK Runtime Environment (build 1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09)
OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode)

ubuntu@5a99103747c6:~$ python -V
Python 3.8.10

ubuntu@5a99103747c6:~$ bazel version
WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).
Extracting Bazel installation...
Build label: 6.2.1
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 2 16:59:58 2023 (1685725198)
Build timestamp: 1685725198
Build timestamp as int: 1685725198
```

## Explore the Bazel Dockerfile

If you want to check how the Bazel Docker image is built, you can find its Dockerfile at [bazelbuild/continuous-integration/bazel/oci](https://github.com/bazelbuild/continuous-integration/tree/master/bazel/oci).

---

## Integrating Bazel with IDEs
- URL: https://bazel.build/install/ide
- Source: install/ide.mdx
- Slug: /install/ide

This page covers how to integrate Bazel with IDEs, such as IntelliJ, Android
Studio, and CLion (or build your own IDE plugin). It also includes links to
installation and plugin details.

IDEs integrate with Bazel in a variety of ways, from features that allow Bazel
executions from within the IDE, to awareness of Bazel structures such as syntax
highlighting of the `BUILD` files.

If you are interested in developing an editor or IDE plugin for Bazel, please
join the `#ide` channel on the [Bazel Slack](https://slack.bazel.build) or start
a discussion on [GitHub](https://github.com/bazelbuild/bazel/discussions).

## IDEs and editors

### IntelliJ, Android Studio, and CLion

[Official plugin](http://ij.bazel.build) for IntelliJ, Android Studio, and
CLion. The plugin is [open source](https://github.com/bazelbuild/intellij).

This is the open source version of the plugin used internally at Google.

Features:

* Interop with language-specific plugins. Supported languages include Java,
  Scala, and Python.
* Import `BUILD` files into the IDE with semantic awareness of Bazel targets.
* Make your IDE aware of Starlark, the language used for Bazel's `BUILD` and
  `.bzl`files
* Build, test, and execute binaries directly from the IDE
* Create configurations for debugging and running binaries.

To install, go to the IDE's plugin browser and search for `Bazel`.

To manually install older versions, download the zip files from JetBrains'
Plugin Repository and install the zip file from the IDE's plugin browser:

*  [Android Studio
   plugin](https://plugins.jetbrains.com/plugin/9185-android-studio-with-bazel)
*  [IntelliJ
   plugin](https://plugins.jetbrains.com/plugin/8609-intellij-with-bazel)
*  [CLion plugin](https://plugins.jetbrains.com/plugin/9554-clion-with-bazel)

### Xcode

[rules_xcodeproj](https://github.com/buildbuddy-io/rules_xcodeproj),
[Tulsi](https://tulsi.bazel.build), and
[XCHammer](https://github.com/pinterest/xchammer) generate Xcode
projects from Bazel `BUILD` files.

### Visual Studio Code

Official plugin for VS Code.

Features:

* Bazel Build Targets tree
* Starlark debugger for `.bzl` files during a build (set breakpoints, step
  through code, inspect variables, and so on)

Find [the plugin on the Visual Studio
marketplace](https://marketplace.visualstudio.com/items?itemName=BazelBuild.vscode-bazel)
or the [OpenVSX marketplace](https://open-vsx.org/extension/BazelBuild/vscode-bazel).
The plugin is [open source](https://github.com/bazelbuild/vscode-bazel).

See also: [Autocomplete for Source Code](#autocomplete-for-source-code)

### Atom

Find the [`language-bazel` package](https://atom.io/packages/language-bazel)
on the Atom package manager.

See also: [Autocomplete for Source Code](#autocomplete-for-source-code)

### Vim

See [`bazelbuild/vim-bazel` on GitHub](https://github.com/bazelbuild/vim-bazel)

See also: [Autocomplete for Source Code](#autocomplete-for-source-code)

### Emacs

See [`bazelbuild/bazel-emacs-mode` on
GitHub](https://github.com/bazelbuild/emacs-bazel-mode)

See also: [Autocomplete for Source Code](#autocomplete-for-source-code)

### Visual Studio

[Lavender](https://github.com/tmandry/lavender) is an experimental project for
generating Visual Studio projects that use Bazel for building.

### Eclipse

[Bazel Eclipse Feature](https://github.com/salesforce/bazel-eclipse)
is a set of plugins for importing Bazel packages into an Eclipse workspace as
Eclipse projects.

## Autocomplete for Source Code

### C Language Family (C++, C, Objective-C, Objective-C++, and CUDA)

[`kiron1/bazel-compile-commands`](https://github.com/kiron1/bazel-compile-commands)
run `bazel-compile-commands //...` in a Bazel workspace to generate a `compile_commands.json` file.
The `compile_commands.json` file enables tools like `clang-tidy`, `clangd` (LSP) and other IDEs to
provide autocomplete, smart navigation, quick fixes, and more. The tool is written in C++ and
consumes the Protobuf output of Bazel to extract the compile commands.

[`hedronvision/bazel-compile-commands-extractor`](https://github.com/hedronvision/bazel-compile-commands-extractor) enables autocomplete, smart navigation, quick fixes, and more in a wide variety of extensible editors, including VSCode, Vim, Emacs, Atom, and Sublime. It lets language servers, like clangd and ccls, and other types of tooling, draw upon Bazel's understanding of how `cc` and `objc` code will be compiled, including how it configures cross-compilation for other platforms.

### Java

[`georgewfraser/java-language-server`](https://github.com/georgewfraser/java-language-server) - Java Language Server (LSP) with support for Bazel-built projects

## Automatically run build and test on file change

[Bazel watcher](https://github.com/bazelbuild/bazel-watcher) is a
tool for building Bazel targets when source files change.

## Building your own IDE plugin

Read the [**IDE support** blog
post](https://blog.bazel.build/2016/06/10/ide-support.html) to learn more about
the Bazel APIs to use when building an IDE plugin.

---

## Installing Bazel on macOS
- URL: https://bazel.build/install/os-x
- Source: install/os-x.mdx
- Slug: /install/os-x

This page describes how to install Bazel on macOS and set up your environment.

You can install Bazel on macOS using one of the following methods:

*   *Recommended*: [Use Bazelisk](/install/bazelisk)
*   [Use Homebrew](#install-on-mac-os-x-homebrew)
*   [Use the binary installer](#install-with-installer-mac-os-x)
*   [Compile Bazel from source](/install/compile-source)

Bazel comes with two completion scripts. After installing Bazel, you can:

*   Access the [bash completion script](/install/completion#bash)
*   Install the [zsh completion script](/install/completion#zsh)

<h2 id="install-on-mac-os-x-homebrew">Installing using Homebrew</h2>

### Step 1: Install Homebrew on macOS

Install [Homebrew](https://brew.sh/) (a one-time step):

```posix-terminal
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

### Step 2: Install Bazel via Homebrew

Install the Bazel package via Homebrew as follows:

```posix-terminal
brew install bazel
```

All set! You can confirm Bazel is installed successfully by running the
following command:

```posix-terminal
bazel --version
```

Once installed, you can upgrade to a newer version of Bazel using the
following command:

```posix-terminal
brew upgrade bazel
```

<h2 id="install-with-installer-mac-os-x">Installing using the binary installer</h2>

The binary installers are on Bazel's
[GitHub releases page](https://github.com/bazelbuild/bazel/releases).

The installer contains the Bazel binary. Some additional libraries
must also be installed for Bazel to work.

### Step 1: Install Xcode command line tools

If you don't intend to use `ios_*` rules, it is sufficient to install the Xcode
command line tools package by using `xcode-select`:

```posix-terminal
xcode-select --install
```

Otherwise, for `ios_*` rule support, you must have Xcode 6.1 or later with iOS
SDK 8.1 installed on your system.

Download Xcode from the
[App Store](https://apps.apple.com/us/app/xcode/id497799835) or the
[Apple Developer site](https://developer.apple.com/download/more/?=xcode).

Once Xcode is installed, accept the license agreement for all users with the
following command:

```posix-terminal
sudo xcodebuild -license accept
```

### Step 2: Download the Bazel installer

Next, download the Bazel binary installer named
`bazel-<version>-installer-darwin-x86_64.sh` from the
[Bazel releases page on GitHub](https://github.com/bazelbuild/bazel/releases).

**On macOS Catalina or newer (macOS >= 11)**, due to Apple's new app signing requirements,
you need to download the installer from the terminal using `curl`, replacing
the version variable with the Bazel version you want to download:

```posix-terminal
export BAZEL_VERSION=5.2.0

curl -fLO "https://github.com/bazelbuild/bazel/releases/download/{{ '<var>' }}$BAZEL_VERSION{{ '</var>' }}/bazel-{{ '<var>' }}$BAZEL_VERSION{{ '</var>' }}-installer-darwin-x86_64.sh"
```

This is a temporary workaround until the macOS release flow supports
signing ([#9304](https://github.com/bazelbuild/bazel/issues/9304)).

### Step 3: Run the installer

Run the Bazel installer as follows:

```posix-terminal
chmod +x "bazel-{{ '<var>' }}$BAZEL_VERSION{{ '</var>' }}-installer-darwin-x86_64.sh"

./bazel-{{ '<var>' }}$BAZEL_VERSION{{ '</var>' }}-installer-darwin-x86_64.sh --user
```

The `--user` flag installs Bazel to the `$HOME/bin` directory on your system and
sets the `.bazelrc` path to `$HOME/.bazelrc`. Use the `--help` command to see
additional installation options.

If you are **on macOS Catalina or newer (macOS >= 11)** and get an error that _**“bazel-real” cannot be
opened because the developer cannot be verified**_, you need to re-download
the installer from the terminal using `curl` as a workaround; see Step 2 above.

### Step 4: Set up your environment

If you ran the Bazel installer with the `--user` flag as above, the Bazel
executable is installed in your `<var>HOME</var>/bin` directory.
It's a good idea to add this directory to your default paths, as follows:

```posix-terminal
export PATH="{{ '<var>' }}PATH{{ '</var>' }}:{{ '<var>' }}HOME{{ '</var>' }}/bin"
```

You can also add this command to your `~/.bashrc`, `~/.zshrc`, or `~/.profile`
file.

All set! You can confirm Bazel is installed successfully by running the
following command:

```posix-terminal
bazel --version
```
To update to a newer release of Bazel, download and install the desired version.

---

## Installing Bazel on openSUSE Tumbleweed & Leap
- URL: https://bazel.build/install/suse
- Source: install/suse.mdx
- Slug: /install/suse

This page describes how to install Bazel on openSUSE Tumbleweed and Leap.

`NOTE:` The Bazel team does not officially maintain openSUSE support. For issues
using Bazel on openSUSE please file a ticket at [bugzilla.opensuse.org](https://bugzilla.opensuse.org/).

Packages are provided for openSUSE Tumbleweed and Leap. You can find all
available Bazel versions via openSUSE's [software search](https://software.opensuse.org/search?utf8=%E2%9C%93&baseproject=ALL&q=bazel).

The commands below must be run either via `sudo` or while logged in as `root`.

## Installing Bazel on openSUSE

Run the following commands to install the package. If you need a specific
version, you can install it via the specific `bazelXXX` package, otherwise,
just `bazel` is enough:

To install the latest version of Bazel, run:

```posix-terminal
zypper install bazel
```

You can also install a specific version of Bazel by specifying the package
version with `bazel<var>version</var>`. For example, to install
Bazel 4.2, run:

```posix-terminal
zypper install bazel4.2
```

---

## Installing Bazel on Ubuntu
- URL: https://bazel.build/install/ubuntu
- Source: install/ubuntu.mdx
- Slug: /install/ubuntu

This page describes the options for installing Bazel on Ubuntu.
It also provides links to the Bazel completion scripts and the binary installer,
if needed as a backup option (for example, if you don't have admin access).

Supported Ubuntu Linux platforms:

*   22.04 (LTS)
*   20.04 (LTS)
*   18.04 (LTS)

Bazel should be compatible with other Ubuntu releases and Debian
"stretch" and above, but is untested and not guaranteed to work.

Install Bazel on Ubuntu using one of the following methods:

*   *Recommended*: [Use Bazelisk](/install/bazelisk)
*   [Use our custom APT repository](#install-on-ubuntu)
*   [Use the binary installer](#binary-installer)
*   [Use the Bazel Docker container](#docker-container)
*   [Compile Bazel from source](/install/compile-source)

**Note:** For Arm-based systems, the APT repository does not contain an `arm64`
release, and there is no binary installer available. Either use Bazelisk or
compile from source.

Bazel comes with two completion scripts. After installing Bazel, you can:

*   Access the [bash completion script](/install/completion#bash)
*   Install the [zsh completion script](/install/completion#zsh)

## Using Bazel's apt repository

### Step 1: Add Bazel distribution URI as a package source

**Note:** This is a one-time setup step.

```posix-terminal
sudo apt install apt-transport-https curl gnupg -y

curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor >bazel-archive-keyring.gpg

sudo mv bazel-archive-keyring.gpg /usr/share/keyrings

echo "deb [arch=amd64 signed-by=/usr/share/keyrings/bazel-archive-keyring.gpg] https://storage.googleapis.com/bazel-apt stable jdk1.8" | sudo tee /etc/apt/sources.list.d/bazel.list
```

The component name "jdk1.8" is kept only for legacy reasons and doesn't relate
to supported or included JDK versions. Bazel releases are Java-version agnostic.
Changing the "jdk1.8" component name would break existing users of the repo.

### Step 2: Install and update Bazel

```posix-terminal
sudo apt update && sudo apt install bazel
```

Once installed, you can upgrade to a newer version of Bazel as part of your normal system updates:

```posix-terminal
sudo apt update && sudo apt full-upgrade
```

The `bazel` package always installs the latest stable version of Bazel. You
can install specific, older versions of Bazel in addition to the latest one,
such as this:

```posix-terminal
sudo apt install bazel-1.0.0
```

This installs Bazel 1.0.0 as `/usr/bin/bazel-1.0.0` on your system. This
can be useful if you need a specific version of Bazel to build a project, for
example because it uses a `.bazelversion` file to explicitly state with which
Bazel version it should be built.

Optionally, you can set `bazel` to a specific version by creating a symlink:

```posix-terminal
sudo ln -s /usr/bin/bazel-1.0.0 /usr/bin/bazel

bazel --version  # 1.0.0
```

### Step 3: Install a JDK (optional)

Bazel includes a private, bundled JRE as its runtime and doesn't require you to
install any specific version of Java.

However, if you want to build Java code using Bazel, you have to install a JDK.

```posix-terminal
sudo apt install default-jdk
```

## Using the binary installer

Generally, you should use the apt repository, but the binary installer
can be useful if you don't have admin permissions on your machine or
can't add custom repositories.

The binary installers can be downloaded from Bazel's [GitHub releases page](https://github.com/bazelbuild/bazel/releases).

The installer contains the Bazel binary and extracts it into your `$HOME/bin`
folder. Some additional libraries must be installed manually for Bazel to work.

### Step 1: Install required packages

Bazel needs a C++ compiler and unzip / zip in order to work:

```posix-terminal
sudo apt install g++ unzip zip
```

If you want to build Java code using Bazel, install a JDK:

```posix-terminal
sudo apt-get install default-jdk
```

### Step 2: Run the installer

Next, download the Bazel binary installer named `bazel-<var>version</var>-installer-linux-x86_64.sh`
from the [Bazel releases page on GitHub](https://github.com/bazelbuild/bazel/releases).

Run it as follows:

```posix-terminal
chmod +x bazel-{{ '<var>' }}version{{ '</var>' }}-installer-linux-x86_64.sh

./bazel-{{ '<var>' }}version{{ '</var>' }}-installer-linux-x86_64.sh --user
```

The `--user` flag installs Bazel to the `$HOME/bin` directory on your system and
sets the `.bazelrc` path to `$HOME/.bazelrc`. Use the `--help` command to see
additional installation options.

### Step 3: Set up your environment

If you ran the Bazel installer with the `--user` flag as above, the Bazel
executable is installed in your `$HOME/bin` directory.
It's a good idea to add this directory to your default paths, as follows:

```posix-terminal
export PATH="$PATH:$HOME/bin"
```

You can also add this command to your `~/.bashrc` or `~/.zshrc` file to make it
permanent.

## Using the Bazel Docker container

We publish Docker container with Bazel installed for each Bazel version at `gcr.io/bazel-public/bazel`.
You can use the Docker container as follows:

```
$ docker pull gcr.io/bazel-public/bazel:<bazel version>
```

The Docker container is built by [these steps](https://github.com/bazelbuild/continuous-integration/tree/master/bazel/oci).

---

## Installing Bazel on Windows
- URL: https://bazel.build/install/windows
- Source: install/windows.mdx
- Slug: /install/windows

This page describes the requirements and steps to install Bazel on Windows.
It also includes troubleshooting and other ways to install Bazel, such as
using Chocolatey or Scoop.

## Installing Bazel

This section covers the prerequisites, environment setup, and detailed
steps during installation on Windows.

### Check your system

Recommended: 64 bit Windows 10, version 1703 (Creators Update) or newer

To check your Windows version:

* Click the Start button.
* Type `winver` in the search box and press Enter.
* You should see the About Windows box with your Windows version information.

### Install the prerequisites

*   [Microsoft Visual C++ Redistributable](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)

### Download Bazel

*Recommended*: [Use Bazelisk](/install/bazelisk)


Alternatively you can:

*   [Download the Bazel binary (`bazel-<var>version</var>-windows-x86_64.exe`) from
 GitHub](https://github.com/bazelbuild/bazel/releases).
*   [Install Bazel from Chocolatey](#chocolately)
*   [Install Bazel from Scoop](#scoop)
*   [Build Bazel from source](/install/compile-source)

### Set up your environment

To make Bazel easily accessible from command prompts or PowerShell by default, you can rename the Bazel binary to `bazel.exe` and add it to your default paths.

```posix-terminal
set PATH=%PATH%;{{ '<var>' }}path to the Bazel binary{{ '</var>' }}
```

You can also change your system `PATH` environment variable to make it permanent. Check out how to [set environment variables](/configure/windows#set-environment-variables).

### Done

"Success: You've installed Bazel."

To check the installation is correct, try to run:

```posix-terminal
bazel {{ '<var>' }}version{{ '</var>' }}
```

Next, you can check out more tips and guidance here:

*   [Installing compilers and language runtimes](#install-compilers)
*   [Troubleshooting](#troubleshooting)
*   [Best practices on Windows](/configure/windows#best-practices)
*   [Tutorials](/start/#tutorials)

## Installing compilers and language runtimes

Depending on which languages you want to build, you will need:

*   [MSYS2 x86_64](https://www.msys2.org/)

    MSYS2 is a software distro and building platform for Windows. It contains Bash and common Unix
    tools (like `grep`, `tar`, `git`).

    You will need MSYS2 to build, test, or run targets that depend on Bash. Typically these are
    `genrule`, `sh_binary`, `sh_test`, but there may be more (such as Starlark rules). Bazel shows an
    error if a build target needs Bash but Bazel could not locate it.

*   Common MSYS2 packages

    You will likely need these to build and run targets that depend on Bash. MSYS2 does not install
    these tools by default, so you need to install them manually. Projects that depend on Bash tools in `PATH` need this step (for example TensorFlow).

    Open the MSYS2 terminal and run this command:

    ```posix-terminal
    pacman -S zip unzip patch diffutils git
    ```

    Optional: If you want to use Bazel from CMD or Powershell and still be able
    to use Bash tools, make sure to add
    `<var>MSYS2_INSTALL_PATH</var>/usr/bin` to your
    `PATH` environment variable.

*   [Build Tools for Visual Studio 2019](https://aka.ms/buildtools)

    You will need this to build C++ code on Windows.

    Also supported:

    *   Visual C++ Build Tools 2017 (or newer) and Windows 10 SDK

*   [Java SE Development Kit 11 (JDK) for Windows x64](https://www.oracle.com/java/technologies/javase-jdk11-downloads.html)

    You will need this to build Java code on Windows.

    Also supported: Java 8, 9, and 10

*   [Python 3.6 for Windows x86-64](https://www.python.org/downloads/windows/)

    You will need this to build Python code on Windows.

    Also supported: Python 2.7 or newer for Windows x86-64

## Troubleshooting

### Bazel does not find Bash or bash.exe

**Possible reasons**:

*   you installed MSYS2 not under the default install path

*   you installed MSYS2 i686 instead of MSYS2 x86\_64

*   you installed MSYS instead of MSYS2

**Solution**:

Ensure you installed MSYS2 x86\_64.

If that doesn't help:

1.  Go to Start Menu &gt; Settings.

2.  Find the setting "Edit environment variables for your account"

3.  Look at the list on the top ("User variables for &lt;username&gt;"), and click the "New..."
    button below it.

4.  For "Variable name", enter `BAZEL_SH`

5.  Click "Browse File..."

6.  Navigate to the MSYS2 directory, then to `usr\bin` below it.

    For example, this might be `C:\msys64\usr\bin` on your system.

7.  Select the `bash.exe` or `bash` file and click OK

8.  The "Variable value" field now has the path to `bash.exe`. Click OK to close the window.

9.  Done.

    If you open a new cmd.exe or PowerShell terminal and run Bazel now, it will find Bash.

### Bazel does not find Visual Studio or Visual C++

**Possible reasons**:

*   you installed multiple versions of Visual Studio

*   you installed and removed various versions of Visual Studio

*   you installed various versions of the Windows SDK

*   you installed Visual Studio not under the default install path

**Solution**:

1.  Go to Start Menu &gt; Settings.

2.  Find the setting "Edit environment variables for your account"

3.  Look at the list on the top ("User variables for &lt;username&gt;"), and click the "New..."
    button below it.

4.  For "Variable name", enter `BAZEL_VC`

5.  Click "Browse Directory..."

6.  Navigate to the `VC` directory of Visual Studio.

    For example, this might be `C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC`
    on your system.

7.  Select the `VC` folder and click OK

8.  The "Variable value" field now has the path to `VC`. Click OK to close the window.

9.  Done.

    If you open a new cmd.exe or PowerShell terminal and run Bazel now, it will find Visual C++.

## Other ways to install Bazel

### Using Chocolatey

1.  Install the [Chocolatey](https://chocolatey.org) package manager

2.  Install the Bazel package:

    ```posix-terminal
    choco install bazel
    ```

    This command will install the latest available version of Bazel and
    its dependencies, such as the MSYS2 shell. This will not install Visual C++
    though.

See [Chocolatey installation and package maintenance
guide](/contribute/windows-chocolatey-maintenance) for more
information about the Chocolatey package.

### Using Scoop

1.  Install the [Scoop](https://scoop.sh/) package manager using the following PowerShell command:

    ```posix-terminal
    iex (new-object net.webclient).downloadstring('https://get.scoop.sh')
    ```

2.  Install the Bazel package:

    ```posix-terminal
    scoop install bazel
    ```

See [Scoop installation and package maintenance
guide](/contribute/windows-scoop-maintenance) for more
information about the Scoop package.

### Build from source

To build Bazel from scratch instead of installing, see [Compiling from source](/install/compile-source).

---

## Migrating to Bazel
- URL: https://bazel.build/migrate
- Source: migrate/index.mdx
- Slug: /migrate

This page links to migration guides for Bazel.

*  [Maven](/migrate/maven)
*  [Xcode](/migrate/xcode)

---

## Migrating from Maven to Bazel
- URL: https://bazel.build/migrate/maven
- Source: migrate/maven.mdx
- Slug: /migrate/maven

This page describes how to migrate from Maven to Bazel, including the
prerequisites and installation steps. It describes the differences between Maven
and Bazel, and provides a migration example using the Guava project.

When migrating from any build tool to Bazel, it's best to have both build tools
running in parallel until you have fully migrated your development team, CI
system, and any other relevant systems. You can run Maven and Bazel in the same
repository.

Note: While Bazel supports downloading and publishing Maven artifacts with
[rules_jvm_external](https://github.com/bazelbuild/rules_jvm_external)
, it does not directly support Maven-based plugins. Maven plugins can't be
directly run by Bazel since there's no Maven compatibility layer.

## Before you begin

*   [Install Bazel](/install) if it's not yet installed.
*   If you're new to Bazel, go through the tutorial [Introduction to Bazel:
    Build Java](/start/java) before you start migrating. The tutorial explains
    Bazel's concepts, structure, and label syntax.

## Differences between Maven and Bazel

*   Maven uses top-level `pom.xml` file(s). Bazel supports multiple build files
    and multiple targets per `BUILD` file, allowing for builds that are more
    incremental than Maven's.
*   Maven takes charge of steps for the deployment process. Bazel does not
    automate deployment.
*   Bazel enables you to express dependencies between languages.
*   As you add new sections to the project, with Bazel you may need to add new
    `BUILD` files. Best practice is to add a `BUILD` file to each new Java
    package.

## Migrate from Maven to Bazel

The steps below describe how to migrate your project to Bazel:

1.  [Create the MODULE.bazel file](#1-build)
2.  [Create one BUILD file](#2-build)
3.  [Create more BUILD files](#3-build)
4.  [Build using Bazel](#4-build)

Examples below come from a migration of the [Guava
project](https://github.com/google/guava) from Maven to Bazel. The
Guava project used is release `v31.1`. The examples using Guava do not walk
through each step in the migration, but they do show the files and contents that
are generated or added manually for the migration.

```
$ git clone https://github.com/google/guava.git && cd guava
$ git checkout v31.1
```

### 1. Create the MODULE.bazel file

Create a file named `MODULE.bazel` at the root of your project. If your project
has no external dependencies, this file can be empty.

If your project depends on files or packages that are not in one of the
project's directories, specify these external dependencies in the MODULE.bazel
file. You can use `rules_jvm_external` to manage dependencies from Maven. For
instructions about using this ruleset, see [the
README](https://github.com/bazelbuild/rules_jvm_external/#rules_jvm_external)
.

#### Guava project example: external dependencies

You can list the external dependencies of the [Guava
project](https://github.com/google/guava) with the
[`rules_jvm_external`](https://github.com/bazelbuild/rules_jvm_external)
ruleset.

Add the following snippet to the `MODULE.bazel` file:

```python
bazel_dep(name = "rules_jvm_external", version = "6.2")
maven = use_extension("@rules_jvm_external//:extensions.bzl", "maven")
maven.install(
    artifacts = [
        "com.google.code.findbugs:jsr305:3.0.2",
        "com.google.errorprone:error_prone_annotations:2.11.0",
        "com.google.j2objc:j2objc-annotations:1.3",
        "org.codehaus.mojo:animal-sniffer-annotations:1.20",
        "org.checkerframework:checker-qual:3.12.0",
    ],
    repositories = [
        "https://repo1.maven.org/maven2",
    ],
)
use_repo(maven, "maven")
```

### 2. Create one BUILD file

Now that you have your workspace defined and external dependencies (if
applicable) listed, you need to create `BUILD` files to describe how your
project should be built. Unlike Maven with its one `pom.xml` file, Bazel can use
many `BUILD` files to build a project. These files specify multiple build
targets, which allow Bazel to produce incremental builds.

Add `BUILD` files in stages. Start with adding one `BUILD` file at the root of
your project and using it to do an initial build using Bazel. Then, you refine
your build by adding more `BUILD` files with more granular targets.

1.  In the same directory as your `MODULE.bazel` file, create a text file and
    name it `BUILD`.

2.  In this `BUILD` file, use the appropriate rule to create one target to build
    your project. Here are some tips:

    *   Use the appropriate rule:
        *   To build projects with a single Maven module, use the
            `java_library` rule as follows:

            ```python
            java_library(
               name = "everything",
               srcs = glob(["src/main/java/**/*.java"]),
               resources = glob(["src/main/resources/**"]),
               deps = ["//:all-external-targets"],
            )
            ```

        *   To build projects with multiple Maven modules, use the
            `java_library` rule as follows:

            ```python
            java_library(
               name = "everything",
               srcs = glob([
                     "Module1/src/main/java/**/*.java",
                     "Module2/src/main/java/**/*.java",
                     ...
               ]),
               resources = glob([
                     "Module1/src/main/resources/**",
                     "Module2/src/main/resources/**",
                     ...
               ]),
               deps = ["//:all-external-targets"],
            )
            ```

        *   To build binaries, use the `java_binary` rule:

            ```python
            java_binary(
               name = "everything",
               srcs = glob(["src/main/java/**/*.java"]),
               resources = glob(["src/main/resources/**"]),
               deps = ["//:all-external-targets"],
               main_class = "com.example.Main"
            )
            ```

        *   Specify the attributes:
            *   `name`: Give the target a meaningful name. In the examples
                above, the target is called "everything."
            *   `srcs`: Use globbing to list all .java files in your project.
            *   `resources`: Use globbing to list all resources in your project.
            *   `deps`: You need to determine which external dependencies your
                project needs.
        *   Take a look at the [example below of this top-level BUILD
            file](#guava-2) from the migration of the Guava project.

3.  Now that you have a `BUILD` file at the root of your project, build your
    project to ensure that it works. On the command line, from your workspace
    directory, use `bazel build //:everything` to build your project with Bazel.

    The project has now been successfully built with Bazel. You will need to add
    more `BUILD` files to allow incremental builds of the project.

#### Guava project example: start with one BUILD file

When migrating the Guava project to Bazel, initially one `BUILD` file is used to
build the entire project. Here are the contents of this initial `BUILD` file in
the workspace directory:

```python
java_library(
    name = "everything",
    srcs = glob([
        "guava/src/**/*.java",
        "futures/failureaccess/src/**/*.java",
    ]),
    javacopts = ["-XepDisableAllChecks"],
    deps = [
        "@maven//:com_google_code_findbugs_jsr305",
        "@maven//:com_google_errorprone_error_prone_annotations",
        "@maven//:com_google_j2objc_j2objc_annotations",
        "@maven//:org_checkerframework_checker_qual",
        "@maven//:org_codehaus_mojo_animal_sniffer_annotations",
    ],
)
```

### 3. Create more BUILD files (optional)

Bazel does work with just one `BUILD file`, as you saw after completing your
first build. You should still consider breaking the build into smaller chunks by
adding more `BUILD` files with granular targets.

Multiple `BUILD` files with multiple targets will give the build increased
granularity, allowing:

*   increased incremental builds of the project,
*   increased parallel execution of the build,
*   better maintainability of the build for future users, and
*   control over visibility of targets between packages, which can prevent
    issues such as libraries containing implementation details leaking into
    public APIs.

Tips for adding more `BUILD` files:

*   You can start by adding a `BUILD` file to each Java package. Start with Java
    packages that have the fewest dependencies and work you way up to packages
    with the most dependencies.
*   As you add `BUILD` files and specify targets, add these new targets to the
    `deps` sections of targets that depend on them. Note that the `glob()`
    function does not cross package boundaries, so as the number of packages
    grows the files matched by `glob()` will shrink.
*   Any time you add a `BUILD` file to a `main` directory, ensure that you add a
    `BUILD` file to the corresponding `test` directory.
*   Take care to limit visibility properly between packages.
*   To simplify troubleshooting errors in your setup of `BUILD` files, ensure
    that the project continues to build with Bazel as you add each build file.
    Run `bazel build //...` to ensure all of your targets still build.

### 4. Build using Bazel

You've been building using Bazel as you add `BUILD` files to validate the setup
of the build.

When you have `BUILD` files at the desired granularity, you can use Bazel to
produce all of your builds.

---

## Migrating from Xcode to Bazel
- URL: https://bazel.build/migrate/xcode
- Source: migrate/xcode.mdx
- Slug: /migrate/xcode

This page describes how to build or test an Xcode project with Bazel. It
describes the differences between Xcode and Bazel, and provides the steps for
converting an Xcode project to a Bazel project. It also provides troubleshooting
solutions to address common errors.

## Differences between Xcode and Bazel

*   Bazel requires you to explicitly specify every build target and its
    dependencies, plus the corresponding build settings via build rules.

*   Bazel requires all files on which the project depends to be present within
    the workspace directory or specified as dependencies in the `MODULE.bazel`
    file.

*   When building Xcode projects with Bazel, the `BUILD` file(s) become the
    source of truth. If you work on the project in Xcode, you must generate a
    new version of the Xcode project that matches the `BUILD` files using
    [rules_xcodeproj](https://github.com/buildbuddy-io/rules_xcodeproj/)
    whenever you update the `BUILD` files. Certain changes to the `BUILD` files
    such as adding dependencies to a target don't require regenerating the
    project which can speed up development. If you're not using Xcode, the
    `bazel build` and `bazel test` commands provide build and test capabilities
    with certain limitations described later in this guide.

## Before you begin

Before you begin, do the following:

1.  [Install Bazel](/install) if you have not already done so.

2.  If you're not familiar with Bazel and its concepts, complete the [iOS app
    tutorial](/start/ios-app)). You should understand the Bazel workspace,
    including the `MODULE.bazel` and `BUILD` files, as well as the concepts of
    targets, build rules, and Bazel packages.

3.  Analyze and understand the project's dependencies.

### Analyze project dependencies

Unlike Xcode, Bazel requires you to explicitly declare all dependencies for
every target in the `BUILD` file.

For more information on external dependencies, see [Working with external
dependencies](/docs/external).

## Build or test an Xcode project with Bazel

To build or test an Xcode project with Bazel, do the following:

1.  [Create the `MODULE.bazel` file](#create-workspace)

2.  [(Experimental) Integrate SwiftPM dependencies](#integrate-swiftpm)

3.  [Create a `BUILD` file:](#create-build-file)

    a.  [Add the application target](#add-app-target)

    b.  [(Optional) Add the test target(s)](#add-test-target)

    c.  [Add the library target(s)](#add-library-target)

4.  [(Optional) Granularize the build](#granularize-build)

5.  [Run the build](#run-build)

6.  [Generate the Xcode project with rules_xcodeproj](#generate-the-xcode-project-with-rules_xcodeproj)

### Step 1: Create the `MODULE.bazel` file

Create a `MODULE.bazel` file in a new directory. This directory becomes the
Bazel workspace root. If the project uses no external dependencies, this file
can be empty. If the project depends on files or packages that are not in one of
the project's directories, specify these external dependencies in the
`MODULE.bazel` file.

Note: Place the project source code within the directory tree containing the
`MODULE.bazel` file.

### Step 2: (Experimental) Integrate SwiftPM dependencies

To integrate SwiftPM dependencies into the Bazel workspace with
[swift_bazel](https://github.com/cgrindel/swift_bazel), you must
convert them into Bazel packages as described in the [following
tutorial](https://chuckgrindel.com/swift-packages-in-bazel-using-swift_bazel/)
.

Note: SwiftPM support is a manual process with many variables. SwiftPM
integration with Bazel has not been fully verified and is not officially
supported.

### Step 3: Create a `BUILD` file

Once you have defined the workspace and external dependencies, you need to
create a `BUILD` file that tells Bazel how the project is structured. Create the
`BUILD` file at the root of the Bazel workspace and configure it to do an
initial build of the project as follows:

*   [Step 3a: Add the application target](#step-3a-add-the-application-target)
*   [Step 3b: (Optional) Add the test target(s)](#step-3b-optional-add-the-test-target-s)
*   [Step 3c: Add the library target(s)](#step-3c-add-the-library-target-s)

**Tip:** To learn more about packages and other Bazel concepts, see [Workspaces,
packages, and targets](/concepts/build-ref).

#### Step 3a: Add the application target

Add a
[`macos_application`](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-macos.md#macos_application)
or an
[`ios_application`](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-ios.md#ios_application)
rule target. This target builds a macOS or iOS application bundle, respectively.
In the target, specify the following at the minimum:

*   `bundle_id` - the bundle ID (reverse-DNS path followed by app name) of the
    binary.

*   `provisioning_profile` - provisioning profile from your Apple Developer
    account (if building for an iOS device device).

*   `families` (iOS only) - whether to build the application for iPhone, iPad,
    or both.

*   `infoplists` - list of .plist files to merge into the final Info.plist file.

*   `minimum_os_version` - the minimum version of macOS or iOS that the
    application supports. This ensures Bazel builds the application with the
    correct API levels.

#### Step 3b: (Optional) Add the test target(s)

Bazel's [Apple build
rules](https://github.com/bazelbuild/rules_apple) support running
unit and UI tests on all Apple platforms. Add test targets as follows:

*   [`macos_unit_test`](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-macos.md#macos_unit_test)
    to run library-based and application-based unit tests on a macOS.

*   [`ios_unit_test`](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-ios.md#ios_unit_test)
    to build and run library-based unit tests on iOS.

*   [`ios_ui_test`](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-ios.md#ios_ui_test)
    to build and run user interface tests in the iOS simulator.

*   Similar test rules exist for
    [tvOS](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-tvos.md),
    [watchOS](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-watchos.md)
    and
    [visionOS](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-visionos.md).

At the minimum, specify a value for the `minimum_os_version` attribute. While
other packaging attributes, such as `bundle_identifier` and `infoplists`,
default to most commonly used values, ensure that those defaults are compatible
with the project and adjust them as necessary. For tests that require the iOS
simulator, also specify the `ios_application` target name as the value of the
`test_host` attribute.

#### Step 3c: Add the library target(s)

Add an [`objc_library`](/reference/be/objective-c#objc_library) target for each
Objective-C library and a
[`swift_library`](https://github.com/bazelbuild/rules_swift/blob/master/doc/rules.md#swift_library)
target for each Swift library on which the application and/or tests depend.

Add the library targets as follows:

*   Add the application library targets as dependencies to the application
    targets.

*   Add the test library targets as dependencies to the test targets.

*   List the implementation sources in the `srcs` attribute.

*   List the headers in the `hdrs` attribute.

Note: You can use the [`glob`](/reference/be/functions#glob) function to include
all sources and/or headers of a certain type. Use it carefully as it might
include files you do not want Bazel to build.

You can browse existing examples for various types of applications directly in
the [rules_apple examples
directory](https://github.com/bazelbuild/rules_apple/tree/master/examples/). For
example:

*   [macOS application targets](https://github.com/bazelbuild/rules_apple/tree/master/examples/macos)

*   [iOS applications targets](https://github.com/bazelbuild/rules_apple/tree/master/examples/ios)

*   [Multi platform applications (macOS, iOS, watchOS, tvOS)](https://github.com/bazelbuild/rules_apple/tree/master/examples/multi_platform)

For more information on build rules, see [Apple Rules for
Bazel](https://github.com/bazelbuild/rules_apple).

At this point, it is a good idea to test the build:

`bazel build //:<application_target>`

### Step 4: (Optional) Granularize the build

If the project is large, or as it grows, consider chunking it into multiple
Bazel packages. This increased granularity provides:

*   Increased incrementality of builds,

*   Increased parallelization of build tasks,

*   Better maintainability for future users,

*   Better control over source code visibility across targets and packages. This
    prevents issues such as libraries containing implementation details leaking
    into public APIs.

Tips for granularizing the project:

*   Put each library in its own Bazel package. Start with those requiring the
    fewest dependencies and work your way up the dependency tree.

*   As you add `BUILD` files and specify targets, add these new targets to the
    `deps` attributes of targets that depend on them.

*   The `glob()` function does not cross package boundaries, so as the number of
    packages grows the files matched by `glob()` will shrink.

*   When adding a `BUILD` file to a `main` directory, also add a `BUILD` file to
    the corresponding `test` directory.

*   Enforce healthy visibility limits across packages.

*   Build the project after each major change to the `BUILD` files and fix build
    errors as you encounter them.

### Step 5: Run the build

Run the fully migrated build to ensure it completes with no errors or warnings.
Run every application and test target individually to more easily find sources
of any errors that occur.

For example:

```posix-terminal
bazel build //:my-target
```

### Step 6: Generate the Xcode project with rules_xcodeproj

When building with Bazel, the `MODULE.bazel` and `BUILD` files become the source
of truth about the build. To make Xcode aware of this, you must generate a
Bazel-compatible Xcode project using
[rules_xcodeproj](https://github.com/buildbuddy-io/rules_xcodeproj#features)
.

### Troubleshooting

Bazel errors can arise when it gets out of sync with the selected Xcode version,
like when you apply an update. Here are some things to try if you're
experiencing errors with Xcode, for example "Xcode version must be specified to
use an Apple CROSSTOOL".

*   Manually run Xcode and accept any terms and conditions.

*   Use Xcode select to indicate the correct version, accept the license, and
    clear Bazel's state.

```posix-terminal
  sudo xcode-select -s /Applications/Xcode.app/Contents/Developer

  sudo xcodebuild -license

  bazel sync --configure
```

*   If this does not work, you may also try running `bazel clean --expunge`.

Note: If you've saved your Xcode to a different path, you can use `xcode-select
-s` to point to that path.

---

## Action Graph Query (aquery)
- URL: https://bazel.build/query/aquery
- Source: query/aquery.mdx
- Slug: /query/aquery

The `aquery` command allows you to query for actions in your build graph.
It operates on the post-analysis Configured Target Graph and exposes
information about **Actions, Artifacts and their relationships.**

`aquery` is useful when you are interested in the properties of the Actions/Artifacts
generated from the Configured Target Graph. For example, the actual commands run
and their inputs/outputs/mnemonics.

The tool accepts several command-line [options](#command-options).
Notably, the aquery command runs on top of a regular Bazel build and inherits
the set of options available during a build.

It supports the same set of functions that is also available to traditional
`query` but `siblings`, `buildfiles` and
`tests`.

An example `aquery` output (without specific details):

```
$ bazel aquery 'deps(//some:label)'
action 'Writing file some_file_name'
  Mnemonic: ...
  Target: ...
  Configuration: ...
  ActionKey: ...
  Inputs: [...]
  Outputs: [...]
```

## Basic syntax

A simple example of the syntax for `aquery` is as follows:

`bazel aquery "aquery_function(function(//target))"`

The query expression (in quotes) consists of the following:

*   `aquery_function(...)`: functions specific to `aquery`.
    More details [below](#using-aquery-functions).
*   `function(...)`: the standard [functions](/query/language#functions)
    as traditional `query`.
*   `//target` is the label to the interested target.

```
# aquery examples:
# Get the action graph generated while building //src/target_a
$ bazel aquery '//src/target_a'

# Get the action graph generated while building all dependencies of //src/target_a
$ bazel aquery 'deps(//src/target_a)'

# Get the action graph generated while building all dependencies of //src/target_a
# whose inputs filenames match the regex ".*cpp".
$ bazel aquery 'inputs(".*cpp", deps(//src/target_a))'
```

## Using aquery functions

There are three `aquery` functions:

*   `inputs`: filter actions by inputs.
*   `outputs`: filter actions by outputs
*   `mnemonic`: filter actions by mnemonic

`expr ::= inputs(word, expr)`

  The `inputs` operator returns the actions generated from building `expr`,
  whose input filenames match the regex provided by `word`.

`$ bazel aquery 'inputs(".*cpp", deps(//src/target_a))'`

`outputs` and `mnemonic` functions share a similar syntax.

You can also combine functions to achieve the AND operation. For example:

```
  $ bazel aquery 'mnemonic("Cpp.*", (inputs(".*cpp", inputs("foo.*", //src/target_a))))'
```

  The above command would find all actions involved in building `//src/target_a`,
  whose mnemonics match `"Cpp.*"` and inputs match the patterns
  `".*cpp"` and `"foo.*"`.

Important: aquery functions can't be nested inside non-aquery functions.
Conceptually, this makes sense since the output of aquery functions is Actions,
not Configured Targets.

An example of the syntax error produced:

```
        $ bazel aquery 'deps(inputs(".*cpp", //src/target_a))'
        ERROR: aquery filter functions (inputs, outputs, mnemonic) produce actions,
        and therefore can't be the input of other function types: deps
        deps(inputs(".*cpp", //src/target_a))
```

## Options

### Build options

`aquery` runs on top of a regular Bazel build and thus inherits the set of
[options](/reference/command-line-reference#build-options)
available during a build.

### Aquery options

#### `--output=(text|summary|commands|proto|jsonproto|textproto), default=text`

The default output format (`text`) is human-readable,
use `proto`, `textproto`, or `jsonproto` for machine-readable format.
The proto message is `analysis.ActionGraphContainer`.

The `commands` output format prints a list of build commands with
one command per line.

In general, do not depend on the order of output. For more information,
see the [core query ordering contract](/query/language#graph-order).

#### `--include_commandline, default=true`

Includes the content of the action command lines in the output (potentially large).

#### `--include_artifacts, default=true`

Includes names of the action inputs and outputs in the output (potentially large).

#### `--include_aspects, default=true`

Whether to include Aspect-generated actions in the output.

#### `--include_param_files, default=false`

Include the content of the param files used in the command (potentially large).

Warning: Enabling this flag will automatically enable the `--include_commandline` flag.

#### `--include_file_write_contents, default=false`

Include file contents for the `actions.write()` action and the contents of the
manifest file for the `SourceSymlinkManifest` action The file contents is
returned in the `file_contents` field with `--output=`xxx`proto`.
With `--output=text`, the output has
```
FileWriteContents: [<base64-encoded file contents>]
```
line

#### `--skyframe_state, default=false`

Without performing extra analysis, dump the Action Graph from Skyframe.

Note: Specifying a target with `--skyframe_state` is currently not supported.
This flag is only available with `--output=proto` or `--output=textproto`.

## Other tools and features

### Querying against the state of Skyframe

[Skyframe](/reference/skyframe) is the evaluation and
incrementality model of Bazel. On each instance of Bazel server, Skyframe stores the dependency graph
constructed from the previous runs of the [Analysis phase](/run/build#analysis).

In some cases, it is useful to query the Action Graph on Skyframe.
An example use case would be:

1.  Run `bazel build //target_a`
2.  Run `bazel build //target_b`
3.  File `foo.out` was generated.

_As a Bazel user, I want to determine if `foo.out` was generated from building
`//target_a` or `//target_b`_.

One could run `bazel aquery 'outputs("foo.out", //target_a)'` and
`bazel aquery 'outputs("foo.out", //target_b)'` to figure out the action responsible
for creating `foo.out`, and in turn the target. However, the number of different
targets previously built can be larger than 2, which makes running multiple `aquery`
commands a hassle.

As an alternative, the `--skyframe_state` flag can be used:

```
  # List all actions on Skyframe's action graph
  $ bazel aquery --output=proto --skyframe_state

  # or

  # List all actions on Skyframe's action graph, whose output matches "foo.out"
  $ bazel aquery --output=proto --skyframe_state 'outputs("foo.out")'
```

With `--skyframe_state` mode, `aquery` takes the content of the Action Graph
that Skyframe keeps on the instance of Bazel, (optionally) performs filtering on it and
outputs the content, without re-running the analysis phase.

#### Special considerations

##### Output format

`--skyframe_state` is currently only available for `--output=proto`
and `--output=textproto`

##### Non-inclusion of target labels in the query expression

Currently, `--skyframe_state` queries the whole action graph that exists on Skyframe,
regardless of the targets. Having the target label specified in the query together with
`--skyframe_state` is considered a syntax error:

```
  # WRONG: Target Included
  $ bazel aquery --output=proto --skyframe_state **//target_a**
  ERROR: Error while parsing '//target_a)': Specifying build target(s) [//target_a] with --skyframe_state is currently not supported.

  # WRONG: Target Included
  $ bazel aquery --output=proto --skyframe_state 'inputs(".*.java", **//target_a**)'
  ERROR: Error while parsing '//target_a)': Specifying build target(s) [//target_a] with --skyframe_state is currently not supported.

  # CORRECT: Without Target
  $ bazel aquery --output=proto --skyframe_state
  $ bazel aquery --output=proto --skyframe_state 'inputs(".*.java")'
```

### Comparing aquery outputs

You can compare the outputs of two different aquery invocations using the `aquery_differ` tool.
For instance: when you make some changes to your rule definition and want to verify that the
command lines being run did not change. `aquery_differ` is the tool for that.

The tool is available in the [bazelbuild/bazel](https://github.com/bazelbuild/bazel/tree/master/tools/aquery_differ) repository.
To use it, clone the repository to your local machine. An example usage:

```
  $ bazel run //tools/aquery_differ -- \
  --before=/path/to/before.proto \
  --after=/path/to/after.proto \
  --input_type=proto \
  --attrs=cmdline \
  --attrs=inputs
```

The above command returns the difference between the `before` and `after` aquery outputs:
which actions were present in one but not the other, which actions have different
command line/inputs in each aquery output, ...). The result of running the above command would be:

```
  Aquery output 'after' change contains an action that generates the following outputs that aquery output 'before' change doesn't:
  ...
  /list of output files/
  ...

  [cmdline]
  Difference in the action that generates the following output(s):
    /path/to/abc.out
  --- /path/to/before.proto
  +++ /path/to/after.proto
  @@ -1,3 +1,3 @@
    ...
    /cmdline diff, in unified diff format/
    ...
```

#### Command options

`--before, --after`: The aquery output files to be compared

`--input_type=(proto|text_proto), default=proto`: the format of the input
files. Support is provided for `proto` and `textproto` aquery output.

`--attrs=(cmdline|inputs), default=cmdline`: the attributes of actions
to be compared.

### Aspect-on-aspect

It is possible for [Aspects](/extending/aspects)
to be applied on top of each other. The aquery output of the action generated by
these Aspects would then include the _Aspect path_, which is the sequence of
Aspects applied to the target which generated the action.

An example of Aspect-on-Aspect:

```
  t0
  ^
  | <- a1
  t1
  ^
  | <- a2
  t2
```

Let t<sub>i</sub> be a target of rule r<sub>i</sub>, which applies an Aspect a<sub>i</sub>
to its dependencies.

Assume that a2 generates an action X when applied to target t0. The text output of
`bazel aquery --include_aspects 'deps(//t2)'` for action X would be:

```
  action ...
  Mnemonic: ...
  Target: //my_pkg:t0
  Configuration: ...
  AspectDescriptors: [//my_pkg:rule.bzl%**a2**(foo=...)
    -> //my_pkg:rule.bzl%**a1**(bar=...)]
  ...
```

This means that action `X` was generated by Aspect `a2` applied onto
`a1(t0)`, where `a1(t0)` is the result of Aspect `a1` applied
onto target `t0`.

Each `AspectDescriptor` has the following format:

```
  AspectClass([param=value,...])
```

`AspectClass` could be the name of the Aspect class (for native Aspects) or
`bzl_file%aspect_name` (for Starlark Aspects). `AspectDescriptor` are
sorted in topological order of the
[dependency graph](/extending/aspects#aspect_basics).

### Linking with the JSON profile

While aquery provides information about the actions being run in a build (why they're being run,
their inputs/outputs), the [JSON profile](/rules/performance#performance-profiling)
tells us the timing and duration of their execution.
It is possible to combine these 2 sets of information via a common denominator: an action's primary output.

To include actions' outputs in the JSON profile, generate the profile with
`--experimental_include_primary_output --noslim_profile`.
Slim profiles are incompatible with the inclusion of primary outputs. An action's primary output
is included by default by aquery.

We don't currently provide a canonical tool to combine these 2 data sources, but you should be
able to build your own script with the above information.

## Known issues

### Handling shared actions

Sometimes actions are
[shared](https://source.bazel.build/bazel/+/master:src/main/java/com/google/devtools/build/lib/actions/Actions.java;l=59;drc=146d51aa1ec9dcb721a7483479ef0b1ac21d39f1)
between configured targets.

In the execution phase, those shared actions are
[simply considered as one](https://source.bazel.build/bazel/+/master:src/main/java/com/google/devtools/build/lib/actions/Actions.java;l=241;drc=003b8734036a07b496012730964ac220f486b61f) and only executed once.
However, aquery operates on the pre-execution, post-analysis action graph, and hence treats these
like separate actions whose output Artifacts have the exact same `execPath`. As a result,
equivalent Artifacts appear duplicated.

The list of aquery issues/planned features can be found on
[GitHub](https://github.com/bazelbuild/bazel/labels/team-Performance).

## FAQs

### The ActionKey remains the same even though the content of an input file changed.

In the context of aquery, the `ActionKey` refers to the `String` gotten from
[ActionAnalysisMetadata#getKey](https://source.bazel.build/bazel/+/master:src/main/java/com/google/devtools/build/lib/actions/ActionAnalysisMetadata.java;l=89;drc=8b856f5484f0117b2aebc302f849c2a15f273310):

```
  Returns a string encoding all of the significant behaviour of this Action that might affect the
  output. The general contract of `getKey` is this: if the work to be performed by the
  execution of this action changes, the key must change.

  ...

  Examples of changes that should affect the key are:

  - Changes to the BUILD file that materially affect the rule which gave rise to this Action.
  - Changes to the command-line options, environment, or other global configuration resources
      which affect the behaviour of this kind of Action (other than changes to the names of the
      input/output files, which are handled externally).
  - An upgrade to the build tools which changes the program logic of this kind of Action
      (typically this is achieved by incorporating a UUID into the key, which is changed each
      time the program logic of this action changes).
  Note the following exception: for actions that discover inputs, the key must change if any
  input names change or else action validation may falsely validate.
```

This excludes the changes to the content of the input files, and is not to be confused with
[RemoteCacheClient#ActionKey](https://source.bazel.build/bazel/+/master:src/main/java/com/google/devtools/build/lib/remote/common/RemoteCacheClient.java;l=38;drc=21577f202eb90ce94a337ebd2ede824d609537b6).

## Updates

For any issues/feature requests, please file an issue [here](https://github.com/bazelbuild/bazel/issues/new).

---

## Configurable Query (cquery)
- URL: https://bazel.build/query/cquery
- Source: query/cquery.mdx
- Slug: /query/cquery

`cquery` is a variant of [`query`](/query/language) that correctly handles
[`select()`](/docs/configurable-attributes) and build options' effects on the
build graph.

It achieves this by running over the results of Bazel's [analysis
phase](/extending/concepts#evaluation-model),
which integrates these effects. `query`, by contrast, runs over the results of
Bazel's loading phase, before options are evaluated.

For example:

```
$ cat > tree/BUILD &lt;&lt;EOF
sh_library(
    name = "ash",
    deps = select({
        ":excelsior": [":manna-ash"],
        ":americana": [":white-ash"],
        "//conditions:default": [":common-ash"],
    }),
)
sh_library(name = "manna-ash")
sh_library(name = "white-ash")
sh_library(name = "common-ash")
config_setting(
    name = "excelsior",
    values = \{"define": "species=excelsior"\},
)
config_setting(
    name = "americana",
    values = \{"define": "species=americana"\},
)
EOF
```

```
# Traditional query: query doesn't know which select() branch you will choose,
# so it conservatively lists all of possible choices, including all used config_settings.
$ bazel query "deps(//tree:ash)" --noimplicit_deps
//tree:americana
//tree:ash
//tree:common-ash
//tree:excelsior
//tree:manna-ash
//tree:white-ash

# cquery: cquery lets you set build options at the command line and chooses
# the exact dependencies that implies (and also the config_setting targets).
$ bazel cquery "deps(//tree:ash)" --define species=excelsior --noimplicit_deps
//tree:ash (9f87702)
//tree:manna-ash (9f87702)
//tree:americana (9f87702)
//tree:excelsior (9f87702)
```

Each result includes a [unique identifier](#configurations) `(9f87702)` of
the [configuration](/reference/glossary#configuration) the
target is built with.

Since `cquery` runs over the configured target graph. it doesn't have insight
into artifacts like build actions nor access to [`test_suite`](/reference/be/general#test_suite)
rules as they are not configured targets. For the former, see [`aquery`](/query/aquery).

## Basic syntax

A simple `cquery` call looks like:

`bazel cquery "function(//target)"`

The query expression `"function(//target)"` consists of the following:

*   **`function(...)`** is the function to run on the target. `cquery`
    supports most
    of `query`'s [functions](/query/language#functions), plus a
    few new ones.
*   **`//target`** is the expression fed to the function. In this example, the
    expression is a simple target. But the query language also allows nesting of functions.
    See the [Query guide](/query/guide) for examples.


`cquery` requires a target to run through the [loading and analysis](/extending/concepts#evaluation-model)
phases. Unless otherwise specified, `cquery` parses the target(s) listed in the
query expression. See [`--universe_scope`](#universe-scope)
for querying dependencies of top-level build targets.

## Configurations

The line:

```
//tree:ash (9f87702)
```

means `//tree:ash` was built in a configuration with ID `9f87702`. For most
targets, this is an opaque hash of the build option values defining the
configuration.

To see the configuration's complete contents, run:

```
$ bazel config 9f87702
```

`9f87702` is a prefix of the complete ID. This is because complete IDs are
SHA-256 hashes, which are long and hard to follow. `cquery` understands any valid
prefix of a complete ID, similar to
[Git short hashes](https://git-scm.com/book/en/v2/Git-Tools-Revision-Selection#_revision_selection).
 To see complete IDs, run `$ bazel config`.

## Target pattern evaluation

`//foo` has a different meaning for `cquery` than for `query`. This is because
`cquery` evaluates _configured_ targets and the build graph may have multiple
configured versions of `//foo`.

For `cquery`, a target pattern in the query expression evaluates
to every configured target with a label that matches that pattern. Output is
deterministic, but `cquery` makes no ordering guarantee beyond the
[core query ordering contract](/query/language#graph-order).

This produces subtler results for query expressions than with `query`.
For example, the following can produce multiple results:

```
# Analyzes //foo in the target configuration, but also analyzes
# //genrule_with_foo_as_tool which depends on an exec-configured
# //foo. So there are two configured target instances of //foo in
# the build graph.
$ bazel cquery //foo --universe_scope=//foo,//genrule_with_foo_as_tool
//foo (9f87702)
//foo (exec)
```

If you want to precisely declare which instance to query over, use
the [`config`](#config) function.

See `query`'s [target pattern
documentation](/query/language#target-patterns) for more information on target
patterns.

## Functions

Of the [set of functions](/query/language#functions "list of query functions")
supported by `query`, `cquery` supports all but
[`allrdeps`](/query/language#allrdeps),
[`buildfiles`](/query/language#buildfiles),
[`rbuildfiles`](/query/language#rbuildfiles),
[`siblings`](/query/language#siblings), [`tests`](/query/language#tests), and
[`visible`](/query/language#visible).

`cquery` also introduces the following new functions:

### config

`expr ::= config(expr, word)`

The `config` operator attempts to find the configured target for
the label denoted by the first argument and configuration specified by the
second argument.

Valid values for the second argument are `null` or a
[custom configuration hash](#configurations). Hashes can be retrieved from `$
bazel config` or a previous `cquery`'s output.

Examples:

```
$ bazel cquery "config(//bar, 3732cc8)" --universe_scope=//foo
```

```
$ bazel cquery "deps(//foo)"
//bar (exec)
//baz (exec)

$ bazel cquery "config(//baz, 3732cc8)"
```

If not all results of the first argument can be found in the specified
configuration, only those that can be found are returned. If no results
can be found in the specified configuration, the query fails.

## Options

### Build options

`cquery` runs over a regular Bazel build and thus inherits the set of
[options](/reference/command-line-reference#build-options) available during a
build.

###  Using cquery options

#### `--universe_scope` (comma-separated list)

Often, the dependencies of configured targets go through
[transitions](/extending/rules#configurations),
which causes their configuration to differ from their dependent. This flag
allows you to query a target as if it were built as a dependency or a transitive
dependency of another target. For example:

```
# x/BUILD
genrule(
     name = "my_gen",
     srcs = ["x.in"],
     outs = ["x.cc"],
     cmd = "$(locations :tool) $&lt; >$@",
     tools = [":tool"],
)
cc_binary(
    name = "tool",
    srcs = ["tool.cpp"],
)
```

Genrules configure their tools in the
[exec configuration](/extending/rules#configurations)
so the following queries would produce the following outputs:

<table class="table table-condensed table-bordered table-params">
  <thead>
    <tr>
      <th>Query</th>
      <th>Target Built</th>
      <th>Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>bazel cquery "//x:tool"</td>
      <td>//x:tool</td>
      <td>//x:tool(targetconfig)</td>
    </tr>
    <tr>
      <td>bazel cquery "//x:tool" --universe_scope="//x:my_gen"</td>
      <td>//x:my_gen</td>
      <td>//x:tool(execconfig)</td>
    </tr>
  </tbody>
</table>

If this flag is set, its contents are built. _If it's not set, all targets
mentioned in the query expression are built_ instead. The transitive closure of
the built targets are used as the universe of the query. Either way, the targets
to be built must be buildable at the top level (that is, compatible with
top-level options). `cquery` returns results in the transitive closure of these
top-level targets.

Even if it's possible to build all targets in a query expression at the top
level, it may be beneficial to not do so. For example, explicitly setting
`--universe_scope` could prevent building targets multiple times in
configurations you don't care about. It could also help specify which
configuration version of a target you're looking for. You should set this flag
if your query expression is more complex than `deps(//foo)`.

#### `--implicit_deps` (boolean, default=True)

Setting this flag to false filters out all results that aren't explicitly set in
the BUILD file and instead set elsewhere by Bazel. This includes filtering
resolved toolchains.

#### `--tool_deps` (boolean, default=True)

Setting this flag to false filters out all configured targets for which the
path from the queried target to them crosses a transition between the target
configuration and the
[non-target configurations](/extending/rules#configurations). If the queried
target is in the target configuration, setting `--notool_deps` will only return
targets that also are in the target configuration. If the queried target is in a
non-target configuration, setting `--notool_deps` will only return targets also
in non-target configurations. This setting generally does not affect filtering
of resolved toolchains.

#### `--include_aspects` (boolean, default=True)

Include dependencies added by [aspects](/extending/aspects).

If this flag is disabled, `cquery somepath(X, Y)` and
`cquery deps(X) | grep 'Y'` omit Y if X only depends on it through an aspect.

## Output formats

By default, cquery outputs results in a dependency-ordered list of label and configuration pairs.
There are other options for exposing the results as well.

### Transitions

```
--transitions=lite
--transitions=full
```

Configuration [transitions](/extending/rules#configurations)
are used to build targets underneath the top level targets in different
configurations than the top level targets.

For example, a target might impose a transition to the exec configuration on all
dependencies in its `tools` attribute. These are known as attribute
transitions. Rules can also impose transitions on their own configurations,
known as rule class transitions. This output format outputs information about
these transitions such as what type they are and the effect they have on build
options.

This output format is triggered by the `--transitions` flag which by default is
set to `NONE`. It can be set to `FULL` or `LITE` mode. `FULL` mode outputs
information about rule class transitions and attribute transitions including a
detailed diff of the options before and after the transition. `LITE` mode
outputs the same information without the options diff.

### Protocol message output

```
--output=proto
```

This option causes the resulting targets to be printed in a binary protocol
buffer form. The definition of the protocol buffer can be found at
[src/main/protobuf/analysis_v2.proto](https://github.com/bazelbuild/bazel/blob/master/src/main/protobuf/analysis_v2.proto).

`CqueryResult` is the top level message containing the results of the cquery. It
has a list of `ConfiguredTarget` messages and a list of `Configuration`
messages. Each `ConfiguredTarget` has a `configuration_id` whose value is equal
to that of the `id` field from the corresponding `Configuration` message.

#### --[no]proto:include_configurations

By default, cquery results return configuration information as part of each
configured target. If you'd like to omit this information and get proto output
that is formatted exactly like query's proto output, set this flag to false.

See [query's proto output documentation](/query/language#output-formats)
for more proto output-related options.

Note: While selects are resolved both at the top level of returned
targets and within attributes, all possible inputs for selects are still
included as `rule_input` fields.

### Graph output

```
--output=graph
```

This option generates output as a Graphviz-compatible .dot file. See `query`'s
[graph output documentation](/query/language#display-result-graph) for details. `cquery`
also supports [`--graph:node_limit`](/query/language#graph-nodelimit) and
[`--graph:factored`](/query/language#graph-factored).

### Files output

```
--output=files
```

This option prints a list of the output files produced by each target matched
by the query similar to the list printed at the end of a `bazel build`
invocation. The output contains only the files advertised in the requested
output groups as determined by the
[`--output_groups`](/reference/command-line-reference#flag--output_groups) flag.
It does include source files.

All paths emitted by this output format are relative to the
[execroot](https://bazel.build/remote/output-directories), which can be obtained
via `bazel info execution_root`. If the `bazel-out` convenience symlink exists,
paths to files in the main repository also resolve relative to the workspace
directory.

Note: The output of `bazel cquery --output=files //pkg:foo` contains the output
files of `//pkg:foo` in *all* configurations that occur in the build (also see
the [section on target pattern evaluation](#target-pattern-evaluation)). If that
is not desired, wrap you query in [`config(..., target)`](#config).

### Defining the output format using Starlark

```
--output=starlark
```

This output format calls a [Starlark](/rules/language)
function for each configured target in the query result, and prints the value
returned by the call. The `--starlark:file` flag specifies the location of a
Starlark file that defines a function named `format` with a single parameter,
`target`. This function is called for each [Target](/rules/lib/builtins/Target)
in the query result. Alternatively, for convenience, you may specify just the
body of a function declared as `def format(target): return expr` by using the
`--starlark:expr` flag.

#### 'cquery' Starlark dialect

The cquery Starlark environment differs from a BUILD or .bzl file. It includes
all core Starlark
[built-in constants and functions](https://github.com/bazelbuild/starlark/blob/master/spec.md#built-in-constants-and-functions),
plus a few cquery-specific ones described below, but not (for example) `glob`,
`native`, or `rule`, and it does not support load statements.

##### build_options(target)

`build_options(target)` returns a map whose keys are build option identifiers
(see [Configurations](/extending/config)) and whose values are their Starlark
values. Build options whose values are not legal Starlark values are omitted
from this map.

If the target is an input file, `build_options(target)` returns None, as input
file targets have a null configuration.

##### providers(target)

`providers(target)` returns a map whose keys are names of
[providers](/extending/rules#providers)
(for example, `"DefaultInfo"`) and whose values are their Starlark values.
Providers whose values are not legal Starlark values are omitted from this map.

#### Examples

Print a space-separated list of the base names of all files produced by `//foo`:

```
  bazel cquery //foo --output=starlark \
    --starlark:expr="' '.join([f.basename for f in providers(target)['DefaultInfo'].files.to_list()])"
```

Print a space-separated list of the paths of all files produced by **rule** targets in
`//bar` and its subpackages:

```
  bazel cquery 'kind(rule, //bar/...)' --output=starlark \
    --starlark:expr="' '.join([f.path for f in providers(target)['DefaultInfo'].files.to_list()])"
```

Print a list of the mnemonics of all actions registered by `//foo`.

```
  bazel cquery //foo --output=starlark \
    --starlark:expr="[a.mnemonic for a in target.actions]"
```

Print a list of compilation outputs registered by a `cc_library` `//baz`.

```
  bazel cquery //baz --output=starlark \
    --starlark:expr="[f.path for f in target.output_groups.compilation_outputs.to_list()]"
```

Print the value of the command line option `--javacopt` when building `//foo`.

```
  bazel cquery //foo --output=starlark \
    --starlark:expr="build_options(target)['//command_line_option:javacopt']"
```

Print the label of each target with exactly one output. This example uses
Starlark functions defined in a file.

```
  $ cat example.cquery

  def has_one_output(target):
    return len(providers(target)["DefaultInfo"].files.to_list()) == 1

  def format(target):
    if has_one_output(target):
      return target.label
    else:
      return ""

  $ bazel cquery //baz --output=starlark --starlark:file=example.cquery
```

Print the label of each target which is strictly Python 3. This example uses
Starlark functions defined in a file.

```
  $ cat example.cquery

  def format(target):
    p = providers(target)
    py_info = p.get("PyInfo")
    if py_info and py_info.has_py3_only_sources:
      return target.label
    else:
      return ""

  $ bazel cquery //baz --output=starlark --starlark:file=example.cquery
```

Extract a value from a user defined Provider.

```
  $ cat some_package/my_rule.bzl

  MyRuleInfo = provider(fields=\{"color": "the name of a color"\})

  def _my_rule_impl(ctx):
      ...
      return [MyRuleInfo(color="red")]

  my_rule = rule(
      implementation = _my_rule_impl,
      attrs = {...},
  )

  $ cat example.cquery

  def format(target):
    p = providers(target)
    my_rule_info = p.get("//some_package:my_rule.bzl%MyRuleInfo'")
    if my_rule_info:
      return my_rule_info.color
    return ""

  $ bazel cquery //baz --output=starlark --starlark:file=example.cquery
```

## cquery vs. query

`cquery` and `query` complement each other and excel in
different niches. Consider the following to decide which is right for you:

*  `cquery` follows specific `select()` branches to
    model the exact graph you build. `query` doesn't know which
    branch the build chooses, so overapproximates by including all branches.
*   `cquery`'s precision requires building more of the graph than
    `query` does. Specifically, `cquery`
    evaluates _configured targets_ while `query` only
    evaluates _targets_. This takes more time and uses more memory.
*   `cquery`'s interpretation of
    the [query language](/query/language) introduces ambiguity
    that `query` avoids. For example,
    if `"//foo"` exists in two configurations, which one
    should `cquery "deps(//foo)"` use?
    The [`config`](#config) function can help with this.

## Non-deterministic output

`cquery` does not automatically wipe the build graph from previous commands.
It's therefore prone to picking up results from past queries.

For example, `genrule` exerts an exec transition on its `tools` attribute -
that is, it configures its tools in the [exec configuration]
(https://bazel.build/rules/rules#configurations).

You can see the lingering effects of that transition below.

```
$ cat > foo/BUILD <<<EOF
genrule(
    name = "my_gen",
    srcs = ["x.in"],
    outs = ["x.cc"],
    cmd = "$(locations :tool) $< >$@",
    tools = [":tool"],
)
cc_library(
    name = "tool",
)
EOF

# Evaluate //foo:tool in the target configuration.
$ blaze cquery "//foo:tool"
//foo:tool (473ccb7)

# Evaluate a genrule that adds an exec-configured //foo:tool to the build graph.
$ blaze cquery "somepath(//foo:my_gen, //foo:tool)"
//foo:my_gen (473ccb7)
//foo:tool (c932dde)

# The first command now shows both versions.
$ blaze cquery "//foo:tool"
//foo:tool (473ccb7)
//foo:tool (c932dde)
```

This may or may not be desirable behavior depending on what you're trying to
evaluate.

To disable this, run `blaze clean` before your `cquery` to ensure a fresh
analysis graph.

## Troubleshooting

### Recursive target patterns (`/...`)

If you encounter:

```
$ bazel cquery --universe_scope=//foo:app "somepath(//foo:app, //foo/...)"
ERROR: Error doing post analysis query: Evaluation failed: Unable to load package '[foo]'
because package is not in scope. Check that all target patterns in query expression are within the
--universe_scope of this query.
```

this incorrectly suggests package `//foo` isn't in scope even though
`--universe_scope=//foo:app` includes it. This is due to design limitations in
`cquery`. As a workaround, explicitly include `//foo/...` in the universe
scope:

```
$ bazel cquery --universe_scope=//foo:app,//foo/... "somepath(//foo:app, //foo/...)"
```

If that doesn't work (for example, because some target in `//foo/...` can't
build with the chosen build flags), manually unwrap the pattern into its
constituent packages with a pre-processing query:

```
# Replace "//foo/..." with a subshell query call (not cquery!) outputting each package, piped into
# a sed call converting "&lt;pkg&gt;" to "//&lt;pkg&gt;:*", piped into a "+"-delimited line merge.
# Output looks like "//foo:*+//foo/bar:*+//foo/baz".
#
$  bazel cquery --universe_scope=//foo:app "somepath(//foo:app, $(bazel query //foo/...
--output=package | sed -e 's/^/\/\//' -e 's/$/:*/' | paste -sd "+" -))"
```

---

## Query guide
- URL: https://bazel.build/query/guide
- Source: query/guide.mdx
- Slug: /query/guide

This page covers how to get started using Bazel's query language to trace
dependencies in your code.

For a language details and `--output` flag details, please see the
reference manuals, [Bazel query reference](/query/language)
and [Bazel cquery reference](/query/cquery). You can get help by
typing `bazel help query` or `bazel help cquery` on the
command line.

To execute a query while ignoring errors such as missing targets, use the
`--keep_going` flag.

## Finding the dependencies of a rule

To see the dependencies of `//foo`, use the
`deps` function in bazel query:

```
$ bazel query "deps(//foo)"
//foo:foo
//foo:foo-dep
...
```

This is the set of all targets required to build `//foo`.

## Tracing the dependency chain between two packages

The library `//third_party/zlib:zlibonly` isn't in the BUILD file for
`//foo`, but it is an indirect dependency. How can
we trace this dependency path?  There are two useful functions here:
`allpaths` and `somepath`. You may also want to exclude
tooling dependencies with `--notool_deps` if you care only about
what is included in the artifact you built, and not every possible job.

To visualize the graph of all dependencies, pipe the bazel query output through
  the `dot` command-line tool:

```
$ bazel query "allpaths(//foo, third_party/...)" --notool_deps --output graph | dot -Tsvg > /tmp/deps.svg
```

Note: `dot` supports other image formats, just replace `svg` with the
format identifier, for example, `png`.

When a dependency graph is big and complicated, it can be helpful start with a single path:

```
$ bazel query "somepath(//foo:foo, third_party/zlib:zlibonly)"
//foo:foo
//translations/tools:translator
//translations/base:base
//third_party/py/MySQL:MySQL
//third_party/py/MySQL:_MySQL.so
//third_party/mysql:mysql
//third_party/zlib:zlibonly
```

If you do not specify `--output graph` with `allpaths`,
you will get a flattened list of the dependency graph.

```
$ bazel query "allpaths(//foo, third_party/...)"
  ...many errors detected in BUILD files...
//foo:foo
//translations/tools:translator
//translations/tools:aggregator
//translations/base:base
//tools/pkg:pex
//tools/pkg:pex_phase_one
//tools/pkg:pex_lib
//third_party/python:python_lib
//translations/tools:messages
//third_party/py/xml:xml
//third_party/py/xml:utils/boolean.so
//third_party/py/xml:parsers/sgmlop.so
//third_party/py/xml:parsers/pyexpat.so
//third_party/py/MySQL:MySQL
//third_party/py/MySQL:_MySQL.so
//third_party/mysql:mysql
//third_party/openssl:openssl
//third_party/zlib:zlibonly
//third_party/zlib:zlibonly_v1_2_3
//third_party/python:headers
//third_party/openssl:crypto
```

### Aside: implicit dependencies

The BUILD file for `//foo` never references
`//translations/tools:aggregator`. So, where's the direct dependency?

Certain rules include implicit dependencies on additional libraries or tools.
For example, to build a `genproto` rule, you need first to build the Protocol
Compiler, so every `genproto` rule carries an implicit dependency on the
protocol compiler. These dependencies are not mentioned in the build file,
but added in by the build tool. The full set of implicit dependencies is
  currently undocumented. Using `--noimplicit_deps` allows you to filter out
  these deps from your query results. For cquery, this will include resolved toolchains.

## Reverse dependencies

You might want to know the set of targets that depends on some target. For instance,
if you're going to change some code, you might want to know what other code
you're about to break. You can use `rdeps(u, x)` to find the reverse
dependencies of the targets in `x` within the transitive closure of `u`.

Bazel's [Sky Query](/query/language#sky-query)
supports the `allrdeps` function which allows you to query reverse dependencies
in a universe you specify.

## Miscellaneous uses

You can use `bazel query` to analyze many dependency relationships.

### What exists ...

#### What packages exist beneath `foo`?

```bazel query 'foo/...' --output package```

#### What rules are defined in the `foo` package?

```bazel query 'kind(rule, foo:*)' --output label_kind```

#### What files are generated by rules in the `foo` package?

```bazel query 'kind("generated file", //foo:*)'```

#### What targets are generated by starlark macro `foo`?

```bazel query 'attr(generator_function, foo, //path/to/search/...)'```

#### What's the set of BUILD files needed to build `//foo`?

```bazel query 'buildfiles(deps(//foo))' | cut -f1 -d:```

#### What are the individual tests that a `test_suite` expands to?

```bazel query 'tests(//foo:smoke_tests)'```

#### Which of those are C++ tests?

```bazel query 'kind(cc_.*, tests(//foo:smoke_tests))'```

#### Which of those are small?  Medium?  Large?

```
bazel query 'attr(size, small, tests(//foo:smoke_tests))'

bazel query 'attr(size, medium, tests(//foo:smoke_tests))'

bazel query 'attr(size, large, tests(//foo:smoke_tests))'
```

#### What are the tests beneath `foo` that match a pattern?

```bazel query 'filter("pa?t", kind(".*_test rule", //foo/...))'```

The pattern is a regex and is applied to the full name of the rule. It's similar to doing

```bazel query 'kind(".*_test rule", //foo/...)' | grep -E 'pa?t'```

#### What package contains file `path/to/file/bar.java`?

``` bazel query path/to/file/bar.java --output=package```

#### What is the build label for `path/to/file/bar.java?`

```bazel query path/to/file/bar.java```

#### What rule target(s) contain file `path/to/file/bar.java` as a source?

```
fullname=$(bazel query path/to/file/bar.java)
bazel query "attr('srcs', $fullname, ${fullname//:*/}:*)"
```

### What package dependencies exist ...

#### What packages does `foo` depend on? (What do I need to check out to build `foo`)

```bazel query 'buildfiles(deps(//foo:foo))' --output package```

Note: `buildfiles` is required in order to correctly obtain all files
referenced by `subinclude`; see the reference manual for details.

#### What packages does the `foo` tree depend on, excluding `foo/contrib`?

```bazel query 'deps(foo/... except foo/contrib/...)' --output package```

### What rule dependencies exist ...

#### What genproto rules does bar depend upon?

```bazel query 'kind(genproto, deps(bar/...))'```

#### Find the definition of some JNI (C++) library that is transitively depended upon by a Java binary rule in the servlet tree.

```bazel query 'some(kind(cc_.*library, deps(kind(java_binary, //java/com/example/frontend/...))))' --output location```

##### ...Now find the definitions of all the Java binaries that depend on them

```bazel query 'let jbs = kind(java_binary, //java/com/example/frontend/...) in
  let cls = kind(cc_.*library, deps($jbs)) in
    $jbs intersect allpaths($jbs, $cls)'
```

### What file dependencies exist ...

#### What's the complete set of Java source files required to build foo?

Source files:

```bazel query 'kind("source file", deps(//path/to/target/foo/...))' | grep java$```

Generated files:

```bazel query 'kind("generated file", deps(//path/to/target/foo/...))' | grep java$```

#### What is the complete set of Java source files required to build QUX's tests? 

Source files:

```bazel query 'kind("source file", deps(kind(".*_test rule", javatests/com/example/qux/...)))' | grep java$```

Generated files:

```bazel query 'kind("generated file", deps(kind(".*_test rule", javatests/com/example/qux/...)))' | grep java$```

### What differences in dependencies between X and Y exist ...

#### What targets does `//foo` depend on that `//foo:foolib` does not?

```bazel query 'deps(//foo) except deps(//foo:foolib)'```

#### What C++ libraries do the `foo` tests depend on that the `//foo` production binary does _not_ depend on?

```bazel query 'kind("cc_library", deps(kind(".*test rule", foo/...)) except deps(//foo))'```

### Why does this dependency exist ...

#### Why does `bar` depend on `groups2`?

```bazel query 'somepath(bar/...,groups2/...:*)'```

Once you have the results of this query, you will often find that a single
target stands out as being an unexpected or egregious and undesirable
dependency of `bar`. The query can then be further refined to:

#### Show me a path from `docker/updater:updater_systest` (a `py_test`) to some `cc_library` that it depends upon:

```bazel query 'let cc = kind(cc_library, deps(docker/updater:updater_systest)) in
  somepath(docker/updater:updater_systest, $cc)'```

#### Why does library `//photos/frontend:lib` depend on two variants of the same library `//third_party/jpeglib` and `//third_party/jpeg`?

This query boils down to: "show me the subgraph of `//photos/frontend:lib` that
depends on both libraries". When shown in topological order, the last element
of the result is the most likely culprit.

```bazel query 'allpaths(//photos/frontend:lib, //third_party/jpeglib)
                intersect
               allpaths(//photos/frontend:lib, //third_party/jpeg)'
//photos/frontend:lib
//photos/frontend:lib_impl
//photos/frontend:lib_dispatcher
//photos/frontend:icons
//photos/frontend/modules/gadgets:gadget_icon
//photos/thumbnailer:thumbnail_lib
//third_party/jpeg/img:renderer
```

### What depends on  ...

#### What rules under bar depend on Y?

```bazel query 'bar/... intersect allpaths(bar/..., Y)'```

Note: `X intersect allpaths(X, Y)` is the general idiom for the query "which X
depend on Y?" If expression X is non-trivial, it may be convenient to bind a
name to it using `let` to avoid duplication.

#### What targets directly depend on T, in T's package?

```bazel query 'same_pkg_direct_rdeps(T)'```

### How do I break a dependency ...

{/* TODO find a convincing value of X to plug in here */}

#### What dependency paths do I have to break to make `bar` no longer depend on X?

To output the graph to a `svg` file:

```bazel query 'allpaths(bar/...,X)' --output graph | dot -Tsvg &gt; /tmp/dep.svg```

### Misc

#### How many sequential steps are there in the `//foo-tests` build?

Unfortunately, the query language can't currently give you the longest path
from x to y, but it can find the (or rather _a_) most distant node from the
starting point, or show you the _lengths_ of the longest path from x to every
y that it depends on. Use `maxrank`:

```bazel query 'deps(//foo-tests)' --output maxrank | tail -1
85 //third_party/zlib:zutil.c```

The result indicates that there exist paths of length 85 that must occur in
order in this build.

---

## Be Nav
- URL: https://bazel.build/reference/be/be-nav
- Source: reference/be/be-nav.mdx
- Slug: /reference/be/be-nav

\*\*Build Encyclopedia\*\*
- [Overview](/reference/be/overview.html)
- [Concepts](#be-menu)  - [Common Definitions](/reference/be/common-definitions.html)
  - ["Make" variables](/reference/be/make-variables.html)
- [Rules](#be-rules)  - [Functions](/reference/be/functions.html)
  - [C / C++](/reference/be/c-cpp.html)
  - [Java](/reference/be/java.html)
  - [Objective-C](/reference/be/objective-c.html)
  - [Protocol Buffer](/reference/be/protocol-buffer.html)
  - [Python](/reference/be/python.html)
  - [Shell](/reference/be/shell.html)
  - [Extra Actions](/reference/be/extra-actions.html)
  - [General](/reference/be/general.html)
  - [Platforms and Toolchains](/reference/be/platforms-and-toolchains.html)
  - [AppEngine](https://github.com/bazelbuild/rules_appengine)
  - [Apple (Swift, iOS, macOS, tvOS, visionOS, watchOS)](https://github.com/bazelbuild/rules_apple)
  - [C#](https://github.com/bazelbuild/rules_dotnet)
  - [D](https://github.com/bazelbuild/rules_d)
  - [Docker](https://github.com/bazelbuild/rules_docker)
  - [Groovy](https://github.com/bazelbuild/rules_groovy)
  - [Go](https://github.com/bazelbuild/rules_go)
  - [JavaScript (Closure)](https://github.com/bazelbuild/rules_closure)
  - [Jsonnet](https://github.com/bazelbuild/rules_jsonnet)
  - [Packaging](/reference/be/pkg.html)
  - [Rust](https://github.com/bazelbuild/rules_rust)
  - [Sass](https://github.com/bazelbuild/rules_sass)
  - [Scala](https://github.com/bazelbuild/rules_scala)

---

## C / C++ Rules
- URL: https://bazel.build/reference/be/c-cpp
- Source: reference/be/c-cpp.mdx
- Slug: /reference/be/c-cpp

## Rules

- [cc\_binary](#cc_binary)
- [cc\_import](#cc_import)
- [cc\_library](#cc_library)
- [cc\_shared\_library](#cc_shared_library)
- [cc\_static\_library](#cc_static_library)
- [cc\_test](#cc_test)
- [cc\_toolchain](#cc_toolchain)
- [fdo\_prefetch\_hints](#fdo_prefetch_hints)
- [fdo\_profile](#fdo_profile)
- [memprof\_profile](#memprof_profile)
- [propeller\_optimize](#propeller_optimize)

## cc\_binary

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/cc_binary.bzl)

```
cc_binary(name, deps, srcs, data, additional_linker_inputs, args, aspect_hints, compatible_with, conlyopts, copts, cxxopts, defines, deprecation, distribs, dynamic_deps, env, exec_compatible_with, exec_group_compatible_with, exec_properties, features, hdrs_check, includes, licenses, link_extra_lib, linkopts, linkshared, linkstatic, local_defines, malloc, module_interfaces, nocopts, output_licenses, package_metadata, reexport_deps, restricted_to, stamp, tags, target_compatible_with, testonly, toolchains, visibility, win_def_file)
```

It produces an executable binary.

The `name` of the target should be the same as the name of the
source file that is the main entry point of the application (minus the extension).
For example, if your entry point is in `main.cc`, then your name should
be `main`.

#### Implicit output targets

- `name.stripped` (only built if explicitly requested): A stripped
   version of the binary. `strip -g` is run on the binary to remove debug
   symbols. Additional strip options can be provided on the command line using
   `--stripopt=-foo`.
- `name.dwp` (only built if explicitly requested): If
   [Fission](https://gcc.gnu.org/wiki/DebugFission) is enabled: a debug
   information package file suitable for debugging remotely deployed binaries. Else: an
   empty file.

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries to be linked in to the binary target.

These can be `cc_library` or `objc_library`
targets.

It is also allowed to
put linker scripts (.lds) into deps, and reference them in
[linkopts](#cc_binary.linkopts).
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of C and C++ files that are processed to create the library target.
These are C/C++ source and header files, either non-generated (normal source
code) or generated.

All `.cc`, `.c`, and `.cpp` files will
be compiled. These might be generated files: if a named file is in
the `outs` of some other rule, this `cc_library`
will automatically depend on that other rule.

Pure assembler files (.s, .asm) are not preprocessed and are typically built using
the assembler. Preprocessed assembly files (.S) are preprocessed and are typically built
using the C/C++ compiler.

A `.h` file will not be compiled, but will be available for
inclusion by sources in this rule. Both `.cc` and
`.h` files can directly include headers listed in
these `srcs` or in the `hdrs` of this rule or any
rule listed in the `deps` argument.

All `#include` d files must be mentioned in the
`hdrs` attribute of this or referenced `cc_library`
rules, or they should be listed in `srcs` if they are private
to this library. See ["Header inclusion checking"](#hdrs) for
a more detailed description.

`.so`, `.lo`, and `.a` files are
pre-compiled files. Your library might have these as
`srcs` if it uses third-party code for which we don't
have source code.

If the `srcs` attribute includes the label of another rule,
`cc_library` will use the output files of that rule as source files to
compile. This is useful for one-off generation of source code (for more than occasional
use, it's better to implement a Starlark rule class and use the `cc_common`
API)

Permitted `srcs` file types:

- C and C++ source files: `.c`, `.cc`, `.cpp`,
   `.cxx`, `.c++`, `.C`
- C and C++ header files: `.h`, `.hh`, `.hpp`,
   `.hxx`, `.inc`, `.inl`, `.H`
- Assembler with C preprocessor: `.S`
- Archive: `.a`, `.pic.a`
- "Always link" library: `.lo`, `.pic.lo`
- Shared library, versioned or unversioned: `.so`,
   `.so.version`
- Object file: `.o`, `.pic.o`

... and any rules that produce those files (e.g. `cc_embed_data`).
Different extensions denote different programming languages in
accordance with gcc convention.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this library at runtime.

See general comments about `data`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

If a `data` is the name of a generated file, then this
`cc_library` rule automatically depends on the generating
rule.

If a `data` is a rule name, then this
`cc_library` rule automatically depends on that rule,
and that rule's `outs` are automatically added to
this `cc_library`'s data files.

Your C++ code can access these data files like so:

```lang-starlark

  const std::string path = devtools_build::GetDataDependencyFilepath(
      "my/test/data/file");

```

`additional_linker_inputs`

List of [labels](/concepts/labels); default is `[]`

 Pass these files to the C++ linker command.

For example, compiled Windows .res files can be provided here to be embedded in
the binary target.

`conlyopts`

List of strings; default is `[]`

 Add these options to the C compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
 `copts`

List of strings; default is `[]`

 Add these options to the C/C++ compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).

Each string in this attribute is added in the given order to `COPTS` before
compiling the binary target. The flags take effect only for compiling this target, not
its dependencies, so be careful about header files included elsewhere.
All paths should be relative to the workspace, not to the current package.
This attribute should not be needed outside of `third_party`.

If the package declares the [feature](/reference/be/functions.html#package.features) `no_copts_tokenization`, Bourne shell tokenization applies only to strings
that consist of a single "Make" variable.

`cxxopts`

List of strings; default is `[]`

 Add these options to the C++ compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
 `defines`

List of strings; default is `[]`

 List of defines to add to the compile line.
Subject to ["Make" variable](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
Each string, which must consist of a single Bourne shell token,
is prepended with `-D` and added to the compile command line to this target,
as well as to every rule that depends on it. Be very careful, since this may have
far-reaching effects. When in doubt, add define values to
[`local_defines`](#cc_binary.local_defines) instead.
 `distribs`

List of strings; default is `[]`

`dynamic_deps`

List of [labels](/concepts/labels); default is `[]`

 These are other `cc_shared_library` dependencies the current target depends on.

The `cc_shared_library` implementation will use the list of
`dynamic_deps` (transitively, i.e. also the `dynamic_deps` of the
current target's `dynamic_deps`) to decide which `cc_libraries` in
the transitive `deps` should not be linked in because they are already provided
by a different `cc_shared_library`.


`hdrs_check`

String; default is `""`

 Deprecated, no-op.
 `includes`

List of strings; default is `[]`

 List of include dirs to be added to the compile line.
Subject to ["Make variable"](/reference/be/make-variables) substitution.
Each string is prepended with the package path and passed to the C++ toolchain for
expansion via the "include\_paths" CROSSTOOL feature. A toolchain running on a POSIX system
with typical feature definitions will produce
`-isystem path_to_package/include_entry`.
This should only be used for third-party libraries that
do not conform to the Google style of writing #include statements.
Unlike [COPTS](#cc_binary.copts), these flags are added for this rule
and every rule that depends on it. (Note: not the rules it depends upon!) Be
very careful, since this may have far-reaching effects. When in doubt, add
"-I" flags to [COPTS](#cc_binary.copts) instead.

The added `include` paths will include generated files as well as
files in the source tree.

`link_extra_lib`

[Label](/concepts/labels); default is `"@bazel_tools//tools/cpp:link_extra_lib"`

 Control linking of extra libraries.

By default, C++ binaries are linked against `//tools/cpp:link_extra_lib`,
which by default depends on the label flag `//tools/cpp:link_extra_libs`.
Without setting the flag, this library is empty by default. Setting the label flag
allows linking optional dependencies, such as overrides for weak symbols, interceptors
for shared library functions, or special runtime libraries (for malloc replacements,
prefer `malloc` or `--custom_malloc`). Setting this attribute to
`None` disables this behaviour.

`linkopts`

List of strings; default is `[]`

 Add these flags to the C++ linker command.
Subject to ["Make" variable](make-variables.html) substitution,
[Bourne shell tokenization](common-definitions.html#sh-tokenization) and
[label expansion](common-definitions.html#label-expansion).
Each string in this attribute is added to `LINKOPTS` before
linking the binary target.

Each element of this list that does not start with `$` or `-` is
assumed to be the label of a target in `deps`. The
list of files generated by that target is appended to the linker
options. An error is reported if the label is invalid, or is
not declared in `deps`.

`linkshared`

Boolean; default is `False`

 Create a shared library.
To enable this attribute, include `linkshared=True` in your rule. By default
this option is off.

The presence of this flag means that linking occurs with the `-shared` flag
to `gcc`, and the resulting shared library is suitable for loading into for
example a Java program. However, for build purposes it will never be linked into the
dependent binary, as it is assumed that shared libraries built with a
[cc\_binary](#cc_binary) rule are only loaded manually by other programs, so
it should not be considered a substitute for the [cc\_library](#cc_library)
rule. For sake of scalability we recommend avoiding this approach altogether and
simply letting `java_library` depend on `cc_library` rules
instead.

If you specify both `linkopts=['-static']` and `linkshared=True`,
you get a single completely self-contained unit. If you specify both
`linkstatic=True` and `linkshared=True`, you get a single, mostly
self-contained unit.

`linkstatic`

Boolean; default is `True`

 For [`cc_binary`](/reference/be/c-cpp.html#cc_binary) and
[`cc_test`](/reference/be/c-cpp.html#cc_test): link the binary in static
mode. For `cc_library.link_static`: see below.

By default this option is on for `cc_binary` and off for the rest.

If enabled and this is a binary or test, this option tells the build tool to link in
`.a`'s instead of `.so`'s for user libraries whenever possible.
System libraries such as libc (but _not_ the C/C++ runtime libraries,
see below) are still linked dynamically, as are libraries for which
there is no static library. So the resulting executable will still be dynamically
linked, hence only _mostly_ static.

There are really three different ways to link an executable:

- STATIC with fully\_static\_link feature, in which everything is linked statically;
   e.g. " `gcc -static foo.o libbar.a libbaz.a -lm`".


   This mode is enabled by specifying `fully_static_link` in the
   [`features`](/reference/be/common-definitions#features) attribute.
- STATIC, in which all user libraries are linked statically (if a static
   version is available), but where system libraries (excluding C/C++ runtime libraries)
   are linked dynamically, e.g. " `gcc foo.o libfoo.a libbaz.a -lm`".


   This mode is enabled by specifying `linkstatic=True`.
- DYNAMIC, in which all libraries are linked dynamically (if a dynamic version is
   available), e.g. " `gcc foo.o libfoo.so libbaz.so -lm`".


   This mode is enabled by specifying `linkstatic=False`.

If the `linkstatic` attribute or `fully_static_link` in
`features` is used outside of `//third_party`
please include a comment near the rule to explain why.

The `linkstatic` attribute has a different meaning if used on a
[`cc_library()`](/reference/be/c-cpp.html#cc_library) rule.
For a C++ library, `linkstatic=True` indicates that only
static linking is allowed, so no `.so` will be produced. linkstatic=False does
not prevent static libraries from being created. The attribute is meant to control the
creation of dynamic libraries.

There should be very little code built with `linkstatic=False` in production.
If `linkstatic=False`, then the build tool will create symlinks to
depended-upon shared libraries in the `*.runfiles` area.

`local_defines`

List of strings; default is `[]`

 List of defines to add to the compile line.
Subject to ["Make" variable](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
Each string, which must consist of a single Bourne shell token,
is prepended with `-D` and added to the compile command line for this target,
but not to its dependents.
 `malloc`

[Label](/concepts/labels); default is `"@bazel_tools//tools/cpp:malloc"`

 Override the default dependency on malloc.

By default, C++ binaries are linked against `//tools/cpp:malloc`,
which is an empty library so the binary ends up using libc malloc.
This label must refer to a `cc_library`. If compilation is for a non-C++
rule, this option has no effect. The value of this attribute is ignored if
`linkshared=True` is specified.

`module_interfaces`

List of [labels](/concepts/labels); default is `[]`

 The list of files are regarded as C++20 Modules Interface.

C++ Standard has no restriction about module interface file extension

- Clang use cppm
- GCC can use any source file extension
- MSVC use ixx

The use is guarded by the flag
`--experimental_cpp_modules`.

`nocopts`

String; default is `""`

 Remove matching options from the C++ compilation command.
Subject to ["Make" variable](/reference/be/make-variables) substitution.
The value of this attribute is interpreted as a regular expression.
Any preexisting `COPTS` that match this regular expression
(including values explicitly specified in the rule's [copts](#cc_binary.copts) attribute)
will be removed from `COPTS` for purposes of compiling this rule.
This attribute should not be needed or used
outside of `third_party`. The values are not preprocessed
in any way other than the "Make" variable substitution.
 `reexport_deps`

List of [labels](/concepts/labels); default is `[]`

`stamp`

Integer; default is `-1`

 Whether to encode build information into the binary. Possible values:

- `stamp = 1`: Always stamp the build information into the binary, even in
   [`--nostamp`](/docs/user-manual#flag--stamp) builds. **This**
  **setting should be avoided**, since it potentially kills remote caching for the
   binary and any downstream actions that depend on it.

- `stamp = 0`: Always replace build information by constant values. This
   gives good build result caching.

- `stamp = -1`: Embedding of build information is controlled by the
   [`--[no]stamp`](/docs/user-manual#flag--stamp) flag.


Stamped binaries are _not_ rebuilt unless their dependencies change.

`win_def_file`

[Label](/concepts/labels); default is `None`

 The Windows DEF file to be passed to linker.

This attribute should only be used when Windows is the target platform.
It can be used to [export symbols](https://msdn.microsoft.com/en-us/library/d91k01sh.aspx) during linking a shared library.

## cc\_import

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/cc_import.bzl)

```
cc_import(name, deps, data, hdrs, alwayslink, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, includes, interface_library, linkopts, objects, package_metadata, pic_objects, pic_static_library, restricted_to, shared_library, static_library, strip_include_prefix, system_provided, tags, target_compatible_with, testonly, toolchains, visibility)
```

`cc_import` rules allows users to import precompiled C/C++ libraries.

The following are the typical use cases:

1\. Linking a static library

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  static_library = "libmylib.a",
  # If alwayslink is turned on,
  # libmylib.a will be forcely linked into any binary that depends on it.
  # alwayslink = 1,
)

```

2\. Linking a shared library (Unix)

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  shared_library = "libmylib.so",
)

```

3\. Linking a shared library with interface library

On Unix:

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  # libmylib.ifso is an interface library for libmylib.so which will be passed to linker
  interface_library = "libmylib.ifso",
  # libmylib.so will be available for runtime
  shared_library = "libmylib.so",
)

```

On Windows:

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  # mylib.lib is an import library for mylib.dll which will be passed to linker
  interface_library = "mylib.lib",
  # mylib.dll will be available for runtime
  shared_library = "mylib.dll",
)

```

4\. Linking a shared library with `system_provided=True`

On Unix:

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  interface_library = "libmylib.ifso", # Or we can also use libmylib.so as its own interface library
  # libmylib.so is provided by system environment, for example it can be found in LD_LIBRARY_PATH.
  # This indicates that Bazel is not responsible for making libmylib.so available.
  system_provided = 1,
)

```

On Windows:

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  # mylib.lib is an import library for mylib.dll which will be passed to linker
  interface_library = "mylib.lib",
  # mylib.dll is provided by system environment, for example it can be found in PATH.
  # This indicates that Bazel is not responsible for making mylib.dll available.
  system_provided = 1,
)

```

5\. Linking to static or shared library

On Unix:

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  static_library = "libmylib.a",
  shared_library = "libmylib.so",
)

```

On Windows:

```lang-starlark

cc_import(
  name = "mylib",
  hdrs = ["mylib.h"],
  static_library = "libmylib.lib", # A normal static library
  interface_library = "mylib.lib", # An import library for mylib.dll
  shared_library = "mylib.dll",
)

```

The remaining is the same on Unix and Windows:

```lang-starlark

# first will link to libmylib.a (or libmylib.lib)
cc_binary(
  name = "first",
  srcs = ["first.cc"],
  deps = [":mylib"],
  linkstatic = 1, # default value
)

# second will link to libmylib.so (or libmylib.lib)
cc_binary(
  name = "second",
  srcs = ["second.cc"],
  deps = [":mylib"],
  linkstatic = 0,
)

```

`cc_import` supports an include attribute. For example:

```lang-starlark

cc_import(
  name = "curl_lib",
  hdrs = glob(["vendor/curl/include/curl/*.h"]),
  includes = ["vendor/curl/include"],
  shared_library = "vendor/curl/lib/.libs/libcurl.dylib",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries that the target depends upon.
See general comments about `deps`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).
 `hdrs`

List of [labels](/concepts/labels); default is `[]`

 The list of header files published by
this precompiled library to be directly included by sources in dependent rules.
 `alwayslink`

Boolean; default is `False`

 If 1, any binary that depends (directly or indirectly) on this C++
precompiled library will link in all the object files archived in the static library,
even if some contain no symbols referenced by the binary.
This is useful if your code isn't explicitly called by code in
the binary, e.g., if your code registers to receive some callback
provided by some service.

If alwayslink doesn't work with VS 2017 on Windows, that is due to a
[known issue](https://github.com/bazelbuild/bazel/issues/3949),
please upgrade your VS 2017 to the latest version.

`includes`

List of strings; default is `[]`

 List of include dirs to be added to the compile line.
Subject to ["Make variable"](/reference/be/make-variables) substitution.
Each string is prepended with the package path and passed to the C++ toolchain for
expansion via the "include\_paths" CROSSTOOL feature. A toolchain running on a POSIX system
with typical feature definitions will produce
`-isystem path_to_package/include_entry`.
This should only be used for third-party libraries that
do not conform to the Google style of writing #include statements.
Unlike [COPTS](#cc_binary.copts), these flags are added for this rule
and every rule that depends on it. (Note: not the rules it depends upon!) Be
very careful, since this may have far-reaching effects. When in doubt, add
"-I" flags to [COPTS](#cc_binary.copts) instead.

The default `include` path doesn't include generated
files. If you need to `#include` a generated header
file, list it in the `srcs`.

`interface_library`

[Label](/concepts/labels); default is `None`

 A single interface library for linking the shared library.

Permitted file types:
`.ifso`,
`.tbd`,
`.lib`,
`.so`
or `.dylib`

`linkopts`

List of strings; default is `[]`

 Add these flags to the C++ linker command.
Subject to ["Make" variable](make-variables.html) substitution,
[Bourne shell tokenization](common-definitions.html#sh-tokenization) and
[label expansion](common-definitions.html#label-expansion).
Each string in this attribute is added to `LINKOPTS` before
linking the binary target.

Each element of this list that does not start with `$` or `-` is
assumed to be the label of a target in `deps`. The
list of files generated by that target is appended to the linker
options. An error is reported if the label is invalid, or is
not declared in `deps`.

`objects`

List of [labels](/concepts/labels); default is `[]`

`pic_objects`

List of [labels](/concepts/labels); default is `[]`

`pic_static_library`

[Label](/concepts/labels); default is `None`

`shared_library`

[Label](/concepts/labels); default is `None`

 A single precompiled shared library. Bazel ensures it is available to the
binary that depends on it during runtime.

Permitted file types:
`.so`,
`.dll`
or `.dylib`

`static_library`

[Label](/concepts/labels); default is `None`

 A single precompiled static library.

Permitted file types:
`.a`,
`.pic.a`
or `.lib`

`strip_include_prefix`

String; default is `""`

 The prefix to strip from the paths of the headers of this rule.

When set, the headers in the `hdrs` attribute of this rule are accessible
at their path with this prefix cut off.

If it's a relative path, it's taken as a package-relative one. If it's an absolute one,
it's understood as a repository-relative path.

The prefix in the `include_prefix` attribute is added after this prefix is
stripped.

This attribute is only legal under `third_party`.


`system_provided`

Boolean; default is `False`

 If 1, it indicates the shared library required at runtime is provided by the system. In
this case, `interface_library` should be specified and
`shared_library` should be empty.


## cc\_library

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/cc_library.bzl)

```
cc_library(name, deps, srcs, data, hdrs, additional_compiler_inputs, additional_linker_inputs, alwayslink, aspect_hints, compatible_with, conlyopts, copts, cxxopts, defines, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, hdrs_check, implementation_deps, include_prefix, includes, licenses, linkopts, linkstamp, linkstatic, local_defines, module_interfaces, package_metadata, restricted_to, strip_include_prefix, tags, target_compatible_with, testonly, textual_hdrs, toolchains, visibility, win_def_file)
```

Use `cc_library()` for C++-compiled libraries.
The result is either a `.so`, `.lo`,
or `.a`, depending on what is needed.

If you build something with static linking that depends on
a `cc_library`, the output of a depended-on library rule
is the `.a` file. If you specify
`alwayslink=True`, you get the `.lo` file.

The actual output file name is `libfoo.so` for
the shared library, where _foo_ is the name of the rule. The
other kinds of libraries end with `.lo` and `.a`,
respectively. If you need a specific shared library name, for
example, to define a Python module, use a genrule to copy the library
to the desired name.

#### Header inclusion checking

All header files that are used in the build must be declared in
the `hdrs` or `srcs` of `cc_*` rules.
This is enforced.

For `cc_library` rules, headers in `hdrs` comprise the
public interface of the library and can be directly included both
from the files in `hdrs` and `srcs` of the library
itself as well as from files in `hdrs` and `srcs`
of `cc_*` rules that list the library in their `deps`.
Headers in `srcs` must only be directly included from the files
in `hdrs` and `srcs` of the library itself. When
deciding whether to put a header into `hdrs` or `srcs`,
you should ask whether you want consumers of this library to be able to
directly include it. This is roughly the same decision as
between `public` and `private` visibility in programming languages.

`cc_binary` and `cc_test` rules do not have an exported
interface, so they also do not have a `hdrs` attribute. All headers
that belong to the binary or test directly should be listed in
the `srcs`.

To illustrate these rules, look at the following example.

```lang-starlark

cc_binary(
    name = "foo",
    srcs = [
        "foo.cc",
        "foo.h",
    ],
    deps = [":bar"],
)

cc_library(
    name = "bar",
    srcs = [
        "bar.cc",
        "bar-impl.h",
    ],
    hdrs = ["bar.h"],
    deps = [":baz"],
)

cc_library(
    name = "baz",
    srcs = [
        "baz.cc",
        "baz-impl.h",
    ],
    hdrs = ["baz.h"],
)

```

The allowed direct inclusions in this example are listed in the table below.
For example `foo.cc` is allowed to directly
include `foo.h` and `bar.h`, but not `baz.h`.

Including fileAllowed inclusionsfoo.hbar.hfoo.ccfoo.h bar.hbar.hbar-impl.h baz.hbar-impl.hbar.h baz.hbar.ccbar.h bar-impl.h baz.hbaz.hbaz-impl.hbaz-impl.hbaz.hbaz.ccbaz.h baz-impl.h

The inclusion checking rules only apply to _direct_
inclusions. In the example above `foo.cc` is allowed to
include `bar.h`, which may include `baz.h`, which in
turn is allowed to include `baz-impl.h`. Technically, the
compilation of a `.cc` file may transitively include any header
file in the `hdrs` or `srcs` in
any `cc_library` in the transitive `deps` closure. In
this case the compiler may read `baz.h` and `baz-impl.h`
when compiling `foo.cc`, but `foo.cc` must not
contain `#include "baz.h"`. For that to be
allowed, `baz` must be added to the `deps`
of `foo`.

Bazel depends on toolchain support to enforce the inclusion checking rules.
The `layering_check` feature has to be supported by the toolchain
and requested explicitly, for example via the
`--features=layering_check` command-line flag or the
`features` parameter of the
[`package`](/reference/be/functions.html#package) function. The toolchains
provided by Bazel only support this feature with clang on Unix and macOS.

#### Examples

We use the `alwayslink` flag to force the linker to link in
this code although the main binary code doesn't reference it.

```lang-starlark

cc_library(
    name = "ast_inspector_lib",
    srcs = ["ast_inspector_lib.cc"],
    hdrs = ["ast_inspector_lib.h"],
    visibility = ["//visibility:public"],
    deps = ["//third_party/llvm/llvm/tools/clang:frontend"],
    # alwayslink as we want to be able to call things in this library at
    # debug time, even if they aren't used anywhere in the code.
    alwayslink = 1,
)

```

The following example comes from
`third_party/python2_4_3/BUILD`.
Some of the code uses the `dl` library (to load
another, dynamic library), so this
rule specifies the `-ldl` link option to link the
`dl` library.

```lang-starlark

cc_library(
    name = "python2_4_3",
    linkopts = [
        "-ldl",
        "-lutil",
    ],
    deps = ["//third_party/expat"],
)

```

The following example comes from `third_party/kde/BUILD`.
We keep pre-built `.so` files in the depot.
The header files live in a subdirectory named `include`.

```lang-starlark

cc_library(
    name = "kde",
    srcs = [
        "lib/libDCOP.so",
        "lib/libkdesu.so",
        "lib/libkhtml.so",
        "lib/libkparts.so",
        ...more .so files...,
    ],
    includes = ["include"],
    deps = ["//third_party/X11"],
)

```

The following example comes from `third_party/gles/BUILD`.
Third-party code often needs some `defines` and
`linkopts`.

```lang-starlark

cc_library(
    name = "gles",
    srcs = [
        "GLES/egl.h",
        "GLES/gl.h",
        "ddx.c",
        "egl.c",
    ],
    defines = [
        "USE_FLOAT",
        "__GL_FLOAT",
        "__GL_COMMON",
    ],
    linkopts = ["-ldl"],  # uses dlopen(), dl library
    deps = [
        "es",
        "//third_party/X11",
    ],
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries that the library target depends upon.

These can be `cc_library` or `objc_library` targets.

See general comments about `deps`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

These should be names of C++ library rules.
When you build a binary that links this rule's library,
you will also link the libraries in `deps`.

Despite the "deps" name, not all of this library's clients
belong here. Run-time data dependencies belong in `data`.
Source files generated by other rules belong in `srcs`.

To link in a pre-compiled third-party library, add its name to
the `srcs` instead.

To depend on something without linking it to this library, add its
name to the `data` instead.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of C and C++ files that are processed to create the library target.
These are C/C++ source and header files, either non-generated (normal source
code) or generated.

All `.cc`, `.c`, and `.cpp` files will
be compiled. These might be generated files: if a named file is in
the `outs` of some other rule, this `cc_library`
will automatically depend on that other rule.

Pure assembler files (.s, .asm) are not preprocessed and are typically built using
the assembler. Preprocessed assembly files (.S) are preprocessed and are typically built
using the C/C++ compiler.

A `.h` file will not be compiled, but will be available for
inclusion by sources in this rule. Both `.cc` and
`.h` files can directly include headers listed in
these `srcs` or in the `hdrs` of this rule or any
rule listed in the `deps` argument.

All `#include` d files must be mentioned in the
`hdrs` attribute of this or referenced `cc_library`
rules, or they should be listed in `srcs` if they are private
to this library. See ["Header inclusion checking"](#hdrs) for
a more detailed description.

`.so`, `.lo`, and `.a` files are
pre-compiled files. Your library might have these as
`srcs` if it uses third-party code for which we don't
have source code.

If the `srcs` attribute includes the label of another rule,
`cc_library` will use the output files of that rule as source files to
compile. This is useful for one-off generation of source code (for more than occasional
use, it's better to implement a Starlark rule class and use the `cc_common`
API)

Permitted `srcs` file types:

- C and C++ source files: `.c`, `.cc`, `.cpp`,
   `.cxx`, `.c++`, `.C`
- C and C++ header files: `.h`, `.hh`, `.hpp`,
   `.hxx`, `.inc`, `.inl`, `.H`
- Assembler with C preprocessor: `.S`
- Archive: `.a`, `.pic.a`
- "Always link" library: `.lo`, `.pic.lo`
- Shared library, versioned or unversioned: `.so`,
   `.so.version`
- Object file: `.o`, `.pic.o`

... and any rules that produce those files (e.g. `cc_embed_data`).
Different extensions denote different programming languages in
accordance with gcc convention.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this library at runtime.

See general comments about `data`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

If a `data` is the name of a generated file, then this
`cc_library` rule automatically depends on the generating
rule.

If a `data` is a rule name, then this
`cc_library` rule automatically depends on that rule,
and that rule's `outs` are automatically added to
this `cc_library`'s data files.

Your C++ code can access these data files like so:

```lang-starlark

  const std::string path = devtools_build::GetDataDependencyFilepath(
      "my/test/data/file");

```

`hdrs`

List of [labels](/concepts/labels); default is `[]`

 The list of header files published by
this library to be directly included by sources in dependent rules.

This is the strongly preferred location for declaring header files that
describe the interface for the library. These headers will be made
available for inclusion by sources in this rule or in dependent rules.
Headers not meant to be included by a client of this library should be
listed in the `srcs` attribute instead, even if they are
included by a published header. See ["Header inclusion\
checking"](#hdrs) for a more detailed description.

Permitted `headers` file types:
`.h`,
`.hh`,
`.hpp`,
`.hxx`.

`additional_compiler_inputs`

List of [labels](/concepts/labels); default is `[]`

 Any additional files you might want to pass to the compiler command line, such as sanitizer
ignorelists, for example. Files specified here can then be used in copts with the
$(location) function.
 `additional_linker_inputs`

List of [labels](/concepts/labels); default is `[]`

 Pass these files to the C++ linker command.

For example, compiled Windows .res files can be provided here to be embedded in
the binary target.

`alwayslink`

Boolean; default is `False`

 If 1, any binary that depends (directly or indirectly) on this C++
library will link in all the object files for the files listed in
`srcs`, even if some contain no symbols referenced by the binary.
This is useful if your code isn't explicitly called by code in
the binary, e.g., if your code registers to receive some callback
provided by some service.

If alwayslink doesn't work with VS 2017 on Windows, that is due to a
[known issue](https://github.com/bazelbuild/bazel/issues/3949),
please upgrade your VS 2017 to the latest version.

`conlyopts`

List of strings; default is `[]`

 Add these options to the C compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
 `copts`

List of strings; default is `[]`

 Add these options to the C/C++ compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).

Each string in this attribute is added in the given order to `COPTS` before
compiling the binary target. The flags take effect only for compiling this target, not
its dependencies, so be careful about header files included elsewhere.
All paths should be relative to the workspace, not to the current package.
This attribute should not be needed outside of `third_party`.

If the package declares the [feature](/reference/be/functions.html#package.features) `no_copts_tokenization`, Bourne shell tokenization applies only to strings
that consist of a single "Make" variable.

`cxxopts`

List of strings; default is `[]`

 Add these options to the C++ compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
 `defines`

List of strings; default is `[]`

 List of defines to add to the compile line.
Subject to ["Make" variable](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
Each string, which must consist of a single Bourne shell token,
is prepended with `-D` and added to the compile command line to this target,
as well as to every rule that depends on it. Be very careful, since this may have
far-reaching effects. When in doubt, add define values to
[`local_defines`](#cc_binary.local_defines) instead.
 `hdrs_check`

String; default is `""`

 Deprecated, no-op.
 `implementation_deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries that the library target depends on. Unlike with
`deps`, the headers and include paths of these libraries (and all their
transitive deps) are only used for compilation of this library, and not libraries that
depend on it. Libraries specified with `implementation_deps` are still linked in
binary targets that depend on this library.
 `include_prefix`

String; default is `""`

 The prefix to add to the paths of the headers of this rule.

When set, the headers in the `hdrs` attribute of this rule are accessible
at is the value of this attribute prepended to their repository-relative path.

The prefix in the `strip_include_prefix` attribute is removed before this
prefix is added.

This attribute is only legal under `third_party`.


`includes`

List of strings; default is `[]`

 List of include dirs to be added to the compile line.
Subject to ["Make variable"](/reference/be/make-variables) substitution.
Each string is prepended with the package path and passed to the C++ toolchain for
expansion via the "include\_paths" CROSSTOOL feature. A toolchain running on a POSIX system
with typical feature definitions will produce
`-isystem path_to_package/include_entry`.
This should only be used for third-party libraries that
do not conform to the Google style of writing #include statements.
Unlike [COPTS](#cc_binary.copts), these flags are added for this rule
and every rule that depends on it. (Note: not the rules it depends upon!) Be
very careful, since this may have far-reaching effects. When in doubt, add
"-I" flags to [COPTS](#cc_binary.copts) instead.

The added `include` paths will include generated files as well as
files in the source tree.

`linkopts`

List of strings; default is `[]`

 See [`cc_binary.linkopts`](/reference/be/c-cpp.html#cc_binary.linkopts).
The `linkopts` attribute is also applied to any target that
depends, directly or indirectly, on this library via `deps`
attributes (or via other attributes that are treated similarly:
the [`malloc`](/reference/be/c-cpp.html#cc_binary.malloc)
attribute of [`cc_binary`](/reference/be/c-cpp.html#cc_binary)). Dependency
linkopts take precedence over dependent linkopts (i.e. dependency linkopts
appear later in the command line). Linkopts specified in
[`--linkopt`](../user-manual.html#flag--linkopt)
take precedence over rule linkopts.

Note that the `linkopts` attribute only applies
when creating `.so` files or executables, not
when creating `.a` or `.lo` files.
So if the `linkstatic=True` attribute is set, the
`linkopts` attribute has no effect on the creation of
this library, only on other targets which depend on this library.

Also, it is important to note that "-Wl,-soname" or "-Xlinker -soname"
options are not supported and should never be specified in this attribute.

The `.so` files produced by `cc_library`
rules are not linked against the libraries that they depend
on. If you're trying to create a shared library for use
outside of the main repository, e.g. for manual use
with `dlopen()` or `LD_PRELOAD`,
it may be better to use a `cc_binary` rule
with the `linkshared=True` attribute.
See [`cc_binary.linkshared`](/reference/be/c-cpp.html#cc_binary.linkshared).

`linkstamp`

[Label](/concepts/labels); default is `None`

 Simultaneously compiles and links the specified C++ source file into the final
binary. This trickery is required to introduce timestamp
information into binaries; if we compiled the source file to an
object file in the usual way, the timestamp would be incorrect.
A linkstamp compilation may not include any particular set of
compiler flags and so should not depend on any particular
header, compiler option, or other build variable.
_This option should only be needed in the_
_`base` package._`linkstatic`

Boolean; default is `False`

 For [`cc_binary`](/reference/be/c-cpp.html#cc_binary) and
[`cc_test`](/reference/be/c-cpp.html#cc_test): link the binary in static
mode. For `cc_library.link_static`: see below.

By default this option is on for `cc_binary` and off for the rest.

If enabled and this is a binary or test, this option tells the build tool to link in
`.a`'s instead of `.so`'s for user libraries whenever possible.
System libraries such as libc (but _not_ the C/C++ runtime libraries,
see below) are still linked dynamically, as are libraries for which
there is no static library. So the resulting executable will still be dynamically
linked, hence only _mostly_ static.

There are really three different ways to link an executable:

- STATIC with fully\_static\_link feature, in which everything is linked statically;
   e.g. " `gcc -static foo.o libbar.a libbaz.a -lm`".


   This mode is enabled by specifying `fully_static_link` in the
   [`features`](/reference/be/common-definitions#features) attribute.
- STATIC, in which all user libraries are linked statically (if a static
   version is available), but where system libraries (excluding C/C++ runtime libraries)
   are linked dynamically, e.g. " `gcc foo.o libfoo.a libbaz.a -lm`".


   This mode is enabled by specifying `linkstatic=True`.
- DYNAMIC, in which all libraries are linked dynamically (if a dynamic version is
   available), e.g. " `gcc foo.o libfoo.so libbaz.so -lm`".


   This mode is enabled by specifying `linkstatic=False`.

If the `linkstatic` attribute or `fully_static_link` in
`features` is used outside of `//third_party`
please include a comment near the rule to explain why.

The `linkstatic` attribute has a different meaning if used on a
[`cc_library()`](/reference/be/c-cpp.html#cc_library) rule.
For a C++ library, `linkstatic=True` indicates that only
static linking is allowed, so no `.so` will be produced. linkstatic=False does
not prevent static libraries from being created. The attribute is meant to control the
creation of dynamic libraries.

There should be very little code built with `linkstatic=False` in production.
If `linkstatic=False`, then the build tool will create symlinks to
depended-upon shared libraries in the `*.runfiles` area.

`local_defines`

List of strings; default is `[]`

 List of defines to add to the compile line.
Subject to ["Make" variable](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
Each string, which must consist of a single Bourne shell token,
is prepended with `-D` and added to the compile command line for this target,
but not to its dependents.
 `module_interfaces`

List of [labels](/concepts/labels); default is `[]`

 The list of files are regarded as C++20 Modules Interface.

C++ Standard has no restriction about module interface file extension

- Clang use cppm
- GCC can use any source file extension
- MSVC use ixx

The use is guarded by the flag
`--experimental_cpp_modules`.

`strip_include_prefix`

String; default is `""`

 The prefix to strip from the paths of the headers of this rule.

When set, the headers in the `hdrs` attribute of this rule are accessible
at their path with this prefix cut off.

If it's a relative path, it's taken as a package-relative one. If it's an absolute one,
it's understood as a repository-relative path.

The prefix in the `include_prefix` attribute is added after this prefix is
stripped.

This attribute is only legal under `third_party`.


`textual_hdrs`

List of [labels](/concepts/labels); default is `[]`

 The list of header files published by
this library to be textually included by sources in dependent rules.

This is the location for declaring header files that cannot be compiled on their own;
that is, they always need to be textually included by other source files to build valid
code.

`win_def_file`

[Label](/concepts/labels); default is `None`

 The Windows DEF file to be passed to linker.

This attribute should only be used when Windows is the target platform.
It can be used to [export symbols](https://msdn.microsoft.com/en-us/library/d91k01sh.aspx) during linking a shared library.

## cc\_shared\_library

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/cc_shared_library.bzl)

```
cc_shared_library(name, deps, additional_linker_inputs, aspect_hints, compatible_with, deprecation, dynamic_deps, exec_compatible_with, exec_group_compatible_with, exec_properties, experimental_disable_topo_sort_do_not_use_remove_before_7_0, exports_filter, features, package_metadata, restricted_to, roots, shared_lib_name, static_deps, tags, target_compatible_with, testonly, toolchains, user_link_flags, visibility, win_def_file)
```

It produces a shared library.

#### Example

```
cc_shared_library(
    name = "foo_shared",
    deps = [
        ":foo",
    ],
    dynamic_deps = [
        ":bar_shared",
    ],
    additional_linker_inputs = [
        ":foo.lds",
    ],
    user_link_flags = [
        "-Wl,--version-script=$(location :foo.lds)",
    ],
)
cc_library(
    name = "foo",
    srcs = ["foo.cc"],
    hdrs = ["foo.h"],
    deps = [
        ":bar",
        ":baz",
    ],
)
cc_shared_library(
    name = "bar_shared",
    shared_lib_name = "bar.so",
    deps = [":bar"],
)
cc_library(
    name = "bar",
    srcs = ["bar.cc"],
    hdrs = ["bar.h"],
)
cc_library(
    name = "baz",
    srcs = ["baz.cc"],
    hdrs = ["baz.h"],
)

```

In the example `foo_shared` statically links `foo`
and `baz`, the latter being a transitive dependency. It doesn't
link `bar` because it is already provided dynamically by the
`dynamic_dep` `bar_shared`.

`foo_shared` uses a linker script \*.lds file to control which
symbols should be exported. The `cc_shared_library` rule logic does
not control which symbols get exported, it only uses what is assumed to be
exported to give errors during analysis phase if two shared libraries export the
same targets.

Every direct dependency of `cc_shared_library` is assumed to be
exported. Therefore, Bazel assumes during analysis that `foo` is being
exported by `foo_shared`. `baz` is not assumed to be exported
by `foo_shared`. Every target matched by the `exports_filter`
is also assumed to be exported.

Every single `cc_library` in the example should appear at most in one
`cc_shared_library`. If we wanted to link `baz` also into
`bar_shared` we would need to add
`tags = ["LINKABLE_MORE_THAN_ONCE"]` to `baz`.

Due to the `shared_lib_name` attribute, the file produced by
`bar_shared` will have the name `bar.so` as opposed
to the name `libbar.so` that it would have by default on Linux.

#### Errors

##### `Two shared libraries in dependencies export the same symbols.`

This will happen whenever you are creating a target with two different
`cc_shared_library` dependencies that export the same target. To fix this
you need to stop the libraries from being exported in one of the
`cc_shared_library` dependencies.

##### `Two shared libraries in dependencies link the same library statically`

This will happen whenever you are creating a new `cc_shared_library` with two
different `cc_shared_library` dependencies that link the same target statically.
Similar to the error with exports.

One way to fix this is to stop linking the library into one of the
`cc_shared_library` dependencies. At the same time, the one that still links it
needs to export the library so that the one not linking it keeps visibility to
the symbols. Another way is to pull out a third library that exports the target.
A third way is to tag the culprit `cc_library` with `LINKABLE_MORE_THAN_ONCE`
but this fix should be rare and you should absolutely make sure that the
`cc_library` is indeed safe to link more than once.

##### ``'//foo:foo' is already linked statically in '//bar:bar' but not exported` ``

This means that a library in the transitive closure of your `deps` is reachable
without going through one of the `cc_shared_library` dependencies but is already
linked into a different `cc_shared_library` in `dynamic_deps` and is not
exported.

The solution is to export it from the `cc_shared_library` dependency or pull out
a third `cc_shared_library` that exports it.

##### `Do not place libraries which only contain a precompiled dynamic library in deps. `

If you have a precompiled dynamic library, this doesn't need to and cannot be
linked statically into the current `cc_shared_library` target that you are
currently creating. Therefore, it doesn't belong in `deps` of the
`cc_shared_library`. If this precompiled dynamic library is a dependency of one
of your `cc_libraries`, then the `cc_library` needs to depend on it
directly.

##### `Trying to export a library already exported by a different shared library`

You will see this error if on the current rule you are claiming to export a
target that is already being exported by one of your dynamic dependencies.

To fix this, remove the target from `deps` and just rely on it from the dynamic
dependency or make sure that the `exports_filter` doesn't catch this target.

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 Top level libraries that will unconditionally be statically linked into the shared library
after being whole-archived.

Any transitive library dependency of these direct deps will be linked into this shared
library as long as they have not already been linked by a `cc_shared_library`
in `dynamic_deps`.

During analysis, the rule implementation will consider any target listed in
`deps` as being exported by the shared library in order to give errors when
multiple `cc_shared_libraries` export the same targets. The rule implementation
does not take care of informing the linker about which symbols should be exported by the
shared object. The user should take care of this via linker scripts or visibility
declarations in the source code.

The implementation will also trigger errors whenever the same library is linked statically
into more than one `cc_shared_library`. This can be avoided by adding
`"LINKABLE_MORE_THAN_ONCE"` to the `cc_library.tags` or by listing
the \`cc\_library\` as an export of one of the shared libraries so that one can be made a
`dynamic_dep` of the other.

`additional_linker_inputs`

List of [labels](/concepts/labels); default is `[]`

 Any additional files that you may want to pass to the linker, for example, linker scripts.
You have to separately pass any linker flags that the linker needs in order to be aware
of this file. You can do so via the `user_link_flags` attribute.
 `dynamic_deps`

List of [labels](/concepts/labels); default is `[]`

 These are other `cc_shared_library` dependencies the current target depends on.

The `cc_shared_library` implementation will use the list of
`dynamic_deps` (transitively, i.e. also the `dynamic_deps` of the
current target's `dynamic_deps`) to decide which `cc_libraries` in
the transitive `deps` should not be linked in because they are already provided
by a different `cc_shared_library`.

`experimental_disable_topo_sort_do_not_use_remove_before_7_0`

Boolean; default is `False`

`exports_filter`

List of strings; default is `[]`

 This attribute contains a list of targets that are claimed to be exported by the current
shared library.

Any target `deps` is already understood to be exported by the shared library.
This attribute should be used to list any targets that are exported by the shared library
but are transitive dependencies of `deps`.

Note that this attribute is not actually adding a dependency edge to those targets, the
dependency edge should instead be created by `deps`.The entries in this
attribute are just strings. Keep in mind that when placing a target in this attribute,
this is considered a claim that the shared library exports the symbols from that target.
The `cc_shared_library` logic doesn't actually handle telling the linker which
symbols should be exported.

The following syntax is allowed:

`//foo:__pkg__` to account for any target in foo/BUILD

`//foo:__subpackages__` to account for any target in foo/BUILD or any other
package below foo/ like foo/bar/BUILD

`roots`

List of [labels](/concepts/labels); default is `[]`

`shared_lib_name`

String; default is `""`

 By default cc\_shared\_library will use a name for the shared library output file based on
the target's name and the platform. This includes an extension and sometimes a prefix.
Sometimes you may not want the default name, for example, when loading C++ shared libraries
for Python the default lib\* prefix is often not desired, in which case you can use this
attribute to choose a custom name.
 `static_deps`

List of strings; default is `[]`

`user_link_flags`

List of strings; default is `[]`

 Any additional flags that you may want to pass to the linker. For example, to make the
linker aware of a linker script passed via additional\_linker\_inputs you can use the
following:

```lang-starlark

 cc_shared_library(
    name = "foo_shared",
    additional_linker_inputs = select({
      "//src/conditions:linux": [
        ":foo.lds",
        ":additional_script.txt",
      ],
      "//conditions:default": []}),
    user_link_flags = select({
      "//src/conditions:linux": [
        "-Wl,-rpath,kittens",
        "-Wl,--version-script=$(location :foo.lds)",
        "-Wl,--script=$(location :additional_script.txt)",
      ],
      "//conditions:default": []}),
      ...
 )

```

`win_def_file`

[Label](/concepts/labels); default is `None`

 The Windows DEF file to be passed to linker.

This attribute should only be used when Windows is the target platform.
It can be used to [export symbols](https://msdn.microsoft.com/en-us/library/d91k01sh.aspx) during linking a shared library.

## cc\_static\_library

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/cc_static_library.bzl)

```
cc_static_library(name, deps, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

 Produces a static library from a list of targets and their transitive dependencies.

The resulting static library contains the object files of the targets listed in
`deps` as well as their transitive dependencies, with preference given to
`PIC` objects.

#### Output groups

##### `linkdeps`

A text file containing the labels of those transitive dependencies of targets listed in
`deps` that did not contribute any object files to the static library, but do
provide at least one static, dynamic or interface library. The resulting static library
may require these libraries to be available at link time.

##### `linkopts`

A text file containing the user-provided `linkopts` of all transitive
dependencies of targets listed in `deps`.

#### Duplicate symbols

By default, the `cc_static_library` rule checks that the resulting static
library does not contain any duplicate symbols. If it does, the build fails with an error
message that lists the duplicate symbols and the object files containing them.

This check can be disabled per target or per package by setting
`features = ["-symbol_check"]` or globally via
`--features=-symbol_check`.

##### Toolchain support for `symbol_check`

The auto-configured C++ toolchains shipped with Bazel support the
`symbol_check` feature on all platforms. Custom toolchains can add support for
it in one of two ways:

- Implementing the `ACTION_NAMES.validate_static_library` action and
   enabling it with the `symbol_check` feature. The tool set in the action is
   invoked with two arguments, the static library to check for duplicate symbols and the
   path of a file that must be created if the check passes.
- Having the `symbol_check` feature add archiver flags that cause the
   action creating the static library to fail on duplicate symbols.

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of targets to combine into a static library, including all their transitive
dependencies.

Dependencies that do not provide any object files are not included in the static
library, but their labels are collected in the file provided by the
`linkdeps` output group.

## cc\_test

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/cc_test.bzl)

```
cc_test(name, deps, srcs, data, additional_linker_inputs, args, aspect_hints, compatible_with, conlyopts, copts, cxxopts, defines, deprecation, distribs, dynamic_deps, env, env_inherit, exec_compatible_with, exec_group_compatible_with, exec_properties, features, flaky, hdrs_check, includes, licenses, link_extra_lib, linkopts, linkshared, linkstatic, local, local_defines, malloc, module_interfaces, nocopts, package_metadata, reexport_deps, restricted_to, shard_count, size, stamp, tags, target_compatible_with, testonly, timeout, toolchains, visibility, win_def_file)
```

A `cc_test()` rule compiles a test. Here, a test
is a binary wrapper around some testing code.

_By default, C++ tests are dynamically linked._

To statically link a unit test, specify
[`linkstatic=True`](/reference/be/c-cpp.html#cc_binary.linkstatic).
It would probably be good to comment why your test needs
`linkstatic`; this is probably not obvious.

#### Implicit output targets

- `name.stripped` (only built if explicitly requested): A stripped
   version of the binary. `strip -g` is run on the binary to remove debug
   symbols. Additional strip options can be provided on the command line using
   `--stripopt=-foo`.
- `name.dwp` (only built if explicitly requested): If
   [Fission](https://gcc.gnu.org/wiki/DebugFission) is enabled: a debug
   information package file suitable for debugging remotely deployed binaries. Else: an
   empty file.

See the [cc\_binary()](/reference/be/c-cpp.html#cc_binary_args) arguments, except that
the `stamp` argument is set to 0 by default for tests and
that `cc_test` has extra [attributes common to all test rules (\*\_test)](/reference/be/common-definitions#common-attributes-tests).

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries to be linked in to the binary target.

These can be `cc_library` or `objc_library`
targets.

It is also allowed to
put linker scripts (.lds) into deps, and reference them in
[linkopts](#cc_binary.linkopts).
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of C and C++ files that are processed to create the library target.
These are C/C++ source and header files, either non-generated (normal source
code) or generated.

All `.cc`, `.c`, and `.cpp` files will
be compiled. These might be generated files: if a named file is in
the `outs` of some other rule, this `cc_library`
will automatically depend on that other rule.

Pure assembler files (.s, .asm) are not preprocessed and are typically built using
the assembler. Preprocessed assembly files (.S) are preprocessed and are typically built
using the C/C++ compiler.

A `.h` file will not be compiled, but will be available for
inclusion by sources in this rule. Both `.cc` and
`.h` files can directly include headers listed in
these `srcs` or in the `hdrs` of this rule or any
rule listed in the `deps` argument.

All `#include` d files must be mentioned in the
`hdrs` attribute of this or referenced `cc_library`
rules, or they should be listed in `srcs` if they are private
to this library. See ["Header inclusion checking"](#hdrs) for
a more detailed description.

`.so`, `.lo`, and `.a` files are
pre-compiled files. Your library might have these as
`srcs` if it uses third-party code for which we don't
have source code.

If the `srcs` attribute includes the label of another rule,
`cc_library` will use the output files of that rule as source files to
compile. This is useful for one-off generation of source code (for more than occasional
use, it's better to implement a Starlark rule class and use the `cc_common`
API)

Permitted `srcs` file types:

- C and C++ source files: `.c`, `.cc`, `.cpp`,
   `.cxx`, `.c++`, `.C`
- C and C++ header files: `.h`, `.hh`, `.hpp`,
   `.hxx`, `.inc`, `.inl`, `.H`
- Assembler with C preprocessor: `.S`
- Archive: `.a`, `.pic.a`
- "Always link" library: `.lo`, `.pic.lo`
- Shared library, versioned or unversioned: `.so`,
   `.so.version`
- Object file: `.o`, `.pic.o`

... and any rules that produce those files (e.g. `cc_embed_data`).
Different extensions denote different programming languages in
accordance with gcc convention.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this library at runtime.

See general comments about `data`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

If a `data` is the name of a generated file, then this
`cc_library` rule automatically depends on the generating
rule.

If a `data` is a rule name, then this
`cc_library` rule automatically depends on that rule,
and that rule's `outs` are automatically added to
this `cc_library`'s data files.

Your C++ code can access these data files like so:

```lang-starlark

  const std::string path = devtools_build::GetDataDependencyFilepath(
      "my/test/data/file");

```

`additional_linker_inputs`

List of [labels](/concepts/labels); default is `[]`

 Pass these files to the C++ linker command.

For example, compiled Windows .res files can be provided here to be embedded in
the binary target.

`conlyopts`

List of strings; default is `[]`

 Add these options to the C compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
 `copts`

List of strings; default is `[]`

 Add these options to the C/C++ compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).

Each string in this attribute is added in the given order to `COPTS` before
compiling the binary target. The flags take effect only for compiling this target, not
its dependencies, so be careful about header files included elsewhere.
All paths should be relative to the workspace, not to the current package.
This attribute should not be needed outside of `third_party`.

If the package declares the [feature](/reference/be/functions.html#package.features) `no_copts_tokenization`, Bourne shell tokenization applies only to strings
that consist of a single "Make" variable.

`cxxopts`

List of strings; default is `[]`

 Add these options to the C++ compilation command.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
 `defines`

List of strings; default is `[]`

 List of defines to add to the compile line.
Subject to ["Make" variable](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
Each string, which must consist of a single Bourne shell token,
is prepended with `-D` and added to the compile command line to this target,
as well as to every rule that depends on it. Be very careful, since this may have
far-reaching effects. When in doubt, add define values to
[`local_defines`](#cc_binary.local_defines) instead.
 `distribs`

List of strings; default is `[]`

`dynamic_deps`

List of [labels](/concepts/labels); default is `[]`

 These are other `cc_shared_library` dependencies the current target depends on.

The `cc_shared_library` implementation will use the list of
`dynamic_deps` (transitively, i.e. also the `dynamic_deps` of the
current target's `dynamic_deps`) to decide which `cc_libraries` in
the transitive `deps` should not be linked in because they are already provided
by a different `cc_shared_library`.


`hdrs_check`

String; default is `""`

 Deprecated, no-op.
 `includes`

List of strings; default is `[]`

 List of include dirs to be added to the compile line.
Subject to ["Make variable"](/reference/be/make-variables) substitution.
Each string is prepended with the package path and passed to the C++ toolchain for
expansion via the "include\_paths" CROSSTOOL feature. A toolchain running on a POSIX system
with typical feature definitions will produce
`-isystem path_to_package/include_entry`.
This should only be used for third-party libraries that
do not conform to the Google style of writing #include statements.
Unlike [COPTS](#cc_binary.copts), these flags are added for this rule
and every rule that depends on it. (Note: not the rules it depends upon!) Be
very careful, since this may have far-reaching effects. When in doubt, add
"-I" flags to [COPTS](#cc_binary.copts) instead.

The added `include` paths will include generated files as well as
files in the source tree.

`link_extra_lib`

[Label](/concepts/labels); default is `"@bazel_tools//tools/cpp:link_extra_lib"`

 Control linking of extra libraries.

By default, C++ binaries are linked against `//tools/cpp:link_extra_lib`,
which by default depends on the label flag `//tools/cpp:link_extra_libs`.
Without setting the flag, this library is empty by default. Setting the label flag
allows linking optional dependencies, such as overrides for weak symbols, interceptors
for shared library functions, or special runtime libraries (for malloc replacements,
prefer `malloc` or `--custom_malloc`). Setting this attribute to
`None` disables this behaviour.

`linkopts`

List of strings; default is `[]`

 Add these flags to the C++ linker command.
Subject to ["Make" variable](make-variables.html) substitution,
[Bourne shell tokenization](common-definitions.html#sh-tokenization) and
[label expansion](common-definitions.html#label-expansion).
Each string in this attribute is added to `LINKOPTS` before
linking the binary target.

Each element of this list that does not start with `$` or `-` is
assumed to be the label of a target in `deps`. The
list of files generated by that target is appended to the linker
options. An error is reported if the label is invalid, or is
not declared in `deps`.

`linkshared`

Boolean; default is `False`

 Create a shared library.
To enable this attribute, include `linkshared=True` in your rule. By default
this option is off.

The presence of this flag means that linking occurs with the `-shared` flag
to `gcc`, and the resulting shared library is suitable for loading into for
example a Java program. However, for build purposes it will never be linked into the
dependent binary, as it is assumed that shared libraries built with a
[cc\_binary](#cc_binary) rule are only loaded manually by other programs, so
it should not be considered a substitute for the [cc\_library](#cc_library)
rule. For sake of scalability we recommend avoiding this approach altogether and
simply letting `java_library` depend on `cc_library` rules
instead.

If you specify both `linkopts=['-static']` and `linkshared=True`,
you get a single completely self-contained unit. If you specify both
`linkstatic=True` and `linkshared=True`, you get a single, mostly
self-contained unit.

`linkstatic`

Boolean; default is `False`

 For [`cc_binary`](/reference/be/c-cpp.html#cc_binary) and
[`cc_test`](/reference/be/c-cpp.html#cc_test): link the binary in static
mode. For `cc_library.link_static`: see below.

By default this option is on for `cc_binary` and off for the rest.

If enabled and this is a binary or test, this option tells the build tool to link in
`.a`'s instead of `.so`'s for user libraries whenever possible.
System libraries such as libc (but _not_ the C/C++ runtime libraries,
see below) are still linked dynamically, as are libraries for which
there is no static library. So the resulting executable will still be dynamically
linked, hence only _mostly_ static.

There are really three different ways to link an executable:

- STATIC with fully\_static\_link feature, in which everything is linked statically;
   e.g. " `gcc -static foo.o libbar.a libbaz.a -lm`".


   This mode is enabled by specifying `fully_static_link` in the
   [`features`](/reference/be/common-definitions#features) attribute.
- STATIC, in which all user libraries are linked statically (if a static
   version is available), but where system libraries (excluding C/C++ runtime libraries)
   are linked dynamically, e.g. " `gcc foo.o libfoo.a libbaz.a -lm`".


   This mode is enabled by specifying `linkstatic=True`.
- DYNAMIC, in which all libraries are linked dynamically (if a dynamic version is
   available), e.g. " `gcc foo.o libfoo.so libbaz.so -lm`".


   This mode is enabled by specifying `linkstatic=False`.

If the `linkstatic` attribute or `fully_static_link` in
`features` is used outside of `//third_party`
please include a comment near the rule to explain why.

The `linkstatic` attribute has a different meaning if used on a
[`cc_library()`](/reference/be/c-cpp.html#cc_library) rule.
For a C++ library, `linkstatic=True` indicates that only
static linking is allowed, so no `.so` will be produced. linkstatic=False does
not prevent static libraries from being created. The attribute is meant to control the
creation of dynamic libraries.

There should be very little code built with `linkstatic=False` in production.
If `linkstatic=False`, then the build tool will create symlinks to
depended-upon shared libraries in the `*.runfiles` area.

`local_defines`

List of strings; default is `[]`

 List of defines to add to the compile line.
Subject to ["Make" variable](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
Each string, which must consist of a single Bourne shell token,
is prepended with `-D` and added to the compile command line for this target,
but not to its dependents.
 `malloc`

[Label](/concepts/labels); default is `"@bazel_tools//tools/cpp:malloc"`

 Override the default dependency on malloc.

By default, C++ binaries are linked against `//tools/cpp:malloc`,
which is an empty library so the binary ends up using libc malloc.
This label must refer to a `cc_library`. If compilation is for a non-C++
rule, this option has no effect. The value of this attribute is ignored if
`linkshared=True` is specified.

`module_interfaces`

List of [labels](/concepts/labels); default is `[]`

 The list of files are regarded as C++20 Modules Interface.

C++ Standard has no restriction about module interface file extension

- Clang use cppm
- GCC can use any source file extension
- MSVC use ixx

The use is guarded by the flag
`--experimental_cpp_modules`.

`nocopts`

String; default is `""`

 Remove matching options from the C++ compilation command.
Subject to ["Make" variable](/reference/be/make-variables) substitution.
The value of this attribute is interpreted as a regular expression.
Any preexisting `COPTS` that match this regular expression
(including values explicitly specified in the rule's [copts](#cc_binary.copts) attribute)
will be removed from `COPTS` for purposes of compiling this rule.
This attribute should not be needed or used
outside of `third_party`. The values are not preprocessed
in any way other than the "Make" variable substitution.
 `reexport_deps`

List of [labels](/concepts/labels); default is `[]`

`stamp`

Integer; default is `0`

 Whether to encode build information into the binary. Possible values:

- `stamp = 1`: Always stamp the build information into the binary, even in
   [`--nostamp`](/docs/user-manual#flag--stamp) builds. **This**
  **setting should be avoided**, since it potentially kills remote caching for the
   binary and any downstream actions that depend on it.

- `stamp = 0`: Always replace build information by constant values. This
   gives good build result caching.

- `stamp = -1`: Embedding of build information is controlled by the
   [`--[no]stamp`](/docs/user-manual#flag--stamp) flag.


Stamped binaries are _not_ rebuilt unless their dependencies change.

`win_def_file`

[Label](/concepts/labels); default is `None`

 The Windows DEF file to be passed to linker.

This attribute should only be used when Windows is the target platform.
It can be used to [export symbols](https://msdn.microsoft.com/en-us/library/d91k01sh.aspx) during linking a shared library.

## cc\_toolchain

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/cc_toolchain.bzl)

```
cc_toolchain(name, all_files, ar_files, as_files, aspect_hints, compatible_with, compiler_files, compiler_files_without_includes, coverage_files, deprecation, dwp_files, dynamic_runtime_lib, exec_compatible_with, exec_group_compatible_with, exec_properties, exec_transition_for_inputs, features, libc_top, licenses, linker_files, module_map, objcopy_files, output_licenses, package_metadata, restricted_to, static_runtime_lib, strip_files, supports_header_parsing, supports_param_files, tags, target_compatible_with, testonly, toolchain_config, toolchain_identifier, toolchains, visibility)
```

Represents a C++ toolchain.

This rule is responsible for:



- Collecting all artifacts needed for C++ actions to run. This is done by
   attributes such as `all_files`, `compiler_files`,
   `linker_files`, or other attributes ending with `_files`). These are
   most commonly filegroups globbing all required files.

- Generating correct command lines for C++ actions. This is done using
   `CcToolchainConfigInfo` provider (details below).


Use `toolchain_config` attribute to configure the C++ toolchain.
See also this
[page](https://bazel.build/docs/cc-toolchain-config-reference) for elaborate C++ toolchain configuration and toolchain selection documentation.

Use `tags = ["manual"]` in order to prevent toolchains from being built and configured
unnecessarily when invoking `bazel build //...`

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`all_files`

[Label](/concepts/labels); required

 Collection of all cc\_toolchain artifacts. These artifacts will be added as inputs to all
rules\_cc related actions (with the exception of actions that are using more precise sets of
artifacts from attributes below). Bazel assumes that `all_files` is a superset
of all other artifact-providing attributes (e.g. linkstamp compilation needs both compile
and link files, so it takes `all_files`).

This is what `cc_toolchain.files` contains, and this is used by all Starlark
rules using C++ toolchain.

`ar_files`

[Label](/concepts/labels); default is `None`

 Collection of all cc\_toolchain artifacts required for archiving actions.
 `as_files`

[Label](/concepts/labels); default is `None`

 Collection of all cc\_toolchain artifacts required for assembly actions.
 `compiler_files`

[Label](/concepts/labels); required

 Collection of all cc\_toolchain artifacts required for compile actions.
 `compiler_files_without_includes`

[Label](/concepts/labels); default is `None`

 Collection of all cc\_toolchain artifacts required for compile actions in case when
input discovery is supported (currently Google-only).
 `coverage_files`

[Label](/concepts/labels); default is `None`

 Collection of all cc\_toolchain artifacts required for coverage actions. If not specified,
all\_files are used.
 `dwp_files`

[Label](/concepts/labels); required

 Collection of all cc\_toolchain artifacts required for dwp actions.
 `dynamic_runtime_lib`

[Label](/concepts/labels); default is `None`

 Dynamic library artifact for the C++ runtime library (e.g. libstdc++.so).

This will be used when 'static\_link\_cpp\_runtimes' feature is enabled, and we're linking
dependencies dynamically.

`exec_transition_for_inputs`

Boolean; default is `False`

 Deprecated. No-op.
 `libc_top`

[Label](/concepts/labels); default is `None`

 A collection of artifacts for libc passed as inputs to compile/linking actions.
 `linker_files`

[Label](/concepts/labels); required

 Collection of all cc\_toolchain artifacts required for linking actions.
 `module_map`

[Label](/concepts/labels); default is `None`

 Module map artifact to be used for modular builds.
 `objcopy_files`

[Label](/concepts/labels); required

 Collection of all cc\_toolchain artifacts required for objcopy actions.
 `output_licenses`

List of strings; default is `[]`

`static_runtime_lib`

[Label](/concepts/labels); default is `None`

 Static library artifact for the C++ runtime library (e.g. libstdc++.a).

This will be used when 'static\_link\_cpp\_runtimes' feature is enabled, and we're linking
dependencies statically.

`strip_files`

[Label](/concepts/labels); required

 Collection of all cc\_toolchain artifacts required for strip actions.
 `supports_header_parsing`

Boolean; default is `False`

 Set to True when cc\_toolchain supports header parsing actions.
 `supports_param_files`

Boolean; default is `True`

 Set to True when cc\_toolchain supports using param files for linking actions.
 `toolchain_config`

[Label](/concepts/labels); required

 The label of the rule providing `cc_toolchain_config_info`.
 `toolchain_identifier`

String; default is `""`

 The identifier used to match this cc\_toolchain with the corresponding
crosstool\_config.toolchain.

Until issue [#5380](https://github.com/bazelbuild/bazel/issues/5380) is fixed
this is the recommended way of associating `cc_toolchain` with
`CROSSTOOL.toolchain`. It will be replaced by the `toolchain_config`
attribute ( [#5380](https://github.com/bazelbuild/bazel/issues/5380)).

## fdo\_prefetch\_hints

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/fdo/fdo_prefetch_hints.bzl)

```
fdo_prefetch_hints(name, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, package_metadata, profile, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

Represents an FDO prefetch hints profile that is either in the workspace.
Examples:

```lang-starlark

fdo_prefetch_hints(
    name = "hints",
    profile = "//path/to/hints:profile.afdo",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`profile`

[Label](/concepts/labels); required

 Label of the hints profile. The hints file has the .afdo extension
The label can also point to an fdo\_absolute\_path\_profile rule.


## fdo\_profile

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/fdo/fdo_profile.bzl)

```
fdo_profile(name, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, memprof_profile, package_metadata, profile, proto_profile, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

Represents an FDO profile that is in the workspace.
Example:

```lang-starlark

fdo_profile(
    name = "fdo",
    profile = "//path/to/fdo:profile.zip",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`memprof_profile`

[Label](/concepts/labels); default is `None`

 Label of the MemProf profile. The profile is expected to have
either a .profdata extension (for an indexed/symbolized memprof
profile), or a .zip extension for a zipfile containing a memprof.profdata
file.
 `profile`

[Label](/concepts/labels); required

 Label of the FDO profile or a rule which generates it. The FDO file can have one of the
following extensions: .profraw for unindexed LLVM profile, .profdata for indexed LLVM
profile, .zip that holds an LLVM profraw profile, .afdo for AutoFDO profile, .xfdo for
XBinary profile. The label can also point to an fdo\_absolute\_path\_profile rule.
 `proto_profile`

[Label](/concepts/labels); default is `None`

 Label of the protobuf profile.


## memprof\_profile

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/fdo/memprof_profile.bzl)

```
memprof_profile(name, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, package_metadata, profile, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

Represents a MEMPROF profile that is in the workspace.
Example:

```lang-starlark

memprof_profile(
    name = "memprof",
    profile = "//path/to/memprof:profile.afdo",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`profile`

[Label](/concepts/labels); required

 Label of the MEMPROF profile. The profile is expected to have
either a .profdata extension (for an indexed/symbolized memprof
profile), or a .zip extension for a zipfile containing a memprof.profdata
file.
The label can also point to an fdo\_absolute\_path\_profile rule.


## propeller\_optimize

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/cc/fdo/propeller_optimize.bzl)

```
propeller_optimize(name, aspect_hints, cc_profile, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, ld_profile, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

Represents a Propeller optimization profile in the workspace.
Example:

```lang-starlark

propeller_optimize(
    name = "layout",
    cc_profile = "//path:cc_profile.txt",
    ld_profile = "//path:ld_profile.txt"
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`cc_profile`

[Label](/concepts/labels); required

 Label of the profile passed to the various compile actions. This file has
the .txt extension.
 `ld_profile`

[Label](/concepts/labels); required

 Label of the profile passed to the link action. This file has
the .txt extension.

---

## Common definitions
- URL: https://bazel.build/reference/be/common-definitions
- Source: reference/be/common-definitions.mdx
- Slug: /reference/be/common-definitions

This section defines various terms and concepts that are common to
many functions or build rules.

## Contents

- [Bourne shell tokenization](#sh-tokenization)
- [Label Expansion](#label-expansion)
- [Typical attributes defined by most build rules](#typical-attributes)
- [Attributes common to all build rules](#common-attributes)
- [Attributes common to all test rules (\*\_test)](#common-attributes-tests)
- [Attributes common to all binary rules (\*\_binary)](#common-attributes-binaries)
- [Configurable attributes](#configurable-attributes)
- [Implicit output targets](#implicit-outputs)

## Bourne shell tokenization

Certain string attributes of some rules are split into multiple
words according to the tokenization rules of the Bourne shell:
unquoted spaces delimit separate words, and single- and
double-quotes characters and backslashes are used to prevent
tokenization.

Those attributes that are subject to this tokenization are
explicitly indicated as such in their definitions in this document.

Attributes subject to "Make" variable expansion and Bourne shell
tokenization are typically used for passing arbitrary options to
compilers and other tools. Examples of such attributes are
`cc_library.copts` and `java_library.javacopts`.
Together these substitutions allow a
single string variable to expand into a configuration-specific list
of option words.

## Label expansion

Some string attributes of a very few rules are subject to label
expansion: if those strings contain a valid label as a
substring, such as `//mypkg:target`, and that label is a
declared prerequisite of the current rule, it is expanded into the
pathname of the file represented by the
[target](https://bazel.build/reference/glossary#target) `//mypkg:target`.

Example attributes include `genrule.cmd` and
`cc_binary.linkopts`. The details may vary significantly in
each case, over such issues as: whether relative labels are
expanded; how labels that expand to multiple files are
treated, etc. Consult the rule attribute documentation for
specifics.

## Typical attributes defined by most build rules

This section describes attributes that are defined by many build rules,
but not all.

AttributeDescription`data`

List of [labels](/concepts/labels); default is `[]`

Files needed by this rule at runtime. May list file or rule targets. Generally
allows any target.

The default outputs and runfiles of targets in the `data` attribute
should appear in the `*.runfiles` area of any executable which is
output by or has a runtime dependency on this target. This may include data
files or binaries used when this target's
[`srcs`](#typical.srcs) are executed. See the
[data dependencies](/concepts/dependencies#data-dependencies)
section for more information about how to depend on and use data files.

New rules should define a `data` attribute if they process
inputs which might use other inputs at runtime. Rules' implementation functions
must also [populate the target's\
runfiles](https://bazel.build/rules/rules#runfiles) from the outputs and runfiles of any `data` attribute,
as well as runfiles from any dependency attribute which provides either
source code or runtime dependencies.

`deps`

List of [labels](/concepts/labels); default is `[]`

Dependencies for this target. Generally should only list rule targets. (Though
some rules permit files to be listed directly in `deps`, this
should be avoided when possible.)

Language-specific rules generally limit the listed targets to those with
specific [providers](https://bazel.build/extending/rules#providers).

The precise semantics of what it means for a target to depend on another using
`deps` are specific to the kind of rule, and the rule-specific
documentation goes into more detail. For rules which process source code,
`deps` generally specifies code dependencies used by the code in
[`srcs`](#typical.srcs).

Most often, a `deps` dependency is used to allow one module to use
symbols defined in another module written in the same programming language and
separately compiled. Cross-language dependencies are also permitted in many
cases: For example, a `java_library` target may depend on C++ code
in a `cc_library` target, by listing the latter in the
`deps` attribute. See the definition of
[dependencies](/concepts/build-ref#deps)
for more information.

`licenses`

List of strings; [nonconfigurable](#configurable-attributes);
default is `["none"]`

A list of license-type strings to be used for this particular target.

This is part of a deprecated licensing API that Bazel no longer uses. Don't
use this.

`srcs`

List of [labels](/concepts/labels); default is `[]`

Files processed or included by this rule. Generally lists files directly, but
may list rule targets (like `filegroup` or `genrule`) to
include their default outputs.

Language-specific rules often require that the listed files have particular
file extensions.

## Attributes common to all build rules

This section describes attributes that are implicitly added to all build
rules.

AttributeDescription`aspect_hints`

List of [labels](/concepts/labels); default is `[]`

A list of arbitrary labels which is exposed to [aspects](/extending/aspects) (in
particular - aspects invoked by this rule's reverse dependencies), but isn't exposed to this rule's
own implementation. Consult documentation for language-specific rule sets for details about what
effect a particular aspect hint would have.

You could think of an aspect hint as a richer alternative to a [tag](#common.tags):
while a tag conveys only a boolean state (the tag is either present or absent in the
`tags` list), an aspect hint can convey arbitrary structured information in its
[providers](/extending/rules#providers).

In practice, aspect hints are used for interoperability between different language-specific
rule sets. For example, imagine you have a `mylang_binary` target which needs to depend
on an `otherlang_library` target. The MyLang-specific logic needs some additional
information about the OtherLang target in order to use it, but `otherlang_library`
doesn't provide this information because it knows nothing about MyLang. One solution might be for
the MyLang rule set to define a `mylang_hint` rule which can be used to encode that
additional information; the user can add the hint to their `otherlang_library`'s
`aspect_hints`, and `mylang_binary` can use an aspect to collect the
additional information from a MyLang-specific provider in the `mylang_hint`.

For a concrete example, see
[`swift_interop_hint`](https://github.com/bazelbuild/rules_swift/blob/master/doc/rules.md#swift_interop_hint)
and [`swift_overlay`](https://github.com/bazelbuild/rules_swift/blob/master/doc/rules.md#swift_overlay)
in `rules_swift`.

Best practices:

- Targets listed in `aspect_hints` should be lightweight and minimal.
- Language-specific logic should consider only aspect hints having providers relevant to that
   language, and should ignore any other aspect hints.

`compatible_with`

List of [labels](/concepts/labels);
[nonconfigurable](#configurable-attributes); default is `[]`

The list of environments this target can be built for, in addition to
default-supported environments.

This is part of Bazel's constraint system, which lets users declare which
targets can and cannot depend on each other. For example, externally deployable
binaries shouldn't depend on libraries with company-secret code. See
[ConstraintSemantics](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/analysis/constraints/ConstraintSemantics.java#L46) for details.

`deprecation`

String; [nonconfigurable](#configurable-attributes); default is `None`

An explanatory warning message associated with this target.
Typically this is used to notify users that a target has become obsolete,
or has become superseded by another rule, is private to a package, or is
perhaps considered harmful for some reason. It is a good idea to include
some reference (like a webpage, a bug number or example migration CLs) so
that one can easily find out what changes are required to avoid the message.
If there is a new target that can be used as a drop in replacement, it is a
good idea to just migrate all users of the old target.

This attribute has no effect on the way things are built, but it
may affect a build tool's diagnostic output. The build tool issues a
warning when a rule with a `deprecation` attribute is
depended upon by a target in another package.

Intra-package dependencies are exempt from this warning, so that,
for example, building the tests of a deprecated rule does not
encounter a warning.

If a deprecated target depends on another deprecated target, no warning
message is issued.

Once people have stopped using it, the target can be removed.

`exec_compatible_with`

List of [labels](/concepts/labels);
[nonconfigurable](#configurable-attributes); default is `[]`

A list of
`constraint_values`
that must be present in the execution platform of this target's default exec
group. This is in addition to any constraints already set by the rule type.
Constraints are used to restrict the list of available execution platforms.

For more details, see
the description of
[toolchain resolution](/docs/toolchains#toolchain-resolution).
and
[exec groups](/extending/exec-groups)

`exec_group_compatible_with`

Dictionary of strings to lists of [labels](/concepts/labels);
[nonconfigurable](#configurable-attributes); default is `{}`

A dictionary of exec group names to lists of
`constraint_values`
that must be present in the execution platform for the given exec group. This
is in addition to any constraints already set on the exec group's definition.
Constraints are used to restrict the list of available execution platforms.

For more details, see
the description of
[toolchain resolution](/docs/toolchains#toolchain-resolution).
and
[exec groups](/extending/exec-groups)

`exec_properties`

Dictionary of strings; default is `{}`

A dictionary of strings that will be added to the `exec_properties` of a platform selected for this target. See `exec_properties` of the [platform](platforms-and-toolchains.html#platform) rule.

If a key is present in both the platform and target-level properties, the value will be taken from the target.

Keys can be prefixed with the name of an execution group followed by a `.` to apply them only to that particular exec group.

`features`

List of _feature_ strings; default is `[]`

A feature is string tag that can be enabled or disabled on a target. The
meaning of a feature depends on the rule itself.

This `features` attribute is combined with the [package](/reference/be/functions.html#package) level `features` attribute. For example, if
the features \["a", "b"\] are enabled on the package level, and a target's
`features` attribute contains \["-a", "c"\], the features enabled for the
rule will be "b" and "c".
[See example](https://github.com/bazelbuild/examples/blob/main/rules/features/BUILD).

`package_metadata`

List of [labels](/concepts/labels);
[nonconfigurable](#configurable-attributes); default is the package's
`default_package_metadata`

A list of labels that are associated metadata about this target.
Typically, the labels are simple rules that return a provider of
constant values. Rules and aspects may use these labels to perform some
additional analysis on the build graph.

The canonical use case is that of
[rules\_license](https://github.com/bazelbuild/rules_license).
For that use case, `package_metadata` and
`default_package_metadata` is used to attach information
about a package's licence or version to targets. An aspect applied
to a top-level binary can be used to gather those and produce
compliance reports.

`restricted_to`

List of [labels](/concepts/labels);
[nonconfigurable](#configurable-attributes); default is `[]`

The list of environments this target can be built for, _instead_ of
default-supported environments.

This is part of Bazel's constraint system. See
`compatible_with`
for details.

`tags`

List of strings; [nonconfigurable](#configurable-attributes);
default is `[]`

_Tags_ can be used on any rule. _Tags_ on test and
`test_suite` rules are useful for categorizing the tests.
_Tags_ on non-test targets are used to control sandboxed execution of
`genrule` s and

[Starlark](/rules/concepts)
actions, and for parsing by humans and/or external tools.

Bazel modifies the behavior of its sandboxing code if it finds the following
keywords in the `tags` attribute of any test or `genrule`
target, or the keys of `execution_requirements` for any Starlark
action.

- `no-sandbox` keyword results in the action or test never being
   sandboxed; it can still be cached or run remotely - use `no-cache`
   or `no-remote` to prevent either or both of those.

- `no-cache` keyword results in the action or test never being
   cached (locally or remotely). Note: for the purposes of this tag, the disk cache
   is considered a local cache, whereas the HTTP and gRPC caches are considered
   remote. Other caches, such as Skyframe or the persistent action cache, are not
   affected.

- `no-remote-cache` keyword results in the action or test never being
   cached remotely (but it may be cached locally; it may also be executed remotely).
   Note: for the purposes of this tag, the disk cache is considered a local cache,
   whereas the HTTP and gRPC caches are considered remote. Other caches, such as
   Skyframe or the persistent action cache, are not affected.
   If a combination of local disk cache and remote cache are used (combined cache),
   it's treated as a remote cache and disabled entirely unless `--incompatible_remote_results_ignore_disk`
   is set in which case the local components will be used.

- `no-remote-exec` keyword results in the action or test never being
   executed remotely (but it may be cached remotely).

- `no-remote` keyword prevents the action or test from being executed remotely or
   cached remotely. This is equivalent to using both
   `no-remote-cache` and `no-remote-exec`.

- `no-remote-cache-upload` keyword disables upload part of remote caching of a spawn.
   it does not disable remote execution.

- `local` keyword precludes the action or test from being remotely cached,
   remotely executed, or run inside the sandbox.
   For genrules and tests, marking the rule with the `local = True`
   attribute has the same effect.

- `requires-network` keyword allows access to the external
   network from inside the sandbox. This tag only has an effect if sandboxing
   is enabled.

- `block-network` keyword blocks access to the external
   network from inside the sandbox. In this case, only communication
   with localhost is allowed. This tag only has an effect if sandboxing is
   enabled.

- `requires-fakeroot` runs the test or action as uid and gid 0 (i.e., the root
   user). This is only supported on Linux. This tag takes precedence over the
   `--sandbox_fake_username` command-line option.


_Tags_ on tests are generally used to annotate a test's role in your
debug and release process. Typically, tags are most useful for C++ and Python
tests, which lack any runtime annotation ability. The use of tags and size
elements gives flexibility in assembling suites of tests based around codebase
check-in policy.

Bazel modifies test running behavior if it finds the following keywords in the
`tags` attribute of the test rule:

- `exclusive` will force the test to be run in the
   "exclusive" mode, ensuring that no other tests are running at the
   same time. Such tests will be executed in serial fashion after all build
   activity and non-exclusive tests have been completed. Remote execution is
   disabled for such tests because Bazel doesn't have control over what's
   running on a remote machine.

- `exclusive-if-local` will force the test to be run in the
   "exclusive" mode if it is executed locally, but will run the test in parallel if it's
   executed remotely.

- `manual` keyword will exclude the target from expansion of target pattern wildcards
   ( `...`, `:*`, `:all`, etc.) and `test_suite` rules
   which do not list the test explicitly when computing the set of top-level targets to build/run
   for the `build`, `test`, and `coverage` commands. It does not
   affect target wildcard or test suite expansion in other contexts, including the
   `query` command. Note that `manual` does not imply that a target should
   not be built/run automatically by continuous build/test systems. For example, it may be
   desirable to exclude a target from `bazel test ...` because it requires specific
   Bazel flags, but still have it included in properly-configured presubmit or continuous test
   runs.


- `external` keyword will force test to be unconditionally
   executed (regardless of `--cache_test_results`
   value).


See
[Tag Conventions](/reference/test-encyclopedia#tag-conventions)
 in the Test Encyclopedia for more conventions on tags attached to test targets.
`target_compatible_with`

List of [labels](/concepts/labels); default is `[]`

A list of
`constraint_value` s
that must be present in the target platform for this target to be considered
_compatible_. This is in addition to any constraints already set by the
rule type. If the target platform does not satisfy all listed constraints then
the target is considered _incompatible_. Incompatible targets are
skipped for building and testing when the target pattern is expanded
(e.g. `//...`, `:all`). When explicitly specified on the
command line, incompatible targets cause Bazel to print an error and cause a
build or test failure.

Targets that transitively depend on incompatible targets are themselves
considered incompatible. They are also skipped for building and testing.

An empty list (which is the default) signifies that the target is compatible
with all platforms.

All rules other than [Workspace Rules](workspace.html) support this
attribute.
For some rules this attribute has no effect. For example, specifying
`target_compatible_with` for a
`cc_toolchain` is not useful.

See the
[Platforms](/docs/platforms#skipping-incompatible-targets)
page for more information about incompatible target skipping.

`testonly`

Boolean; [nonconfigurable](#configurable-attributes); default is `False`
except for test and test suite targets

If `True`, only testonly targets (such as tests) can depend on this target.

Equivalently, a rule that is not `testonly` is not allowed to
depend on any rule that is `testonly`.

Tests ( `*_test` rules)
and test suites ( [test\_suite](/reference/be/general.html#test_suite) rules)
are `testonly` by default.

This attribute is intended to mean that the target should not be
contained in binaries that are released to production.

Because testonly is enforced at build time, not run time, and propagates
virally through the dependency tree, it should be applied judiciously. For
example, stubs and fakes that
are useful for unit tests may also be useful for integration tests
involving the same binaries that will be released to production, and
therefore should probably not be marked testonly. Conversely, rules that
are dangerous to even link in, perhaps because they unconditionally
override normal behavior, should definitely be marked testonly.

`toolchains`

List of [labels](/concepts/labels);
[nonconfigurable](#configurable-attributes); default is `[]`

The set of targets whose [Make variables](/reference/be/make-variables) this target is
allowed to access. These targets are either instances of rules that provide
`TemplateVariableInfo` or special targets for toolchain types built into Bazel. These
include:

- `@bazel_tools//tools/cpp:toolchain_type`
- `@rules_java//toolchains:current_java_runtime`

Note that this is distinct from the concept of
[toolchain resolution](/docs/toolchains#toolchain-resolution)
that is used by rule implementations for platform-dependent configuration. You cannot use this
attribute to determine which specific `cc_toolchain` or `java_toolchain` a
target will use.

`visibility`

List of [labels](/concepts/labels);
[nonconfigurable](#configurable-attributes);
default varies

The `visibility` attribute controls whether the target can be
depended on by targets in other locations. See the documentation for
[visibility](/concepts/visibility).

For targets declared directly in a BUILD file or in legacy macros called from
a BUILD file, the default value is the package's
`default_visibility`
if specified, or else `["//visibility:private"]`. For targets
declared in one or more symbolic macros, the default value is always just
`["//visibility:private"]` (which makes it useable only within the
package containing the macro's code).

## Attributes common to all test rules (\*\_test)

This section describes attributes that are common to all test rules.

AttributeDescription`args`

List of strings; subject to
[$(location)](/reference/be/make-variables#predefined_label_variables) and
["Make variable"](/reference/be/make-variables) substitution, and
[Bourne shell tokenization](#sh-tokenization); default is `[]`

Command line arguments that Bazel passes to the target when it is
executed with `bazel test`.

These arguments are passed before any `--test_arg` values
specified on the `bazel test` command line.

`env`

Dictionary of strings; values are subject to
[$(location)](/reference/be/make-variables#predefined_label_variables) and
["Make variable"](/reference/be/make-variables) substitution; default is `{}`

Specifies additional environment variables to set when the test is executed by
`bazel test`.

This attribute only applies to native rules, like `cc_test`,
`py_test`, and `sh_test`. It does not apply to
Starlark-defined test rules. For your own Starlark rules, you can add an "env"
attribute and use it to populate a

[RunEnvironmentInfo](/rules/lib/providers/RunEnvironmentInfo.html)
Provider.

[TestEnvironment](/rules/lib/toplevel/testing#TestEnvironment)

 Provider.

`env_inherit`

List of strings; default is `[]`

Specifies additional environment variables to inherit from the
external environment when the test is executed by `bazel test`.

This attribute only applies to native rules, like `cc_test`, `py_test`,
and `sh_test`. It does not apply to Starlark-defined test rules.

`size`

String `"enormous"`, `"large"`, `"medium"`, or
`"small"`; [nonconfigurable](#configurable-attributes);
default is `"medium"`

Specifies a test target's "heaviness": how much time/resources it needs to run.

Unit tests are considered "small", integration tests "medium", and end-to-end tests "large" or
"enormous". Bazel uses the size to determine a default timeout, which can be overridden using the
`timeout` attribute. The timeout is for all tests in the BUILD target, not for each
individual test. When the test is run locally, the `size` is additionally used for
scheduling purposes: Bazel tries to respect `--local_{ram,cpu}_resources` and not
overwhelm the local machine by running lots of heavy tests at the same time.

Test sizes correspond to the following default timeouts and assumed peak local resource
usages:

SizeRAM (in MB)CPU (in CPU cores)Default timeoutsmall201short (1 minute)medium1001moderate (5 minutes)large3001long (15 minutes)enormous8001eternal (60 minutes)

The environment variable
`TEST_SIZE` will be set to
the value of this attribute when spawning the test.

`timeout`

String `"short"`, `"moderate"`, `"long"`, or
`"eternal"`; [nonconfigurable](#configurable-attributes); default is derived
from the test's `size` attribute

How long the test is expected to run before returning.

While a test's size attribute controls resource estimation, a test's
timeout may be set independently. If not explicitly specified, the
timeout is based on the [test's size](#test.size). The test
timeout can be overridden with the `--test_timeout` flag, e.g. for
running under certain conditions which are known to be slow. Test timeout values
correspond to the following time periods:

Timeout ValueTime Periodshort1 minutemoderate5 minuteslong15 minuteseternal60 minutes

For times other than the above, the test timeout can be overridden with the
`--test_timeout` bazel flag, e.g. for manually running under
conditions which are known to be slow. The `--test_timeout` values
are in seconds. For example `--test_timeout=120` will set the test
timeout to two minutes.

The environment variable
`TEST_TIMEOUT` will be set
to the test timeout (in seconds) when spawning the test.

`flaky`

Boolean; [nonconfigurable](#configurable-attributes);
default is `False`

Marks test as flaky.

If set, executes the test up to three times, marking it as failed only if it
fails each time. By default, this attribute is set to False and the test is
executed only once. Note, that use of this attribute is generally discouraged -
tests should pass reliably when their assertions are upheld.

`shard_count`

Non-negative integer less than or equal to 50; default is `-1`

Specifies the number of parallel shards
to use to run the test.

If set, this value will override any heuristics used to determine the number of
parallel shards with which to run the test. Note that for some test
rules, this parameter may be required to enable sharding
in the first place. Also see `--test_sharding_strategy`.

If test sharding is enabled, the environment variable `
TEST_TOTAL_SHARDS
` will be set to this value when spawning the test.

Sharding requires the test runner to support the test sharding protocol.
If it does not, then it will most likely run every test in every shard, which
is not what you want.

See
[Test Sharding](/reference/test-encyclopedia#test-sharding)
in the Test Encyclopedia for details on sharding.

`local`

Boolean; [nonconfigurable](#configurable-attributes);
default is `False`

Forces the test to be run locally, without sandboxing.

Setting this to True is equivalent to providing "local" as a tag
( `tags=["local"]`).

## Attributes common to all binary rules (\*\_binary)

This section describes attributes that are common to all binary rules.

AttributeDescription`args`

List of strings; subject to
[$(location)](/reference/be/make-variables#predefined_label_variables) and
["Make variable"](/reference/be/make-variables) substitution, and
[Bourne shell tokenization](#sh-tokenization);
[nonconfigurable](#configurable-attributes);
default is `[]`

Command line arguments that Bazel will pass to the target when it is executed
either by the `run` command or as a test. These arguments are
passed before the ones that are specified on the `bazel run` or
`bazel test` command line.

_NOTE: The arguments are not passed when you run the target_
_outside of Bazel (for example, by manually executing the binary in_
_`bazel-bin/`)._

`env`

Dictionary of strings; values are subject to
[$(location)](/reference/be/make-variables#predefined_label_variables) and
["Make variable"](/reference/be/make-variables) substitution; default is `{}`

Specifies additional environment variables to set when the target is
executed by `bazel run`.

This attribute only applies to native rules, like `cc_binary`, `py_binary`,
and `sh_binary`. It does not apply to Starlark-defined executable rules. For your own
Starlark rules, you can add an "env" attribute and use it to populate a

[RunEnvironmentInfo](/rules/lib/providers/RunEnvironmentInfo.html)

Provider.

_NOTE: The environment variables are not set when you run the target_
_outside of Bazel (for example, by manually executing the binary in_
_`bazel-bin/`)._

`output_licenses`

List of strings; default is `[]`

The licenses of the output files that this binary generates.

This is part of a deprecated licensing API that Bazel no longer uses. Don't
use this.

## Configurable attributes

Most attributes are "configurable", meaning that their values may change when
the target is built in different ways. Specifically, configurable attributes
may vary based on the flags passed to the Bazel command line, or what
downstream dependency is requesting the target. This can be used, for
instance, to customize the target for multiple platforms or compilation modes.

The following example declares different sources for different target
architectures. Running `bazel build :multiplatform_lib --cpu x86`
will build the target using `x86_impl.cc`, while substituting
`--cpu arm` will instead cause it to use `arm_impl.cc`.

```
cc_library(
    name = "multiplatform_lib",
    srcs = select({
        ":x86_mode": ["x86_impl.cc"],
        ":arm_mode": ["arm_impl.cc"]
    })
)
config_setting(
    name = "x86_mode",
    values = { "cpu": "x86" }
)
config_setting(
    name = "arm_mode",
    values = { "cpu": "arm" }
)

```

The [`select()`](/reference/be/functions.html#select) function
chooses among different alternative values for a configurable attribute based
on which [`config_setting`](/reference/be/general.html#config_setting)
or [`constraint_value`](/reference/be/platforms-and-toolchains.html#constraint_value)
criteria the target's configuration satisfies.

Bazel evaluates configurable attributes after processing macros and before
processing rules (technically, between the
[loading and analysis phases](https://bazel.build/rules/concepts#evaluation-model)).
Any processing before `select()` evaluation doesn't know which
branch the `select()` chooses. Macros, for example, can't change
their behavior based on the chosen branch, and `bazel query` can
only make conservative guesses about a target's configurable dependencies. See
[this FAQ](https://bazel.build/docs/configurable-attributes#faq)
for more on using `select()` with rules and macros.

Attributes marked `nonconfigurable` in their documentation cannot
use this feature. Usually an attribute is nonconfigurable because Bazel
internally needs to know its value before it can determine how to resolve a
`select()`.

See [Configurable Build Attributes](https://bazel.build/docs/configurable-attributes) for a detailed overview.

## Implicit output targets

_Implicit outputs in C++ are deprecated. Please refrain from using it_
_in other languages where possible. We don't have a deprecation path yet_
_but they will eventually be deprecated too._

When you define a build rule in a BUILD file, you are explicitly
declaring a new, named rule target in a package. Many build rule
functions also _implicitly_ entail one or more output file
targets, whose contents and meaning are rule-specific.

For example, when you explicitly declare a
`java_binary(name='foo', ...)` rule, you are also
_implicitly_ declaring an output file
target `foo_deploy.jar` as a member of the same package.
(This particular target is a self-contained Java archive suitable
for deployment.)

Implicit output targets are first-class members of the global
target graph. Just like other targets, they are built on demand,
either when specified in the top-level built command, or when they
are necessary prerequisites for other build targets. They can be
referenced as dependencies in BUILD files, and can be observed in
the output of analysis tools such as `bazel query`.

For each kind of build rule, the rule's documentation contains a
special section detailing the names and contents of any implicit
outputs entailed by a declaration of that kind of rule.

An important but somewhat subtle distinction between the
two namespaces used by the build system:
[labels](/concepts/labels) identify _targets_,
which may be rules or files, and file targets may be divided into
either source (or input) file targets and derived (or output) file
targets. These are the things you can mention in BUILD files,
build from the command-line, or examine using `bazel query`;
this is the _target namespace_. Each file target corresponds
to one actual file on disk (the "file system namespace"); each rule
target may correspond to zero, one or more actual files on disk.
There may be files on disk that have no corresponding target; for
example, `.o` object files produced during C++ compilation
cannot be referenced from within BUILD files or from the command line.
In this way, the build tool may hide certain implementation details of
how it does its job. This is explained more fully in
the [BUILD Concept Reference](/concepts/build-ref).

---

## Extra Actions Rules
- URL: https://bazel.build/reference/be/extra-actions
- Source: reference/be/extra-actions.mdx
- Slug: /reference/be/extra-actions

## Rules

- [action\_listener](#action_listener)
- [extra\_action](#extra_action)

## action\_listener

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/extra/ActionListenerRule.java)

```
action_listener(name, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, extra_actions, features, licenses, mnemonics, package_metadata, restricted_to, tags, target_compatible_with, testonly, visibility)
```

**WARNING:** Extra actions are deprecated. Use
[aspects](https://bazel.build/rules/aspects)
instead.

An `action_listener` rule doesn't produce any output itself.
Instead, it allows tool developers to insert
[`extra_action`](/reference/be/extra-actions.html#extra_action) s into the build system,
by providing a mapping from action to [`extra_action`](/reference/be/extra-actions.html#extra_action).

This rule's arguments map action mnemonics to
[`extra_action`](/reference/be/extra-actions.html#extra_action) rules.

By specifying the option [`--experimental_action_listener=<label>`](/docs/user-manual#flag--experimental_action_listener),
the build will use the specified `action_listener` to insert
[`extra_action`](/reference/be/extra-actions.html#extra_action) s into the build graph.

#### Example

```
action_listener(
    name = "index_all_languages",
    mnemonics = [
        "Javac",
        "CppCompile",
        "Python",
    ],
    extra_actions = [":indexer"],
)

action_listener(
    name = "index_java",
    mnemonics = ["Javac"],
    extra_actions = [":indexer"],
)

extra_action(
    name = "indexer",
    tools = ["//my/tools:indexer"],
    cmd = "$(location //my/tools:indexer)" +
          "--extra_action_file=$(EXTRA_ACTION_FILE)",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`extra_actions`

List of [labels](/concepts/labels); required

 A list of `extra_action` targets
 this `action_listener` should add to the build graph.
 E.g. `[ "//my/tools:analyzer" ]`.

 `mnemonics`

List of strings; required

 A list of action mnemonics this `action_listener` should listen
 for, e.g. `[ "Javac" ]`.


Mnemonics are not a public interface.
There's no guarantee that the mnemonics and their actions don't change.


## extra\_action

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/extra/ExtraActionRule.java)

```
extra_action(name, data, aspect_hints, cmd, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, licenses, out_templates, package_metadata, requires_action_output, restricted_to, tags, target_compatible_with, testonly, toolchains, tools, visibility)
```

**WARNING:** Extra actions are deprecated. Use
[aspects](https://bazel.build/rules/aspects)
instead.

An `extra_action` rule doesn't produce any meaningful output
when specified as a regular build target. Instead, it allows tool developers
to insert additional actions into the build graph that shadow existing actions.

See [`action_listener`](/reference/be/extra-actions.html#action_listener) for details
on how to enable `extra_action` s.

The `extra_action` s run as a command-line. The command-line tool gets
access to a file containing a protocol buffer as $(EXTRA\_ACTION\_FILE)
with detailed information on the original action it is shadowing.
It also has access to all the input files the original action has access to.
See `extra_actions_base.proto`
for details on the data stored inside the protocol buffer. Each proto file
contains an ExtraActionInfo message.

Just like all other actions, extra actions are sandboxed, and should be designed to handle that.

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

 You may refer to this rule by `label` in the `extra_actions` argument
 of [` action_listener`](/reference/be/extra-actions.html#action_listener) rules.

 `cmd`

String; required

 The command to run.


Like [genrule cmd attribute](/reference/be/general.html#genrule.cmd) with the following
differences:


1. No heuristic label expansion. Only labels using $(location ...) are expanded.


2. An additional pass is applied to the string to replace all
    occurrences of the outputs created from the `out_templates`
    attribute. All occurrences of `$(output out_template)`
    are replaced with the path to the file denoted by `label`.



    E.g. out\_template `$(ACTION_ID).analysis`
    can be matched with `$(output $(ACTION_ID).analysis)`.



    In effect, this is the same substitution as `$(location)`
    but with a different scope.



`out_templates`

List of strings; default is `[]`

 A list of templates for files generated by the `extra_action` command.


The template can use the following variables:


- $(ACTION\_ID), an id uniquely identifying this `extra_action`.
   Used to generate a unique output file.


`requires_action_output`

Boolean; default is `False`

 Indicates this `extra_action` requires the output of the
 original action to be present as input to this `extra_action`.


When true (default false), the extra\_action can assume that the
original action outputs are available as part of its inputs.


`tools`

List of [labels](/concepts/labels); default is `[]`

 A list of `tool` dependencies for this rule.


See the definition of [dependencies](/concepts/build-ref#deps) for more
information.


The build system ensures these prerequisites are built before running the
`extra_action` command; they are built using the
[`exec` configuration](/docs/user-manual#configurations),
since they must run as a tool during the build itself. The path of an individual
`tools` target `//x:y` can be obtained using
`$(location //x:y)`.


All tools and their data dependencies are consolidated into a single tree
within which the command can use relative paths. The working directory will
be the root of that unified tree.

---

## General Rules
- URL: https://bazel.build/reference/be/general
- Source: reference/be/general.mdx
- Slug: /reference/be/general

## Rules

- [alias](#alias)
- [config\_setting](#config_setting)
- [filegroup](#filegroup)
- [genquery](#genquery)
- [genrule](#genrule)
- [starlark\_doc\_extract](#starlark_doc_extract)
- [test\_suite](#test_suite)

## alias

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/Alias.java)

```
alias(name, actual, aspect_hints, compatible_with, deprecation, features, package_metadata, restricted_to, tags, target_compatible_with, testonly, visibility)
```

The `alias` rule creates another name a rule can be referred to as.

Aliasing only works for "regular" targets. In particular, `package_group`
and `test_suite` cannot be aliased.

Aliasing may be of help in large repositories where renaming a target would require making
changes to lots of files. You can also use alias rule to store a
[select](/reference/be/functions.html#select) function call if you want to reuse that logic for
multiple targets.

The alias rule has its own visibility declaration. In all other respects, it behaves
like the rule it references (e.g. testonly _on the alias_ is ignored; the testonly-ness
of the referenced rule is used instead) with some minor exceptions:



- Tests are not run if their alias is mentioned on the command line. To define an alias
   that runs the referenced test, use a [`test_suite`](#test_suite)
   rule with a single target in its [`tests`](#test_suite.tests)
   attribute.

- When defining environment groups, the aliases to `environment` rules are not
   supported. They are not supported in the `--target_environment` command line
   option, either.


#### Examples

```
filegroup(
    name = "data",
    srcs = ["data.txt"],
)

alias(
    name = "other",
    actual = ":data",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`actual`

[Label](/concepts/labels); required

 The target this alias refers to. It does not need to be a rule, it can also be an input
 file.



## config\_setting

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/config/ConfigRuleClasses.java)

```
config_setting(name, aspect_hints, constraint_values, define_values, deprecation, features, flag_values, licenses, package_metadata, tags, testonly, values, visibility)
```

Matches an expected configuration state (expressed as build flags or platform constraints) for
the purpose of triggering configurable attributes. See [select](/reference/be/functions.html#select) for
how to consume this rule and [Configurable attributes](/reference/be/common-definitions#configurable-attributes) for an overview of the general feature.



#### Examples

The following matches any build that sets `--compilation_mode=opt` or
`-c opt` (either explicitly at the command line or implicitly from .bazelrc files):


```
  config_setting(
      name = "simple",
      values = {"compilation_mode": "opt"}
  )

```

The following matches any build that targets ARM and applies the custom define
`FOO=bar` (for instance, `bazel build --cpu=arm --define FOO=bar ...`):


```
  config_setting(
      name = "two_conditions",
      values = {
          "cpu": "arm",
          "define": "FOO=bar"
      }
  )

```

The following matches any build that sets
[user-defined flag](https://bazel.build/rules/config#user-defined-build-settings) `--//custom_flags:foo=1` (either explicitly at the command line or implicitly from
.bazelrc files):


```
  config_setting(
      name = "my_custom_flag_is_set",
      flag_values = { "//custom_flags:foo": "1" },
  )

```

The following matches any build that targets a platform with an x86\_64 architecture and glibc
version 2.25, assuming the existence of a `constraint_value` with label
`//example:glibc_2_25`. Note that a platform still matches if it defines additional
constraint values beyond these two.


```
  config_setting(
      name = "64bit_glibc_2_25",
      constraint_values = [
          "@platforms//cpu:x86_64",
          "//example:glibc_2_25",
      ]
  )

```

 In all these cases, it's possible for the configuration to change within the build, for example if
 a target needs to be built for a different platform than its dep. This means that even when a
 `config_setting` doesn't match the top-level command-line flags, it may still match
 some build targets.



#### Notes

- See [select](/reference/be/functions.html#select) for what happens when multiple
   `config_setting` s match the current configuration state.

- For flags that support shorthand forms (e.g. `--compilation_mode` vs.
   `-c`), `values` definitions must use the full form. These automatically
   match invocations using either form.

- If a flag takes multiple values (like `--copt=-Da --copt=-Db` or a list-typed
   [Starlark flag](https://bazel.build/rules/config#user-defined-build-settings)), `values = { "flag": "a" }` matches if `"a"` is
   present _anywhere_ in the actual list.


  `values = { "myflag": "a,b" }` works the same way: this matches
   `--myflag=a --myflag=b`, `--myflag=a --myflag=b --myflag=c`,
   `--myflag=a,b`, and `--myflag=c,b,a`. Exact semantics vary between
   flags. For example, `--copt` doesn't support multiple values _in the same_
  _instance_: `--copt=a,b` produces `["a,b"]` while `--copt=a
          --copt=b` produces `["a", "b"]` (so `values = { "copt": "a,b" }`
   matches the former but not the latter). But `--ios_multi_cpus` (for Apple rules)
   _does_: `-ios_multi_cpus=a,b` and `ios_multi_cpus=a --ios_multi_cpus=b
          ` both produce `["a", "b"]`. Check flag definitions and test your
   conditions carefully to verify exact expectations.


- If you need to define conditions that aren't modeled by built-in build flags, use
   [Starlark-defined flags](https://bazel.build/rules/config#user-defined-build-settings). You can also use `--define`, but this offers weaker
   support and is not recommended. See
   [here](/reference/be/common-definitions#configurable-attributes) for more discussion.

- Avoid repeating identical `config_setting` definitions in different packages.
   Instead, reference a common `config_setting` that defined in a canonical package.

- [`values`](general.html#config_setting.values),
   [`define_values`](general.html#config_setting.define_values), and
   [`constraint_values`](general.html#config_setting.constraint_values)
   can be used in any combination in the same `config_setting` but at least one must
   be set for any given `config_setting`.


### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`constraint_values`

List of [labels](/concepts/labels); [nonconfigurable](common-definitions.html#configurable-attributes); default is `[]`

 The minimum set of `constraint_values` that the target platform must specify
 in order to match this `config_setting`. (The execution platform is not
 considered here.) Any additional constraint values that the platform has are ignored. See
 [Configurable Build Attributes](https://bazel.build/docs/configurable-attributes#platforms) for details.



If two `config_setting` s match in the same `select` and one has
all the same flags and `constraint_setting` s as the other plus additional ones,
the one with more settings is chosen. This is known as "specialization". For example,
a `config_setting` matching `x86` and `Linux` specializes
a `config_setting` matching `x86`.



If two `config_setting` s match and both have `constraint_value` s
not present in the other, this is an error.



`define_values`

Dictionary: String -> String; [nonconfigurable](common-definitions.html#configurable-attributes); default is `{}`

 The same as [`values`](/reference/be/general.html#config_setting.values) but
 specifically for the `--define` flag.



`--define` is special because its syntax ( `--define KEY=VAL`)
means `KEY=VAL` is a _value_ from a Bazel flag perspective.


That means:



```
            config_setting(
                name = "a_and_b",
                values = {
                    "define": "a=1",
                    "define": "b=2",
                })

```

doesn't work because the same key ( `define`) appears twice in the
dictionary. This attribute solves that problem:



```
            config_setting(
                name = "a_and_b",
                define_values = {
                    "a": "1",
                    "b": "2",
                })

```

correctly matches `bazel build //foo --define a=1 --define b=2`.



`--define` can still appear in
[`values`](/reference/be/general.html#config_setting.values) with normal flag syntax,
and can be mixed freely with this attribute as long as dictionary keys remain distinct.



`flag_values`

Dictionary: [label](/concepts/labels) -\> String; [nonconfigurable](common-definitions.html#configurable-attributes); default is `{}`

 The same as [`values`](/reference/be/general.html#config_setting.values) but
 for [user-defined build flags](https://bazel.build/rules/config#user-defined-build-settings).



This is a distinct attribute because user-defined flags are referenced as labels while
built-in flags are referenced as arbitrary strings.



`values`

Dictionary: String -> String; [nonconfigurable](common-definitions.html#configurable-attributes); default is `{}`

 The set of configuration values that match this rule (expressed as build flags)



This rule inherits the configuration of the configured target that
references it in a `select` statement. It is considered to
"match" a Bazel invocation if, for every entry in the dictionary, its
configuration matches the entry's expected value. For example
`values = {"compilation_mode": "opt"}` matches the invocations
`bazel build --compilation_mode=opt ...` and
`bazel build -c opt ...` on target-configured rules.


For convenience's sake, configuration values are specified as build flags (without
the preceding `"--"`). But keep in mind that the two are not the same. This
is because targets can be built in multiple configurations within the same
build. For example, an exec configuration's "cpu" matches the value of
`--host_cpu`, not `--cpu`. So different instances of the
same `config_setting` may match the same invocation differently
depending on the configuration of the rule using them.


If a flag is not explicitly set at the command line, its default value is used.
If a key appears multiple times in the dictionary, only the last instance is used.
If a key references a flag that can be set multiple times on the command line (e.g.
`bazel build --copt=foo --copt=bar --copt=baz ...`), a match occurs if
_any_ of those settings match.


## filegroup

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/filegroup/FilegroupRule.java)

```
filegroup(name, srcs, data, aspect_hints, compatible_with, deprecation, features, licenses, output_group, package_metadata, restricted_to, tags, target_compatible_with, testonly, visibility)
```

Use `filegroup` to gather the outputs of a set of targets under a single
label.

`filegroup` is not a substitute for listing targets on the command line or
in an attribute of another rule, because targets have many properties other than their
outputs, which are not collected in the same way. However, it's still useful in quite
a few cases, for example, in the `srcs` attribute of a genrule, or
the `data` attribute of a \*\_binary rule.

Using `filegroup` is encouraged instead of referencing directories directly.
Directly referencing directories is discouraged because the build system does not have
full knowledge of all files below the directory, so it may not rebuild when these files change.
When combined with [glob](/reference/be/functions.html#glob), `filegroup` can ensure that all
files are explicitly known to the build system.

#### Examples

To create a `filegroup` consisting of two source files, do

```
filegroup(
    name = "mygroup",
    srcs = [
        "a_file.txt",
        "//a/library:target",
        "//a/binary:target",
    ],
)

```

Or, use a `glob` to fully crawl a testdata directory:

```
filegroup(
    name = "exported_testdata",
    srcs = glob([
        "testdata/*.dat",
        "testdata/logs/**/*.log",
    ]),
)

```

To make use of these definitions, reference the `filegroup` with a label from any rule:

```
cc_library(
    name = "my_library",
    srcs = ["foo.cc"],
    data = [
        "//my_package:exported_testdata",
        "//my_package:mygroup",
    ],
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of targets that are members of the file group.


It is common to use the result of a [glob](/reference/be/functions.html#glob) expression for
the value of the `srcs` attribute.


`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this rule at runtime.


Targets named in the `data` attribute will be added to the
`runfiles` of this `filegroup` rule. When the
`filegroup` is referenced in the `data` attribute of
another rule its `runfiles` will be added to the `runfiles`
of the depending rule. See the [data dependencies](/concepts/dependencies#data-dependencies)
section and [general documentation of\
`data`](/reference/be/common-definitions#common.data) for more information about how to depend on and use data files.


`output_group`

String; default is `""`

 The output group from which to gather artifacts from sources. If this attribute is
 specified, artifacts from the specified output group of the dependencies will be exported
 instead of the default output group.


An "output group" is a category of output artifacts of a target, specified in that
rule's implementation.


## genquery

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/genquery/GenQueryRule.java)

```
genquery(name, deps, data, aspect_hints, compatible_with, compressed_output, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, expression, features, licenses, opts, package_metadata, restricted_to, scope, strict, tags, target_compatible_with, testonly, visibility)
```

`genquery()` runs a query specified in the
[Bazel query language](/reference/query) and dumps the result
into a file.


In order to keep the build consistent, the query is allowed only to visit
the transitive closure of the targets specified in the `scope`
attribute. Queries violating this rule will fail during execution if
`strict` is unspecified or true (if `strict` is false,
the out of scope targets will simply be skipped with a warning). The
easiest way to make sure this does not happen is to mention the same labels
in the scope as in the query expression.


The only difference between the queries allowed here and on the command
line is that queries containing wildcard target specifications (e.g.
`//pkg:*` or `//pkg:all`) are not allowed here.
The reasons for this are two-fold: first, because `genquery` has
to specify a scope to prevent targets outside the transitive closure of the
query to influence its output; and, second, because `BUILD` files
do not support wildcard dependencies (e.g. `deps=["//a/..."]`
is not allowed).


The genquery's output is ordered lexicographically in order to enforce deterministic output,
with the exception of `--output=graph|minrank|maxrank` or when `somepath`
is used as the top-level function.


The name of the output file is the name of the rule.


#### Examples

This example writes the list of the labels in the transitive closure of the
specified target to a file.


```
genquery(
    name = "kiwi-deps",
    expression = "deps(//kiwi:kiwi_lib)",
    scope = ["//kiwi:kiwi_lib"],
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`compressed_output`

Boolean; default is `False`

 If `True`, query output is written in GZIP file format. This setting can be used
 to avoid spikes in Bazel's memory use when the query output is expected to be large. Bazel
 already internally compresses query outputs greater than 220 bytes regardless of
 the value of this setting, so setting this to `True` may not reduce retained
 heap. However, it allows Bazel to skip _decompression_ when writing the output file,
 which can be memory-intensive.

 `expression`

String; required

 The query to be executed. In contrast to the command line and other places in BUILD files,
 labels here are resolved relative to the root directory of the workspace. For example, the
 label `:b` in this attribute in the file `a/BUILD` will refer to the
 target `//:b`.

 `opts`

List of strings; default is `[]`

 The options that are passed to the query engine. These correspond to the command line
 options that can be passed to `bazel query`. Some query options are not allowed
 here: `--keep_going`, `--query_file`, `--universe_scope`,
 `--order_results` and `--order_output`. Options not specified here
 will have their default values just like on the command line of `bazel query`.

 `scope`

List of [labels](/concepts/labels); required

 The scope of the query. The query is not allowed to touch targets outside the transitive
 closure of these targets.

 `strict`

Boolean; default is `True`

 If true, targets whose queries escape the transitive closure of their scopes will fail to
 build. If false, Bazel will print a warning and skip whatever query path led it outside of
 the scope, while completing the rest of the query.



## genrule

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/bazel/rules/genrule/BazelGenRuleRule.java)

```
genrule(name, srcs, outs, aspect_hints, cmd, cmd_bash, cmd_bat, cmd_ps, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, executable, features, licenses, local, message, output_licenses, output_to_bindir, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, tools, visibility)
```

A `genrule` generates one or more files using a user-defined Bash command.

Genrules are generic build rules that you can use if there's no specific rule for the task.
For example, you could run a Bash one-liner. If however you need to compile C++ files, stick
to the existing `cc_*` rules, because all the heavy lifting has already been done
for you.

Note that genrule requires a shell to interpret the command argument.
It is also easy to reference arbitrary programs available on the PATH, however this makes the
command non-hermetic and may not be reproducible.
If you only need to run a single tool, consider using
[run\_binary](https://github.com/bazelbuild/bazel-skylib/blob/main/docs/run_binary_doc.md)
instead.

Like every other action, the action created by genrules should not assume anything about their
working directory; all Bazel guarantees is that their declared inputs will be available at the
path that `$(location)` returns for their label. For example, if the action is run in a
sandbox or remotely, the implementation of the sandbox or the remote execution will determine the
working directory. If run directly (using the `standalone` strategy), the working
directory will be the execution root, i.e. the result of `bazel info execution_root`.

Do not use a genrule for running tests. There are special dispensations for tests and test
results, including caching policies and environment variables. Tests generally need to be run
after the build is complete and on the target architecture, whereas genrules are executed during
the build and on the exec architecture (the two may be different). If you need a general purpose
testing rule, use [`sh_test`](/reference/be/shell.html#sh_test).

#### Cross-compilation Considerations

_See [the user manual](/docs/user-manual#configurations) for more info about_
_cross-compilation._

While genrules run during a build, their outputs are often used after the build, for deployment or
testing. Consider the example of compiling C code for a microcontroller: the compiler accepts C
source files and generates code that runs on a microcontroller. The generated code obviously
cannot run on the CPU that was used for building it, but the C compiler (if compiled from source)
itself has to.

The build system uses the exec configuration to describe the machine(s) on which the build runs
and the target configuration to describe the machine(s) on which the output of the build is
supposed to run. It provides options to configure each of these and it segregates the
corresponding files into separate directories to avoid conflicts.

For genrules, the build system ensures that dependencies are built appropriately:
`srcs` are built (if necessary) for the _target_ configuration,
`tools` are built for the _exec_ configuration, and the output is considered to
be for the _target_ configuration. It also provides ["Make" variables](/reference/be/make-variables) that genrule commands can pass to the corresponding tools.

It is intentional that genrule defines no `deps` attribute: other built-in rules use
language-dependent meta information passed between the rules to automatically determine how to
handle dependent rules, but this level of automation is not possible for genrules. Genrules work
purely at the file and runfiles level.

#### Special Cases

_Exec-exec compilation_: in some cases, the build system needs to run genrules such that the
output can also be executed during the build. If for example a genrule builds some custom compiler
which is subsequently used by another genrule, the first one has to produce its output for the
exec configuration, because that's where the compiler will run in the other genrule. In this case,
the build system does the right thing automatically: it builds the `srcs` and
`outs` of the first genrule for the exec configuration instead of the target
configuration. See [the user manual](/docs/user-manual#configurations) for more
info.

_JDK & C++ Tooling_: to use a tool from the JDK or the C++ compiler suite, the build system
provides a set of variables to use. See ["Make" variable](/reference/be/make-variables) for
details.

#### Genrule Environment

The genrule command is executed by a Bash shell that is configured to fail when a command
or a pipeline fails, using `set -e -o pipefail`.

The build tool executes the Bash command in a sanitized process environment that
defines only core variables such as `PATH`, `PWD`,
`TMPDIR`, and a few others.

To ensure that builds are reproducible, most variables defined in the user's shell
environment are not passed though to the genrule's command. However, Bazel (but not
Blaze) passes through the value of the user's `PATH` environment variable.

Any change to the value of `PATH` will cause Bazel to re-execute the command
on the next build.


A genrule command should not access the network except to connect processes that are
children of the command itself, though this is not currently enforced.

The build system automatically deletes any existing output files, but creates any necessary parent
directories before it runs a genrule. It also removes any output files in case of a failure.

#### General Advice

- Do ensure that tools run by a genrule are deterministic and hermetic. They should not write
   timestamps to their output, and they should use stable ordering for sets and maps, as well as
   write only relative file paths to the output, no absolute paths. Not following this rule will
   lead to unexpected build behavior (Bazel not rebuilding a genrule you thought it would) and
   degrade cache performance.
- Do use `$(location)` extensively, for outputs, tools and sources. Due to the
   segregation of output files for different configurations, genrules cannot rely on hard-coded
   and/or absolute paths.
- Do write a common Starlark macro in case the same or very similar genrules are used in
   multiple places. If the genrule is complex, consider implementing it in a script or as a
   Starlark rule. This improves readability as well as testability.
- Do make sure that the exit code correctly indicates success or failure of the genrule.
- Do not write informational messages to stdout or stderr. While useful for debugging, this can
   easily become noise; a successful genrule should be silent. On the other hand, a failing genrule
   should emit good error messages.
- `$$` evaluates to a `$`, a literal dollar-sign, so in order to invoke a
   shell command containing dollar-signs such as `ls $(dirname $x)`, one must escape it
   thus: `ls $$(dirname $$x)`.
- Avoid creating symlinks and directories. Bazel doesn't copy over the directory/symlink
   structure created by genrules and its dependency checking of directories is unsound.
- When referencing the genrule in other rules, you can use either the genrule's label or the
   labels of individual output files. Sometimes the one approach is more readable, sometimes the
   other: referencing outputs by name in a consuming rule's `srcs` will avoid
   unintentionally picking up other outputs of the genrule, but can be tedious if the genrule
   produces many outputs.

#### Examples

This example generates `foo.h`. There are no sources, because the command doesn't take
any input. The "binary" run by the command is a perl script in the same package as the genrule.

```
genrule(
    name = "foo",
    srcs = [],
    outs = ["foo.h"],
    cmd = "./$(location create_foo.pl) > \"$@\"",
    tools = ["create_foo.pl"],
)

```

The following example shows how to use a [`filegroup`](/reference/be/general.html#filegroup) and the outputs of another `genrule`. Note that using `$(SRCS)` instead
of explicit `$(location)` directives would also work; this example uses the latter for
sake of demonstration.

```
genrule(
    name = "concat_all_files",
    srcs = [
        "//some:files",  # a filegroup with multiple files in it ==> $(locations)
        "//other:gen",   # a genrule with a single output ==> $(location)
    ],
    outs = ["concatenated.txt"],
    cmd = "cat $(locations //some:files) $(location //other:gen) > $@",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

You may refer to this rule by name in the
 `srcs` or `deps` section of other `BUILD`
 rules. If the rule generates source files, you should use the
 `srcs` attribute.

 `srcs`

List of [labels](/concepts/labels); default is `[]`

 A list of inputs for this rule, such as source files to process.


_This attributes is not suitable to list tools executed by the `cmd`; use_
_the [`tools`](/reference/be/general.html#genrule.tools) attribute for them instead._

The build system ensures these prerequisites are built before running the genrule
command; they are built using the same configuration as the original build request. The
names of the files of these prerequisites are available to the command as a
space-separated list in `$(SRCS)`; alternatively the path of an individual
`srcs` target `//x:y` can be obtained using `$(location
          //x:y)`, or using `$<` provided it's the only entry in
`srcs`.


`outs`

List of [filenames](/concepts/build-ref#filename); [nonconfigurable](common-definitions.html#configurable-attributes); required

 A list of files generated by this rule.


Output files must not cross package boundaries.
Output filenames are interpreted as relative to the package.


If the `executable` flag is set, `outs` must contain exactly one
label.


The genrule command is expected to create each output file at a predetermined location.
The location is available in `cmd` using [genrule-specific "Make"\
variables](/reference/be/make-variables#predefined_genrule_variables) ( `$@`, `$(OUTS)`, `$(@D)` or `
          $(RULEDIR)`) or using [`$(location)`](/reference/be/make-variables#predefined_label_variables) substitution.


`cmd`

String; default is `""`

 The command to run.
 Subject to [`$(location)\
        `](/reference/be/make-variables#predefined_label_variables) and ["Make" variable](/reference/be/make-variables) substitution.


1. First [`$(location)\
               `](/reference/be/make-variables#predefined_label_variables) substitution is applied, replacing all occurrences of `$(location
               label)` and of `$(locations label)` (and similar
    constructions using related variables `execpath`, `execpaths`,
    `rootpath` and `rootpaths`).

2. Next, ["Make" variables](/reference/be/make-variables) are expanded. Note that
    predefined variables `$(JAVA)`, `$(JAVAC)` and
    `$(JAVABASE)` expand under the _exec_ configuration, so Java invocations
    that run as part of a build step can correctly load shared libraries and other
    dependencies.

3. Finally, the resulting command is executed using the Bash shell. If its exit code is
    non-zero the command is considered to have failed.


 This is the fallback of `cmd_bash`, `cmd_ps` and `cmd_bat`,
 if none of them are applicable.


If the command line length exceeds the platform limit (64K on Linux/macOS, 8K on Windows),
then genrule will write the command to a script and execute that script to work around. This
applies to all cmd attributes ( `cmd`, `cmd_bash`, `cmd_ps`,
`cmd_bat`).


`cmd_bash`

String; default is `""`

 The Bash command to run.


This attribute has higher priority than `cmd`. The command is expanded and
runs in the exact same way as the `cmd` attribute.


`cmd_bat`

String; default is `""`

 The Batch command to run on Windows.


This attribute has higher priority than `cmd` and `cmd_bash`.
The command runs in the similar way as the `cmd` attribute, with the
following differences:


- This attribute only applies on Windows.

- The command runs with `cmd.exe /c` with the following default arguments:

  - `/S` \- strip first and last quotes and execute everything else as is.

  - `/E:ON` \- enable extended command set.

  - `/V:ON` \- enable delayed variable expansion

  - `/D` \- ignore AutoRun registry entries.
- After [$(location)](/reference/be/make-variables#predefined_label_variables) and
   ["Make" variable](/reference/be/make-variables) substitution, the paths will be
   expanded to Windows style paths (with backslash).


`cmd_ps`

String; default is `""`

 The Powershell command to run on Windows.


This attribute has higher priority than `cmd`, `cmd_bash` and
`cmd_bat`. The command runs in the similar way as the `cmd`
attribute, with the following differences:


- This attribute only applies on Windows.

- The command runs with `powershell.exe /c`.


To make Powershell easier to use and less error-prone, we run the following
commands to set up the environment before executing Powershell command in genrule.


- `Set-ExecutionPolicy -Scope CurrentUser RemoteSigned` \- allow running
   unsigned scripts.

- `$errorActionPreference='Stop'` \- In case there are multiple commands
   separated by `;`, the action exits immediately if a Powershell CmdLet fails,
   but this does **NOT** work for external command.

- `$PSDefaultParameterValues['*:Encoding'] = 'utf8'` \- change the default
   encoding from utf-16 to utf-8.


`executable`

Boolean; [nonconfigurable](common-definitions.html#configurable-attributes); default is `False`

 Declare output to be executable.


Setting this flag to True means the output is an executable file and can be run using the
`run` command. The genrule must produce exactly one output in this case.
If this attribute is set, `run` will try executing the file regardless of
its content.


Declaring data dependencies for the generated executable is not supported.

`local`

Boolean; default is `False`

If set to True, this option forces this `genrule` to run using the "local"
strategy, which means no remote execution, no sandboxing, no persistent workers.


This is equivalent to providing 'local' as a tag ( `tags=["local"]`).


`message`

String; default is `""`

 A progress message.


A progress message that will be printed as this build step is executed. By default, the
message is "Generating _output_" (or something equally bland) but you may provide a
more specific one. Use this attribute instead of `echo` or other print
statements in your `cmd` command, as this allows the build tool to control
whether such progress messages are printed or not.


`output_licenses`

List of strings; default is `[]`

 See [`common attributes\
        `](/reference/be/common-definitions#binary.output_licenses)`output_to_bindir`

Boolean; [nonconfigurable](common-definitions.html#configurable-attributes); default is `False`

If set to True, this option causes output files to be written into the `bin`
directory instead of the `genfiles` directory.


`toolchains`

List of [labels](/concepts/labels); [nonconfigurable](common-definitions.html#configurable-attributes); default is `[]`

The set of targets whose [Make variables](/reference/be/make-variables) this genrule
is allowed to access, or the [`toolchain_type`](/docs/toolchains)
targets that this genrule will access.


Toolchains accessed via `toolchain_type` must also provide a
`TemplateVariableInfo` provider, which the target can use to access toolchain
details.


`tools`

List of [labels](/concepts/labels); default is `[]`

 A list of _tool_ dependencies for this rule. See the definition of
 [dependencies](/concepts/build-ref#deps) for more information.

The build system ensures these prerequisites are built before running the genrule command;
they are built using the [_exec_\
configuration](/contribute/guide#configurations), since these tools are executed as part of the build. The path of an
individual `tools` target `//x:y` can be obtained using
`$(location //x:y)`.


Any `*_binary` or tool to be executed by `cmd` must appear in this
list, not in `srcs`, to ensure they are built in the correct configuration.


## starlark\_doc\_extract

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/starlarkdocextract/StarlarkDocExtractRule.java)

```
starlark_doc_extract(name, deps, src, data, allow_unused_doc_comments, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, licenses, package_metadata, render_main_repo_name, restricted_to, symbol_names, tags, target_compatible_with, testonly, visibility)
```

`starlark_doc_extract()` extracts documentation for rules, functions (including
macros), aspects, and providers defined or re-exported in a given `.bzl` or
`.scl` file. The output of this rule is a `ModuleInfo` binary proto as defined
in
[stardoc\_output.proto](https://github.com/bazelbuild/bazel/blob/master/src/main/protobuf/stardoc_output.proto)
in the Bazel source tree.

#### Implicit output targets

- `name.binaryproto` (the default output): A
   `ModuleInfo` binary proto.
- `name.textproto` (only built if explicitly requested): the text
   proto version of `name.binaryproto`.

Note: the exact output of this rule is not a stable public API. For example, the set of
natively-defined common rule attributes and their docstrings may change even with minor Bazel
releases. For this reason, documentation generated for user-defined rules is not stable across Bazel
releases, so we suggest taking care that any "golden tests" based on outputs of this rule are only
run with a single Bazel version.



### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 A list of targets wrapping the Starlark files which are `load()`-ed by
 `src`. These targets _should_ under normal usage be
 [`bzl_library`](https://github.com/bazelbuild/bazel-skylib/blob/main/bzl_library.bzl)
 targets, but the `starlark_doc_extract` rule does not enforce that, and accepts
 any target which provides Starlark files in its `DefaultInfo`.



Note that the wrapped Starlark files must be files in the source tree; Bazel cannot
`load()` generated files.



`src`

[Label](/concepts/labels); required

 A Starlark file from which to extract documentation.



Note that this must be a file in the source tree; Bazel cannot `load()`
generated files.



`allow_unused_doc_comments`

Boolean; default is `False`

 If true, allow and silently ignore doc comments (comments starting with `#:`)
 which are not attached to any global variable, or which are attached to a variable whose
 value's documentation should be provided in a different way (for example, in a docstring for
 a function, or via `rule(doc = ...)` for a rule).

 `render_main_repo_name`

Boolean; default is `False`

 If true, render labels in the main repository in emitted documentation with a repo component
 (in other words, `//foo:bar.bzl` will be emitted as
 `@main_repo_name//foo:bar.bzl`).


The name to use for the main repository is obtained from `module(name = ...)`
in the main repository's `MODULE.bazel` file (if Bzlmod is enabled), or from
`workspace(name = ...)` in the main repository's `WORKSPACE` file.


This attribute should be set to `False` when generating documentation for
Starlark files which are intended to be used only within the same repository, and to
`True` when generating documentation for Starlark files which are intended to be
used from other repositories.



`symbol_names`

List of strings; default is `[]`

 An optional list of qualified names of exported functions, rules, providers, or aspects (or
 structs in which they are nested) for which to extract documentation. Here, a _qualified_
_name_ means the name under which an entity is made available to a user of the module,
 including any structs in which the entity is nested for namespacing.



`starlark_doc_extract` emits documentation for an entity if and only if


1. each component of the entity's qualified name is public (in other words, the first
    character of each component of the qualified name is alphabetic, not `"_"`);
    _and_
   1. _either_ the `symbol_names` list is empty (which is the default
          case), _or_
   2. the entity's qualified name, or the qualified name of a struct in which the entity
       is nested, is in the `symbol_names` list.

## test\_suite

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/test/TestSuiteRule.java)

```
test_suite(name, aspect_hints, compatible_with, deprecation, features, licenses, package_metadata, restricted_to, tags, target_compatible_with, testonly, tests, visibility)
```

A `test_suite` defines a set of tests that are considered "useful" to humans. This
allows projects to define sets of tests, such as "tests you must run before checkin", "our
project's stress tests" or "all small tests." The `bazel test` command respects this sort
of organization: For an invocation like `bazel test //some/test:suite`, Bazel first
enumerates all test targets transitively included by the `//some/test:suite` target (we
call this "test\_suite expansion"), then Bazel builds and tests those targets.

#### Examples

A test suite to run all of the small tests in the current package.

```
test_suite(
    name = "small_tests",
    tags = ["small"],
)

```

A test suite that runs a specified set of tests:

```
test_suite(
    name = "smoke_tests",
    tests = [
        "system_unittest",
        "public_api_unittest",
    ],
)

```

A test suite to run all tests in the current package which are not flaky.

```
test_suite(
    name = "non_flaky_test",
    tags = ["-flaky"],
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`tags`

List of strings; [nonconfigurable](common-definitions.html#configurable-attributes); default is `[]`

 List of text tags such as "small" or "database" or "-flaky". Tags may be any valid string.


Tags which begin with a "-" character are considered negative tags. The
preceding "-" character is not considered part of the tag, so a suite tag
of "-small" matches a test's "small" size. All other tags are considered
positive tags.


Optionally, to make positive tags more explicit, tags may also begin with the
"+" character, which will not be evaluated as part of the text of the tag. It
merely makes the positive and negative distinction easier to read.


Only test rules that match **all** of the positive tags and **none** of the negative
tags will be included in the test suite. Note that this does not mean that error checking
for dependencies on tests that are filtered out is skipped; the dependencies on skipped
tests still need to be legal (e.g. not blocked by visibility constraints).


The `manual` tag keyword is treated differently than the above by the
"test\_suite expansion" performed by the `bazel test` command on invocations
involving wildcard
[target patterns](https://bazel.build/docs/build#specifying-build-targets).
There, `test_suite` targets tagged "manual" are filtered out (and thus not
expanded). This behavior is consistent with how `bazel build` and
`bazel test` handle wildcard target patterns in general. Note that this is
explicitly different from how `bazel query 'tests(E)'` behaves, as suites are
always expanded by the `tests` query function, regardless of the
`manual` tag.


Note that a test's `size` is considered a tag for the purpose of filtering.


If you need a `test_suite` that contains tests with mutually exclusive tags
(e.g. all small and medium tests), you'll have to create three `test_suite`
rules: one for all small tests, one for all medium tests, and one that includes the
previous two.


`tests`

List of [labels](/concepts/labels); [nonconfigurable](common-definitions.html#configurable-attributes); default is `[]`

 A list of test suites and test targets of any language.


Any `*_test` is accepted here, independent of the language. No
`*_binary` targets are accepted however, even if they happen to run a test.
Filtering by the specified `tags` is only done for tests listed directly in
this attribute. If this attribute contains `test_suite` s, the tests inside
those will not be filtered by this `test_suite` (they are considered to be
filtered already).


If the `tests` attribute is unspecified or empty, the rule will default to
including all test rules in the current BUILD file that are not tagged as
`manual`. These rules are still subject to `tag` filtering.

---

## Java Rules
- URL: https://bazel.build/reference/be/java
- Source: reference/be/java.mdx
- Slug: /reference/be/java

## Rules

- [java\_binary](#java_binary)
- [java\_import](#java_import)
- [java\_library](#java_library)
- [java\_test](#java_test)
- [java\_package\_configuration](#java_package_configuration)
- [java\_plugin](#java_plugin)
- [java\_runtime](#java_runtime)
- [java\_single\_jar](#java_single_jar)
- [java\_toolchain](#java_toolchain)

## java\_binary

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/bazel/rules/bazel_java_binary.bzl)

```
java_binary(name, deps, srcs, data, resources, add_exports, add_opens, args, aspect_hints, bootclasspath, classpath_resources, compatible_with, create_executable, deploy_env, deploy_manifest_lines, deprecation, env, exec_compatible_with, exec_group_compatible_with, exec_properties, features, javacopts, jvm_flags, launcher, licenses, main_class, neverlink, output_licenses, package_metadata, plugins, resource_strip_prefix, restricted_to, runtime_deps, stamp, tags, target_compatible_with, testonly, toolchains, use_launcher, use_testrunner, visibility)
```

Builds a Java archive ("jar file"), plus a wrapper shell script with the same name as the rule.
The wrapper shell script uses a classpath that includes, among other things, a jar file for each
library on which the binary depends. When running the wrapper shell script, any nonempty
`JAVABIN` environment variable will take precedence over the version specified via
Bazel's `--java_runtime_version` flag.

The wrapper script accepts several unique flags. Refer to
`java_stub_template.txt`
for a list of configurable flags and environment variables accepted by the wrapper.

#### Implicit output targets

- `name.jar`: A Java archive, containing the class files and other
   resources corresponding to the binary's direct dependencies.
- `name-src.jar`: An archive containing the sources ("source
   jar").
- `name_deploy.jar`: A Java archive suitable for deployment (only
   built if explicitly requested).


   Building the `<name>_deploy.jar` target for your rule
   creates a self-contained jar file with a manifest that allows it to be run with the
   `java -jar` command or with the wrapper script's `--singlejar`
   option. Using the wrapper script is preferred to `java -jar` because it
   also passes the [JVM flags](#java_binary-jvm_flags) and the options
   to load native libraries.



   The deploy jar contains all the classes that would be found by a classloader that
   searched the classpath from the binary's wrapper script from beginning to end. It also
   contains the native libraries needed for dependencies. These are automatically loaded
   into the JVM at runtime.


  If your target specifies a [launcher](#java_binary.launcher)
   attribute, then instead of being a normal JAR file, the \_deploy.jar will be a
   native binary. This will contain the launcher plus any native (C++) dependencies of
   your rule, all linked into a static binary. The actual jar file's bytes will be
   appended to that native binary, creating a single binary blob containing both the
   executable and the Java code. You can execute the resulting jar file directly
   like you would execute any native binary.

- `name_deploy-src.jar`: An archive containing the sources
   collected from the transitive closure of the target. These will match the classes in the
   `deploy.jar` except where jars have no matching source jar.

It is good practice to use the name of the source file that is the main entry point of the
application (minus the extension). For example, if your entry point is called
`Main.java`, then your name could be `Main`.

A `deps` attribute is not allowed in a `java_binary` rule without
[`srcs`](#java_binary-srcs); such a rule requires a
[`main_class`](#java_binary-main_class) provided by
[`runtime_deps`](#java_binary-runtime_deps).

The following code snippet illustrates a common mistake:

```lang-starlark

java_binary(
    name = "DontDoThis",
    srcs = [
        ...,
        "GeneratedJavaFile.java",  # a generated .java file
    ],
    deps = [":generating_rule",],  # rule that generates that file
)

```

Do this instead:

```lang-starlark

java_binary(
    name = "DoThisInstead",
    srcs = [
        ...,
        ":generating_rule",
    ],
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries to be linked in to the target.
See general comments about `deps` at
[Typical attributes defined by\
most build rules](common-definitions.html#typical-attributes).
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of source files that are processed to create the target.
This attribute is almost always required; see exceptions below.

Source files of type `.java` are compiled. In case of generated
`.java` files it is generally advisable to put the generating rule's name
here instead of the name of the file itself. This not only improves readability but
makes the rule more resilient to future changes: if the generating rule generates
different files in the future, you only need to fix one place: the `outs` of
the generating rule. You should not list the generating rule in `deps`
because it is a no-op.

Source files of type `.srcjar` are unpacked and compiled. (This is useful if
you need to generate a set of `.java` files with a genrule.)

Rules: if the rule (typically `genrule` or `filegroup`) generates
any of the files listed above, they will be used the same way as described for source
files.

This argument is almost always required, except if a
[`main_class`](#java_binary.main_class) attribute specifies a
class on the runtime classpath or you specify the `runtime_deps` argument.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this library at runtime.
See general comments about `data`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).
 `resources`

List of [labels](/concepts/labels); default is `[]`

 A list of data files to include in a Java jar.

Resources may be source files or generated files.

If resources are specified, they will be bundled in the jar along with the usual
`.class` files produced by compilation. The location of the resources inside
of the jar file is determined by the project structure. Bazel first looks for Maven's
[standard directory layout](https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html),
(a "src" directory followed by a "resources" directory grandchild). If that is not
found, Bazel then looks for the topmost directory named "java" or "javatests" (so, for
example, if a resource is at `<workspace root>/x/java/y/java/z`, the
path of the resource will be `y/java/z`. This heuristic cannot be overridden,
however, the `resource_strip_prefix` attribute can be used to specify a
specific alternative directory for resource files.


`add_exports`

List of strings; default is `[]`

 Allow this library to access the given `module` or `package`.

This corresponds to the javac and JVM --add-exports= flags.


`add_opens`

List of strings; default is `[]`

 Allow this library to reflectively access the given `module` or
`package`.

This corresponds to the javac and JVM --add-opens= flags.


`bootclasspath`

[Label](/concepts/labels); default is `None`

 Restricted API, do not use!
 `classpath_resources`

List of [labels](/concepts/labels); default is `[]`

 _DO NOT USE THIS OPTION UNLESS THERE IS NO OTHER WAY)_

A list of resources that must be located at the root of the java tree. This attribute's
only purpose is to support third-party libraries that require that their resources be
found on the classpath as exactly `"myconfig.xml"`. It is only allowed on
binaries and not libraries, due to the danger of namespace conflicts.

`create_executable`

Boolean; default is `True`

 Deprecated, use `java_single_jar` instead.
 `deploy_env`

List of [labels](/concepts/labels); default is `[]`

 A list of other `java_binary` targets which represent the deployment
environment for this binary.
Set this attribute when building a plugin which will be loaded by another
`java_binary`.

 Setting this attribute excludes all dependencies from
the runtime classpath (and the deploy jar) of this binary that are shared between this
binary and the targets specified in `deploy_env`.
 `deploy_manifest_lines`

List of strings; default is `[]`

 A list of lines to add to the `META-INF/manifest.mf` file generated for the
`*_deploy.jar` target. The contents of this attribute are _not_ subject
to ["Make variable"](make-variables.html) substitution.
 `javacopts`

List of strings; default is `[]`

 Extra compiler options for this binary.
Subject to ["Make variable"](make-variables.html) substitution and
[Bourne shell tokenization](common-definitions.html#sh-tokenization).

These compiler options are passed to javac after the global compiler options.

`jvm_flags`

List of strings; default is `[]`

 A list of flags to embed in the wrapper script generated for running this binary.
Subject to [$(location)](/reference/be/make-variables#location) and
["Make variable"](make-variables.html) substitution, and
[Bourne shell tokenization](common-definitions.html#sh-tokenization).

The wrapper script for a Java binary includes a CLASSPATH definition
(to find all the dependent jars) and invokes the right Java interpreter.
The command line generated by the wrapper script includes the name of
the main class followed by a `"$@"` so you can pass along other
arguments after the classname. However, arguments intended for parsing
by the JVM must be specified _before_ the classname on the command
line. The contents of `jvm_flags` are added to the wrapper
script before the classname is listed.

Note that this attribute has _no effect_ on `*_deploy.jar`
outputs.

`launcher`

[Label](/concepts/labels); default is `None`

 Specify a binary that will be used to run your Java program instead of the
normal `bin/java` program included with the JDK.
The target must be a `cc_binary`. Any `cc_binary` that
implements the
[Java Invocation API](http://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/invocation.html) can be specified as a value for this attribute.

By default, Bazel will use the normal JDK launcher (bin/java or java.exe).

The related [`\
--java_launcher`](/docs/user-manual#flag--java_launcher) Bazel flag affects only those
`java_binary` and `java_test` targets that have
_not_ specified a `launcher` attribute.

Note that your native (C++, SWIG, JNI) dependencies will be built differently
depending on whether you are using the JDK launcher or another launcher:

- If you are using the normal JDK launcher (the default), native dependencies are
  built as a shared library named `{name}_nativedeps.so`, where
  `{name}` is the `name` attribute of this java\_binary rule.
  Unused code is _not_ removed by the linker in this configuration.
- If you are using any other launcher, native (C++) dependencies are statically
  linked into a binary named `{name}_nativedeps`, where `{name}`
  is the `name` attribute of this java\_binary rule. In this case,
  the linker will remove any code it thinks is unused from the resulting binary,
  which means any C++ code accessed only via JNI may not be linked in unless
  that `cc_library` target specifies `alwayslink = True`.

When using any launcher other than the default JDK launcher, the format
of the `*_deploy.jar` output changes. See the main
[java\_binary](#java_binary) docs for details.

`main_class`

String; default is `""`

 Name of class with `main()` method to use as entry point.
If a rule uses this option, it does not need a `srcs=[...]` list.
Thus, with this attribute one can make an executable from a Java library that already
contains one or more `main()` methods.

The value of this attribute is a class name, not a source file. The class must be
available at runtime: it may be compiled by this rule (from `srcs`) or
provided by direct or transitive dependencies (through `runtime_deps` or
`deps`). If the class is unavailable, the binary will fail at runtime; there
is no build-time check.

`neverlink`

Boolean; default is `False`

`plugins`

List of [labels](/concepts/labels); default is `[]`

 Java compiler plugins to run at compile-time.
Every `java_plugin` specified in this attribute will be run whenever this rule
is built. A library may also inherit plugins from dependencies that use
`exported_plugins`. Resources
generated by the plugin will be included in the resulting jar of this rule.
 `resource_strip_prefix`

String; default is `""`

 The path prefix to strip from Java resources.

If specified, this path prefix is stripped from every file in the `resources`
attribute. It is an error for a resource file not to be under this directory. If not
specified (the default), the path of resource file is determined according to the same
logic as the Java package of source files. For example, a source file at
`stuff/java/foo/bar/a.txt` will be located at `foo/bar/a.txt`.

`runtime_deps`

List of [labels](/concepts/labels); default is `[]`

 Libraries to make available to the final binary or test at runtime only.
Like ordinary `deps`, these will appear on the runtime classpath, but unlike
them, not on the compile-time classpath. Dependencies needed only at runtime should be
listed here. Dependency-analysis tools should ignore targets that appear in both
`runtime_deps` and `deps`.
 `stamp`

Integer; default is `-1`

 Whether to encode build information into the binary. Possible values:

- `stamp = 1`: Always stamp the build information into the binary, even in
   [`--nostamp`](/docs/user-manual#flag--stamp) builds. **This**
  **setting should be avoided**, since it potentially kills remote caching for the
   binary and any downstream actions that depend on it.

- `stamp = 0`: Always replace build information by constant values. This
   gives good build result caching.

- `stamp = -1`: Embedding of build information is controlled by the
   [`--[no]stamp`](/docs/user-manual#flag--stamp) flag.


Stamped binaries are _not_ rebuilt unless their dependencies change.

`use_launcher`

Boolean; default is `True`

 Whether the binary should use a custom launcher.

If this attribute is set to false, the
[launcher](/reference/be/java.html#java_binary.launcher) attribute and the related
[`--java_launcher`](/docs/user-manual#flag--java_launcher) flag
will be ignored for this target.


`use_testrunner`

Boolean; default is `False`

 Use the test runner (by default
`com.google.testing.junit.runner.BazelTestRunner`) class as the
main entry point for a Java program, and provide the test class
to the test runner as a value of `bazel.test_suite`
system property.

You can use this to override the default
behavior, which is to use test runner for
`java_test` rules,
and not use it for `java_binary` rules. It is unlikely
you will want to do this. One use is for `AllTest`
rules that are invoked by another rule (to set up a database
before running the tests, for example). The `AllTest`
rule must be declared as a `java_binary`, but should
still use the test runner as its main entry point.

The name of a test runner class can be overridden with `main_class` attribute.


## java\_import

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/bazel/rules/bazel_java_import.bzl)

```
java_import(name, deps, data, add_exports, add_opens, aspect_hints, compatible_with, constraints, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, exports, features, jars, licenses, neverlink, package_metadata, proguard_specs, restricted_to, runtime_deps, srcjar, tags, target_compatible_with, testonly, toolchains, visibility)
```

This rule allows the use of precompiled `.jar` files as
libraries for `java_library` and
`java_binary` rules.

#### Examples

```lang-starlark

    java_import(
        name = "maven_model",
        jars = [
            "maven_model/maven-aether-provider-3.2.3.jar",
            "maven_model/maven-model-3.2.3.jar",
            "maven_model/maven-model-builder-3.2.3.jar",
        ],
    )

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries to be linked in to the target.
See [java\_library.deps](/reference/be/java.html#java_library.deps).
 `data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this rule at runtime.
 `add_exports`

List of strings; default is `[]`

 Allow this library to access the given `module` or `package`.

This corresponds to the javac and JVM --add-exports= flags.


`add_opens`

List of strings; default is `[]`

 Allow this library to reflectively access the given `module` or
`package`.

This corresponds to the javac and JVM --add-opens= flags.


`constraints`

List of strings; default is `[]`

 Extra constraints imposed on this rule as a Java library.
 `exports`

List of [labels](/concepts/labels); default is `[]`

 Targets to make available to users of this rule.
See [java\_library.exports](/reference/be/java.html#java_library.exports).
 `jars`

List of [labels](/concepts/labels); required

 The list of JAR files provided to Java targets that depend on this target.
 `neverlink`

Boolean; default is `False`

 Only use this library for compilation and not at runtime.
Useful if the library will be provided by the runtime environment
during execution. Examples of libraries like this are IDE APIs
for IDE plug-ins or `tools.jar` for anything running on
a standard JDK.
 `proguard_specs`

List of [labels](/concepts/labels); default is `[]`

 Files to be used as Proguard specification.
These will describe the set of specifications to be used by Proguard. If specified,
they will be added to any `android_binary` target depending on this library.

The files included here must only have idempotent rules, namely -dontnote, -dontwarn,
assumenosideeffects, and rules that start with -keep. Other options can only appear in
`android_binary`'s proguard\_specs, to ensure non-tautological merges.
 `runtime_deps`

List of [labels](/concepts/labels); default is `[]`

 Libraries to make available to the final binary or test at runtime only.
See [java\_library.runtime\_deps](/reference/be/java.html#java_library.runtime_deps).
 `srcjar`

[Label](/concepts/labels); default is `None`

 A JAR file that contains source code for the compiled JAR files.


## java\_library

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/bazel/rules/bazel_java_library.bzl)

```
java_library(name, deps, srcs, data, resources, add_exports, add_opens, aspect_hints, bootclasspath, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, exported_plugins, exports, features, javabuilder_jvm_flags, javacopts, licenses, neverlink, package_metadata, plugins, proguard_specs, resource_strip_prefix, restricted_to, runtime_deps, tags, target_compatible_with, testonly, toolchains, visibility)
```

This rule compiles and links sources into a `.jar` file.

#### Implicit outputs

- `libname.jar`: A Java archive containing the class files.
- `libname-src.jar`: An archive containing the sources ("source
   jar").

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of libraries to link into this library.
See general comments about `deps` at
[Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

The jars built by `java_library` rules listed in `deps` will be on
the compile-time classpath of this rule. Furthermore the transitive closure of their
`deps`, `runtime_deps` and `exports` will be on the
runtime classpath.

By contrast, targets in the `data` attribute are included in the runfiles but
on neither the compile-time nor runtime classpath.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of source files that are processed to create the target.
This attribute is almost always required; see exceptions below.

Source files of type `.java` are compiled. In case of generated
`.java` files it is generally advisable to put the generating rule's name
here instead of the name of the file itself. This not only improves readability but
makes the rule more resilient to future changes: if the generating rule generates
different files in the future, you only need to fix one place: the `outs` of
the generating rule. You should not list the generating rule in `deps`
because it is a no-op.

Source files of type `.srcjar` are unpacked and compiled. (This is useful if
you need to generate a set of `.java` files with a genrule.)

Rules: if the rule (typically `genrule` or `filegroup`) generates
any of the files listed above, they will be used the same way as described for source
files.

Source files of type `.properties` are treated as resources.

All other files are ignored, as long as there is at least one file of a
file type described above. Otherwise an error is raised.

This argument is almost always required, except if you specify the `runtime_deps` argument.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this library at runtime.
See general comments about `data` at
[Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

When building a `java_library`, Bazel doesn't put these files anywhere; if the
`data` files are generated files then Bazel generates them. When building a
test that depends on this `java_library` Bazel copies or links the
`data` files into the runfiles area.

`resources`

List of [labels](/concepts/labels); default is `[]`

 A list of data files to include in a Java jar.

Resources may be source files or generated files.

If resources are specified, they will be bundled in the jar along with the usual
`.class` files produced by compilation. The location of the resources inside
of the jar file is determined by the project structure. Bazel first looks for Maven's
[standard directory layout](https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html),
(a "src" directory followed by a "resources" directory grandchild). If that is not
found, Bazel then looks for the topmost directory named "java" or "javatests" (so, for
example, if a resource is at `<workspace root>/x/java/y/java/z`, the
path of the resource will be `y/java/z`. This heuristic cannot be overridden,
however, the `resource_strip_prefix` attribute can be used to specify a
specific alternative directory for resource files.


`add_exports`

List of strings; default is `[]`

 Allow this library to access the given `module` or `package`.

This corresponds to the javac and JVM --add-exports= flags.


`add_opens`

List of strings; default is `[]`

 Allow this library to reflectively access the given `module` or
`package`.

This corresponds to the javac and JVM --add-opens= flags.


`bootclasspath`

[Label](/concepts/labels); default is `None`

 Restricted API, do not use!
 `exported_plugins`

List of [labels](/concepts/labels); default is `[]`

 The list of `java_plugin` s (e.g. annotation
processors) to export to libraries that directly depend on this library.

The specified list of `java_plugin` s will be applied to any library which
directly depends on this library, just as if that library had explicitly declared these
labels in `plugins`.

`exports`

List of [labels](/concepts/labels); default is `[]`

 Exported libraries.

Listing rules here will make them available to parent rules, as if the parents explicitly
depended on these rules. This is not true for regular (non-exported) `deps`.

Summary: a rule _X_ can access the code in _Y_ if there exists a dependency
path between them that begins with a `deps` edge followed by zero or more
`exports` edges. Let's see some examples to illustrate this.

Assume _A_ depends on _B_ and _B_ depends on _C_. In this case
C is a _transitive_ dependency of A, so changing C's sources and rebuilding A will
correctly rebuild everything. However A will not be able to use classes in C. To allow
that, either A has to declare C in its `deps`, or B can make it easier for A
(and anything that may depend on A) by declaring C in its (B's) `exports`
attribute.

The closure of exported libraries is available to all direct parent rules. Take a slightly
different example: A depends on B, B depends on C and D, and also exports C but not D.
Now A has access to C but not to D. Now, if C and D exported some libraries, C' and D'
respectively, A could only access C' but not D'.

Important: an exported rule is not a regular dependency. Sticking to the previous example,
if B exports C and wants to also use C, it has to also list it in its own
`deps`.

`javabuilder_jvm_flags`

List of strings; default is `[]`

 Restricted API, do not use!
 `javacopts`

List of strings; default is `[]`

 Extra compiler options for this library.
Subject to ["Make variable"](make-variables.html) substitution and
[Bourne shell tokenization](common-definitions.html#sh-tokenization).

These compiler options are passed to javac after the global compiler options.

`neverlink`

Boolean; default is `False`

 Whether this library should only be used for compilation and not at runtime.
Useful if the library will be provided by the runtime environment during execution. Examples
of such libraries are the IDE APIs for IDE plug-ins or `tools.jar` for anything
running on a standard JDK.

Note that `neverlink = True` does not prevent the compiler from inlining material
from this library into compilation targets that depend on it, as permitted by the Java
Language Specification (e.g., `static final` constants of `String`
or of primitive types). The preferred use case is therefore when the runtime library is
identical to the compilation library.

If the runtime library differs from the compilation library then you must ensure that it
differs only in places that the JLS forbids compilers to inline (and that must hold for
all future versions of the JLS).

`plugins`

List of [labels](/concepts/labels); default is `[]`

 Java compiler plugins to run at compile-time.
Every `java_plugin` specified in this attribute will be run whenever this rule
is built. A library may also inherit plugins from dependencies that use
`exported_plugins`. Resources
generated by the plugin will be included in the resulting jar of this rule.
 `proguard_specs`

List of [labels](/concepts/labels); default is `[]`

 Files to be used as Proguard specification.
These will describe the set of specifications to be used by Proguard. If specified,
they will be added to any `android_binary` target depending on this library.

The files included here must only have idempotent rules, namely -dontnote, -dontwarn,
assumenosideeffects, and rules that start with -keep. Other options can only appear in
`android_binary`'s proguard\_specs, to ensure non-tautological merges.
 `resource_strip_prefix`

String; default is `""`

 The path prefix to strip from Java resources.

If specified, this path prefix is stripped from every file in the `resources`
attribute. It is an error for a resource file not to be under this directory. If not
specified (the default), the path of resource file is determined according to the same
logic as the Java package of source files. For example, a source file at
`stuff/java/foo/bar/a.txt` will be located at `foo/bar/a.txt`.

`runtime_deps`

List of [labels](/concepts/labels); default is `[]`

 Libraries to make available to the final binary or test at runtime only.
Like ordinary `deps`, these will appear on the runtime classpath, but unlike
them, not on the compile-time classpath. Dependencies needed only at runtime should be
listed here. Dependency-analysis tools should ignore targets that appear in both
`runtime_deps` and `deps`.


## java\_test

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/bazel/rules/bazel_java_test.bzl)

```
java_test(name, deps, srcs, data, resources, add_exports, add_opens, args, aspect_hints, bootclasspath, classpath_resources, compatible_with, create_executable, deploy_manifest_lines, deprecation, env, env_inherit, exec_compatible_with, exec_group_compatible_with, exec_properties, features, flaky, javacopts, jvm_flags, launcher, licenses, local, main_class, neverlink, package_metadata, plugins, resource_strip_prefix, restricted_to, runtime_deps, shard_count, size, stamp, tags, target_compatible_with, test_class, testonly, timeout, toolchains, use_launcher, use_testrunner, visibility)
```

A `java_test()` rule compiles a Java test. A test is a binary wrapper around your
test code. The test runner's main method is invoked instead of the main class being compiled.

#### Implicit output targets

- `name.jar`: A Java archive.
- `name_deploy.jar`: A Java archive suitable
   for deployment. (Only built if explicitly requested.) See the description of the
   `name_deploy.jar` output from
   [java\_binary](#java_binary) for more details.

See the section on `java_binary()` arguments. This rule also
supports all [attributes common\
to all test rules (\*\_test)](https://bazel.build/reference/be/common-definitions#common-attributes-tests).

#### Examples

```lang-starlark

java_library(
    name = "tests",
    srcs = glob(["*.java"]),
    deps = [
        "//java/com/foo/base:testResources",
        "//java/com/foo/testing/util",
    ],
)

java_test(
    name = "AllTests",
    size = "small",
    runtime_deps = [
        ":tests",
        "//util/mysql",
    ],
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries to be linked in to the target.
See general comments about `deps` at
[Typical attributes defined by\
most build rules](common-definitions.html#typical-attributes).
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of source files that are processed to create the target.
This attribute is almost always required; see exceptions below.

Source files of type `.java` are compiled. In case of generated
`.java` files it is generally advisable to put the generating rule's name
here instead of the name of the file itself. This not only improves readability but
makes the rule more resilient to future changes: if the generating rule generates
different files in the future, you only need to fix one place: the `outs` of
the generating rule. You should not list the generating rule in `deps`
because it is a no-op.

Source files of type `.srcjar` are unpacked and compiled. (This is useful if
you need to generate a set of `.java` files with a genrule.)

Rules: if the rule (typically `genrule` or `filegroup`) generates
any of the files listed above, they will be used the same way as described for source
files.

This argument is almost always required, except if a
[`main_class`](#java_binary.main_class) attribute specifies a
class on the runtime classpath or you specify the `runtime_deps` argument.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this library at runtime.
See general comments about `data`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).
 `resources`

List of [labels](/concepts/labels); default is `[]`

 A list of data files to include in a Java jar.

Resources may be source files or generated files.

If resources are specified, they will be bundled in the jar along with the usual
`.class` files produced by compilation. The location of the resources inside
of the jar file is determined by the project structure. Bazel first looks for Maven's
[standard directory layout](https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html),
(a "src" directory followed by a "resources" directory grandchild). If that is not
found, Bazel then looks for the topmost directory named "java" or "javatests" (so, for
example, if a resource is at `<workspace root>/x/java/y/java/z`, the
path of the resource will be `y/java/z`. This heuristic cannot be overridden,
however, the `resource_strip_prefix` attribute can be used to specify a
specific alternative directory for resource files.


`add_exports`

List of strings; default is `[]`

 Allow this library to access the given `module` or `package`.

This corresponds to the javac and JVM --add-exports= flags.


`add_opens`

List of strings; default is `[]`

 Allow this library to reflectively access the given `module` or
`package`.

This corresponds to the javac and JVM --add-opens= flags.


`bootclasspath`

[Label](/concepts/labels); default is `None`

 Restricted API, do not use!
 `classpath_resources`

List of [labels](/concepts/labels); default is `[]`

 _DO NOT USE THIS OPTION UNLESS THERE IS NO OTHER WAY)_

A list of resources that must be located at the root of the java tree. This attribute's
only purpose is to support third-party libraries that require that their resources be
found on the classpath as exactly `"myconfig.xml"`. It is only allowed on
binaries and not libraries, due to the danger of namespace conflicts.

`create_executable`

Boolean; default is `True`

 Deprecated, use `java_single_jar` instead.
 `deploy_manifest_lines`

List of strings; default is `[]`

 A list of lines to add to the `META-INF/manifest.mf` file generated for the
`*_deploy.jar` target. The contents of this attribute are _not_ subject
to ["Make variable"](make-variables.html) substitution.
 `javacopts`

List of strings; default is `[]`

 Extra compiler options for this binary.
Subject to ["Make variable"](make-variables.html) substitution and
[Bourne shell tokenization](common-definitions.html#sh-tokenization).

These compiler options are passed to javac after the global compiler options.

`jvm_flags`

List of strings; default is `[]`

 A list of flags to embed in the wrapper script generated for running this binary.
Subject to [$(location)](/reference/be/make-variables#location) and
["Make variable"](make-variables.html) substitution, and
[Bourne shell tokenization](common-definitions.html#sh-tokenization).

The wrapper script for a Java binary includes a CLASSPATH definition
(to find all the dependent jars) and invokes the right Java interpreter.
The command line generated by the wrapper script includes the name of
the main class followed by a `"$@"` so you can pass along other
arguments after the classname. However, arguments intended for parsing
by the JVM must be specified _before_ the classname on the command
line. The contents of `jvm_flags` are added to the wrapper
script before the classname is listed.

Note that this attribute has _no effect_ on `*_deploy.jar`
outputs.

`launcher`

[Label](/concepts/labels); default is `None`

 Specify a binary that will be used to run your Java program instead of the
normal `bin/java` program included with the JDK.
The target must be a `cc_binary`. Any `cc_binary` that
implements the
[Java Invocation API](http://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/invocation.html) can be specified as a value for this attribute.

By default, Bazel will use the normal JDK launcher (bin/java or java.exe).

The related [`\
--java_launcher`](/docs/user-manual#flag--java_launcher) Bazel flag affects only those
`java_binary` and `java_test` targets that have
_not_ specified a `launcher` attribute.

Note that your native (C++, SWIG, JNI) dependencies will be built differently
depending on whether you are using the JDK launcher or another launcher:

- If you are using the normal JDK launcher (the default), native dependencies are
  built as a shared library named `{name}_nativedeps.so`, where
  `{name}` is the `name` attribute of this java\_binary rule.
  Unused code is _not_ removed by the linker in this configuration.
- If you are using any other launcher, native (C++) dependencies are statically
  linked into a binary named `{name}_nativedeps`, where `{name}`
  is the `name` attribute of this java\_binary rule. In this case,
  the linker will remove any code it thinks is unused from the resulting binary,
  which means any C++ code accessed only via JNI may not be linked in unless
  that `cc_library` target specifies `alwayslink = True`.

When using any launcher other than the default JDK launcher, the format
of the `*_deploy.jar` output changes. See the main
[java\_binary](#java_binary) docs for details.

`main_class`

String; default is `""`

 Name of class with `main()` method to use as entry point.
If a rule uses this option, it does not need a `srcs=[...]` list.
Thus, with this attribute one can make an executable from a Java library that already
contains one or more `main()` methods.

The value of this attribute is a class name, not a source file. The class must be
available at runtime: it may be compiled by this rule (from `srcs`) or
provided by direct or transitive dependencies (through `runtime_deps` or
`deps`). If the class is unavailable, the binary will fail at runtime; there
is no build-time check.

`neverlink`

Boolean; default is `False`

`plugins`

List of [labels](/concepts/labels); default is `[]`

 Java compiler plugins to run at compile-time.
Every `java_plugin` specified in this attribute will be run whenever this rule
is built. A library may also inherit plugins from dependencies that use
`exported_plugins`. Resources
generated by the plugin will be included in the resulting jar of this rule.
 `resource_strip_prefix`

String; default is `""`

 The path prefix to strip from Java resources.

If specified, this path prefix is stripped from every file in the `resources`
attribute. It is an error for a resource file not to be under this directory. If not
specified (the default), the path of resource file is determined according to the same
logic as the Java package of source files. For example, a source file at
`stuff/java/foo/bar/a.txt` will be located at `foo/bar/a.txt`.

`runtime_deps`

List of [labels](/concepts/labels); default is `[]`

 Libraries to make available to the final binary or test at runtime only.
Like ordinary `deps`, these will appear on the runtime classpath, but unlike
them, not on the compile-time classpath. Dependencies needed only at runtime should be
listed here. Dependency-analysis tools should ignore targets that appear in both
`runtime_deps` and `deps`.
 `stamp`

Integer; default is `0`

 Whether to encode build information into the binary. Possible values:

- `stamp = 1`: Always stamp the build information into the binary, even in
   [`--nostamp`](https://bazel.build/docs/user-manual#stamp) builds. **This**
  **setting should be avoided**, since it potentially kills remote caching for the
   binary and any downstream actions that depend on it.

- `stamp = 0`: Always replace build information by constant values. This
   gives good build result caching.

- `stamp = -1`: Embedding of build information is controlled by the
   [`--[no]stamp`](https://bazel.build/docs/user-manual#stamp) flag.


Stamped binaries are _not_ rebuilt unless their dependencies change.

`test_class`

String; default is `""`

 The Java class to be loaded by the test runner.

By default, if this argument is not defined then the legacy mode is used and the
test arguments are used instead. Set the `--nolegacy_bazel_java_test` flag
to not fallback on the first argument.

This attribute specifies the name of a Java class to be run by
this test. It is rare to need to set this. If this argument is omitted,
it will be inferred using the target's `name` and its
source-root-relative path. If the test is located outside a known
source root, Bazel will report an error if `test_class`
is unset.

For JUnit3, the test class needs to either be a subclass of
`junit.framework.TestCase` or it needs to have a public
static `suite()` method that returns a
`junit.framework.Test` (or a subclass of `Test`).

This attribute allows several `java_test` rules to
share the same `Test`
( `TestCase`, `TestSuite`, ...). Typically
additional information is passed to it
(e.g. via `jvm_flags=['-Dkey=value']`) so that its
behavior differs in each case, such as running a different
subset of the tests. This attribute also enables the use of
Java tests outside the `javatests` tree.

`use_launcher`

Boolean; default is `True`

 Whether the binary should use a custom launcher.

If this attribute is set to false, the
[launcher](/reference/be/java.html#java_binary.launcher) attribute and the related
[`--java_launcher`](/docs/user-manual#flag--java_launcher) flag
will be ignored for this target.


`use_testrunner`

Boolean; default is `True`

 Use the test runner (by default
`com.google.testing.junit.runner.BazelTestRunner`) class as the
main entry point for a Java program, and provide the test class
to the test runner as a value of `bazel.test_suite`
system property.

You can use this to override the default
behavior, which is to use test runner for
`java_test` rules,
and not use it for `java_binary` rules. It is unlikely
you will want to do this. One use is for `AllTest`
rules that are invoked by another rule (to set up a database
before running the tests, for example). The `AllTest`
rule must be declared as a `java_binary`, but should
still use the test runner as its main entry point.

The name of a test runner class can be overridden with `main_class` attribute.


## java\_package\_configuration

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/common/rules/java_package_configuration.bzl)

```
java_package_configuration(name, data, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, javacopts, output_licenses, package_metadata, packages, restricted_to, system, tags, target_compatible_with, testonly, toolchains, visibility)
```

Configuration to apply to a set of packages.
Configurations can be added to
`java_toolchain.javacopts` s.

#### Example:

```lang-starlark

java_package_configuration(
    name = "my_configuration",
    packages = [":my_packages"],
    javacopts = ["-Werror"],
)

package_group(
    name = "my_packages",
    packages = [
        "//com/my/project/...",
        "-//com/my/project/testing/...",
    ],
)

java_toolchain(
    ...,
    package_configuration = [
        ":my_configuration",
    ]
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this configuration at runtime.
 `javacopts`

List of strings; default is `[]`

 Java compiler flags.
 `output_licenses`

List of strings; default is `[]`

`packages`

List of [labels](/concepts/labels); default is `[]`

 The set of `package_group` s
the configuration should be applied to.
 `system`

[Label](/concepts/labels); default is `None`

 Corresponds to javac's --system flag.


## java\_plugin

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/bazel/rules/bazel_java_plugin.bzl)

```
java_plugin(name, deps, srcs, data, resources, add_exports, add_opens, aspect_hints, bootclasspath, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, generates_api, javabuilder_jvm_flags, javacopts, licenses, neverlink, output_licenses, package_metadata, plugins, processor_class, proguard_specs, resource_strip_prefix, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

`java_plugin` defines plugins for the Java compiler run by Bazel. The
only supported kind of plugins are annotation processors. A `java_library` or
`java_binary` rule can run plugins by depending on them via the `plugins`
attribute. A `java_library` can also automatically export plugins to libraries that
directly depend on it using
`exported_plugins`.

#### Implicit output targets

- `libname.jar`: A Java archive.

Arguments are a subset of (and with identical semantics to) those of
[java\_library()](/reference/be/java.html#java_library),
except for the addition of the `processor_class` and
`generates_api` arguments.

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of libraries to link into this library.
See general comments about `deps` at
[Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

The jars built by `java_library` rules listed in `deps` will be on
the compile-time classpath of this rule. Furthermore the transitive closure of their
`deps`, `runtime_deps` and `exports` will be on the
runtime classpath.

By contrast, targets in the `data` attribute are included in the runfiles but
on neither the compile-time nor runtime classpath.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of source files that are processed to create the target.
This attribute is almost always required; see exceptions below.

Source files of type `.java` are compiled. In case of generated
`.java` files it is generally advisable to put the generating rule's name
here instead of the name of the file itself. This not only improves readability but
makes the rule more resilient to future changes: if the generating rule generates
different files in the future, you only need to fix one place: the `outs` of
the generating rule. You should not list the generating rule in `deps`
because it is a no-op.

Source files of type `.srcjar` are unpacked and compiled. (This is useful if
you need to generate a set of `.java` files with a genrule.)

Rules: if the rule (typically `genrule` or `filegroup`) generates
any of the files listed above, they will be used the same way as described for source
files.

Source files of type `.properties` are treated as resources.

All other files are ignored, as long as there is at least one file of a
file type described above. Otherwise an error is raised.

This argument is almost always required, except if you specify the `runtime_deps` argument.

`data`

List of [labels](/concepts/labels); default is `[]`

 The list of files needed by this library at runtime.
See general comments about `data` at
[Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical-attributes).

When building a `java_library`, Bazel doesn't put these files anywhere; if the
`data` files are generated files then Bazel generates them. When building a
test that depends on this `java_library` Bazel copies or links the
`data` files into the runfiles area.

`resources`

List of [labels](/concepts/labels); default is `[]`

 A list of data files to include in a Java jar.

Resources may be source files or generated files.

If resources are specified, they will be bundled in the jar along with the usual
`.class` files produced by compilation. The location of the resources inside
of the jar file is determined by the project structure. Bazel first looks for Maven's
[standard directory layout](https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html),
(a "src" directory followed by a "resources" directory grandchild). If that is not
found, Bazel then looks for the topmost directory named "java" or "javatests" (so, for
example, if a resource is at `<workspace root>/x/java/y/java/z`, the
path of the resource will be `y/java/z`. This heuristic cannot be overridden,
however, the `resource_strip_prefix` attribute can be used to specify a
specific alternative directory for resource files.


`add_exports`

List of strings; default is `[]`

 Allow this library to access the given `module` or `package`.

This corresponds to the javac and JVM --add-exports= flags.


`add_opens`

List of strings; default is `[]`

 Allow this library to reflectively access the given `module` or
`package`.

This corresponds to the javac and JVM --add-opens= flags.


`bootclasspath`

[Label](/concepts/labels); default is `None`

 Restricted API, do not use!
 `generates_api`

Boolean; default is `False`

 This attribute marks annotation processors that generate API code.

If a rule uses an API-generating annotation processor, other rules
depending on it can refer to the generated code only if their
compilation actions are scheduled after the generating rule. This
attribute instructs Bazel to introduce scheduling constraints when
--java\_header\_compilation is enabled.

_WARNING: This attribute affects build_
_performance, use it only if necessary._

`javabuilder_jvm_flags`

List of strings; default is `[]`

 Restricted API, do not use!
 `javacopts`

List of strings; default is `[]`

 Extra compiler options for this library.
Subject to ["Make variable"](make-variables.html) substitution and
[Bourne shell tokenization](common-definitions.html#sh-tokenization).

These compiler options are passed to javac after the global compiler options.

`neverlink`

Boolean; default is `False`

 Whether this library should only be used for compilation and not at runtime.
Useful if the library will be provided by the runtime environment during execution. Examples
of such libraries are the IDE APIs for IDE plug-ins or `tools.jar` for anything
running on a standard JDK.

Note that `neverlink = True` does not prevent the compiler from inlining material
from this library into compilation targets that depend on it, as permitted by the Java
Language Specification (e.g., `static final` constants of `String`
or of primitive types). The preferred use case is therefore when the runtime library is
identical to the compilation library.

If the runtime library differs from the compilation library then you must ensure that it
differs only in places that the JLS forbids compilers to inline (and that must hold for
all future versions of the JLS).

`output_licenses`

List of strings; default is `[]`

`plugins`

List of [labels](/concepts/labels); default is `[]`

 Java compiler plugins to run at compile-time.
Every `java_plugin` specified in this attribute will be run whenever this rule
is built. A library may also inherit plugins from dependencies that use
`exported_plugins`. Resources
generated by the plugin will be included in the resulting jar of this rule.
 `processor_class`

String; default is `""`

 The processor class is the fully qualified type of the class that the Java compiler should
use as entry point to the annotation processor. If not specified, this rule will not
contribute an annotation processor to the Java compiler's annotation processing, but its
runtime classpath will still be included on the compiler's annotation processor path. (This
is primarily intended for use by
[Error Prone plugins](https://errorprone.info/docs/plugins), which are loaded
from the annotation processor path using
[java.util.ServiceLoader](https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html).)
 `proguard_specs`

List of [labels](/concepts/labels); default is `[]`

 Files to be used as Proguard specification.
These will describe the set of specifications to be used by Proguard. If specified,
they will be added to any `android_binary` target depending on this library.

The files included here must only have idempotent rules, namely -dontnote, -dontwarn,
assumenosideeffects, and rules that start with -keep. Other options can only appear in
`android_binary`'s proguard\_specs, to ensure non-tautological merges.
 `resource_strip_prefix`

String; default is `""`

 The path prefix to strip from Java resources.

If specified, this path prefix is stripped from every file in the `resources`
attribute. It is an error for a resource file not to be under this directory. If not
specified (the default), the path of resource file is determined according to the same
logic as the Java package of source files. For example, a source file at
`stuff/java/foo/bar/a.txt` will be located at `foo/bar/a.txt`.

## java\_runtime

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/common/rules/java_runtime.bzl)

```
java_runtime(name, srcs, aspect_hints, compatible_with, default_cds, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, hermetic_srcs, hermetic_static_libs, java, java_home, lib_ct_sym, lib_modules, output_licenses, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, version, visibility)
```

Specifies the configuration for a Java runtime.

#### Example:

```lang-starlark

java_runtime(
    name = "jdk-9-ea+153",
    srcs = glob(["jdk9-ea+153/**"]),
    java_home = "jdk9-ea+153",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 All files in the runtime.
 `default_cds`

[Label](/concepts/labels); default is `None`

 Default CDS archive for hermetic `java_runtime`. When hermetic
is enabled for a `java_binary` target the `java_runtime`
default CDS is packaged in the hermetic deploy JAR.
 `hermetic_srcs`

List of [labels](/concepts/labels); default is `[]`

 Files in the runtime needed for hermetic deployments.
 `hermetic_static_libs`

List of [labels](/concepts/labels); default is `[]`

 The libraries that are statically linked with the launcher for hermetic deployments
 `java`

[Label](/concepts/labels); default is `None`

 The path to the java executable.
 `java_home`

String; default is `""`

 The path to the root of the runtime.
Subject to ["Make" variable](/reference/be/make-variables) substitution.
If this path is absolute, the rule denotes a non-hermetic Java runtime with a well-known
path. In that case, the `srcs` and `java` attributes must be empty.
 `lib_ct_sym`

[Label](/concepts/labels); default is `None`

 The lib/ct.sym file needed for compilation with `--release`. If not specified and
there is exactly one file in `srcs` whose path ends with
`/lib/ct.sym`, that file is used.
 `lib_modules`

[Label](/concepts/labels); default is `None`

 The lib/modules file needed for hermetic deployments.
 `output_licenses`

List of strings; default is `[]`

`version`

Integer; default is `0`

 The feature version of the Java runtime. I.e., the integer returned by
`Runtime.version().feature()`.


## java\_single\_jar

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/java_single_jar.bzl)

```
java_single_jar(name, deps, aspect_hints, compatible_with, compress, deploy_env, deploy_manifest_lines, deprecation, exclude_build_data, exec_compatible_with, exec_group_compatible_with, exec_properties, features, multi_release, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

 Collects Java dependencies and jar files into a single jar

\`java\_single\_jar\` collects Java dependencies and jar files into a single jar.
This is similar to java\_binary with everything related to executables disabled,
and provides an alternative to the java\_binary "deploy jar hack".

\## Example

\`\`\`skylark
load("//tools/build\_defs/java\_single\_jar:java\_single\_jar.bzl", "java\_single\_jar")

java\_single\_jar(
 name = "my\_single\_jar",
 deps = \[
 "//java/com/google/foo",
 "//java/com/google/bar",
 \],
)
\`\`\`

Outputs:
 {name}.jar: A single jar containing all of the inputs.



### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The Java targets (including java\_import and java\_library) to collect
transitive dependencies from. Runtime dependencies are collected via
deps, exports, and runtime\_deps. Resources are also collected.
Native cc\_library or java\_wrap\_cc dependencies are not.
 `compress`

String; default is `"preserve"`

 Whether to always deflate ("yes"), always store ("no"), or pass
through unmodified ("preserve"). The default is "preserve", and is the
most efficient option -- no extra work is done to inflate or deflate.
 `deploy_env`

List of [labels](/concepts/labels); default is `[]`

 A list of \`java\_binary\` or \`java\_single\_jar\` targets which represent
the deployment environment for this binary.

Set this attribute when building a plugin which will be loaded by another
\`java\_binary\`.

\`deploy\_env\` dependencies are excluded from the jar built by this rule.
 `deploy_manifest_lines`

List of strings; default is `[]`

 A list of lines to add to the `META-INF/manifest.mf` file.
 `exclude_build_data`

Boolean; default is `True`

 Whether to omit the build-data.properties file generated
by default.
 `multi_release`

Boolean; default is `True`

 Whether to enable Multi-Release output jars.


## java\_toolchain

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_java/blob/master/java/common/rules/java_toolchain.bzl)

```
java_toolchain(name, android_lint_data, android_lint_jvm_opts, android_lint_opts, android_lint_package_configuration, android_lint_runner, aspect_hints, bootclasspath, compatible_javacopts, compatible_with, deprecation, deps_checker, exec_compatible_with, exec_group_compatible_with, exec_properties, features, forcibly_disable_header_compilation, genclass, header_compiler, header_compiler_builtin_processors, header_compiler_direct, ijar, jacocorunner, java_runtime, javabuilder, javabuilder_data, javabuilder_jvm_opts, javac_supports_multiplex_workers, javac_supports_worker_cancellation, javac_supports_worker_multiplex_sandboxing, javac_supports_workers, javacopts, jspecify_implicit_deps, jspecify_javacopts, jspecify_packages, jspecify_processor, jspecify_processor_class, jspecify_stubs, jvm_opts, licenses, misc, oneversion, oneversion_allowlist, oneversion_allowlist_for_tests, oneversion_whitelist, package_configuration, package_metadata, proguard_allowlister, reduced_classpath_incompatible_processors, restricted_to, singlejar, source_version, tags, target_compatible_with, target_version, testonly, timezone_data, toolchains, tools, turbine_data, turbine_jvm_opts, visibility, xlint)
```

Specifies the configuration for the Java compiler. Which toolchain to be used can be changed through
the --java\_toolchain argument. Normally you should not write those kind of rules unless you want to
tune your Java compiler.

#### Examples

A simple example would be:

```lang-starlark

java_toolchain(
    name = "toolchain",
    source_version = "7",
    target_version = "7",
    bootclasspath = ["//tools/jdk:bootclasspath"],
    xlint = [ "classfile", "divzero", "empty", "options", "path" ],
    javacopts = [ "-g" ],
    javabuilder = ":JavaBuilder_deploy.jar",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`android_lint_data`

List of [labels](/concepts/labels); default is `[]`

 Labels of tools available for label-expansion in android\_lint\_jvm\_opts.
 `android_lint_jvm_opts`

List of strings; default is `[]`

 The list of arguments for the JVM when invoking Android Lint.
 `android_lint_opts`

List of strings; default is `[]`

 The list of Android Lint arguments.
 `android_lint_package_configuration`

List of [labels](/concepts/labels); default is `[]`

 Android Lint Configuration that should be applied to the specified package groups.
 `android_lint_runner`

[Label](/concepts/labels); default is `None`

 Label of the Android Lint runner, if any.
 `bootclasspath`

List of [labels](/concepts/labels); default is `[]`

 The Java target bootclasspath entries. Corresponds to javac's -bootclasspath flag.
 `compatible_javacopts`

null; default is `{}`

 Internal API, do not use!
 `deps_checker`

[Label](/concepts/labels); default is `None`

 Label of the ImportDepsChecker deploy jar.
 `forcibly_disable_header_compilation`

Boolean; default is `False`

 Overrides --java\_header\_compilation to disable header compilation on platforms that do not
support it, e.g. JDK 7 Bazel.
 `genclass`

[Label](/concepts/labels); default is `None`

 Label of the GenClass deploy jar.
 `header_compiler`

[Label](/concepts/labels); default is `None`

 Label of the header compiler. Required if --java\_header\_compilation is enabled.
 `header_compiler_builtin_processors`

List of strings; default is `[]`

 Internal API, do not use!
 `header_compiler_direct`

[Label](/concepts/labels); default is `None`

 Optional label of the header compiler to use for direct classpath actions that do not
include any API-generating annotation processors.

This tool does not support annotation processing.


`ijar`

[Label](/concepts/labels); default is `None`

 Label of the ijar executable.
 `jacocorunner`

[Label](/concepts/labels); default is `None`

 Label of the JacocoCoverageRunner deploy jar.
 `java_runtime`

[Label](/concepts/labels); default is `None`

 The java\_runtime to use with this toolchain. It defaults to java\_runtime
in execution configuration.
 `javabuilder`

[Label](/concepts/labels); default is `None`

 Label of the JavaBuilder deploy jar.
 `javabuilder_data`

List of [labels](/concepts/labels); default is `[]`

 Labels of data available for label-expansion in javabuilder\_jvm\_opts.
 `javabuilder_jvm_opts`

List of strings; default is `[]`

 The list of arguments for the JVM when invoking JavaBuilder.
 `javac_supports_multiplex_workers`

Boolean; default is `True`

 True if JavaBuilder supports running as a multiplex persistent worker, false if it doesn't.
 `javac_supports_worker_cancellation`

Boolean; default is `True`

 True if JavaBuilder supports cancellation of persistent workers, false if it doesn't.
 `javac_supports_worker_multiplex_sandboxing`

Boolean; default is `False`

 True if JavaBuilder supports running as a multiplex persistent worker with sandboxing, false if it doesn't.
 `javac_supports_workers`

Boolean; default is `True`

 True if JavaBuilder supports running as a persistent worker, false if it doesn't.
 `javacopts`

List of strings; default is `[]`

 The list of extra arguments for the Java compiler. Please refer to the Java compiler
documentation for the extensive list of possible Java compiler flags.
 `jspecify_implicit_deps`

[Label](/concepts/labels); default is `None`

 Experimental, do not use!
 `jspecify_javacopts`

List of strings; default is `[]`

 Experimental, do not use!
 `jspecify_packages`

List of [labels](/concepts/labels); default is `[]`

 Experimental, do not use!
 `jspecify_processor`

[Label](/concepts/labels); default is `None`

 Experimental, do not use!
 `jspecify_processor_class`

String; default is `""`

 Experimental, do not use!
 `jspecify_stubs`

List of [labels](/concepts/labels); default is `[]`

 Experimental, do not use!
 `jvm_opts`

List of strings; default is `[]`

 The list of arguments for the JVM when invoking the Java compiler. Please refer to the Java
virtual machine documentation for the extensive list of possible flags for this option.
 `misc`

List of strings; default is `[]`

 Deprecated: use javacopts instead
 `oneversion`

[Label](/concepts/labels); default is `None`

 Label of the one-version enforcement binary.
 `oneversion_allowlist`

[Label](/concepts/labels); default is `None`

 Label of the one-version allowlist.
 `oneversion_allowlist_for_tests`

[Label](/concepts/labels); default is `None`

 Label of the one-version allowlist for tests.
 `oneversion_whitelist`

[Label](/concepts/labels); default is `None`

 Deprecated: use oneversion\_allowlist instead
 `package_configuration`

List of [labels](/concepts/labels); default is `[]`

 Configuration that should be applied to the specified package groups.
 `proguard_allowlister`

[Label](/concepts/labels); default is `"@bazel_tools//tools/jdk:proguard_whitelister"`

 Label of the Proguard allowlister.
 `reduced_classpath_incompatible_processors`

List of strings; default is `[]`

 Internal API, do not use!
 `singlejar`

[Label](/concepts/labels); default is `None`

 Label of the SingleJar deploy jar.
 `source_version`

String; default is `""`

 The Java source version (e.g., '6' or '7'). It specifies which set of code structures
are allowed in the Java source code.
 `target_version`

String; default is `""`

 The Java target version (e.g., '6' or '7'). It specifies for which Java runtime the class
should be build.
 `timezone_data`

[Label](/concepts/labels); default is `None`

 Label of a resource jar containing timezone data. If set, the timezone data is added as an
implicitly runtime dependency of all java\_binary rules.
 `tools`

List of [labels](/concepts/labels); default is `[]`

 Labels of tools available for label-expansion in jvm\_opts.
 `turbine_data`

List of [labels](/concepts/labels); default is `[]`

 Labels of data available for label-expansion in turbine\_jvm\_opts.
 `turbine_jvm_opts`

List of strings; default is `[]`

 The list of arguments for the JVM when invoking turbine.
 `xlint`

List of strings; default is `[]`

 The list of warning to add or removes from default list. Precedes it with a dash to
removes it. Please see the Javac documentation on the -Xlint options for more information.

---

## Make Variables
- URL: https://bazel.build/reference/be/make-variables
- Source: reference/be/make-variables.mdx
- Slug: /reference/be/make-variables

- [Use](#use)
- [Predefined variables](#predefined_variables)
- [Predefined genrule variables](#predefined_genrule_variables)
- [Predefined source/output path variables](#predefined_label_variables)
- [Custom variables](#custom_variables)

"Make" variables are a special class of expandable string variables available
to attributes marked as _"Subject to 'Make variable' substitution"_.

These can be used, for example, to inject specific toolchain paths into
user-constructed build actions.

Bazel provides both _predefined_ variables, which are available to all
targets, and _custom_ variables, which are defined in dependency targets
and only available to targets that depend on them.

The reason for the term "Make" is historical: the syntax and semantics of
these variables were originally intended to match [GNU\
Make](https://www.gnu.org/software/make/manual/html_node/Using-Variables.html).

## Use

Attributes marked as _"Subject to 'Make variable' substitution"_ can
reference the "Make" variable `FOO` as follows:

`my_attr = "prefix $(FOO) suffix"`

In other words, any substring matching `$(FOO)` gets expanded
to `FOO`'s value. If that value is `"bar"`, the final
string becomes:

`my_attr = "prefix bar suffix"`

If `FOO` doesn't correspond to a variable known to the consuming
target, Bazel fails with an error.

"Make" variables whose names are non-letter symbols, such as
`@`, can also be referenced using only a dollar sign, without
the parentheses. For example:

`my_attr = "prefix $@ suffix"`

To write `$` as a string literal (i.e. to prevent variable
expansion), write `$$`.

## Predefined variables

Predefined "Make" variables can be referenced by any attribute marked as
_"Subject to 'Make variable' substitution"_ on any target.

To see the list of these variables and their values for a given set of build
options, run

`bazel info --show_make_env [build options]`

and look at the top output lines with capital letters.

[See an example of predefined variables](https://github.com/bazelbuild/examples/tree/main/make-variables#predefined-variables).

**Toolchain option variables**

- `COMPILATION_MODE`:
   `fastbuild`, `dbg`, or `opt`. ( [more\
   details](https://bazel.build/docs/user-manual#flag--compilation_mode))


**Path variables**

- `BINDIR`: The base of the generated binary tree for the target
   architecture.



   Note that a different tree may be used for programs that run during the
   build on the host architecture, to support cross-compiling.



   If you want to run a tool from within a `genrule`, the
   recommended way to get its path is `$(execpath toolname)`,
   where _toolname_ must be listed in the `genrule`'s
   `tools` attribute.


- `GENDIR`:
   The base of the generated code tree for the target architecture.


**Machine architecture variables**

- `TARGET_CPU`:
   The target architecture's CPU, e.g. `k8`.

## Predefined genrule variables

The following are specially available to `genrule`'s
`cmd` attribute and are
generally important for making that attribute work.

[See an example of predefined genrule variables](https://github.com/bazelbuild/examples/tree/main/make-variables#predefined-genrule-variables).

- `OUTS`: The `genrule`'s `outs` list. If you have
   only one output file, you can also use `$@`.
- `SRCS`: The `genrule`'s `srcs` list (or more
   precisely: the path names of the files corresponding to labels in the
   `srcs` list).
   If you have only one source file, you can also use `$<`.

- `<`: `SRCS`, if it is a single file. Else triggers
   a build error.

- `@`: `OUTS`, if it is a single file. Else triggers a
   build error.

- `RULEDIR`: The output directory of the target, that is, the
   directory corresponding to the name of the package containing the target
   under the `genfiles` or `bin` tree. For
   `//my/pkg:my_genrule` this always ends in `my/pkg`,
   even if `//my/pkg:my_genrule`'s outputs are in subdirectories.


- `@D`: The output directory. If
   [outs](/reference/be/general.html#genrule.outs) has one entry,
   this expands to the directory containing that file. If it has multiple
   entries, this expands to the package's root directory in the
   `genfiles` tree, _even if all output files are in the same_
  _subdirectory_!


  **Note:** Use `RULEDIR` over `@D` because
   `RULEDIR` has simpler semantics and behaves the same way
   regardless of the number of output files.



   If the genrule needs to generate temporary intermediate files (perhaps as
   a result of using some other tool like a compiler), it should attempt to
   write them to `@D` (although `/tmp` will also
   be writable) and remove them before finishing.



   Especially avoid writing to directories containing inputs. They may be on
   read-only filesystems. Even if not, doing so would trash the source tree.



**Note:** If the filenames corresponding to the input labels or the output
filenames contain spaces, `'`, or other special characters (or your
genrule is part of a Starlark macro which downstream users may invoke on such
files), then `$(SRCS)` and `$(OUTS)` are not suitable
for interpolation into a command line, as they do not have the semantics that
`"${@}"` would in Bash.

One workaround is to convert to a Bash array, with


```
mapfile SRCS <<< "$$(sed -e 's/ /\\n/g' <<'genrule_srcs_expansion'
$(SRC)
genrule_srcs_expansion
)
```

and then use `"$$\{SRCS[@]}"` in subsequent command lines in place
of `$(SRCS)`. A more robust option is to write a Starlark rule
instead.

## Predefined source/output path variables

The predefined variables `execpath`, `execpaths`,
`rootpath`, `rootpaths`, `location`, and
`locations` take label parameters (e.g. `$(execpath
//foo:bar)`) and substitute the file paths denoted by that label.

For source files, this is the path relative to your workspace root.
For files that are outputs of rules, this is the file's _output path_
(see the explanation of _output files_ below).

[See an example of predefined path variables](https://github.com/bazelbuild/examples/tree/main/make-variables#predefined-path-variables).

- `execpath`: Denotes the path beneath the

   [execroot](/docs/output_directories)
   where Bazel runs build actions.



   In the above example, Bazel runs all build actions in the directory linked
   by the `bazel-myproject` symlink in your workspace root. The
   source file `empty.source` is linked at the path
   `bazel-myproject/testapp/empty.source`. So its exec path (which
   is the subpath below the root) is `testapp/empty.source`. This
   is the path build actions can use to find the file.



   Output files are staged similarly, but are also prefixed with the subpath
   `bazel-out/cpu-compilation_mode/bin` (or for the outputs of
   tools: `bazel-out/cpu-opt-exec-hash/bin`). In the above example,
   `//testapp:app` is a tool because it appears in
   `show_app_output`'s `tools` attribute.
   So its output file `app` is written to
   `bazel-myproject/bazel-out/cpu-opt-exec-hash/bin/testapp/app`.
   The exec path is thus `
        bazel-out/cpu-opt-exec-hash/bin/testapp/app`. This extra prefix
   makes it possible to build the same target for, say, two different CPUs in
   the same build without the results clobbering each other.



   The label passed to this variable must represent exactly one file. For
   labels representing source files, this is automatically true. For labels
   representing rules, the rule must generate exactly one output. If this is
   false or the label is malformed, the build fails with an error.


- `rootpath`: Denotes the path that a built binary can use to
   find a dependency at runtime relative to the subdirectory of its runfiles
   directory corresponding to the main repository.
   **Note:** This only works if [`--enable_runfiles`](/reference/command-line-reference#flag--enable_runfiles) is enabled, which is not the case on
   Windows by default. Use `rlocationpath` instead for
   cross-platform support.



   This is similar to `execpath` but strips the configuration
   prefixes described above. In the example from above this means both
   `empty.source` and `app` use pure workspace-relative
   paths: `testapp/empty.source` and `testapp/app`.



   The `rootpath` of a file in an external repository
   `repo` will start with `../repo/`, followed by the
   repository-relative path.



   This has the same "one output only" requirements as `execpath`.


- `rlocationpath`: The path a built binary can pass to the `
        Rlocation` function of a runfiles library to find a dependency at
   runtime, either in the runfiles directory (if available) or using the
   runfiles manifest.



   This is similar to `rootpath` in that it does not contain
   configuration prefixes, but differs in that it always starts with the
   name of the repository. In the example from above this means that `
        empty.source` and `app` result in the following
   paths: `myproject/testapp/empty.source` and `
        myproject/testapp/app`.



   The `rlocationpath` of a file in an external repository
   `repo` will start with `repo/`, followed by the
   repository-relative path.



   Passing this path to a binary and resolving it to a file system path using
   the runfiles libraries is the preferred approach to find dependencies at
   runtime. Compared to `rootpath`, it has the advantage that it
   works on all platforms and even if the runfiles directory is not
   available.



   This has the same "one output only" requirements as `execpath`.


- `location`: A synonym for either `execpath` or
   `rootpath`, depending on the attribute being expanded. This is
   legacy pre-Starlark behavior and not recommended unless you really know what
   it does for a particular rule. See [#2475](https://github.com/bazelbuild/bazel/issues/2475#issuecomment-339318016)
   for details.


`execpaths`, `rootpaths`, `rlocationpaths`,
and `locations` are the plural variations of `execpath`,
`rootpath`, `rlocationpath`, and `location`,
respectively. They support labels producing multiple outputs, in which case
each output is listed separated by a space. Zero-output rules and malformed
labels produce build errors.

All referenced labels must appear in the consuming target's `srcs`,
output files, or `deps`. Otherwise the build fails. C++ targets can
also reference labels in `data`.

Labels don't have to be in canonical form: `foo`, `:foo`
and `//somepkg:foo` are all fine.

## Custom variables

Custom "Make" variables can be referenced by any attribute marked as
_"Subject to 'Make variable' substitution"_, but only on targets that
depend on other targets that _define_ these variables.

As best practice all variables should be custom unless there's a really good
reason to bake them into core Bazel. This saves Bazel from having to load
potentially expensive dependencies to supply variables consuming tarets may
not care about.

**C++ toolchain variables**

The following are defined in C++ toolchain rules and available to any rule
that sets `toolchains =
["@bazel_tools//tools/cpp:toolchain_type"]`
Some rules, like `java_binary`, implicitly
include the C++ toolchain in their rule definition. They inherit these variables
automatically.

The built-in C++ rules are much more sophisticated than "run the compiler on
it". In order to support compilation modes as diverse as \*SAN, ThinLTO,
with/without modules, and carefully optimized binaries at the same time as
fast running tests on multiple platforms, the built-in rules go to great
lengths to ensure the correct inputs, outputs, and command-line flags are set
on each of potentially multiple internally generated actions.

These variables are a fallback mechanism to be used by language experts in
rare cases. If you are tempted to use them, please [contact the Bazel devs](https://bazel.build/help) first.

- `ABI`: The C++ ABI version.
- `AR`: The "ar" command from crosstool.
- `C_COMPILER`:
   The C/C++ compiler identifier, e.g. `llvm`.

- `CC`: The C and C++ compiler command.


   We strongly recommended always using `CC_FLAGS` in
   combination with `CC`. Fail to do so at your own risk.


- `CC_FLAGS`: A minimal set of flags for the C/C++
   compiler to be usable by genrules. In particular, this contains flags to
   select the correct architecture if `CC` supports multiple
   architectures.

- `DUMPBIN`: Microsoft COFF Binary File Dumper (dumpbin.exe) from
   from Microsoft Visual Studio.
- `NM`: The "nm" command from crosstool.
- `OBJCOPY`: The objcopy command from the same suite as the C/C++
   compiler.
- `STRIP`: The strip command from the same suite as the C/C++
   compiler.

**Java toolchain variables**

The following are defined in Java toolchain rules and available to any rule
that sets `toolchains =
["@rules_java//toolchains:current_java_runtime"]` (or
`"@rules_java//toolchains:current_host_java_runtime"`
for the host toolchain equivalent).

Most of the tools in the JDK should not be used directly. The built-in Java
rules use much more sophisticated approaches to Java compilation and packaging
than upstream tools can express, such as interface Jars, header interface
Jars, and highly optimized Jar packaging and merging implementations.

These variables are a fallback mechanism to be used by language experts in
rare cases. If you are tempted to use them, please [contact the Bazel devs](https://bazel.build/help) first.

- `JAVA`: The "java" command (a Java virtual
   machine). Avoid this, and use a `java_binary` rule
   instead where possible. May be a relative path. If you must change
   directories before invoking `java`, you need to capture the
   working directory before changing it.

- `JAVABASE`: The base directory containing the
   Java utilities. May be a relative path. It will have a "bin"
   subdirectory.


**Starlark-defined variables**

Rule and [toolchain](/docs/toolchains) writers can define
completely custom variables by returning a
[TemplateVariableInfo](/rules/lib/TemplateVariableInfo)
provider. Any rules depending on these through the
`toolchains` attribute can then read their values:

[See an example of Starlark-defined variables](https://github.com/bazelbuild/examples/tree/main/make-variables#custom-starlark-defined-variables).

---

## Objective-C Rules
- URL: https://bazel.build/reference/be/objective-c
- Source: reference/be/objective-c.mdx
- Slug: /reference/be/objective-c

## Rules

- [objc\_import](#objc_import)
- [objc\_library](#objc_library)

## objc\_import

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/objc/objc_import.bzl)

```
objc_import(name, deps, hdrs, alwayslink, archives, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, includes, package_metadata, restricted_to, sdk_dylibs, sdk_frameworks, sdk_includes, tags, target_compatible_with, testonly, textual_hdrs, toolchains, visibility, weak_sdk_frameworks)
```

This rule encapsulates an already-compiled static library in the form of an
`.a` file. It also allows exporting headers and resources using the same
attributes supported by `objc_library`.

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of targets that this target depend on.
 `hdrs`

List of [labels](/concepts/labels); default is `[]`

 The list of C, C++, Objective-C, and Objective-C++ header files published
by this library to be included by sources in dependent rules.

These headers describe the public interface for the library and will be
made available for inclusion by sources in this rule or in dependent
rules. Headers not meant to be included by a client of this library
should be listed in the srcs attribute instead.

These will be compiled separately from the source if modules are enabled.


`alwayslink`

Boolean; default is `False`

 If 1, any bundle or binary that depends (directly or indirectly) on this
library will link in all the object files for the files listed in
`srcs` and `non_arc_srcs`, even if some contain no
symbols referenced by the binary.
This is useful if your code isn't explicitly called by code in
the binary, e.g., if your code registers to receive some callback
provided by some service.
 `archives`

List of [labels](/concepts/labels); required

 The list of `.a` files provided to Objective-C targets that
depend on this target.
 `includes`

List of strings; default is `[]`

 List of `#include/#import` search paths to add to this target
and all depending targets.

This is to support third party and open-sourced libraries that do not
specify the entire workspace path in their
`#import/#include` statements.

The paths are interpreted relative to the package directory, and the
genfiles and bin roots (e.g. `blaze-genfiles/pkg/includedir`
and `blaze-out/pkg/includedir`) are included in addition to the
actual client root.

Unlike [COPTS](/reference/be/objective-c.html#objc_library.copts), these flags are added for this rule
and every rule that depends on it. (Note: not the rules it depends upon!) Be
very careful, since this may have far-reaching effects. When in doubt, add
"-iquote" flags to [COPTS](/reference/be/objective-c.html#objc_library.copts) instead.


`sdk_dylibs`

List of strings; default is `[]`

 Names of SDK .dylib libraries to link with. For instance, "libz" or
"libarchive".

"libc++" is included automatically if the binary has any C++ or
Objective-C++ sources in its dependency tree. When linking a binary,
all libraries named in that binary's transitive dependency graph are
used.
 `sdk_frameworks`

List of strings; default is `[]`

 Names of SDK frameworks to link with (e.g. "AddressBook", "QuartzCore").

When linking a top level Apple binary, all SDK frameworks listed in that binary's
transitive dependency graph are linked.


`sdk_includes`

List of strings; default is `[]`

 List of `#include/#import` search paths to add to this target
and all depending targets, where each path is relative to
`$(SDKROOT)/usr/include`.
 `textual_hdrs`

List of [labels](/concepts/labels); default is `[]`

 The list of C, C++, Objective-C, and Objective-C++ files that are
included as headers by source files in this rule or by users of this
library. Unlike hdrs, these will not be compiled separately from the
sources.
 `weak_sdk_frameworks`

List of strings; default is `[]`

 Names of SDK frameworks to weakly link with. For instance,
"MediaAccessibility".

In difference to regularly linked SDK frameworks, symbols
from weakly linked frameworks do not cause an error if they
are not present at runtime.


## objc\_library

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/bazel/blob/master/src/main/starlark/builtins_bzl/common/objc/objc_library.bzl)

```
objc_library(name, deps, srcs, data, hdrs, alwayslink, aspect_hints, compatible_with, conlyopts, copts, cxxopts, defines, deprecation, enable_modules, exec_compatible_with, exec_group_compatible_with, exec_properties, features, implementation_deps, includes, linkopts, module_map, module_name, non_arc_srcs, package_metadata, pch, restricted_to, sdk_dylibs, sdk_frameworks, sdk_includes, stamp, tags, target_compatible_with, testonly, textual_hdrs, toolchains, visibility, weak_sdk_frameworks)
```

This rule produces a static library from the given Objective-C source files.

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of targets that this target depend on.
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of C, C++, Objective-C, and Objective-C++ source and header
files, and/or (\`.s\`, \`.S\`, or \`.asm\`) assembly source files, that are processed to create
the library target.
These are your checked-in files, plus any generated files.
Source files are compiled into .o files with Clang. Header files
may be included/imported by any source or header in the srcs attribute
of this target, but not by headers in hdrs or any targets that depend
on this rule.
Additionally, precompiled .o files may be given as srcs. Be careful to
ensure consistency in the architecture of provided .o files and that of the
build to avoid missing symbol linker errors.
 `hdrs`

List of [labels](/concepts/labels); default is `[]`

 The list of C, C++, Objective-C, and Objective-C++ header files published
by this library to be included by sources in dependent rules.

These headers describe the public interface for the library and will be
made available for inclusion by sources in this rule or in dependent
rules. Headers not meant to be included by a client of this library
should be listed in the srcs attribute instead.

These will be compiled separately from the source if modules are enabled.


`alwayslink`

Boolean; default is `False`

 If 1, any bundle or binary that depends (directly or indirectly) on this
library will link in all the object files for the files listed in
`srcs` and `non_arc_srcs`, even if some contain no
symbols referenced by the binary.
This is useful if your code isn't explicitly called by code in
the binary, e.g., if your code registers to receive some callback
provided by some service.
 `conlyopts`

List of strings; default is `[]`

 Extra flags to pass to the compiler for C files.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
These flags will only apply to this target, and not those upon which
it depends, or those which depend on it.

Note that for the generated Xcode project, directory paths specified using "-I" flags in
copts are parsed out, prepended with "$(WORKSPACE\_ROOT)/" if they are relative paths, and
added to the header search paths for the associated Xcode target.


`copts`

List of strings; default is `[]`

 Extra flags to pass to the compiler.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
These flags will only apply to this target, and not those upon which
it depends, or those which depend on it.

Note that for the generated Xcode project, directory paths specified using "-I" flags in
copts are parsed out, prepended with "$(WORKSPACE\_ROOT)/" if they are relative paths, and
added to the header search paths for the associated Xcode target.


`cxxopts`

List of strings; default is `[]`

 Extra flags to pass to the compiler for Objective-C++ and C++ files.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
These flags will only apply to this target, and not those upon which
it depends, or those which depend on it.

Note that for the generated Xcode project, directory paths specified using "-I" flags in
copts are parsed out, prepended with "$(WORKSPACE\_ROOT)/" if they are relative paths, and
added to the header search paths for the associated Xcode target.


`defines`

List of strings; default is `[]`

 Extra `-D` flags to pass to the compiler. They should be in
the form `KEY=VALUE` or simply `KEY` and are
passed not only to the compiler for this target (as `copts`
are) but also to all `objc_` dependers of this target.
Subject to ["Make variable"](/reference/be/make-variables) substitution and
[Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization).
 `enable_modules`

Boolean; default is `False`

 Enables clang module support (via -fmodules).
Setting this to 1 will allow you to @import system headers and other targets:
@import UIKit;
@import path\_to\_package\_target;
 `implementation_deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other libraries that the library target depends on. Unlike with
`deps`, the headers and include paths of these libraries (and all their
transitive deps) are only used for compilation of this library, and not libraries that
depend on it. Libraries specified with `implementation_deps` are still linked
in binary targets that depend on this library.
 `includes`

List of strings; default is `[]`

 List of `#include/#import` search paths to add to this target
and all depending targets.

This is to support third party and open-sourced libraries that do not
specify the entire workspace path in their
`#import/#include` statements.

The paths are interpreted relative to the package directory, and the
genfiles and bin roots (e.g. `blaze-genfiles/pkg/includedir`
and `blaze-out/pkg/includedir`) are included in addition to the
actual client root.

Unlike [COPTS](/reference/be/objective-c.html#objc_library.copts), these flags are added for this rule
and every rule that depends on it. (Note: not the rules it depends upon!) Be
very careful, since this may have far-reaching effects. When in doubt, add
"-iquote" flags to [COPTS](/reference/be/objective-c.html#objc_library.copts) instead.


`linkopts`

List of strings; default is `[]`

 Extra flags to pass to the linker.
 `module_map`

[Label](/concepts/labels); default is `None`

 custom Clang module map for this target. Use of a custom module map is discouraged. Most
users should use module maps generated by Bazel.
If specified, Bazel will not generate a module map for this target, but will pass the
provided module map to the compiler.
 `module_name`

String; default is `""`

 Sets the module name for this target. By default the module name is the target path with
all special symbols replaced by \_, e.g. //foo/baz:bar can be imported as foo\_baz\_bar.
 `non_arc_srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of Objective-C files that are processed to create the
library target that DO NOT use ARC.
The files in this attribute are treated very similar to those in the
srcs attribute, but are compiled without ARC enabled.
 `pch`

[Label](/concepts/labels); default is `None`

 Header file to prepend to every source file being compiled (both arc
and non-arc).
Use of pch files is actively discouraged in BUILD files, and this should be
considered deprecated. Since pch files are not actually precompiled this is not
a build-speed enhancement, and instead is just a global dependency. From a build
efficiency point of view you are actually better including what you need directly
in your sources where you need it.
 `sdk_dylibs`

List of strings; default is `[]`

 Names of SDK .dylib libraries to link with. For instance, "libz" or
"libarchive".

"libc++" is included automatically if the binary has any C++ or
Objective-C++ sources in its dependency tree. When linking a binary,
all libraries named in that binary's transitive dependency graph are
used.
 `sdk_frameworks`

List of strings; default is `[]`

 Names of SDK frameworks to link with (e.g. "AddressBook", "QuartzCore").

When linking a top level Apple binary, all SDK frameworks listed in that binary's
transitive dependency graph are linked.


`sdk_includes`

List of strings; default is `[]`

 List of `#include/#import` search paths to add to this target
and all depending targets, where each path is relative to
`$(SDKROOT)/usr/include`.
 `stamp`

Boolean; default is `False`

`textual_hdrs`

List of [labels](/concepts/labels); default is `[]`

 The list of C, C++, Objective-C, and Objective-C++ files that are
included as headers by source files in this rule or by users of this
library. Unlike hdrs, these will not be compiled separately from the
sources.
 `weak_sdk_frameworks`

List of strings; default is `[]`

 Names of SDK frameworks to weakly link with. For instance,
"MediaAccessibility".

In difference to regularly linked SDK frameworks, symbols
from weakly linked frameworks do not cause an error if they
are not present at runtime.

---

## Bazel BUILD Encyclopedia of Functions
- URL: https://bazel.build/reference/be/overview
- Source: reference/be/overview.mdx
- Slug: /reference/be/overview

## Concepts and terminology

- [Common definitions](/reference/be/common-definitions)  - [Bourne shell tokenization](/reference/be/common-definitions#sh-tokenization)
  - [Label expansion](/reference/be/common-definitions#label-expansion)
  - [Typical attributes for most rules](/reference/be/common-definitions#typical-attributes)
  - [Common attributes for all rules](/reference/be/common-definitions#common-attributes)
  - [Common attributes for tests](/reference/be/common-definitions#common-attributes-tests)
  - [Common attributes for binaries](/reference/be/common-definitions#common-attributes-binaries)
  - [Configurable attributes](/reference/be/common-definitions#configurable-attributes)
  - [Implicit output targets](/reference/be/common-definitions#implicit-outputs)
- ["Make" variables](/reference/be/make-variables)  - [Use](/reference/be/make-variables#use)

## Functions

- [package](/reference/be/functions.html#package)
- [package\_group](/reference/be/functions.html#package_group)
- [exports\_files](/reference/be/functions.html#exports_files)
- [glob](/reference/be/functions.html#glob)
- [select](/reference/be/functions.html#select)
- [workspace](/rules/lib/globals/workspace#workspace)

## Rules

Native rules ship with the Bazel binary and do not require a `load` statement.
Native rules are available globally in BUILD files. In .bzl files, you can find them in
the `native` module.

For non-native Starlark rules that ship separately from Bazel, see the list of
[recommended rules](/rules/rules#recommended-rules).

### Language-specific native rules

LanguageFlagsBinary rulesLibrary rulesTest rulesOther rulesC / C++[cc\_binary](c-cpp.html#cc_binary)

[cc\_import](c-cpp.html#cc_import)

[cc\_library](c-cpp.html#cc_library)

[cc\_shared\_library](c-cpp.html#cc_shared_library)

[cc\_static\_library](c-cpp.html#cc_static_library)

[cc\_test](c-cpp.html#cc_test)

[cc\_toolchain](c-cpp.html#cc_toolchain)

[fdo\_prefetch\_hints](c-cpp.html#fdo_prefetch_hints)

[fdo\_profile](c-cpp.html#fdo_profile)

[memprof\_profile](c-cpp.html#memprof_profile)

[propeller\_optimize](c-cpp.html#propeller_optimize)

Java[java\_binary](java.html#java_binary)

[java\_import](java.html#java_import)

[java\_library](java.html#java_library)

[java\_test](java.html#java_test)

[java\_package\_configuration](java.html#java_package_configuration)

[java\_plugin](java.html#java_plugin)

[java\_runtime](java.html#java_runtime)

[java\_single\_jar](java.html#java_single_jar)

[java\_toolchain](java.html#java_toolchain)

Objective-C[objc\_import](objective-c.html#objc_import)

[objc\_library](objective-c.html#objc_library)

Protocol Buffer[cc\_proto\_library](protocol-buffer.html#cc_proto_library)

[java\_lite\_proto\_library](protocol-buffer.html#java_lite_proto_library)

[java\_proto\_library](protocol-buffer.html#java_proto_library)

[proto\_library](protocol-buffer.html#proto_library)

[py\_proto\_library](protocol-buffer.html#py_proto_library)

[proto\_lang\_toolchain](protocol-buffer.html#proto_lang_toolchain)

[proto\_toolchain](protocol-buffer.html#proto_toolchain)

Python[py\_binary](python.html#py_binary)

[py\_library](python.html#py_library)

[py\_test](python.html#py_test)

[py\_runtime](python.html#py_runtime)

Shell[sh\_binary](shell.html#sh_binary)

[sh\_library](shell.html#sh_library)

[sh\_test](shell.html#sh_test)

### Language-agnostic native rules

FamilyRulesExtra Actions

- [action\_listener](extra-actions.html#action_listener)
- [extra\_action](extra-actions.html#extra_action)

General

- [alias](general.html#alias)
- [config\_setting](general.html#config_setting)
- [filegroup](general.html#filegroup)
- [genquery](general.html#genquery)
- [genrule](general.html#genrule)
- [starlark\_doc\_extract](general.html#starlark_doc_extract)
- [test\_suite](general.html#test_suite)

Platforms and Toolchains

- [constraint\_setting](platforms-and-toolchains.html#constraint_setting)
- [constraint\_value](platforms-and-toolchains.html#constraint_value)
- [platform](platforms-and-toolchains.html#platform)
- [toolchain](platforms-and-toolchains.html#toolchain)
- [toolchain\_type](platforms-and-toolchains.html#toolchain_type)

---

## Protocol Buffer Rules
- URL: https://bazel.build/reference/be/protocol-buffer
- Source: reference/be/protocol-buffer.mdx
- Slug: /reference/be/protocol-buffer

## Rules

- [cc\_proto\_library](#cc_proto_library)
- [java\_lite\_proto\_library](#java_lite_proto_library)
- [java\_proto\_library](#java_proto_library)
- [proto\_library](#proto_library)
- [py\_proto\_library](#py_proto_library)
- [proto\_lang\_toolchain](#proto_lang_toolchain)
- [proto\_toolchain](#proto_toolchain)

## cc\_proto\_library

[View rule sourceopen\_in\_new](https://github.com/protocolbuffers/protobuf/tree/v29.0-rc2/bazel/private/bazel_cc_proto_library.bzl)

```
cc_proto_library(name, deps, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

`cc_proto_library` generates C++ code from `.proto` files.

`deps` must point to [`proto_library\
`](protocol-buffer.html#proto_library) rules.

Example:

```lang-starlark

cc_library(
    name = "lib",
    deps = [":foo_cc_proto"],
)

cc_proto_library(
    name = "foo_cc_proto",
    deps = [":foo_proto"],
)

proto_library(
    name = "foo_proto",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of [`proto_library`](protocol-buffer.html#proto_library)
rules to generate C++ code for.


## java\_lite\_proto\_library

[View rule sourceopen\_in\_new](https://github.com/protocolbuffers/protobuf/tree/v29.0-rc2/bazel/private/java_lite_proto_library.bzl)

```
java_lite_proto_library(name, deps, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

`java_lite_proto_library` generates Java code from `.proto` files.

`deps` must point to [`proto_library\
`](protocol-buffer.html#proto_library) rules.

Example:

```lang-starlark

java_library(
    name = "lib",
    runtime_deps = [":foo"],
)

java_lite_proto_library(
    name = "foo",
    deps = [":bar"],
)

proto_library(
    name = "bar",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of [`proto_library`](protocol-buffer.html#proto_library)
rules to generate Java code for.


## java\_proto\_library

[View rule sourceopen\_in\_new](https://github.com/protocolbuffers/protobuf/tree/v29.0-rc2/bazel/private/bazel_java_proto_library_rule.bzl)

```
java_proto_library(name, deps, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, licenses, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

`java_proto_library` generates Java code from `.proto` files.

`deps` must point to [`proto_library\
`](protocol-buffer.html#proto_library) rules.

Example:

```lang-starlark

java_library(
    name = "lib",
    runtime_deps = [":foo_java_proto"],
)

java_proto_library(
    name = "foo_java_proto",
    deps = [":foo_proto"],
)

proto_library(
    name = "foo_proto",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of [`proto_library`](protocol-buffer.html#proto_library)
rules to generate Java code for.


## proto\_library

[View rule sourceopen\_in\_new](https://github.com/protocolbuffers/protobuf/tree/v29.0-rc2/bazel/private/proto_library_rule.bzl)

```
proto_library(name, deps, srcs, data, allow_exports, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, exports, extension_declarations, features, import_prefix, licenses, option_deps, package_metadata, restricted_to, strip_import_prefix, tags, target_compatible_with, testonly, toolchains, visibility)
```

Use `proto_library` to define libraries of protocol buffers which
may be used from multiple languages. A `proto_library` may be listed
in the `deps` clause of supported rules, such as
`java_proto_library`.

When compiled on the command-line, a `proto_library` creates a file
named `foo-descriptor-set.proto.bin`, which is the descriptor set for
the messages the rule srcs. The file is a serialized
`FileDescriptorSet`, which is described in
[https://developers.google.com/protocol-buffers/docs/techniques#self-description](https://developers.google.com/protocol-buffers/docs/techniques#self-description).

It only contains information about the `.proto` files directly
mentioned by a `proto_library` rule; the collection of transitive
descriptor sets is available through the
`[ProtoInfo].transitive_descriptor_sets` Starlark provider.
See documentation in `proto_info.bzl`.

Recommended code organization:

- One `proto_library` rule per `.proto` file.

- A file named `foo.proto` will be in a rule named `foo_proto`,
   which is located in the same package.

- A `[language]_proto_library` that wraps a `proto_library`
   named `foo_proto` should be called `foo_[language]_proto`,
   and be located in the same package.


### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other `proto_library` rules that the target depends upon.
A `proto_library` may only depend on other `proto_library`
targets. It may not depend on language-specific libraries.
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of `.proto` and `.protodevel` files that are
processed to create the target. This is usually a non empty list. One usecase
where `srcs` can be empty is an _alias-library_. This is a
proto\_library rule having one or more other proto\_library in `deps`.
This pattern can be used to e.g. export a public api under a persistent name.
 `allow_exports`

[Label](/concepts/labels); default is `None`

 An optional allowlist that prevents proto library to be reexported or used in
lang\_proto\_library that is not in one of the listed packages.
 `exports`

List of [labels](/concepts/labels); default is `[]`

 List of proto\_library targets that can be referenced via "import public" in the
proto source.
It's an error if you use "import public" but do not list the corresponding library
in the exports attribute.
Note that you have list the library both in deps and exports since not all
lang\_proto\_library implementations have been changed yet.
 `extension_declarations`

List of [labels](/concepts/labels); default is `[]`

 List of files containing extension declarations. This attribute is only allowed
for use with MessageSet.
 `import_prefix`

String; default is `""`

 The prefix to add to the paths of the .proto files in this rule.

When set, the .proto source files in the `srcs` attribute of this rule are
accessible at is the value of this attribute prepended to their repository-relative path.

The prefix in the `strip_import_prefix` attribute is removed before this
prefix is added.


`option_deps`

List of [labels](/concepts/labels); default is `[]`

 The list of other `proto_library` rules that the target depends upon for options only.
A `proto_library` may only depend on other `proto_library`
targets. It may not depend on language-specific libraries.
 `strip_import_prefix`

String; default is `"/"`

 The prefix to strip from the paths of the .proto files in this rule.

When set, .proto source files in the `srcs` attribute of this rule are
accessible at their path with this prefix cut off.

If it's a relative path (not starting with a slash), it's taken as a package-relative
one. If it's an absolute one, it's understood as a repository-relative path.

The prefix in the `import_prefix` attribute is added after this prefix is
stripped.


## py\_proto\_library

[View rule sourceopen\_in\_new](https://github.com/protocolbuffers/protobuf/tree/v29.0-rc2/bazel/py_proto_library.bzl)

```
py_proto_library(name, deps, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

 Use \`py\_proto\_library\` to generate Python libraries from \`.proto\` files.

 The convention is to name the \`py\_proto\_library\` rule \`foo\_py\_pb2\`,
 when it is wrapping \`proto\_library\` rule \`foo\_proto\`.

 \`deps\` must point to a \`proto\_library\` rule.

 Example:

\`\`\`starlark
py\_library(
 name = "lib",
 deps = \[":foo\_py\_pb2"\],
)

py\_proto\_library(
 name = "foo\_py\_pb2",
 deps = \[":foo\_proto"\],
)

proto\_library(
 name = "foo\_proto",
 srcs = \["foo.proto"\],
)
\`\`\`



### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of \`proto\_library\` rules to generate Python libraries for.

Usually this is just the one target: the proto library of interest.
It can be any target providing \`ProtoInfo\`.


## proto\_lang\_toolchain

[View rule sourceopen\_in\_new](https://github.com/protocolbuffers/protobuf/tree/v29.0-rc2/bazel/private/proto_lang_toolchain_rule.bzl)

```
proto_lang_toolchain(name, allowlist_different_package, aspect_hints, blacklisted_protos, command_line, compatible_with, denylisted_protos, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, mnemonic, output_files, package_metadata, plugin, plugin_format_flag, progress_message, protoc_minimal_do_not_use, restricted_to, runtime, tags, target_compatible_with, testonly, toolchain_type, toolchains, visibility)
```

If using Bazel, please load the rule from [https://github.com/bazelbuild/rules\_proto](https://github.com/bazelbuild/rules_proto).

Specifies how a LANG\_proto\_library rule (e.g., `java_proto_library`) should invoke the
proto-compiler.
Some LANG\_proto\_library rules allow specifying which toolchain to use using command-line flags;
consult their documentation.

Normally you should not write those kind of rules unless you want to
tune your Java compiler.

There's no compiler. The proto-compiler is taken from the proto\_library rule we attach to. It is
passed as a command-line flag to Blaze.
Several features require a proto-compiler to be invoked on the proto\_library rule itself.
It's beneficial to enforce the compiler that LANG\_proto\_library uses is the same as the one
`proto_library` does.

#### Examples

A simple example would be:

```lang-starlark

proto_lang_toolchain(
    name = "javalite_toolchain",
    command_line = "--javalite_out=shared,immutable:$(OUT)",
    plugin = ":javalite_plugin",
    runtime = ":protobuf_lite",
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`allowlist_different_package`

[Label](/concepts/labels); default is `None`

`blacklisted_protos`

List of [labels](/concepts/labels); default is `[]`

 Deprecated. Alias for `denylisted_protos`. Will be removed in a future release.
 `command_line`

String; required

 This value will be passed to proto-compiler to generate the code. Only include the parts
specific to this code-generator/plugin (e.g., do not include -I parameters)

- `$(OUT)` is LANG\_proto\_library-specific. The rules are expected to define
   how they interpret this variable. For Java, for example, $(OUT) will be replaced with
   the src-jar filename to create.

`denylisted_protos`

List of [labels](/concepts/labels); default is `[]`

 No code will be generated for files in the `srcs` attribute of
`denylisted_protos`.
This is used for .proto files that are already linked into proto runtimes, such as
`any.proto`.
 `mnemonic`

String; default is `"GenProto"`

 This value will be set as the mnemonic on protoc action.
 `output_files`

String; default is `"legacy"`

 Controls how `$(OUT)` in `command_line` is formatted, either by
a path to a single file or output directory in case of multiple files.
Possible values are: "single", "multiple".
 `plugin`

[Label](/concepts/labels); default is `None`

 If provided, will be made available to the action that calls the proto-compiler, and will be
passed to the proto-compiler:
`--plugin=protoc-gen-PLUGIN=<executable>.``plugin_format_flag`

String; default is `""`

 If provided, this value will be passed to proto-compiler to use the plugin.
The value must contain a single %s which is replaced with plugin executable.
`--plugin=protoc-gen-PLUGIN=<executable>.``progress_message`

String; default is `"Generating proto_library %{label}"`

 This value will be set as the progress message on protoc action.
 `protoc_minimal_do_not_use`

[Label](/concepts/labels); default is `None`

`runtime`

[Label](/concepts/labels); default is `None`

 A language-specific library that the generated code is compiled against.
The exact behavior is LANG\_proto\_library-specific.
Java, for example, should compile against the runtime.
 `toolchain_type`

[Label](/concepts/labels); default is `None`

## proto\_toolchain

[View rule sourceopen\_in\_new](https://github.com/protocolbuffers/protobuf/tree/v29.0-rc2/bazel/private/proto_toolchain_rule.bzl)

```
proto_toolchain(name, aspect_hints, command_line, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, mnemonic, output_files, package_metadata, progress_message, proto_compiler, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`command_line`

String; default is `"--descriptor_set_out=%s"`

`mnemonic`

String; default is `"GenProtoDescriptorSet"`

`output_files`

String; default is `"single"`

`progress_message`

String; default is `"Generating Descriptor Set proto_library %{label}"`

`proto_compiler`

[Label](/concepts/labels); default is `None`

---

## Python Rules
- URL: https://bazel.build/reference/be/python
- Source: reference/be/python.mdx
- Slug: /reference/be/python

## Rules

- [py\_binary](#py_binary)
- [py\_library](#py_library)
- [py\_test](#py_test)
- [py\_runtime](#py_runtime)

## py\_binary

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_python/tree/0.40.0/python/private/py_binary_rule.bzl)

```
py_binary(name, deps, srcs, data, args, aspect_hints, compatible_with, deprecation, distribs, env, exec_compatible_with, exec_group_compatible_with, exec_properties, features, imports, interpreter_args, legacy_create_init, licenses, main, main_module, output_licenses, package_metadata, precompile, precompile_invalidation_mode, precompile_optimize_level, precompile_source_retention, pyc_collection, pyi_deps, pyi_srcs, python_version, restricted_to, srcs_version, stamp, tags, target_compatible_with, testonly, toolchains, visibility)
```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 List of additional libraries to be linked in to the target.
See comments about
the \[\`deps\` attribute typically defined by
rules\](https://bazel.build/reference/be/common-definitions#typical-attributes).
These are typically \`py\_library\` rules.

Targets that only provide data files used at runtime belong in the \`data\`
attribute.

:::{note}
The order of this list can matter because it affects the order that information
from dependencies is merged in, which can be relevant depending on the ordering
mode of depsets that are merged.

\\* {obj}\`PyInfo.venv\_symlinks\` uses default ordering.

See {obj}\`PyInfo\` for more information about the ordering of its depsets and
how its fields are merged.
:::
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of Python source files that are processed to create the target. This
includes all your checked-in code and may include generated source files. The
\`.py\` files belong in \`srcs\` and library targets belong in \`deps\`. Other binary
files that may be needed at run time belong in \`data\`.
 `data`

List of [labels](/concepts/labels); default is `[]`

 The list of files need by this library at runtime. See comments about
the \[\`data\` attribute typically defined by rules\](https://bazel.build/reference/be/common-definitions#typical-attributes).

There is no \`py\_embed\_data\` like there is \`cc\_embed\_data\` and \`go\_embed\_data\`.
This is because Python has a concept of runtime resources.
 `distribs`

List of strings; default is `[]`

`imports`

List of strings; default is `[]`

 List of import directories to be added to the PYTHONPATH.

Subject to "Make variable" substitution. These import directories will be added
for this rule and all rules that depend on it (note: not the rules this rule
depends on. Each directory will be added to \`PYTHONPATH\` by \`py\_binary\` rules
that depend on this rule. The strings are repo-runfiles-root relative,

Absolute paths (paths that start with \`/\`) and paths that references a path
above the execution root are not allowed and will result in an error.
 `interpreter_args`

List of strings; default is `[]`

 Arguments that are only applicable to the interpreter.

The args an interpreter supports are specific to the interpreter. For
CPython, see https://docs.python.org/3/using/cmdline.html.

:::{note}
Only supported for {obj}\`--bootstrap\_impl=script\`. Ignored otherwise.
:::

:::{seealso}
The {any}\`RULES\_PYTHON\_ADDITIONAL\_INTERPRETER\_ARGS\` environment variable
:::

:::{versionadded} 1.3.0
:::
 `legacy_create_init`

Integer; default is `-1`

 Whether to implicitly create empty \`\_\_init\_\_.py\` files in the runfiles tree.
These are created in every directory containing Python source code or shared
libraries, and every parent directory of those directories, excluding the repo
root directory. The default, \`-1\` (auto), means true unless
\`--incompatible\_default\_to\_explicit\_init\_py\` is used. If false, the user is
responsible for creating (possibly empty) \`\_\_init\_\_.py\` files and adding them to
the \`srcs\` of Python targets as required.
 `main`

[Label](/concepts/labels); default is `None`

 Optional; the name of the source file that is the main entry point of the
application. This file must also be listed in \`srcs\`. If left unspecified,
\`name\`, with \`.py\` appended, is used instead. If \`name\` does not match any
filename in \`srcs\`, \`main\` must be specified.

This is mutually exclusive with {obj}\`main\_module\`.
 `main_module`

String; default is `""`

 Module name to execute as the main program.

When set, \`srcs\` is not required, and it is assumed the module is
provided by a dependency.

See https://docs.python.org/3/using/cmdline.html#cmdoption-m for more
information about running modules as the main program.

This is mutually exclusive with {obj}\`main\`.

:::{versionadded} 1.3.0
:::
 `precompile`

String; default is `"inherit"`

 Whether py source files \*\*for this target\*\* should be precompiled.

Values:

\\* \`inherit\`: Allow the downstream binary decide if precompiled files are used.
\\* \`enabled\`: Compile Python source files at build time.
\\* \`disabled\`: Don't compile Python source files at build time.

:::{seealso}

\\* The {flag}\`--precompile\` flag, which can override this attribute in some cases
 and will affect all targets when building.
\\* The {obj}\`pyc\_collection\` attribute for transitively enabling precompiling on
 a per-target basis.
\\* The \[Precompiling\](precompiling) docs for a guide about using precompiling.
:::
 `precompile_invalidation_mode`

String; default is `"auto"`

 How precompiled files should be verified to be up-to-date with their associated
source files. Possible values are:
\\* \`auto\`: The effective value will be automatically determined by other build
 settings.
\\* \`checked\_hash\`: Use the pyc file if the hash of the source file matches the hash
 recorded in the pyc file. This is most useful when working with code that
 you may modify.
\\* \`unchecked\_hash\`: Always use the pyc file; don't check the pyc's hash against
 the source file. This is most useful when the code won't be modified.

For more information on pyc invalidation modes, see
https://docs.python.org/3/library/py\_compile.html#py\_compile.PycInvalidationMode
 `precompile_optimize_level`

Integer; default is `0`

 The optimization level for precompiled files.

For more information about optimization levels, see the \`compile()\` function's
\`optimize\` arg docs at https://docs.python.org/3/library/functions.html#compile

NOTE: The value \`-1\` means "current interpreter", which will be the interpreter
used \_at build time when pycs are generated\_, not the interpreter used at
runtime when the code actually runs.
 `precompile_source_retention`

String; default is `"inherit"`

 Determines, when a source file is compiled, if the source file is kept
in the resulting output or not. Valid values are:

\\* \`inherit\`: Inherit the value from the {flag}\`--precompile\_source\_retention\` flag.
\\* \`keep\_source\`: Include the original Python source.
\\* \`omit\_source\`: Don't include the original py source.
 `pyc_collection`

String; default is `"inherit"`

 Determines whether pyc files from dependencies should be manually included.

Valid values are:
\\* \`inherit\`: Inherit the value from {flag}\`--precompile\`.
\\* \`include\_pyc\`: Add implicitly generated pyc files from dependencies. i.e.
 pyc files for targets that specify {attr}\`precompile="inherit"\`.
\\* \`disabled\`: Don't add implicitly generated pyc files. Note that
 pyc files may still come from dependencies that enable precompiling at the
 target level.
 `pyi_deps`

List of [labels](/concepts/labels); default is `[]`

 Dependencies providing type definitions the library needs.

These are dependencies that satisfy imports guarded by \`typing.TYPE\_CHECKING\`.
These are build-time only dependencies and not included as part of a runnable
program (packaging rules may include them, however).

:::{versionadded} 1.1.0
:::
 `pyi_srcs`

List of [labels](/concepts/labels); default is `[]`

 Type definition files for the library.

These are typically \`.pyi\` files, but other file types for type-checker specific
formats are allowed. These files are build-time only dependencies and not included
as part of a runnable program (packaging rules may include them, however).

:::{versionadded} 1.1.0
:::
 `python_version`

String; default is `""`

 The Python version this target should use.

The value should be in \`X.Y\` or \`X.Y.Z\` (or compatible) format. If empty or
unspecified, the incoming configuration's {obj}\`--python\_version\` flag is
inherited. For backwards compatibility, the values \`PY2\` and \`PY3\` are
accepted, but treated as an empty/unspecified value.

:::{note}
In order for the requested version to be used, there must be a
toolchain configured to match the Python version. If there isn't, then it
may be silently ignored, or an error may occur, depending on the toolchain
configuration.
:::

:::{versionchanged} 1.1.0

This attribute was changed from only accepting \`PY2\` and \`PY3\` values to
accepting arbitrary Python versions.
:::
 `srcs_version`

String; default is `""`

 Defunct, unused, does nothing.
 `stamp`

Integer; default is `-1`

 Whether to encode build information into the binary. Possible values:

\\* \`stamp = 1\`: Always stamp the build information into the binary, even in
 \`--nostamp\` builds. \*\*This setting should be avoided\*\*, since it potentially kills
 remote caching for the binary and any downstream actions that depend on it.
\\* \`stamp = 0\`: Always replace build information by constant values. This gives
 good build result caching.
\\* \`stamp = -1\`: Embedding of build information is controlled by the
 \`--\[no\]stamp\` flag.

Stamped binaries are not rebuilt unless their dependencies change.

WARNING: Stamping can harm build performance by reducing cache hits and should
be avoided if possible.


## py\_library

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_python/tree/0.40.0/python/private/py_library_rule.bzl)

```
py_library(name, deps, srcs, data, aspect_hints, compatible_with, deprecation, distribs, exec_compatible_with, exec_group_compatible_with, exec_properties, experimental_venvs_site_packages, features, imports, licenses, package_metadata, precompile, precompile_invalidation_mode, precompile_optimize_level, precompile_source_retention, pyi_deps, pyi_srcs, restricted_to, srcs_version, tags, target_compatible_with, testonly, toolchains, visibility)
```

 A library of Python code that can be depended upon.

Default outputs:
\\* The input Python sources
\\* The precompiled artifacts from the sources.

NOTE: Precompilation affects which of the default outputs are included in the
resulting runfiles. See the precompile-related attributes and flags for
more information.

:::{versionchanged} 0.37.0
Source files are no longer added to the runfiles directly.
:::



### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 List of additional libraries to be linked in to the target.
See comments about
the \[\`deps\` attribute typically defined by
rules\](https://bazel.build/reference/be/common-definitions#typical-attributes).
These are typically \`py\_library\` rules.

Targets that only provide data files used at runtime belong in the \`data\`
attribute.

:::{note}
The order of this list can matter because it affects the order that information
from dependencies is merged in, which can be relevant depending on the ordering
mode of depsets that are merged.

\\* {obj}\`PyInfo.venv\_symlinks\` uses default ordering.

See {obj}\`PyInfo\` for more information about the ordering of its depsets and
how its fields are merged.
:::
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of Python source files that are processed to create the target. This
includes all your checked-in code and may include generated source files. The
\`.py\` files belong in \`srcs\` and library targets belong in \`deps\`. Other binary
files that may be needed at run time belong in \`data\`.
 `data`

List of [labels](/concepts/labels); default is `[]`

 The list of files need by this library at runtime. See comments about
the \[\`data\` attribute typically defined by rules\](https://bazel.build/reference/be/common-definitions#typical-attributes).

There is no \`py\_embed\_data\` like there is \`cc\_embed\_data\` and \`go\_embed\_data\`.
This is because Python has a concept of runtime resources.
 `distribs`

List of strings; default is `[]`

`experimental_venvs_site_packages`

[Label](/concepts/labels); default is `None`

 \*\*INTERNAL ATTRIBUTE. SHOULD ONLY BE SET BY rules\_python-INTERNAL CODE.\*\*

:::{include} /\_includes/experimental\_api.md
:::

A flag that decides whether the library should treat its sources as a
site-packages layout.

When the flag is \`yes\`, then the \`srcs\` files are treated as a site-packages
layout that is relative to the \`imports\` attribute. The \`imports\` attribute
can have only a single element. It is a repo-relative runfiles path.

For example, in the \`my/pkg/BUILD.bazel\` file, given
\`srcs=\["site-packages/foo/bar.py"\]\`, specifying
\`imports=\["my/pkg/site-packages"\]\` means \`foo/bar.py\` is the file path
under the binary's venv site-packages directory that should be made available (i.e.
\`import foo.bar\` will work).

\`\_\_init\_\_.py\` files are treated specially to provide basic support for \[implicit
namespace packages\](
https://packaging.python.org/en/latest/guides/packaging-namespace-packages/#native-namespace-packages).
However, the \*content\* of the files cannot be taken into account, merely their
presence or absence. Stated another way: \[pkgutil-style namespace packages\](
https://packaging.python.org/en/latest/guides/packaging-namespace-packages/#pkgutil-style-namespace-packages)
won't be understood as namespace packages; they'll be seen as regular packages. This will
likely lead to conflicts with other targets that contribute to the namespace.

:::{seealso}
This attributes populates {obj}\`PyInfo.venv\_symlinks\`.
:::

:::{versionadded} 1.4.0
:::
:::{versionchanged} 1.5.0
The topological order has been removed and if 2 different versions of the same PyPI
package are observed, the behaviour has no guarantees except that it is deterministic
and that only one package version will be included.
:::
 `imports`

List of strings; default is `[]`

 List of import directories to be added to the PYTHONPATH.

Subject to "Make variable" substitution. These import directories will be added
for this rule and all rules that depend on it (note: not the rules this rule
depends on. Each directory will be added to \`PYTHONPATH\` by \`py\_binary\` rules
that depend on this rule. The strings are repo-runfiles-root relative,

Absolute paths (paths that start with \`/\`) and paths that references a path
above the execution root are not allowed and will result in an error.
 `precompile`

String; default is `"inherit"`

 Whether py source files \*\*for this target\*\* should be precompiled.

Values:

\\* \`inherit\`: Allow the downstream binary decide if precompiled files are used.
\\* \`enabled\`: Compile Python source files at build time.
\\* \`disabled\`: Don't compile Python source files at build time.

:::{seealso}

\\* The {flag}\`--precompile\` flag, which can override this attribute in some cases
 and will affect all targets when building.
\\* The {obj}\`pyc\_collection\` attribute for transitively enabling precompiling on
 a per-target basis.
\\* The \[Precompiling\](precompiling) docs for a guide about using precompiling.
:::
 `precompile_invalidation_mode`

String; default is `"auto"`

 How precompiled files should be verified to be up-to-date with their associated
source files. Possible values are:
\\* \`auto\`: The effective value will be automatically determined by other build
 settings.
\\* \`checked\_hash\`: Use the pyc file if the hash of the source file matches the hash
 recorded in the pyc file. This is most useful when working with code that
 you may modify.
\\* \`unchecked\_hash\`: Always use the pyc file; don't check the pyc's hash against
 the source file. This is most useful when the code won't be modified.

For more information on pyc invalidation modes, see
https://docs.python.org/3/library/py\_compile.html#py\_compile.PycInvalidationMode
 `precompile_optimize_level`

Integer; default is `0`

 The optimization level for precompiled files.

For more information about optimization levels, see the \`compile()\` function's
\`optimize\` arg docs at https://docs.python.org/3/library/functions.html#compile

NOTE: The value \`-1\` means "current interpreter", which will be the interpreter
used \_at build time when pycs are generated\_, not the interpreter used at
runtime when the code actually runs.
 `precompile_source_retention`

String; default is `"inherit"`

 Determines, when a source file is compiled, if the source file is kept
in the resulting output or not. Valid values are:

\\* \`inherit\`: Inherit the value from the {flag}\`--precompile\_source\_retention\` flag.
\\* \`keep\_source\`: Include the original Python source.
\\* \`omit\_source\`: Don't include the original py source.
 `pyi_deps`

List of [labels](/concepts/labels); default is `[]`

 Dependencies providing type definitions the library needs.

These are dependencies that satisfy imports guarded by \`typing.TYPE\_CHECKING\`.
These are build-time only dependencies and not included as part of a runnable
program (packaging rules may include them, however).

:::{versionadded} 1.1.0
:::
 `pyi_srcs`

List of [labels](/concepts/labels); default is `[]`

 Type definition files for the library.

These are typically \`.pyi\` files, but other file types for type-checker specific
formats are allowed. These files are build-time only dependencies and not included
as part of a runnable program (packaging rules may include them, however).

:::{versionadded} 1.1.0
:::
 `srcs_version`

String; default is `""`

 Defunct, unused, does nothing.


## py\_test

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_python/tree/0.40.0/python/private/py_test_rule.bzl)

```
py_test(name, deps, srcs, data, args, aspect_hints, compatible_with, deprecation, distribs, env, env_inherit, exec_compatible_with, exec_group_compatible_with, exec_properties, features, flaky, imports, interpreter_args, legacy_create_init, licenses, local, main, main_module, package_metadata, precompile, precompile_invalidation_mode, precompile_optimize_level, precompile_source_retention, pyc_collection, pyi_deps, pyi_srcs, python_version, restricted_to, shard_count, size, srcs_version, stamp, tags, target_compatible_with, testonly, timeout, toolchains, visibility)
```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 List of additional libraries to be linked in to the target.
See comments about
the \[\`deps\` attribute typically defined by
rules\](https://bazel.build/reference/be/common-definitions#typical-attributes).
These are typically \`py\_library\` rules.

Targets that only provide data files used at runtime belong in the \`data\`
attribute.

:::{note}
The order of this list can matter because it affects the order that information
from dependencies is merged in, which can be relevant depending on the ordering
mode of depsets that are merged.

\\* {obj}\`PyInfo.venv\_symlinks\` uses default ordering.

See {obj}\`PyInfo\` for more information about the ordering of its depsets and
how its fields are merged.
:::
 `srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of Python source files that are processed to create the target. This
includes all your checked-in code and may include generated source files. The
\`.py\` files belong in \`srcs\` and library targets belong in \`deps\`. Other binary
files that may be needed at run time belong in \`data\`.
 `data`

List of [labels](/concepts/labels); default is `[]`

 The list of files need by this library at runtime. See comments about
the \[\`data\` attribute typically defined by rules\](https://bazel.build/reference/be/common-definitions#typical-attributes).

There is no \`py\_embed\_data\` like there is \`cc\_embed\_data\` and \`go\_embed\_data\`.
This is because Python has a concept of runtime resources.
 `distribs`

List of strings; default is `[]`

`imports`

List of strings; default is `[]`

 List of import directories to be added to the PYTHONPATH.

Subject to "Make variable" substitution. These import directories will be added
for this rule and all rules that depend on it (note: not the rules this rule
depends on. Each directory will be added to \`PYTHONPATH\` by \`py\_binary\` rules
that depend on this rule. The strings are repo-runfiles-root relative,

Absolute paths (paths that start with \`/\`) and paths that references a path
above the execution root are not allowed and will result in an error.
 `interpreter_args`

List of strings; default is `[]`

 Arguments that are only applicable to the interpreter.

The args an interpreter supports are specific to the interpreter. For
CPython, see https://docs.python.org/3/using/cmdline.html.

:::{note}
Only supported for {obj}\`--bootstrap\_impl=script\`. Ignored otherwise.
:::

:::{seealso}
The {any}\`RULES\_PYTHON\_ADDITIONAL\_INTERPRETER\_ARGS\` environment variable
:::

:::{versionadded} 1.3.0
:::
 `legacy_create_init`

Integer; default is `-1`

 Whether to implicitly create empty \`\_\_init\_\_.py\` files in the runfiles tree.
These are created in every directory containing Python source code or shared
libraries, and every parent directory of those directories, excluding the repo
root directory. The default, \`-1\` (auto), means true unless
\`--incompatible\_default\_to\_explicit\_init\_py\` is used. If false, the user is
responsible for creating (possibly empty) \`\_\_init\_\_.py\` files and adding them to
the \`srcs\` of Python targets as required.
 `main`

[Label](/concepts/labels); default is `None`

 Optional; the name of the source file that is the main entry point of the
application. This file must also be listed in \`srcs\`. If left unspecified,
\`name\`, with \`.py\` appended, is used instead. If \`name\` does not match any
filename in \`srcs\`, \`main\` must be specified.

This is mutually exclusive with {obj}\`main\_module\`.
 `main_module`

String; default is `""`

 Module name to execute as the main program.

When set, \`srcs\` is not required, and it is assumed the module is
provided by a dependency.

See https://docs.python.org/3/using/cmdline.html#cmdoption-m for more
information about running modules as the main program.

This is mutually exclusive with {obj}\`main\`.

:::{versionadded} 1.3.0
:::
 `precompile`

String; default is `"inherit"`

 Whether py source files \*\*for this target\*\* should be precompiled.

Values:

\\* \`inherit\`: Allow the downstream binary decide if precompiled files are used.
\\* \`enabled\`: Compile Python source files at build time.
\\* \`disabled\`: Don't compile Python source files at build time.

:::{seealso}

\\* The {flag}\`--precompile\` flag, which can override this attribute in some cases
 and will affect all targets when building.
\\* The {obj}\`pyc\_collection\` attribute for transitively enabling precompiling on
 a per-target basis.
\\* The \[Precompiling\](precompiling) docs for a guide about using precompiling.
:::
 `precompile_invalidation_mode`

String; default is `"auto"`

 How precompiled files should be verified to be up-to-date with their associated
source files. Possible values are:
\\* \`auto\`: The effective value will be automatically determined by other build
 settings.
\\* \`checked\_hash\`: Use the pyc file if the hash of the source file matches the hash
 recorded in the pyc file. This is most useful when working with code that
 you may modify.
\\* \`unchecked\_hash\`: Always use the pyc file; don't check the pyc's hash against
 the source file. This is most useful when the code won't be modified.

For more information on pyc invalidation modes, see
https://docs.python.org/3/library/py\_compile.html#py\_compile.PycInvalidationMode
 `precompile_optimize_level`

Integer; default is `0`

 The optimization level for precompiled files.

For more information about optimization levels, see the \`compile()\` function's
\`optimize\` arg docs at https://docs.python.org/3/library/functions.html#compile

NOTE: The value \`-1\` means "current interpreter", which will be the interpreter
used \_at build time when pycs are generated\_, not the interpreter used at
runtime when the code actually runs.
 `precompile_source_retention`

String; default is `"inherit"`

 Determines, when a source file is compiled, if the source file is kept
in the resulting output or not. Valid values are:

\\* \`inherit\`: Inherit the value from the {flag}\`--precompile\_source\_retention\` flag.
\\* \`keep\_source\`: Include the original Python source.
\\* \`omit\_source\`: Don't include the original py source.
 `pyc_collection`

String; default is `"inherit"`

 Determines whether pyc files from dependencies should be manually included.

Valid values are:
\\* \`inherit\`: Inherit the value from {flag}\`--precompile\`.
\\* \`include\_pyc\`: Add implicitly generated pyc files from dependencies. i.e.
 pyc files for targets that specify {attr}\`precompile="inherit"\`.
\\* \`disabled\`: Don't add implicitly generated pyc files. Note that
 pyc files may still come from dependencies that enable precompiling at the
 target level.
 `pyi_deps`

List of [labels](/concepts/labels); default is `[]`

 Dependencies providing type definitions the library needs.

These are dependencies that satisfy imports guarded by \`typing.TYPE\_CHECKING\`.
These are build-time only dependencies and not included as part of a runnable
program (packaging rules may include them, however).

:::{versionadded} 1.1.0
:::
 `pyi_srcs`

List of [labels](/concepts/labels); default is `[]`

 Type definition files for the library.

These are typically \`.pyi\` files, but other file types for type-checker specific
formats are allowed. These files are build-time only dependencies and not included
as part of a runnable program (packaging rules may include them, however).

:::{versionadded} 1.1.0
:::
 `python_version`

String; default is `""`

 The Python version this target should use.

The value should be in \`X.Y\` or \`X.Y.Z\` (or compatible) format. If empty or
unspecified, the incoming configuration's {obj}\`--python\_version\` flag is
inherited. For backwards compatibility, the values \`PY2\` and \`PY3\` are
accepted, but treated as an empty/unspecified value.

:::{note}
In order for the requested version to be used, there must be a
toolchain configured to match the Python version. If there isn't, then it
may be silently ignored, or an error may occur, depending on the toolchain
configuration.
:::

:::{versionchanged} 1.1.0

This attribute was changed from only accepting \`PY2\` and \`PY3\` values to
accepting arbitrary Python versions.
:::
 `srcs_version`

String; default is `""`

 Defunct, unused, does nothing.
 `stamp`

Integer; default is `0`

 Whether to encode build information into the binary. Possible values:

\\* \`stamp = 1\`: Always stamp the build information into the binary, even in
 \`--nostamp\` builds. \*\*This setting should be avoided\*\*, since it potentially kills
 remote caching for the binary and any downstream actions that depend on it.
\\* \`stamp = 0\`: Always replace build information by constant values. This gives
 good build result caching.
\\* \`stamp = -1\`: Embedding of build information is controlled by the
 \`--\[no\]stamp\` flag.

Stamped binaries are not rebuilt unless their dependencies change.

WARNING: Stamping can harm build performance by reducing cache hits and should
be avoided if possible.


## py\_runtime

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_python/tree/0.40.0/python/private/py_runtime_rule.bzl)

```
py_runtime(name, abi_flags, aspect_hints, bootstrap_template, compatible_with, coverage_tool, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, files, implementation_name, interpreter, interpreter_path, interpreter_version_info, package_metadata, pyc_tag, python_version, restricted_to, site_init_template, stage2_bootstrap_template, stub_shebang, supports_build_time_venv, tags, target_compatible_with, testonly, toolchains, visibility, zip_main_template)
```

 Represents a Python runtime used to execute Python code.

A \`py\_runtime\` target can represent either a \*platform runtime\* or an \*in-build
runtime\*. A platform runtime accesses a system-installed interpreter at a known
path, whereas an in-build runtime points to an executable target that acts as
the interpreter. In both cases, an "interpreter" means any executable binary or
wrapper script that is capable of running a Python script passed on the command
line, following the same conventions as the standard CPython interpreter.

A platform runtime is by its nature non-hermetic. It imposes a requirement on
the target platform to have an interpreter located at a specific path. An
in-build runtime may or may not be hermetic, depending on whether it points to
a checked-in interpreter or a wrapper script that accesses the system
interpreter.

Example

\`\`\`
load("@rules\_python//python:py\_runtime.bzl", "py\_runtime")

py\_runtime(
 name = "python-2.7.12",
 files = glob(\["python-2.7.12/\*\*"\]),
 interpreter = "python-2.7.12/bin/python",
)

py\_runtime(
 name = "python-3.6.0",
 interpreter\_path = "/opt/pyenv/versions/3.6.0/bin/python",
)
\`\`\`



### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`abi_flags`

String; default is `""`

 The runtime's ABI flags, i.e. \`sys.abiflags\`.

If not set, then it will be set based on flags.
 `bootstrap_template`

[Label](/concepts/labels); default is `"@rules_python//python/private:bootstrap_template"`

 The bootstrap script template file to use. Should have %python\_binary%,
%workspace\_name%, %main%, and %imports%.

This template, after expansion, becomes the executable file used to start the
process, so it is responsible for initial bootstrapping actions such as finding
the Python interpreter, runfiles, and constructing an environment to run the
intended Python application.

While this attribute is currently optional, it will become required when the
Python rules are moved out of Bazel itself.

The exact variable names expanded is an unstable API and is subject to change.
The API will become more stable when the Python rules are moved out of Bazel
itself.

See @bazel\_tools//tools/python:python\_bootstrap\_template.txt for more variables.
 `coverage_tool`

[Label](/concepts/labels); default is `None`

 This is a target to use for collecting code coverage information from
{rule}\`py\_binary\` and {rule}\`py\_test\` targets.

If set, the target must either produce a single file or be an executable target.
The path to the single file, or the executable if the target is executable,
determines the entry point for the python coverage tool. The target and its
runfiles will be added to the runfiles when coverage is enabled.

The entry point for the tool must be loadable by a Python interpreter (e.g. a
\`.py\` or \`.pyc\` file). It must accept the command line arguments
of \[\`coverage.py\`\](https://coverage.readthedocs.io), at least including
the \`run\` and \`lcov\` subcommands.
 `files`

List of [labels](/concepts/labels); default is `[]`

 For an in-build runtime, this is the set of files comprising this runtime.
These files will be added to the runfiles of Python binaries that use this
runtime. For a platform runtime this attribute must not be set.
 `implementation_name`

String; default is `"cpython"`

 The Python implementation name (\`sys.implementation.name\`)
 `interpreter`

[Label](/concepts/labels); default is `None`

 For an in-build runtime, this is the target to invoke as the interpreter. It
can be either of:

\\* A single file, which will be the interpreter binary. It's assumed such
 interpreters are either self-contained single-file executables or any
 supporting files are specified in \`files\`.
\\* An executable target. The target's executable will be the interpreter binary.
 Any other default outputs (\`target.files\`) and plain files runfiles
 (\`runfiles.files\`) will be automatically included as if specified in the
 \`files\` attribute.

 NOTE: the runfiles of the target may not yet be properly respected/propagated
 to consumers of the toolchain/interpreter, see
 bazel-contrib/rules\_python/issues/1612

For a platform runtime (i.e. \`interpreter\_path\` being set) this attribute must
not be set.
 `interpreter_path`

String; default is `""`

 For a platform runtime, this is the absolute path of a Python interpreter on
the target platform. For an in-build runtime this attribute must not be set.
 `interpreter_version_info`

Dictionary: String -> String; default is `{}`

 Version information about the interpreter this runtime provides.

If not specified, uses {obj}\`--python\_version\`

The supported keys match the names for \`sys.version\_info\`. While the input
values are strings, most are converted to ints. The supported keys are:
 \\* major: int, the major version number
 \\* minor: int, the minor version number
 \\* micro: optional int, the micro version number
 \\* releaselevel: optional str, the release level
 \\* serial: optional int, the serial number of the release

:::{versionchanged} 0.36.0
{obj}\`--python\_version\` determines the default value.
:::
 `pyc_tag`

String; default is `""`

 Optional string; the tag portion of a pyc filename, e.g. the \`cpython-39\` infix
of \`foo.cpython-39.pyc\`. See PEP 3147. If not specified, it will be computed
from \`implementation\_name\` and \`interpreter\_version\_info\`. If no pyc\_tag is
available, then only source-less pyc generation will function correctly.
 `python_version`

String; default is `"PY3"`

 Whether this runtime is for Python major version 2 or 3. Valid values are \`"PY2"\`
and \`"PY3"\`.

The default value is controlled by the \`--incompatible\_py3\_is\_default\` flag.
However, in the future this attribute will be mandatory and have no default
value.
 `site_init_template`

[Label](/concepts/labels); default is `"@rules_python//python/private:site_init_template"`

 The template to use for the binary-specific site-init hook run by the
interpreter at startup.

:::{versionadded} 0.41.0
:::
 `stage2_bootstrap_template`

[Label](/concepts/labels); default is `"@rules_python//python/private:stage2_bootstrap_template"`

 The template to use when two stage bootstrapping is enabled

:::{seealso}
{obj}\`PyRuntimeInfo.stage2\_bootstrap\_template\` and {obj}\`--bootstrap\_impl\`
:::
 `stub_shebang`

String; default is `"#!/usr/bin/env python3"`

 "Shebang" expression prepended to the bootstrapping Python stub script
used when executing {rule}\`py\_binary\` targets.

See https://github.com/bazelbuild/bazel/issues/8685 for
motivation.

Does not apply to Windows.
 `supports_build_time_venv`

Boolean; default is `True`

 Whether this runtime supports virtualenvs created at build time.

See {obj}\`PyRuntimeInfo.supports\_build\_time\_venv\` for docs.

:::{versionadded} 1.5.0
:::
 `zip_main_template`

[Label](/concepts/labels); default is `"@rules_python//python/private:zip_main_template"`

 The template to use for a zip's top-level \`\_\_main\_\_.py\` file.

This becomes the entry point executed when \`python foo.zip\` is run.

:::{seealso}
The {obj}\`PyRuntimeInfo.zip\_main\_template\` field.
:::

---

## Shell Rules
- URL: https://bazel.build/reference/be/shell
- Source: reference/be/shell.mdx
- Slug: /reference/be/shell

## Rules

- [sh\_binary](#sh_binary)
- [sh\_library](#sh_library)
- [sh\_test](#sh_test)

## sh\_binary

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_shell/tree/v0.2.0/shell/private/sh_binary.bzl)

```
sh_binary(name, deps, srcs, data, args, aspect_hints, compatible_with, deprecation, env, env_inherit, exec_compatible_with, exec_group_compatible_with, exec_properties, features, output_licenses, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, use_bash_launcher, visibility)
```

The `sh_binary` rule is used to declare executable shell scripts.
( `sh_binary` is a misnomer: its outputs aren't necessarily binaries.) This rule ensures
that all dependencies are built, and appear in the `runfiles` area at execution time.
We recommend that you name your `sh_binary()` rules after the name of the script minus
the extension (e.g. `.sh`); the rule name and the file name must be distinct.
`sh_binary` respects shebangs, so any available interpreter may be used (eg.
`#!/bin/zsh`)

#### Example

For a simple shell script with no dependencies and some data files:

```
sh_binary(
    name = "foo",
    srcs = ["foo.sh"],
    data = glob(["datafiles/*.txt"]),
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of "library" targets to be aggregated into this target.
See general comments about `deps`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical.deps).

This attribute should be used to list other `sh_library` rules that provide
interpreted program source code depended on by the code in `srcs`. The files
provided by these rules will be present among the `runfiles` of this target.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 The file containing the shell script.

This attribute must be a singleton list, whose element is the shell script.
This script must be executable, and may be a source file or a generated file.
All other files required at runtime (whether scripts or data) belong in the
`data` attribute.

`env_inherit`

List of strings; default is `[]`

`use_bash_launcher`

Boolean; default is `False`

 Use a bash launcher initializing the runfiles library


## sh\_library

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_shell/tree/v0.2.0/shell/private/sh_library.bzl)

```
sh_library(name, deps, srcs, data, aspect_hints, compatible_with, deprecation, exec_compatible_with, exec_group_compatible_with, exec_properties, features, package_metadata, restricted_to, tags, target_compatible_with, testonly, toolchains, visibility)
```

The main use for this rule is to aggregate together a logical
"library" consisting of related scripts—programs in an
interpreted language that does not require compilation or linking,
such as the Bourne shell—and any data those programs need at
run-time. Such "libraries" can then be used from
the `data` attribute of one or
more `sh_binary` rules.

You can use the [`filegroup`](/reference/be/general.html#filegroup) rule to aggregate data
files.

In interpreted programming languages, there's not always a clear
distinction between "code" and "data": after all, the program is
just "data" from the interpreter's point of view. For this reason
this rule has three attributes which are all essentially equivalent:
`srcs`, `deps` and `data`.
The current implementation does not distinguish between the elements of these lists.
All three attributes accept rules, source files and generated files.
It is however good practice to use the attributes for their usual purpose (as with other rules).

#### Examples

```
sh_library(
    name = "foo",
    data = [
        ":foo_service_script",  # an sh_binary with srcs
        ":deploy_foo",  # another sh_binary with srcs
    ],
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of "library" targets to be aggregated into this target.
See general comments about `deps`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical.deps).

This attribute should be used to list other `sh_library` rules that provide
interpreted program source code depended on by the code in `srcs`. The files
provided by these rules will be present among the `runfiles` of this target.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 The list of input files.

This attribute should be used to list shell script source files that belong to
this library. Scripts can load other scripts using the shell's `source`
or `.` command.

## sh\_test

[View rule sourceopen\_in\_new](https://github.com/bazelbuild/rules_shell/tree/v0.2.0/shell/private/sh_test.bzl)

```
sh_test(name, deps, srcs, data, args, aspect_hints, compatible_with, deprecation, env, env_inherit, exec_compatible_with, exec_group_compatible_with, exec_properties, features, flaky, local, package_metadata, restricted_to, shard_count, size, tags, target_compatible_with, testonly, timeout, toolchains, use_bash_launcher, visibility)
```

A `sh_test()` rule creates a test written as a Bourne shell script.

See the [attributes common to all test rules (\*\_test)](/reference/be/common-definitions#common-attributes-tests).

#### Examples

```
sh_test(
    name = "foo_integration_test",
    size = "small",
    srcs = ["foo_integration_test.sh"],
    deps = [":foo_sh_lib"],
    data = glob(["testdata/*.txt"]),
)

```

### Arguments

Attributes`name`

[Name](/concepts/labels#target-names); required

A unique name for this target.

`deps`

List of [labels](/concepts/labels); default is `[]`

 The list of "library" targets to be aggregated into this target.
See general comments about `deps`
at [Typical attributes defined by\
most build rules](/reference/be/common-definitions#typical.deps).

This attribute should be used to list other `sh_library` rules that provide
interpreted program source code depended on by the code in `srcs`. The files
provided by these rules will be present among the `runfiles` of this target.

`srcs`

List of [labels](/concepts/labels); default is `[]`

 The file containing the shell script.

This attribute must be a singleton list, whose element is the shell script.
This script must be executable, and may be a source file or a generated file.
All other files required at runtime (whether scripts or data) belong in the
`data` attribute.

`use_bash_launcher`

Boolean; default is `False`

 Use a bash launcher initializing the runfiles library

---

## Bazel Glossary
- URL: https://bazel.build/reference/glossary
- Source: reference/glossary.mdx
- Slug: /reference/glossary

### Action

A command to run during the build, for example, a call to a compiler that takes
[artifacts](#artifact) as inputs and produces other artifacts as outputs.
Includes metadata like the command line arguments, action key, environment
variables, and declared input/output artifacts.

**See also:** [Rules documentation](/extending/rules#actions)

### Action cache

An on-disk cache that stores a mapping of executed [actions](#action) to the
outputs they created. The cache key is known as the [action key](#action-key). A
core component for Bazel's incrementality model. The cache is stored in the
output base directory and thus survives Bazel server restarts.

### Action graph

An in-memory graph of [actions](#action) and the [artifacts](#artifact) that
these actions read and generate. The graph might include artifacts that exist as
source files (for example, in the file system) as well as generated
intermediate/final artifacts that are not mentioned in `BUILD` files. Produced
during the [analysis phase](#analysis-phase) and used during the [execution
phase](#execution-phase).

### Action graph query (aquery)

A [query](#query-concept) tool that can query over build [actions](#action).
This provides the ability to analyze how [build rules](#rule) translate into the
actual work builds do.

### Action key

The cache key of an [action](#action). Computed based on action metadata, which
might include the command to be executed in the action, compiler flags, library
locations, or system headers, depending on the action. Enables Bazel to cache or
invalidate individual actions deterministically.

### Analysis phase

The second phase of a build. Processes the [target graph](#target-graph)
specified in [`BUILD` files](#build-file) to produce an in-memory [action
graph](#action-graph) that determines the order of actions to run during the
[execution phase](#execution-phase). This is the phase in which rule
implementations are evaluated.

### Artifact

A source file or a generated file. Can also be a directory of files, known as
[tree artifacts](#tree-artifact).

An artifact may be an input to multiple actions, but must only be generated by
at most one action.

An artifact that corresponds to a [file target](#target) can be addressed by a
label.

### Aspect

A mechanism for rules to create additional [actions](#action) in their
dependencies. For example, if target A depends on B, one can apply an aspect on
A that traverses *up* a dependency edge to B, and runs additional actions in B
to generate and collect additional output files. These additional actions are
cached and reused between targets requiring the same aspect. Created with the
`aspect()` Starlark Build API function. Can be used, for example, to generate
metadata for IDEs, and create actions for linting.

**See also:** [Aspects documentation](/extending/aspects)

### Aspect-on-aspect

A composition mechanism whereby aspects can be applied to the results
of other aspects. For example, an aspect that generates information for use by
IDEs can be applied on top of an aspect that generates `.java` files from a
proto.

For an aspect `A` to apply on top of aspect `B`, the [providers](#provider) that
`B` advertises in its [`provides`](/rules/lib/globals#aspect.provides) attribute
must match what `A` declares it wants in its [`required_aspect_providers`](/rules/lib/globals#aspect.required_aspect_providers)
attribute.

### Attribute

A parameter to a [rule](#rule), used to express per-target build information.
Examples include `srcs`, `deps`, and `copts`, which respectively declare a
target's source files, dependencies, and custom compiler options. The particular
attributes available for a given target depend on its rule type.

### .bazelrc

Bazel’s configuration file used to change the default values for [startup
flags](#startup-flags) and [command flags](#command-flags), and to define common
groups of options that can then be set together on the Bazel command line using
a `--config` flag. Bazel can combine settings from multiple bazelrc files
(systemwide, per-workspace, per-user, or from a custom location), and a
`bazelrc` file may also import settings from other `bazelrc` files.

### Blaze

The Google-internal version of Bazel. Google’s main build system for its
mono-repository.

### BUILD File

A `BUILD` file is the main configuration file that tells Bazel what software
outputs to build, what their dependencies are, and how to build them. Bazel
takes a `BUILD` file as input and uses the file to create a graph of dependencies
and to derive the actions that must be completed to build intermediate and final
software outputs. A `BUILD` file marks a directory and any sub-directories not
containing a `BUILD` file as a [package](#package), and can contain
[targets](#target) created by [rules](#rule). The file can also be named
`BUILD.bazel`.

### BUILD.bazel File

See [`BUILD` File](#build-file). Takes precedence over a `BUILD` file in the same
directory.

### .bzl File

A file that defines rules, [macros](#macro), and constants written in
[Starlark](#starlark). These can then be imported into [`BUILD`
files](#build-file) using the `load()` function.

{/* TODO: ### Build event protocol */}

{/* TODO: ### Build flag */}

### Build graph

The dependency graph that Bazel constructs and traverses to perform a build.
Includes nodes like [targets](#target), [configured
targets](#configured-target), [actions](#action), and [artifacts](#artifact). A
build is considered complete when all [artifacts](#artifact) on which a set of
requested targets depend are verified as up-to-date.

### Build setting

A Starlark-defined piece of [configuration](#configuration).
[Transitions](#transition) can set build settings to change a subgraph's
configuration. If exposed to the user as a [command-line flag](#command-flags),
also known as a build flag.

### Clean build

A build that doesn't use the results of earlier builds. This is generally slower
than an [incremental build](#incremental-build) but commonly considered to be
more [correct](#correctness). Bazel guarantees both clean and incremental builds
are always correct.

### Client-server model

The `bazel` command-line client automatically starts a background server on the
local machine to execute Bazel [commands](#command). The server persists across
commands but automatically stops after a period of inactivity (or explicitly via
bazel shutdown). Splitting Bazel into a server and client helps amortize JVM
startup time and supports faster [incremental builds](#incremental-build)
because the [action graph](#action-graph) remains in memory across commands.

### Command

Used on the command line to invoke different Bazel functions, like `bazel
build`, `bazel test`, `bazel run`, and `bazel query`.

### Command flags

A set of flags specific to a [command](#command). Command flags are specified
*after* the command (`bazel build <command flags>`). Flags can be applicable to
one or more commands. For example, `--configure` is a flag exclusively for the
`bazel sync` command, but `--keep_going` is applicable to `sync`, `build`,
`test` and more. Flags are often used for [configuration](#configuration)
purposes, so changes in flag values can cause Bazel to invalidate in-memory
graphs and restart the [analysis phase](#analysis-phase).

### Configuration

Information outside of [rule](#rule) definitions that impacts how rules generate
[actions](#action). Every build has at least one configuration specifying the
target platform, action environment variables, and command-line [build
flags](#command-flags). [Transitions](#transition) may create additional
configurations, such as for host tools or cross-compilation.

**See also:** [Configurations](/extending/rules#configurations)

{/* TODO: ### Configuration fragment */}

### Configuration trimming

The process of only including the pieces of [configuration](#configuration) a
target actually needs. For example, if you build Java binary `//:j` with C++
dependency `//:c`, it's wasteful to include the value of `--javacopt` in the
configuration of `//:c` because changing `--javacopt` unnecessarily breaks C++
build cacheability.

### Configured query (cquery)

A [query](#query-concept) tool that queries over [configured
targets](#configured-target) (after the [analysis phase](#analysis-phase)
completes). This means `select()` and [build flags](#command-flags) (such as
`--platforms`) are accurately reflected in the results.

**See also:** [cquery documentation](/query/cquery)

### Configured target

The result of evaluating a [target](#target) with a
[configuration](#configuration). The [analysis phase](#analysis-phase) produces
this by combining the build's options with the targets that need to be built.
For example, if `//:foo` builds for two different architectures in the same
build, it has two configured targets: `<//:foo, x86>` and `<//:foo, arm>`.

### Correctness

A build is correct when its output faithfully reflects the state of its
transitive inputs. To achieve correct builds, Bazel strives to be
[hermetic](#hermeticity), reproducible, and making [build
analysis](#analysis-phase) and [action execution](#execution-phase)
deterministic.

### Dependency

A directed edge between two [targets](#target). A target `//:foo` has a *target
dependency* on target `//:bar` if `//:foo`'s attribute values contain a
reference to `//:bar`. `//:foo` has an *action dependency* on `//:bar` if an
action in `//:foo` depends on an input [artifact](#artifact) created by an
action in `//:bar`.

In certain contexts, it could also refer to an _external dependency_; see
[modules](#module).

### Depset

A data structure for collecting data on transitive dependencies. Optimized so
that merging depsets is time and space efficient, because it’s common to have
very large depsets (hundreds of thousands of files). Implemented to
recursively refer to other depsets for space efficiency reasons. [Rule](#rule)
implementations should not "flatten" depsets by converting them to lists unless
the rule is at the top level of the build graph. Flattening large depsets incurs
huge memory consumption. Also known as *nested sets* in Bazel's internal
implementation.

**See also:** [Depset documentation](/extending/depsets)

### Disk cache

A local on-disk blob store for the remote caching feature. Can be used in
conjunction with an actual remote blob store.

### Distdir

A read-only directory containing files that Bazel would otherwise fetch from the
internet using repository rules. Enables builds to run fully offline.

### Dynamic execution

An execution strategy that selects between local and remote execution based on
various heuristics, and uses the execution results of the faster successful
method. Certain [actions](#action) are executed faster locally (for example,
linking) and others are faster remotely (for example, highly parallelizable
compilation). A dynamic execution strategy can provide the best possible
incremental and clean build times.

### Execution phase

The third phase of a build. Executes the [actions](#action) in the [action
graph](#action-graph) created during the [analysis phase](#analysis-phase).
These actions invoke executables (compilers, scripts) to read and write
[artifacts](#artifact). *Spawn strategies* control how these actions are
executed: locally, remotely, dynamically, sandboxed, docker, and so on.

### Execution root

A directory in the [workspace](#workspace)’s [output base](#output-base)
directory where local [actions](#action) are executed in
non-[sandboxed](#sandboxing) builds. The directory contents are mostly symlinks
of input [artifacts](#artifact) from the workspace. The execution root also
contains symlinks to external repositories as other inputs and the `bazel-out`
directory to store outputs. Prepared during the [loading phase](#loading-phase)
by creating a *symlink forest* of the directories that represent the transitive
closure of packages on which a build depends. Accessible with `bazel info
execution_root` on the command line.

### File

See [Artifact](#artifact).

### Hermeticity

A build is hermetic if there are no external influences on its build and test
operations, which helps to make sure that results are deterministic and
[correct](#correctness). For example, hermetic builds typically disallow network
access to actions, restrict access to declared inputs, use fixed timestamps and
timezones, restrict access to environment variables, and use fixed seeds for
random number generators

### Incremental build

An incremental build reuses the results of earlier builds to reduce build time
and resource usage. Dependency checking and caching aim to produce correct
results for this type of build. An incremental build is the opposite of a clean
build.

{/* TODO: ### Install base */}

### Label

An identifier for a [target](#target). Generally has the form
`@repo//path/to/package:target`, where `repo` is the (apparent) name of the
[repository](#repository) containing the target, `path/to/package` is the path
to the directory that contains the [`BUILD` file](#build-file) declaring the
target (this directory is also known as the [package](#package)), and `target`
is the name of the target itself. Depending on the situation, parts of this
syntax may be omitted.

**See also**: [Labels](/concepts/labels)

### Loading phase

The first phase of a build where Bazel executes [`BUILD` files](#build-file) to
create [packages](#package). [Macros](#macro) and certain functions like
`glob()` are evaluated in this phase. Interleaved with the second phase of the
build, the [analysis phase](#analysis-phase), to build up a [target
graph](#target-graph).

### Legacy macro

A flavor of [macro](#macro) which is declared as an ordinary
[Starlark](#starlark) function, and which runs as a side effect of executing a
`BUILD` file.

Legacy macros can do anything a function can. This means they can be convenient,
but they can also be harder to read, write, and use. A legacy macro might
unexpectedly mutate its arguments or fail when given a `select()` or ill-typed
argument.

Contrast with [symbolic macros](#symbolic-macro).

**See also:** [Legacy macro documentation](/extending/legacy-macros)

### Macro

A mechanism to compose multiple [rule](#rule) target declarations together under
a single [Starlark](#starlark) callable. Enables reusing common rule declaration
patterns across `BUILD` files. Expanded to the underlying rule target
declarations during the [loading phase](#loading-phase).

Comes in two flavors: [symbolic macros](#symbolic-macro) (since Bazel 8) and
[legacy macros](#legacy-macro).

### Mnemonic

A short, human-readable string selected by a rule author to quickly understand
what an [action](#action) in the rule is doing. Mnemonics can be used as
identifiers for *spawn strategy* selections. Some examples of action mnemonics
are `Javac` from Java rules, `CppCompile` from C++ rules, and
`AndroidManifestMerger` from Android rules.

### Module

A Bazel project that can have multiple versions, each of which can have
dependencies on other modules. This is analogous to familiar concepts in other
dependency management systems, such as a Maven _artifact_, an npm _package_, a
Go _module_, or a Cargo _crate_. Modules form the backbone of Bazel's external
dependency management system.

Each module is backed by a [repo](#repository) with a `MODULE.bazel` file at its
root. This file contains metadata about the module itself (such as its name and
version), its direct dependencies, and various other data including toolchain
registrations and [module extension](#module-extension) input.

Module metadata is hosted in Bazel registries.

**See also:** [Bazel modules](/external/module)

### Module Extension

A piece of logic that can be run to generate [repos](#repository) by reading
inputs from across the [module](#module) dependency graph and invoking [repo
rules](#repository-rule). Module extensions have capabilities similar to repo
rules, allowing them to access the internet, perform file I/O, and so on.

**See also:** [Module extensions](/external/extension)

### Native rules

[Rules](#rule) that are built into Bazel and implemented in Java. Such rules
appear in [`.bzl` files](#bzl-file) as functions in the native module (for
example, `native.cc_library` or `native.java_library`). User-defined rules
(non-native) are created using [Starlark](#starlark).

### Output base

A [workspace](#workspace)-specific directory to store Bazel output files. Used
to separate outputs from the *workspace*'s source tree (the [main
repo](#repository)). Located in the [output user root](#output-user-root).

### Output groups

A group of files that is expected to be built when Bazel finishes building a
target. [Rules](#rule) put their usual outputs in the "default output group"
(e.g the `.jar` file of a `java_library`, `.a` and `.so` for `cc_library`
targets). The default output group is the output group whose
[artifacts](#artifact) are built when a target is requested on the command line.
Rules can define more named output groups that can be explicitly specified in
[`BUILD` files](#build-file) (`filegroup` rule) or the command line
(`--output_groups` flag).

### Output user root

A user-specific directory to store Bazel's outputs. The directory name is
derived from the user's system username. Prevents output file collisions if
multiple users are building the same project on the system at the same time.
Contains subdirectories corresponding to build outputs of individual workspaces,
also known as [output bases](#output-base).

### Package

The set of [targets](#target) defined by a [`BUILD` file](#build-file). A
package's name is the `BUILD` file's path relative to the [repo](#repository)
root. A package can contain subpackages, or subdirectories containing `BUILD`
files, thus forming a package hierarchy.

### Package group

A [target](#target) representing a set of packages. Often used in `visibility`
attribute values.

### Platform

A "machine type" involved in a build. This includes the machine Bazel runs on
(the "host" platform), the machines build tools execute on ("exec" platforms),
and the machines targets are built for ("target platforms").

### Provider

A schema describing a unit of information to pass between
[rule targets](#rule-target) along dependency relationships. Typically this
contains information like compiler options, transitive source or output files,
and build metadata. Frequently used in conjunction with [depsets](#depset) to
efficiently store accumulated transitive data. An example of a built-in provider
is `DefaultInfo`.

Note: The object holding specific data for a given rule target is
referred to as a "provider instance", although sometimes this is conflated with
"provider".

**See also:** [Provider documentation](/extending/rules#providers)

### Query (concept)

The process of analyzing a [build graph](#build-graph) to understand
[target](#target) properties and dependency structures. Bazel supports three
query variants: [query](#query-command), [cquery](#configured-query), and
[aquery](#action-graph-query).

### query (command)

A [query](#query-concept) tool that operates over the build's post-[loading
phase](#loading-phase) [target graph](#target-graph). This is relatively fast,
but can't analyze the effects of `select()`, [build flags](#command-flags),
[artifacts](#artifact), or build [actions](#action).

**See also:** [Query how-to](/query/guide), [Query reference](/query/language)

### Repository

A directory tree with a boundary marker file at its root, containing source
files that can be used in a Bazel build. Often shortened to just **repo**.

A repo boundary marker file can be `MODULE.bazel` (signaling that this repo
represents a Bazel module), `REPO.bazel`, or in legacy contexts, `WORKSPACE` or
`WORKSPACE.bazel`. Any repo boundary marker file will signify the boundary of a
repo; multiple such files can coexist in a directory.

The *main repo* is the repo in which the current Bazel command is being run.

*External repos* are defined by specifying [modules](#module) in `MODULE.bazel`
files, or invoking [repo rules](#repository-rule) in [module
extensions](#module-extension). They can be fetched on demand to a predetermined
"magical" location on disk.

Each repo has a unique, constant *canonical* name, and potentially different
*apparent* names when viewed from other repos.

**See also**: [External dependencies overview](/external/overview)

### Repository cache

A shared content-addressable cache of files downloaded by Bazel for builds,
shareable across [workspaces](#workspace). Enables offline builds after the
initial download. Commonly used to cache files downloaded through [repository
rules](#repository-rule) like `http_archive` and repository rule APIs like
`repository_ctx.download`. Files are cached only if their SHA-256 checksums are
specified for the download.

### Repository rule

A schema for repository definitions that tells Bazel how to materialize (or
"fetch") a [repository](#repository). Often shortened to just **repo rule**.
Repo rules are invoked by Bazel internally to define repos backed by
[modules](#module), or can be invoked by [module extensions](#module-extension).
Repo rules can access the internet or perform file I/O; the most common repo
rule is `http_archive` to download an archive containing source files from the
internet.

**See also:** [Repo rule documentation](/external/repo)

### Reproducibility

The property of a build or test that a set of inputs to the build or test will
always produce the same set of outputs every time, regardless of time, method,
or environment. Note that this does not necessarily imply that the outputs are
[correct](#correctness) or the desired outputs.

### Rule

A schema for defining [rule targets](#rule-target) in a `BUILD` file, such as
`cc_library`. From the perspective of a `BUILD` file author, a rule consists of
a set of [attributes](#attributes) and black box logic. The logic tells the
rule target how to produce output [artifacts](#artifact) and pass information to
other rule targets. From the perspective of `.bzl` authors, rules are the
primary way to extend Bazel to support new programming languages and
environments.

Rules are instantiated to produce rule targets in the
[loading phase](#loading-phase). In the [analysis phase](#analysis-phase) rule
targets communicate information to their downstream dependencies in the form of
[providers](#provider), and register [actions](#action) describing how to
generate their output artifacts. These actions are run in the [execution
phase](#execution-phase).

Note: Historically the term "rule" has been used to refer to a rule target.
This usage was inherited from tools like Make, but causes confusion and should
be avoided for Bazel.

**See also:** [Rules documentation](/extending/rules)

### Rule target

A [target](#target) that is an instance of a rule. Contrasts with file targets
and package groups. Not to be confused with [rule](#rule).

### Runfiles

The runtime dependencies of an executable [target](#target). Most commonly, the
executable is the executable output of a test rule, and the runfiles are runtime
data dependencies of the test. Before the invocation of the executable (during
bazel test), Bazel prepares the tree of runfiles alongside the test executable
according to their source directory structure.

**See also:** [Runfiles documentation](/extending/rules#runfiles)

### Sandboxing

A technique to isolate a running [action](#action) inside a restricted and
temporary [execution root](#execution-root), helping to ensure that it doesn’t
read undeclared inputs or write undeclared outputs. Sandboxing greatly improves
[hermeticity](#hermeticity), but usually has a performance cost, and requires
support from the operating system. The performance cost depends on the platform.
On Linux, it's not significant, but on macOS it can make sandboxing unusable.

### Skyframe

[Skyframe](/reference/skyframe) is the core parallel, functional, and incremental evaluation framework of Bazel.

{/* TODO: ### Spawn strategy */}

### Stamping

A feature to embed additional information into Bazel-built
[artifacts](#artifact). For example, this can be used for source control, build
time and other workspace or environment-related information for release builds.
Enable through the `--workspace_status_command` flag and [rules](/extending/rules) that
support the stamp attribute.

### Starlark

The extension language for writing [rules](/extending/rules) and [macros](#macro). A
restricted subset of Python (syntactically and grammatically) aimed for the
purpose of configuration, and for better performance. Uses the [`.bzl`
file](#bzl-file) extension. [`BUILD` files](#build-file) use an even more
restricted version of Starlark (such as no `def` function definitions), formerly
known as Skylark.

**See also:** [Starlark language documentation](/rules/language)

{/* TODO: ### Starlark rules */}

{/* TODO: ### Starlark rule sandwich */}

### Startup flags

The set of flags specified between `bazel` and the [command](#query-command),
for example, bazel `--host_jvm_debug` build. These flags modify the
[configuration](#configuration) of the Bazel server, so any modification to
startup flags causes a server restart. Startup flags are not specific to any
command.

### Symbolic macro

A flavor of [macro](#macro) which is declared with a [rule](#rule)-like
[attribute](#attribute) schema, allows hiding internal declared
[targets](#target) from their own package, and enforces a predictable naming
pattern on the targets that the macro declares. Designed to avoid some of the
problems seen in large [legacy macro](#legacy-macro) codebases.

**See also:** [Symbolic macro documentation](/extending/macros)

### Target

An object that is defined in a [`BUILD` file](#build-file) and identified by a
[label](#label). Targets represent the buildable units of a workspace from
the perspective of the end user.

A target that is declared by instantiating a [rule](#rule) is called a [rule
target](#rule-target). Depending on the rule, these may be runnable (like
`cc_binary`) or testable (like `cc_test`). Rule targets typically depend on
other targets via their [attributes](#attribute) (such as `deps`); these
dependencies form the basis of the [target graph](#target-graph).

Aside from rule targets, there are also file targets and [package group](#package-group)
targets. File targets correspond to [artifacts](#artifact) that are referenced
within a `BUILD` file. As a special case, the `BUILD` file of any package is
always considered a source file target in that package.

Targets are discovered during the [loading phase](#loading-phase). During the
[analysis phase](#analysis-phase), targets are associated with [build
configurations](#configuration) to form [configured
targets](#configured-target).

### Target graph

An in-memory graph of [targets](#target) and their dependencies. Produced during
the [loading phase](#loading-phase) and used as an input to the [analysis
phase](#analysis-phase).

### Target pattern

A way to specify a group of [targets](#target) on the command line. Commonly
used patterns are `:all` (all rule targets), `:*` (all rule + file targets),
`...` (current [package](#package) and all subpackages recursively). Can be used
in combination, for example, `//...:*` means all rule and file targets in all
packages recursively from the root of the [workspace](#workspace).

### Tests

Rule [targets](#target) instantiated from test rules, and therefore contains a
test executable. A return code of zero from the completion of the executable
indicates test success. The exact contract between Bazel and tests (such as test
environment variables, test result collection methods) is specified in the [Test
Encyclopedia](/reference/test-encyclopedia).

### Toolchain

A set of tools to build outputs for a language. Typically, a toolchain includes
compilers, linkers, interpreters or/and linters. A toolchain can also vary by
platform, that is, a Unix compiler toolchain's components may differ for the
Windows variant, even though the toolchain is for the same language. Selecting
the right toolchain for the platform is known as toolchain resolution.

### Top-level target

A build [target](#target) is top-level if it’s requested on the Bazel command
line. For example, if `//:foo` depends on `//:bar`, and `bazel build //:foo` is
called, then for this build, `//:foo` is top-level, and `//:bar` isn’t
top-level, although both targets will need to be built. An important difference
between top-level and non-top-level targets is that [command
flags](#command-flags) set on the Bazel command line (or via
[.bazelrc](#bazelrc)) will set the [configuration](#configuration) for top-level
targets, but might be modified by a [transition](#transition) for non-top-level
targets.

### Transition

A mapping of [configuration](#configuration) state from one value to another.
Enables [targets](#target) in the [build graph](#build-graph) to have different
configurations, even if they were instantiated from the same [rule](#rule). A
common usage of transitions is with *split* transitions, where certain parts of
the [target graph](#target-graph) is forked with distinct configurations for
each fork. For example, one can build an Android APK with native binaries
compiled for ARM and x86 using split transitions in a single build.

**See also:** [User-defined transitions](/extending/config#user-defined-transitions)

### Tree artifact

An [artifact](#artifact) that represents a collection of files. Since these
files are not themselves artifacts, an [action](#action) operating on them must
instead register the tree artifact as its input or output.

### Visibility

One of two mechanisms for preventing unwanted dependencies in the build system:
*target visibility* for controlling whether a [target](#target) can be depended
upon by other targets; and *load visibility* for controlling whether a `BUILD`
or `.bzl` file may load a given `.bzl` file. Without context, usually
"visibility" refers to target visibility.

**See also:** [Visibility documentation](/concepts/visibility)

### Workspace

The environment shared by all Bazel commands run from the same [main
repository](#repository).

Note that historically the concepts of "repository" and "workspace" have been
conflated; the term "workspace" has often been used to refer to the main
repository, and sometimes even used as a synonym of "repository". Such usage
should be avoided for clarity.

---

## Skyframe
- URL: https://bazel.build/reference/skyframe
- Source: reference/skyframe.mdx
- Slug: /reference/skyframe

The parallel evaluation and incrementality model of Bazel.

## Data model

The data model consists of the following items:

*   `SkyValue`. Also called nodes. `SkyValues` are immutable objects that
    contain all the data built over the course of the build and the inputs of
    the build. Examples are: input files, output files, targets and configured
    targets.
*   `SkyKey`. A short immutable name to reference a `SkyValue`, for example,
    `FILECONTENTS:/tmp/foo` or `PACKAGE://foo`.
*   `SkyFunction`. Builds nodes based on their keys and dependent nodes.
*   Node graph. A data structure containing the dependency relationship between
    nodes.
*   `Skyframe`. Code name for the incremental evaluation framework Bazel is
    based on.

## Evaluation

A build is achieved by evaluating the node that represents the build request.

First, Bazel finds the `SkyFunction` corresponding to the key of the top-level
`SkyKey`. The function then requests the evaluation of the nodes it needs to
evaluate the top-level node, which in turn result in other `SkyFunction` calls,
until the leaf nodes are reached. Leaf nodes are usually ones that represent
input files in the file system. Finally, Bazel ends up with the value of the
top-level `SkyValue`, some side effects (such as output files in the file
system) and a directed acyclic graph of the dependencies between the nodes
involved in the build.

A `SkyFunction` can request `SkyKeys` in multiple passes if it cannot tell in
advance all of the nodes it needs to do its job. A simple example is evaluating
an input file node that turns out to be a symlink: the function tries to read
the file, realizes that it is a symlink, and thus fetches the file system node
representing the target of the symlink. But that itself can be a symlink, in
which case the original function will need to fetch its target, too.

The functions are represented in the code by the interface `SkyFunction` and the
services provided to it by an interface called `SkyFunction.Environment`. These
are the things functions can do:

*   Request the evaluation of another node by way of calling `env.getValue`. If
    the node is available, its value is returned, otherwise, `null` is returned
    and the function itself is expected to return `null`. In the latter case,
    the dependent node is evaluated, and then the original node builder is
    invoked again, but this time the same `env.getValue` call will return a
    non-`null` value.
*   Request the evaluation of multiple other nodes by calling `env.getValues()`.
    This does essentially the same, except that the dependent nodes are
    evaluated in parallel.
*   Do computation during their invocation
*   Have side effects, for example, writing files to the file system. Care needs
    to be taken that two different functions avoid stepping on each other's
    toes. In general, write side effects (where data flows outwards from Bazel)
    are okay, read side effects (where data flows inwards into Bazel without a
    registered dependency) are not, because they are an unregistered dependency
    and as such, can cause incorrect incremental builds.

Well-behaved `SkyFunction` implementations avoid accessing data in any other way
than requesting dependencies (such as by directly reading the file system),
because that results in Bazel not registering the data dependency on the file
that was read, thus resulting in incorrect incremental builds.

Once a function has enough data to do its job, it should return a non-`null`
value indicating completion.

This evaluation strategy has a number of benefits:

*   Hermeticity. If functions only request input data by way of depending on
    other nodes, Bazel can guarantee that if the input state is the same, the
    same data is returned. If all sky functions are deterministic, this means
    that the whole build will also be deterministic.
*   Correct and perfect incrementality. If all the input data of all functions
    is recorded, Bazel can invalidate only the exact set of nodes that need to
    be invalidated when the input data changes.
*   Parallelism. Since functions can only interact with each other by way of
    requesting dependencies, functions that don't depend on each other can be
    run in parallel and Bazel can guarantee that the result is the same as if
    they were run sequentially.

## Incrementality

Since functions can only access input data by depending on other nodes, Bazel
can build up a complete data flow graph from the input files to the output
files, and use this information to only rebuild those nodes that actually need
to be rebuilt: the reverse transitive closure of the set of changed input files.

In particular, two possible incrementality strategies exist: the bottom-up one
and the top-down one. Which one is optimal depends on how the dependency graph
looks like.

*   During bottom-up invalidation, after a graph is built and the set of changed
    inputs is known, all the nodes are invalidated that transitively depend on
    changed files. This is optimal if the same top-level node will be built
    again. Note that bottom-up invalidation requires running `stat()` on all
    input files of the previous build to determine if they were changed. This
    can be improved by using `inotify` or a similar mechanism to learn about
    changed files.

*   During top-down invalidation, the transitive closure of the top-level node
    is checked and only those nodes are kept whose transitive closure is clean.
    This is better if the node graph is large, but the next build only needs a
    small subset of it: bottom-up invalidation would invalidate the larger graph
    of the first build, unlike top-down invalidation, which just walks the small
    graph of second build.

Bazel only does bottom-up invalidation.

To get further incrementality, Bazel uses _change pruning_: if a node is
invalidated, but upon rebuild, it is discovered that its new value is the same
as its old value, the nodes that were invalidated due to a change in this node
are "resurrected".

This is useful, for example, if one changes a comment in a C++ file: then the
`.o` file generated from it will be the same, thus, it is unnecessary to call
the linker again.

## Incremental Linking / Compilation

The main limitation of this model is that the invalidation of a node is an
all-or-nothing affair: when a dependency changes, the dependent node is always
rebuilt from scratch, even if a better algorithm would exist that would mutate
the old value of the node based on the changes. A few examples where this would
be useful:

*   Incremental linking
*   When a single class file changes in a JAR file, it is possible
    modify the JAR file in-place instead of building it from scratch again.

The reason why Bazel does not support these things in a principled way
is twofold:

*   There were limited performance gains.
*   Difficulty to validate that the result of the mutation is the same as that
    of a clean rebuild would be, and Google values builds that are bit-for-bit
    repeatable.

Until now, it was possible to achieve good enough performance by decomposing an
expensive build step and achieving partial re-evaluation that way. For example,
in an Android app, you can split all the classes into multiple groups and dex
them separately. This way, if classes in a group are unchanged, the dexing does
not have to be redone.

## Mapping to Bazel concepts

This is high level summary of the key `SkyFunction` and `SkyValue`
implementations Bazel uses to perform a build:

*   **FileStateValue**. The result of an `lstat()`. For existent files, the
    function also computes additional information in order to detect changes to
    the file. This is the lowest level node in the Skyframe graph and has no
    dependencies.
*   **FileValue**. Used by anything that cares about the actual contents or
    resolved path of a file. Depends on the corresponding `FileStateValue` and
    any symlinks that need to be resolved (such as the `FileValue` for `a/b`
    needs the resolved path of `a` and the resolved path of `a/b`). The
    distinction between `FileValue` and `FileStateValue` is important because
    the latter can be used in cases where the contents of the file are not
    actually needed. For example, the file contents are irrelevant when
    evaluating file system globs (such as `srcs=glob(["*/*.java"])`).
*   **DirectoryListingStateValue**. The result of `readdir()`. Like
    `FileStateValue`, this is the lowest level node and has no dependencies.
*   **DirectoryListingValue**. Used by anything that cares about the entries of
    a directory. Depends on the corresponding `DirectoryListingStateValue`, as
    well as the associated `FileValue` of the directory.
*   **PackageValue**. Represents the parsed version of a BUILD file. Depends on
    the `FileValue` of the associated `BUILD` file, and also transitively on any
    `DirectoryListingValue` that is used to resolve the globs in the package
    (the data structure representing the contents of a `BUILD` file internally).
*   **ConfiguredTargetValue**. Represents a configured target, which is a tuple
    of the set of actions generated during the analysis of a target and
    information provided to dependent configured targets. Depends on the
    `PackageValue` the corresponding target is in, the `ConfiguredTargetValues`
    of direct dependencies, and a special node representing the build
    configuration.
*   **ArtifactValue**. Represents a file in the build, be it a source or an
    output artifact. Artifacts are almost equivalent to files, and are used to
    refer to files during the actual execution of build steps. Source files
    depends on the `FileValue` of the associated node, and output artifacts
    depend on the `ActionExecutionValue` of whatever action generates the
    artifact.
*   **ActionExecutionValue**. Represents the execution of an action. Depends on
    the `ArtifactValues` of its input files. The action it executes is contained
    within its SkyKey, which is contrary to the concept that SkyKeys should be
    small. Note that `ActionExecutionValue` and `ArtifactValue` are unused if
    the execution phase does not run.

As a visual aid, this diagram shows the relationships between
SkyFunction implementations after a build of Bazel itself:

![A graph of SkyFunction implementation relationships](/reference/skyframe.png)

---

## Release Model
- URL: https://bazel.build/release
- Source: release/index.mdx
- Slug: /release

As announced in [the original blog
post](https://blog.bazel.build/2020/11/10/long-term-support-release.html), Bazel
4.0 and higher versions provides support for two release tracks: rolling
releases and long term support (LTS) releases. This page covers the latest
information about Bazel's release model.

## Support matrix

| LTS release | Support stage | Latest version | End of support |
| ----------- | ------------- | -------------- | -------------- |
| Bazel 9 | Rolling| [Check rolling release page](/release/rolling) | N/A |
| Bazel 8 | Active| [8.4.2](https://github.com/bazelbuild/bazel/releases/tag/8.4.2) | December 2027 |
| Bazel 7 | Maintenance| [7.7.0](https://github.com/bazelbuild/bazel/releases/tag/7.7.0) | Dec 2026 |
| Bazel 6 | Maintenance | [6.5.0](https://github.com/bazelbuild/bazel/releases/tag/6.5.0) | Dec 2025 |
| Bazel 5 | Deprecated | [5.4.1](https://github.com/bazelbuild/bazel/releases/tag/5.4.1) | Jan 2025 |
| Bazel 4 | Deprecated | [4.2.4](https://github.com/bazelbuild/bazel/releases/tag/4.2.4) | Jan 2024 |

All Bazel LTS releases can be found on the [release
page](https://github.com/bazelbuild/bazel/releases) on GitHub.

Note: Bazel version older than Bazel 5 are no longer supported, Bazel users are
recommended to upgrade to the latest LTS release or use rolling releases if you
want to keep up with the latest changes at HEAD.

## Release versioning

Bazel uses a _major.minor.patch_ [Semantic
Versioning](https://semver.org/) scheme.

*   A _major release_ contains features that are not backward compatible with
    the previous release. Each major Bazel version is an LTS release.
*   A _minor release_ contains backward-compatible bug fixes and features
    back-ported from the main branch.
*   A _patch release_ contains critical bug fixes.

Additionally, pre-release versions are indicated by appending a hyphen and a
date suffix to the next major version number.

For example, a new release of each type would result in these version numbers:

*   Major: 6.0.0
*   Minor: 6.1.0
*   Patch: 6.1.2
*   Pre-release: 7.0.0-pre.20230502.1

## Support stages

For each major Bazel version, there are four support stages:

*   **Rolling**: This major version is still in pre-release, the Bazel team
    publishes rolling releases from HEAD.
*   **Active**: This major version is the current active LTS release. The Bazel
  team backports important features and bug fixes into its minor releases.
*   **Maintenance**: This major version is an old LTS release in maintenance
    mode. The Bazel team only promises to backport critical bug fixes for
    security issues and OS-compatibility issues into this LTS release.
*   **Deprecated**: The Bazel team no longer provides support for this major
    version, all users should migrate to newer Bazel LTS releases.

## Release cadence

Bazel regularly publish releases for two release tracks.

### Rolling releases

*   Rolling releases are coordinated with Google Blaze release and are released
  from HEAD around every two weeks. It is a preview of the next Bazel LTS
    release.
*   Rolling releases can ship incompatible changes. Incompatible flags are
    recommended for major breaking changes, rolling out incompatible changes
    should follow our [backward compatibility
    policy](/release/backward-compatibility).

### LTS releases

*   _Major release_: A new LTS release is expected to be cut from HEAD roughly
    every
    12 months. Once a new LTS release is out, it immediately enters the Active
    stage, and the previous LTS release enters the Maintenance stage.
*   _Minor release_: New minor verions on the Active LTS track are expected to
    be released once every 2 months.
*   _Patch release_: New patch versions for LTS releases in Active and
    Maintenance stages are expected to be released on demand for critical bug
    fixes.
*   A Bazel LTS release enters the Deprecated stage after being in ​​the
    Maintenance stage for 2 years.

For planned releases, please check our [release
issues](https://github.com/bazelbuild/bazel/issues?q=is%3Aopen+is%3Aissue+label%3Arelease)
on Github.

## Release procedure & policies

For rolling releases, the process is straightforward: about every two weeks, a
new release is created, aligning with the same baseline as the Google internal
Blaze release. Due to the rapid release schedule, we don't backport any changes
to rolling releases.

For LTS releases, the procedure and policies below are followed:

1.  Determine a baseline commit for the release.
    *   For a new major LTS release, the baseline commit is the HEAD of the main
        branch.
    *   For a minor or patch release, the baseline commit is the HEAD of the
        current latest version of the same LTS release.
1.  Create a release branch in the name of `release-<version>` from the baseline
    commit.
1.  Backport changes via PRs to the release branch.
    *   The community can suggest certain commits to be back-ported by replying
   "`@bazel-io flag`" on relevant GitHub issues or PRs to mark them as potential
        release blockers, the Bazel team triages them and decide whether to
        back-port the commits.
    *   Only backward-compatible commits on the main branch can be back-ported,
   additional minor changes to resolve merge conflicts are acceptable.
1.  Backport changes using Cherry-Pick Request Issue for Bazel maintainers.
    *   Bazel maintainers can request to cherry-pick specific commit(s)
        to a release branch. This process is initiated by creating a
        cherry-pick request on GitHub. Here's how to do it.
        1.  Open the [cherry-pick request](https://github.com/bazelbuild/bazel/issues/new?assignees=&labels=&projects=&template=cherry_pick_request.yml)
        2.  Fill in the request details
            *   Title: Provide a concise and descriptive title for the request.
            *   Commit ID(s): Enter the ID(s) of the commit(s) you want to
                cherry-pick. If there are multiple commits, then separate
                them with commas.
            *   Category: Specify the category of the request.
            *   Reviewer(s): For multiple reviewers, separate their GitHub
                ID's with commas.
        3.  Set the milestone
            *   Find the "Milestone" section and click the setting.
            *   Select the appropriate X.Y.Z release blockers. This action
                triggers the cherry-pick bot to process your request
                for the "release-X.Y.Z" branch.
        4.  Submit the Issue
            *   Once all details are filled in and the miestone is set,
                submit the issue.

    *   The cherry-pick bot will process the request and notify
        if the commit(s) are eligible for cherry-picking. If
        the commits are cherry-pickable, which means there's no
        merge conflict while cherry-picking the commit, then
        the bot will create a new pull request. When the pull
        request is approved by a member of the Bazel team, the
        commits are cherry-picked and merged to the release branch.
        For a visual example of a completed cherry-pick request,
        refer to this
        [example](https://github.com/bazelbuild/bazel/issues/20230)
        .

1.  Identify release blockers and fix issues found on the release branch.
    *   The release branch is tested with the same test suite in
        [postsubmit](https://buildkite.com/bazel/bazel-bazel) and
        [downstream test pipeline]
        (https://buildkite.com/bazel/bazel-at-head-plus-downstream)
        on Bazel CI. The Bazel team monitors testing results of the release
        branch and fixes any regressions found.
1.  Create a new release candidate from the release branch when all known
    release blockers are resolved.
    *   The release candidate is announced on
        [bazel-discuss](https://groups.google.com/g/bazel-discuss),
        the Bazel team monitors community bug reports for the candidate.
    *   If new release blockers are identified, go back to the last step and
        create a new release candidate after resolving all the issues.
    *   New features are not allowed to be added to the release branch after the
        first release candidate is created; cherry-picks are limited to critical
        fixes only. If a cherry-pick is needed, the requester must answer the
        following questions: Why is this change critical, and what benefits does
        it provide? What is the likelihood of this change introducing a
        regression?
1.  Push the release candidate as the official release if no further release
    blockers are found
    *   For patch releases, push the release at least two business days after
        the last release candidate is out.
    *   For major and minor releases, push the release two business days after
        the last release candidate is out, but not earlier than one week after
        the first release candidate is out.
    *   The release is only pushed on a day where the next day is a business
        day.
    *   The release is announced on
        [bazel-discuss](https://groups.google.com/g/bazel-discuss),
        the Bazel team monitors and addresses community bug reports for the new
     release.

## Report regressions

If a user finds a regression in a new Bazel release, release candidate or even
Bazel at HEAD, please file a bug on
[GitHub](https://github.com/bazelbuild/bazel/issues). You can use
Bazelisk to bisect the culprit commit and include this information in the bug
report.

For example, if your build succeeds with Bazel 6.1.0 but fails with the second
release candidate of 6.2.0, you can do bisect via

```bash
bazelisk --bisect=6.1.0..release-6.2.0rc2 build //foo:bar
```

You can set `BAZELISK_SHUTDOWN` or `BAZELISK_CLEAN` environment variable to run
corresponding bazel commands to reset the build state if it's needed to
reproduce the issue. For more details, check out documentation about Bazelisk
[bisect feature] (https://github.com/bazelbuild/bazelisk#--bisect).

Remember to upgrade Bazelisk to the latest version to use the bisect
feature.

## Rule compatibility

If you are a rule authors and want to maintain compatibility with different
Bazel versions, please check out the [Rule
Compatibility](/release/rule-compatibility) page.

---

## Backward Compatibility
- URL: https://bazel.build/release/backward-compatibility
- Source: release/backward-compatibility.mdx
- Slug: /release/backward-compatibility

This page provides information about how to handle backward compatibility,
including migrating from one release to another and how to communicate
incompatible changes.

Bazel is evolving. Minor versions released as part of an [LTS major
version](/release#bazel-versioning) are fully backward-compatible. New major LTS
releases may contain incompatible changes that require some migration effort.
For more information about Bazel's release model, please check out the [Release
Model](/release) page.

## Summary

1.  It is recommended to use `--incompatible_*` flags for breaking changes.
1.  For every `--incompatible_*` flag, a GitHub issue explains the change in
    behavior and aims to provide a migration recipe.
1.  Incompatible flags are recommended to be back-ported to the latest LTS
    release without enabling the flag by default.
1.  APIs and behavior guarded by an `--experimental_*` flag can change at any
    time.
1.  Never run production builds with `--experimental_*` or `--incompatible_*`
    flags.

## How to follow this policy

*   [For Bazel users - how to update Bazel](/install/bazelisk)
*   [For contributors - best practices for incompatible changes](/contribute/breaking-changes)
*   [For release managers - how to update issue labels and release](https://github.com/bazelbuild/continuous-integration/tree/master/docs/release-playbook.%6D%64)

## What is stable functionality?

In general, APIs or behaviors without `--experimental_...` flags are considered
stable, supported features in Bazel.

This includes:

*   Starlark language and APIs
*   Rules bundled with Bazel
*   Bazel APIs such as Remote Execution APIs or Build Event Protocol
*   Flags and their semantics

## Incompatible changes and migration recipes

For every incompatible change in a new release, the Bazel team aims to provide a
_migration recipe_ that helps you update your code (`BUILD` and `.bzl` files, as
well as any Bazel usage in scripts, usage of Bazel API, and so on).

Incompatible changes should have an associated `--incompatible_*` flag and a
corresponding GitHub issue.

The incompatible flag and relevant changes are recommended to be back-ported to
the latest LTS release without enabling the flag by default. This allows users
to migrate for the incompatible changes before the next LTS release is
available.

## Communicating incompatible changes

The primary source of information about incompatible changes are GitHub issues
marked with an ["incompatible-change"
label](https://github.com/bazelbuild/bazel/issues?q=label%3Aincompatible-change).

For every incompatible change, the issue specifies the following:

*   Name of the flag controlling the incompatible change
*   Description of the changed functionality
*   Migration recipe

When an incompatible change is ready for migration with Bazel at HEAD
(therefore, also with the next Bazel rolling release), it should be marked with
the `migration-ready` label. The incompatible change issue is closed when the
incompatible flag is flipped at HEAD.

---

## Rolling Releases
- URL: https://bazel.build/release/rolling
- Source: release/rolling.mdx
- Slug: /release/rolling

This page contains an overview of all rolling releases, as per our
[release policy](https://bazel.build/release#rolling-releases).
[Bazelisk](https://github.com/bazelbuild/bazelisk) is the best way to use
these releases.

## Index 

<iframe src="https://releases.bazel.build/rolling.html" style="height: 3000px; width: 100%" ></iframe>

---

## Rule Compatibility
- URL: https://bazel.build/release/rule-compatibility
- Source: release/rule-compatibility.mdx
- Slug: /release/rule-compatibility

Bazel Starlark rules can break compatibility with Bazel LTS releases in the
following two scenarios:

1.  The rule breaks compatibility with future LTS releases because a feature it
    depends on is removed from Bazel at HEAD.
1.  The rule breaks compatibility with the current or older LTS releases because
    a feature it depends on is only available in newer Bazel LTS releases.

Meanwhile, the rule itself can ship incompatible changes for their users as
well. When combined with breaking changes in Bazel, upgrading the rule version
and Bazel version can often be a source of frustration for Bazel users. This
page covers how rules authors should maintain rule compatibility with Bazel to
make it easier for users to upgrade Bazel and rules.

## Manageable migration process

While it's obviously not feasible to guarantee compatibility between every
version of Bazel and every version of the rule, our aim is to ensure that the
migration process remains manageable for Bazel users. A manageable migration
process is defined as a process where **users are not forced to upgrade the
rule's major version and Bazel's major version simultaneously**, thereby
allowing users to handle incompatible changes from one source at a time.

For example, with the following compatibility matrix:

*   Migrating from rules_foo 1.x + Bazel 4.x to rules_foo 2.x + Bazel 5.x is not
    considered manageable, as the users need to upgrade the major version of
    rules_foo and Bazel at the same time.
*   Migrating from rules_foo 2.x + Bazel 5.x to rules_foo 3.x + Bazel 6.x is
    considered manageable, as the users can first upgrade rules_foo from 2.x to
    3.x without changing the major Bazel version, then upgrade Bazel from 5.x to
    6.x.

| | rules_foo 1.x | rules_foo 2.x | rules_foo 3.x | HEAD |
| --- | --- | --- | --- | --- |
| Bazel 4.x | ✅ | ❌ | ❌ | ❌ |
| Bazel 5.x | ❌ | ✅ | ✅ | ❌ |
| Bazel 6.x | ❌ | ❌ | ✅ | ✅ |
| HEAD | ❌ | ❌ | ❌ | ✅ |

❌: No version of the major rule version is compatible with the Bazel LTS
release.

✅: At least one version of the rule is compatible with the latest version of the
Bazel LTS release.

## Best practices

As Bazel rules authors, you can ensure a manageable migration process for users
by following these best practices:

1.  The rule should follow [Semantic
    Versioning](https://semver.org/): minor versions of the same
    major version are backward compatible.
1.  The rule at HEAD should be compatible with the latest Bazel LTS release.
1.  The rule at HEAD should be compatible with Bazel at HEAD. To achieve this,
    you can
    *   Set up your own CI testing with Bazel at HEAD
    *   Add your project to [Bazel downstream
        testing](https://github.com/bazelbuild/continuous-integration/blob/master/docs/downstream-testing.md);
        the Bazel team files issues to your project if breaking changes in Bazel
        affect your project, and you must follow our [downstream project
        policies](https://github.com/bazelbuild/continuous-integration/blob/master/docs/downstream-testing.md#downstream-project-policies)
        to address issues timely.
1.  The latest major version of the rule must be compatible with the latest
    Bazel LTS release.
1.  A new major version of the rule should be compatible with the last Bazel LTS
    release supported by the previous major version of the rule.

Achieving 2. and 3. is the most important task since it allows achieving 4. and
5.  naturally.

To make it easier to keep compatibility with both Bazel at HEAD and the latest
Bazel LTS release, rules authors can:

*   Request backward-compatible features to be back-ported to the latest LTS
    release, check out [release process](/release#release-procedure-policies)
    for more details.
*   Use [bazel_features](https://github.com/bazel-contrib/bazel_features)
    to do Bazel feature detection.

In general, with the recommended approaches, rules should be able to migrate for
Bazel incompatible changes and make use of new Bazel features at HEAD without
dropping compatibility with the latest Bazel LTS release.

---

## Build Event Protocol
- URL: https://bazel.build/remote/bep
- Source: remote/bep.mdx
- Slug: /remote/bep

The [Build Event
Protocol](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/buildeventstream/proto/build_event_stream.proto)
(BEP) allows third-party programs to gain insight into a Bazel invocation. For
example, you could use the BEP to gather information for an IDE
plugin or a dashboard that displays build results.

The protocol is a set of [protocol
buffer](https://developers.google.com/protocol-buffers/) messages with some
semantics defined on top of it. It includes information about build and test
results, build progress, the build configuration and much more. The BEP is
intended to be consumed programmatically and makes parsing Bazel’s
command line output a thing of the past.

The Build Event Protocol represents information about a build as events. A
build event is a protocol buffer message consisting of a build event identifier,
a set of child event identifiers, and a payload.

*  __Build Event Identifier:__ Depending on the kind of build event, it might be
an [opaque
string](https://github.com/bazelbuild/bazel/blob/7.1.0/src/main/java/com/google/devtools/build/lib/buildeventstream/proto/build_event_stream.proto#L131-L140)
or [structured
information](https://github.com/bazelbuild/bazel/blob/7.1.0/src/main/java/com/google/devtools/build/lib/buildeventstream/proto/build_event_stream.proto#L194-L205)
revealing more about the build event. A build event identifier is unique within
a build.

*  __Children:__ A build event may announce other build events, by including
their build event identifiers in its [children
field](https://github.com/bazelbuild/bazel/blob/7.1.0/src/main/java/com/google/devtools/build/lib/buildeventstream/proto/build_event_stream.proto#L1276).
For example, the `PatternExpanded` build event announces the targets it expands
to as children. The protocol guarantees that all events, except for the first
event, are announced by a previous event.

* __Payload:__ The payload contains structured information about a build event,
encoded as a protocol buffer message specific to that event. Note that the
payload might not be the expected type, but could be an `Aborted` message
if the build aborted prematurely.

### Build event graph

All build events form a directed acyclic graph through their parent and child
relationship. Every build event except for the initial build event has one or
more parent events. Please note that not all parent events of a child event must
necessarily be posted before it. When a build is complete (succeeded or failed)
all announced events will have been posted. In case of a Bazel crash or a failed
network transport, some announced build events may never be posted.

The event graph's structure reflects the lifecycle of a command. Every BEP
graph has the following characteristic shape:

1. The root event is always a [`BuildStarted`](/remote/bep-glossary#buildstarted)
   event. All other events are its descendants.
1. Immediate children of the BuildStarted event contain metadata about the
   command.
1. Events containing data produced by the command, such as files built and test
   results, appear before the [`BuildFinished`](/remote/bep-glossary#buildfinished)
   event.
1. The [`BuildFinished`](/remote/bep-glossary#buildfinished) event *may* be followed
   by events containing summary information about the build (for example, metric
   or profiling data).

## Consuming Build Event Protocol

### Consume in binary format

To consume the BEP in a binary format:

1. Have Bazel serialize the protocol buffer messages to a file by specifying the
   option `--build_event_binary_file=/path/to/file`. The file will contain
   serialized protocol buffer messages with each message being length delimited.
   Each message is prefixed with its length encoded as a variable length integer.
   This format can be read using the protocol buffer library’s
   [`parseDelimitedFrom(InputStream)`](https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/AbstractParser#parseDelimitedFrom-java.io.InputStream-)
   method.

2. Then, write a program that extracts the relevant information from the
   serialized protocol buffer message.

### Consume in text or JSON formats

The following Bazel command line flags will output the BEP in
human-readable formats, such as text and JSON:

```
--build_event_text_file
--build_event_json_file
```

## Build Event Service

The [Build Event
Service](https://github.com/googleapis/googleapis/blob/master/google/devtools/build/v1/publish_build_event.proto)
Protocol is a generic [gRPC](https://www.grpc.io) service for publishing build events. The Build Event
Service protocol is independent of the BEP and treats BEP events as opaque bytes.
Bazel ships with a gRPC client implementation of the Build Event Service protocol that
publishes Build Event Protocol events. One can specify the endpoint to send the
events to using the `--bes_backend=HOST:PORT` flag. If your backend uses gRPC,
you must prefix the address with the appropriate scheme: `grpc://` for plaintext
gRPC and `grpcs://` for gRPC with TLS enabled.

### Build Event Service flags

Bazel has several flags related to the Build Event Service protocol, including:

*  `--bes_backend`
*  `--[no]bes_lifecycle_events`
*  `--bes_results_url`
*  `--bes_timeout`
*  `--bes_instance_name`

For a description of each of these flags, see the
[Command-Line Reference](/reference/command-line-reference).

### Authentication and security

Bazel’s Build Event Service implementation also supports authentication and TLS.
These settings can be controlled using the below flags. Please note that these
flags are also used for Bazel’s Remote Execution. This implies that the Build
Event Service and Remote Execution Endpoints need to share the same
authentication and TLS infrastructure.

*  `--[no]google_default_credentials`
*  `--google_credentials`
*  `--google_auth_scopes`
*  `--tls_certificate`
*  `--[no]tls_enabled`

For a description of each of these flags, see the
[Command-Line Reference](/reference/command-line-reference).

### Build Event Service and remote caching

The BEP typically contains many references to log files (test.log, test.xml,
etc. ) stored on the machine where Bazel is running. A remote BES server
typically can't access these files as they are on different machines. A way to
work around this issue is to use Bazel with [remote
caching](/remote/caching).
Bazel will upload all output files to the remote cache (including files
referenced in the BEP) and the BES server can then fetch the referenced files
from the cache.

See [GitHub issue 3689](https://github.com/bazelbuild/bazel/issues/3689) for
more details.

---

## Build Event Protocol Examples
- URL: https://bazel.build/remote/bep-examples
- Source: remote/bep-examples.mdx
- Slug: /remote/bep-examples

The full specification of the Build Event Protocol can be found in its protocol
buffer definition. However, it might be helpful to build up some intuition
before looking at the specification.

Consider a simple Bazel workspace that consists of two empty shell scripts
`foo.sh` and `foo_test.sh` and the following `BUILD` file:

```bash
sh_library(
    name = "foo_lib",
    srcs = ["foo.sh"],
)

sh_test(
    name = "foo_test",
    srcs = ["foo_test.sh"],
    deps = [":foo_lib"],
)
```

When running `bazel test ...` on this project the build graph of the generated
build events will resemble the graph below. The arrows indicate the
aforementioned parent and child relationship. Note that some build events and
most fields have been omitted for brevity.

![bep-graph](/docs/images/bep-graph.png "BEP graph")

**Figure 1.** BEP graph.

Initially, a `BuildStarted` event is published. The event informs us that the
build was invoked through the `bazel test` command and announces child events:

* `OptionsParsed`
* `WorkspaceStatus`
* `CommandLine`
* `UnstructuredCommandLine`
* `BuildMetadata`
* `BuildFinished`
* `PatternExpanded`
* `Progress`

The first three events provide information about how Bazel was invoked.

The `PatternExpanded` build event provides insight
into which specific targets the `...` pattern expanded to:
`//foo:foo_lib` and `//foo:foo_test`. It does so by declaring two
`TargetConfigured` events as children. Note that the `TargetConfigured` event
declares the `Configuration` event as a child event, even though `Configuration`
has been posted before the `TargetConfigured` event.

Besides the parent and child relationship, events may also refer to each other
using their build event identifiers. For example, in the above graph the
`TargetComplete` event refers to the `NamedSetOfFiles` event in its `fileSets`
field.

Build events that refer to files don’t usually embed the file
names and paths in the event. Instead, they contain the build event identifier
of a `NamedSetOfFiles` event, which will then contain the actual file names and
paths. The `NamedSetOfFiles` event allows a set of files to be reported once and
referred to by many targets. This structure is necessary because otherwise in
some cases the Build Event Protocol output size would grow quadratically with
the number of files. A `NamedSetOfFiles` event may also not have all its files
embedded, but instead refer to other `NamedSetOfFiles` events through their
build event identifiers.

Below is an instance of the `TargetComplete` event for the `//foo:foo_lib`
target from the above graph, printed in protocol buffer’s JSON representation.
The build event identifier contains the target as an opaque string and refers to
the `Configuration` event using its build event identifier. The event does not
announce any child events. The payload contains information about whether the
target was built successfully, the set of output files, and the kind of target
built.

```json
{
  "id": {
    "targetCompleted": {
      "label": "//foo:foo_lib",
      "configuration": {
        "id": "544e39a7f0abdb3efdd29d675a48bc6a"
      }
    }
  },
  "completed": {
    "success": true,
    "outputGroup": [{
      "name": "default",
      "fileSets": [{
        "id": "0"
      }]
    }],
    "targetKind": "sh_library rule"
  }
}
```

## Aspect Results in BEP

Ordinary builds evaluate actions associated with `(target, configuration)`
pairs. When building with [aspects](/extending/aspects) enabled, Bazel
additionally evaluates targets associated with `(target, configuration,
aspect)` triples, for each target affected by a given enabled aspect.

Evaluation results for aspects are available in BEP despite the absence of
aspect-specific event types. For each `(target, configuration)` pair with an
applicable aspect, Bazel publishes an additional `TargetConfigured` and
`TargetComplete` event bearing the result from applying the aspect to the
target. For example, if `//:foo_lib` is built with
`--aspects=aspects/myaspect.bzl%custom_aspect`, this event would also appear in
the BEP:

```json
{
  "id": {
    "targetCompleted": {
      "label": "//foo:foo_lib",
      "configuration": {
        "id": "544e39a7f0abdb3efdd29d675a48bc6a"
      },
      "aspect": "aspects/myaspect.bzl%custom_aspect"
    }
  },
  "completed": {
    "success": true,
    "outputGroup": [{
      "name": "default",
      "fileSets": [{
        "id": "1"
      }]
    }]
  }
}
```

Note: The only difference between the IDs is the presence of the `aspect`
field. A tool that does not check the `aspect` ID field and accumulates output
files by target may conflate target outputs with aspect outputs.

## Consuming `NamedSetOfFiles`

Determining the artifacts produced by a given target (or aspect) is a common
BEP use-case that can be done efficiently with some preparation. This section
discusses the recursive, shared structure offered by the `NamedSetOfFiles`
event, which matches the structure of a Starlark [Depset](/extending/depsets).

Consumers must take care to avoid quadratic algorithms when processing
`NamedSetOfFiles` events because large builds can contain tens of thousands of
such events, requiring hundreds of millions operations in a traversal with
quadratic complexity.

![namedsetoffiles-bep-graph](/docs/images/namedsetoffiles-bep-graph.png "NamedSetOfFiles BEP graph")

**Figure 2.** `NamedSetOfFiles` BEP graph.

A `NamedSetOfFiles` event always appears in the BEP stream *before* a
`TargetComplete` or `NamedSetOfFiles` event that references it. This is the
inverse of the "parent-child" event relationship, where all but the first event
appears after at least one event announcing it. A `NamedSetOfFiles` event is
announced by a `Progress` event with no semantics.

Given these ordering and sharing constraints, a typical consumer must buffer all
`NamedSetOfFiles` events until the BEP stream is exhausted. The following JSON
event stream and Python code demonstrate how to populate a map from
target/aspect to built artifacts in the "default" output group, and how to
process the outputs for a subset of built targets/aspects:

```python
named_sets = {}  # type: dict[str, NamedSetOfFiles]
outputs = {}     # type: dict[str, dict[str, set[str]]]

for event in stream:
  kind = event.id.WhichOneof("id")
  if kind == "named_set":
    named_sets[event.id.named_set.id] = event.named_set_of_files
  elif kind == "target_completed":
    tc = event.id.target_completed
    target_id = (tc.label, tc.configuration.id, tc.aspect)
    outputs[target_id] = {}
    for group in event.completed.output_group:
      outputs[target_id][group.name] = {fs.id for fs in group.file_sets}

for result_id in relevant_subset(outputs.keys()):
  visit = outputs[result_id].get("default", [])
  seen_sets = set(visit)
  while visit:
    set_name = visit.pop()
    s = named_sets[set_name]
    for f in s.files:
      process_file(result_id, f)
    for fs in s.file_sets:
      if fs.id not in seen_sets:
        visit.add(fs.id)
        seen_sets.add(fs.id)
```

---

## Build Event Protocol Glossary
- URL: https://bazel.build/remote/bep-glossary
- Source: remote/bep-glossary.mdx
- Slug: /remote/bep-glossary

Each BEP event type has its own semantics, minimally documented in
[build\_event\_stream.proto](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/buildeventstream/proto/build_event_stream.proto).
The following glossary describes each event type.

## Aborted

Unlike other events, `Aborted` does not have a corresponding ID type, because
the `Aborted` event *replaces* events of other types. This event indicates that
the build terminated early and the event ID it appears under was not produced
normally. `Aborted` contains an enum and human-friendly description to explain
why the build did not complete.

For example, if a build is evaluating a target when the user interrupts Bazel,
BEP contains an event like the following:

```json
{
  "id": {
    "targetCompleted": {
      "label": "//:foo",
      "configuration": {
        "id": "544e39a7f0abdb3efdd29d675a48bc6a"
      }
    }
  },
  "aborted": {
    "reason": "USER_INTERRUPTED"
  }
}
```

## ActionExecuted

Provides details about the execution of a specific
[Action](/rules/lib/actions) in a build. By default, this event is
included in the BEP only for failed actions, to support identifying the root cause
of build failures. Users may set the `--build_event_publish_all_actions` flag
to include all `ActionExecuted` events.

## BuildFinished

A single `BuildFinished` event is sent after the command is complete and
includes the exit code for the command. This event provides authoritative
success/failure information.

## BuildMetadata

Contains the parsed contents of the `--build_metadata` flag. This event exists
to support Bazel integration with other tooling by plumbing external data (such as
identifiers).

## BuildMetrics

A single `BuildMetrics` event is sent at the end of every command and includes
counters/gauges useful for quantifying the build tool's behavior during the
command. These metrics indicate work actually done and does not count cached
work that is reused.

Note that `memory_metrics` may not be populated if there was no Java garbage
collection during the command's execution. Users may set the
`--memory_profile=/dev/null` option which forces the garbage
collector to run at the end of the command to populate `memory_metrics`.

```json
{
  "id": {
    "buildMetrics": {}
  },
  "buildMetrics": {
    "actionSummary": {
      "actionsExecuted": "1"
    },
    "memoryMetrics": {},
    "targetMetrics": {
      "targetsLoaded": "9",
      "targetsConfigured": "19"
    },
    "packageMetrics": {
      "packagesLoaded": "5"
    },
    "timingMetrics": {
      "cpuTimeInMs": "1590",
      "wallTimeInMs": "359"
    }
  }
}
```

## BuildStarted

The first event in a BEP stream, `BuildStarted` includes metadata describing the
command before any meaningful work begins.

## BuildToolLogs

A single `BuildToolLogs` event is sent at the end of a command, including URIs
of files generated by the build tool that may aid in understanding or debugging
build tool behavior. Some information may be included inline.

```json
{
  "id": {
    "buildToolLogs": {}
  },
  "lastMessage": true,
  "buildToolLogs": {
    "log": [
      {
        "name": "elapsed time",
        "contents": "MC4xMjEwMDA="
      },
      {
        "name": "process stats",
        "contents": "MSBwcm9jZXNzOiAxIGludGVybmFsLg=="
      },
      {
        "name": "command.profile.gz",
        "uri": "file:///tmp/.cache/bazel/_bazel_foo/cde87985ad0bfef34eacae575224b8d1/command.profile.gz"
      }
    ]
  }
}
```

## CommandLine

The BEP contains multiple `CommandLine` events containing representations of all
command-line arguments (including options and uninterpreted arguments).
Each `CommandLine` event has a label in its `StructuredCommandLineId` that
indicates which representation it conveys; three such events appear in the BEP:

* `"original"`: Reconstructed commandline as Bazel received it from the Bazel
  client, without startup options sourced from .rc files.
* `"canonical"`: The effective commandline with .rc files expanded and
  invocation policy applied.
* `"tool"`: Populated from the `--experimental_tool_command_line` option. This
  is useful to convey the command-line of a tool wrapping Bazel through the BEP.
  This could be a base64-encoded `CommandLine` binary protocol buffer message
  which is used directly, or a string which is parsed but not interpreted (as
  the tool's options may differ from Bazel's).

## Configuration

A `Configuration` event is sent for every [`configuration`](/extending/config)
used in the top-level targets in a build. At least one configuration event is
always be present. The `id` is reused by the `TargetConfigured` and
`TargetComplete` event IDs and is necessary to disambiguate those events in
multi-configuration builds.

```json
{
  "id": {
    "configuration": {
      "id": "a5d130b0966b4a9ca2d32725aa5baf40e215bcfc4d5cdcdc60f5cc5b4918903b"
    }
  },
  "configuration": {
    "mnemonic": "k8-fastbuild",
    "platformName": "k8",
    "cpu": "k8",
    "makeVariable": {
      "COMPILATION_MODE": "fastbuild",
      "TARGET_CPU": "k8",
      "GENDIR": "bazel-out/k8-fastbuild/bin",
      "BINDIR": "bazel-out/k8-fastbuild/bin"
    }
  }
}
```

## ConvenienceSymlinksIdentified

**Experimental.** If the `--experimental_convenience_symlinks_bep_event`
option is set, a single `ConvenienceSymlinksIdentified` event is produced by
`build` commands to indicate how symlinks in the workspace should be managed.
This enables building tools that invoke Bazel remotely then arrange the local
workspace as if Bazel had been run locally.

```json
{
  "id": {
    "convenienceSymlinksIdentified":{}
  },
  "convenienceSymlinksIdentified": {
    "convenienceSymlinks": [
      {
        "path": "bazel-bin",
        "action": "CREATE",
        "target": "execroot/google3/bazel-out/k8-fastbuild/bin"
      },
      {
        "path": "bazel-genfiles",
        "action": "CREATE",
        "target": "execroot/google3/bazel-out/k8-fastbuild/genfiles"
      },
      {
        "path": "bazel-out",
        "action": "CREATE",
        "target": "execroot/google3/bazel-out"
      }
    ]
  }
}
```

## Fetch

Indicates that a Fetch operation occurred as a part of the command execution.
Unlike other events, if a cached fetch result is re-used, this event does not
appear in the BEP stream.

## NamedSetOfFiles

`NamedSetOfFiles` events report a structure matching a
[`depset`](/extending/depsets) of files produced during command evaluation.
Transitively included depsets are identified by `NamedSetOfFilesId`.

For more information on interpreting a stream's `NamedSetOfFiles` events, see the
[BEP examples page](/remote/bep-examples#consuming-namedsetoffiles).

## OptionsParsed

A single `OptionsParsed` event lists all options applied to the command,
separating startup options from command options. It also includes the
[InvocationPolicy](/reference/command-line-reference#flag--invocation_policy), if any.

```json
{
  "id": {
    "optionsParsed": {}
  },
  "optionsParsed": {
    "startupOptions": [
      "--max_idle_secs=10800",
      "--noshutdown_on_low_sys_mem",
      "--connect_timeout_secs=30",
      "--output_user_root=/tmp/.cache/bazel/_bazel_foo",
      "--output_base=/tmp/.cache/bazel/_bazel_foo/a61fd0fbee3f9d6c1e30d54b68655d35",
      "--deep_execroot",
      "--idle_server_tasks",
      "--write_command_log",
      "--nowatchfs",
      "--nofatal_event_bus_exceptions",
      "--nowindows_enable_symlinks",
      "--noclient_debug",
    ],
    "cmdLine": [
      "--enable_platform_specific_config",
      "--build_event_json_file=/tmp/bep.json"
    ],
    "explicitCmdLine": [
      "--build_event_json_file=/tmp/bep.json"
    ],
    "invocationPolicy": {}
  }
}
```

## PatternExpanded

`PatternExpanded` events indicate the set of all targets that match the patterns
supplied on the commandline. For successful commands, a single event is present
with all patterns in the `PatternExpandedId` and all targets in the
`PatternExpanded` event's *children*. If the pattern expands to any
`test_suite`s the set of test targets included by the `test_suite`. For each
pattern that fails to resolve, BEP contains an additional [`Aborted`](#aborted)
event with a `PatternExpandedId` identifying the pattern.

```json
{
  "id": {
    "pattern": {
      "pattern":["//base:all"]
    }
  },
  "children": [
    {"targetConfigured":{"label":"//base:foo"}},
    {"targetConfigured":{"label":"//base:foobar"}}
  ],
  "expanded": {
    "testSuiteExpansions": {
      "suiteLabel": "//base:suite",
      "testLabels": "//base:foo_test"
    }
  }
}
```

## Progress

Progress events contain the standard output and standard error produced by Bazel
during command execution. These events are also auto-generated as needed to
announce events that have not been announced by a logical "parent" event (in
particular, [NamedSetOfFiles](#namedsetoffiles).)

## TargetComplete

For each `(target, configuration, aspect)` combination that completes the
execution phase, a `TargetComplete` event is included in BEP. The event contains
the target's success/failure and the target's requested output groups.

```json
{
  "id": {
    "targetCompleted": {
      "label": "//examples/py:bep",
      "configuration": {
        "id": "a5d130b0966b4a9ca2d32725aa5baf40e215bcfc4d5cdcdc60f5cc5b4918903b"
      }
    }
  },
  "completed": {
    "success": true,
    "outputGroup": [
      {
        "name": "default",
        "fileSets": [
          {
            "id": "0"
          }
        ]
      }
    ]
  }
}
```

## TargetConfigured

For each Target that completes the analysis phase, a `TargetConfigured` event is
included in BEP. This is the authoritative source for a target's "rule kind"
attribute. The configuration(s) applied to the target appear in the announced
*children* of the event.

For example, building with the `--experimental_multi_cpu` options may produce
the following `TargetConfigured` event for a single target with two
configurations:

```json
{
  "id": {
    "targetConfigured": {
      "label": "//starlark_configurations/multi_arch_binary:foo"
    }
  },
  "children": [
    {
      "targetCompleted": {
        "label": "//starlark_configurations/multi_arch_binary:foo",
        "configuration": {
          "id": "c62b30c8ab7b9fc51a05848af9276529842a11a7655c71327ade26d7c894c818"
        }
      }
    },
    {
      "targetCompleted": {
        "label": "//starlark_configurations/multi_arch_binary:foo",
        "configuration": {
          "id": "eae0379b65abce68d54e0924c0ebcbf3d3df26c6e84ef7b2be51e8dc5b513c99"
        }
      }
    }
  ],
  "configured": {
    "targetKind": "foo_binary rule"
  }
}
```

## TargetSummary

For each `(target, configuration)` pair that is executed, a `TargetSummary`
event is included with an aggregate success result encompassing the configured
target's execution and all aspects applied to that configured target.

## TestResult

If testing is requested, a `TestResult` event is sent for each test attempt,
shard, and run per test. This allows BEP consumers to identify precisely which
test actions failed their tests and identify the test outputs (such as logs,
test.xml files) for each test action.

## TestSummary

If testing is requested, a `TestSummary` event is sent for each test `(target,
configuration)`, containing information necessary to interpret the test's
results. The number of attempts, shards and runs per test are included to enable
BEP consumers to differentiate artifacts across these dimensions.  The attempts
and runs per test are considered while producing the aggregate `TestStatus` to
differentiate `FLAKY` tests from `FAILED` tests.

## UnstructuredCommandLine

Unlike [CommandLine](#commandline), this event carries the unparsed commandline
flags in string form as encountered by the build tool after expanding all
[`.bazelrc`](/run/bazelrc) files and
considering the `--config` flag.

The `UnstructuredCommandLine` event may be relied upon to precisely reproduce a
given command execution.

## WorkspaceConfig

A single `WorkspaceConfig` event contains configuration information regarding the
workspace, such as the execution root.

## WorkspaceStatus

A single `WorkspaceStatus` event contains the result of the [workspace status
command](/docs/user-manual#workspace-status).

---

## Debugging Remote Cache Hits for Local Execution
- URL: https://bazel.build/remote/cache-local
- Source: remote/cache-local.mdx
- Slug: /remote/cache-local

This page describes how to investigate cache misses in the context of local
execution.

This page assumes that you have a build and/or test that successfully builds
locally and is set up to utilize remote caching, and that you want to ensure
that the remote cache is being effectively utilized.

For tips on how to check your cache hit rate and how to compare the execution
logs between two Bazel invocations, see
[Debugging Remote Cache Hits for Remote Execution](/remote/cache-remote).
Everything presented in that guide also applies to remote caching with local
execution. However, local execution presents some additional challenges.

## Checking your cache hit rate

Successful remote cache hits will show up in the status line, similar to
[Cache Hits rate with Remote
Execution](/remote/cache-remote#check-cache-hits).

In the standard output of your Bazel run, you will see something like the
following:

```none {:.devsite-disable-click-to-copy}
   INFO: 7 processes: 3 remote cache hit, 4 linux-sandbox.
```

This means that out of 7 attempted actions, 3 got a remote cache hit and 4
actions did not have cache hits and were executed locally using `linux-sandbox`
strategy. Local cache hits are not included in this summary. If you are getting
0 processes (or a number lower than expected), run `bazel clean` followed by
your build/test command.

## Troubleshooting cache hits

If you are not getting the cache hit rate you are expecting, do the following:

### Ensure successful communication with the remote endpoint

To ensure your build is successfully communicating with the remote cache, follow
the steps in this section.

1. Check your output for warnings

   With remote execution, a failure to talk to the remote endpoint would fail
   your build. On the other hand, a cacheable local build would not fail if it
   cannot cache. Check the output of your Bazel invocation for warnings, such
   as:

   ```none 
      WARNING: Error reading from the remote cache:
   ```


   or

   ```none 
      WARNING: Error writing to the remote cache:
   ```


   Such warnings will be followed by the error message detailing the connection
   problem that should help you debug: for example, mistyped endpoint name or
   incorrectly set credentials. Find and address any such errors. If the error
   message you see does not give you enough information, try adding
   `--verbose_failures`.

2. Follow the steps from [Troubleshooting cache hits for remote
   execution](/remote/cache-remote#troubleshooting_cache_hits) to
   ensure that your cache-writing Bazel invocations are able to get cache hits
   on the same machine and across machines.

3. Ensure your cache-reading Bazel invocations can get cache hits.

   a. Since cache-reading Bazel invocations will have a different command-line set
      up, take additional care to ensure that they are properly set up to
      communicate with the remote cache. Ensure the `--remote_cache` flag is set
      and there are no warnings in the output.

   b. Ensure your cache-reading Bazel invocations build the same targets as the
      cache-writing Bazel invocations.

   c. Follow the same steps as to [ensure caching across
      machines](/remote/cache-remote#caching-across-machines),
      to ensure caching from your cache-writing Bazel invocation to your
      cache-reading Bazel invocation.

---

## Debugging Remote Cache Hits for Remote Execution
- URL: https://bazel.build/remote/cache-remote
- Source: remote/cache-remote.mdx
- Slug: /remote/cache-remote

This page describes how to check your cache hit rate and how to investigate
cache misses in the context of remote execution.

This page assumes that you have a build and/or test that successfully
utilizes remote execution, and you want to ensure that you are effectively
utilizing remote cache.

## Checking your cache hit rate

In the standard output of your Bazel run, look at the `INFO` line that lists
processes, which roughly correspond to Bazel actions. That line details
where the action was run. Look for the `remote` label, which indicates an action
executed remotely, `linux-sandbox` for actions executed in a local sandbox,
and other values for other execution strategies. An action whose result came
from a remote cache is displayed as `remote cache hit`.

For example:

```none {:.devsite-disable-click-to-copy}
INFO: 11 processes: 6 remote cache hit, 3 internal, 2 remote.
```

In this example there were 6 remote cache hits, and 2 actions did not have
cache hits and were executed remotely. The 3 internal part can be ignored.
It is typically tiny internal actions, such as creating symbolic links. Local
cache hits are not included in this summary. If you are getting 0 processes
(or a number lower than expected), run `bazel clean` followed by your build/test
command.

## Troubleshooting cache hits

If you are not getting the cache hit rate you are expecting, do the following:

### Ensure re-running the same build/test command produces cache hits

1. Run the build(s) and/or test(s) that you expect to populate the cache. The
   first time a new build is run on a particular stack, you can expect no remote
   cache hits. As part of remote execution, action results are stored in the
   cache and a subsequent run should pick them up.

2. Run `bazel clean`. This command cleans your local cache, which allows
   you to investigate remote cache hits without the results being masked by
   local cache hits.

3. Run the build(s) and test(s) that you are investigating again (on the same
   machine).

4. Check the `INFO` line for cache hit rate. If you see no processes except
   `remote cache hit` and `internal`, then your cache is being correctly populated and
   accessed. In that case, skip to the next section.

5. A likely source of discrepancy is something non-hermetic in the build causing
   the actions to receive different action keys across the two runs. To find
   those actions, do the following:

   a. Re-run the build(s) or test(s) in question to obtain execution logs:

      ```posix-terminal
      bazel clean

      bazel <var>--optional-flags</var> build //<var>your:target</var> --execution_log_compact_file=/tmp/exec1.log
      ```

   b. [Compare the execution logs](#compare-logs) between the
      two runs. Ensure that the actions are identical across the two log files.
      Discrepancies provide a clue about the changes that occurred between the
      runs. Update your build to eliminate those discrepancies.

   If you are able to resolve the caching problems and now the repeated run
   produces all cache hits, skip to the next section.

   If your action IDs are identical but there are no cache hits, then something
   in your configuration is preventing caching. Continue with this section to
   check for common problems.

5. Check that all actions in the execution log have `cacheable` set to true. If
   `cacheable` does not appear in the execution log for a give action, that
   means that the corresponding rule may have a `no-cache` tag in its
   definition in the `BUILD` file. Look at the `mnemonic` and `target_label`
   fields in the execution log to help determine where the action is coming
   from.

6. If the actions are identical and `cacheable` but there are no cache hits, it
   is possible that your command line includes `--noremote_accept_cached` which
   would disable cache lookups for a build.

   If figuring out the actual command line is difficult, use the canonical
   command line from the
   [Build Event Protocol](/remote/bep)
   as follows:

   a. Add `--build_event_text_file=/tmp/bep.txt` to your Bazel command to get
    the text version of the log.

   b. Open the text version of the log and search for the
    `structured_command_line` message with `command_line_label: "canonical"`.
    It will list all the options after expansion.

   c. Search for `remote_accept_cached` and check whether it's set to `false`.

   d. If `remote_accept_cached` is `false`, determine where it is being
      set to `false`: either at the command line or in a
      [bazelrc](/run/bazelrc#bazelrc-file-locations) file.

### Ensure caching across machines

After cache hits are happening as expected on the same machine, run the
same build(s)/test(s) on a different machine. If you suspect that caching is
not happening across machines, do the following:

1. Make a small modification to your build to avoid hitting existing caches.

2. Run the build on the first machine:

   ```posix-terminal
    bazel clean

    bazel ... build ... --execution_log_compact_file=/tmp/exec1.log
   ```

3. Run the build on the second machine, ensuring the modification from step 1
   is included:

   ```posix-terminal
    bazel clean

    bazel ... build ... --execution_log_compact_file=/tmp/exec2.log
   ```

4. [Compare the execution logs](#compare-logs-the-execution-logs) for the two
    runs. If the logs are not identical, investigate your build configurations
    for discrepancies as well as properties from the host environment leaking
    into either of the builds.

## Comparing the execution logs

The execution log contains records of actions executed during the build.
Each record describes both the inputs (not only files, but also command line
arguments, environment variables, etc) and the outputs of the action. Thus,
examination of the log can reveal why an action was reexecuted.

The execution log can be produced in one of three formats:
compact (`--execution_log_compact_file`),
binary (`--execution_log_binary_file`) or JSON (`--execution_log_json_file`).
The compact format is recommended, as it produces much smaller files with very
little runtime overhead. The following instructions work for any format. You
can also convert between them using the `//src/tools/execlog:converter` tool.

To compare logs for two builds that are not sharing cache hits as expected,
do the following:

1. Get the execution logs from each build and store them as `/tmp/exec1.log` and
   `/tmp/exec2.log`.

2. Download the Bazel source code and build the `//src/tools/execlog:parser`
   tool:

       git clone https://github.com/bazelbuild/bazel.git
       cd bazel
       bazel build //src/tools/execlog:parser

3. Use the `//src/tools/execlog:parser` tool to convert the logs into a
   human-readable text format. In this format, the actions in the second log are
   sorted to match the order in the first log, making a comparison easier.

        bazel-bin/src/tools/execlog/parser \
          --log_path=/tmp/exec1.log \
          --log_path=/tmp/exec2.log \
          --output_path=/tmp/exec1.log.txt \
          --output_path=/tmp/exec2.log.txt

4. Use your favourite text differ to diff `/tmp/exec1.log.txt` and
   `/tmp/exec2.log.txt`.

---

## Remote Caching
- URL: https://bazel.build/remote/caching
- Source: remote/caching.mdx
- Slug: /remote/caching

This page covers remote caching, setting up a server to host the cache, and
running builds using the remote cache.

A remote cache is used by a team of developers and/or a continuous integration
(CI) system to share build outputs. If your build is reproducible, the
outputs from one machine can be safely reused on another machine, which can
make builds significantly faster.

## Overview

Bazel breaks a build into discrete steps, which are called actions. Each action
has inputs, output names, a command line, and environment variables. Required
inputs and expected outputs are declared explicitly for each action.

You can set up a server to be a remote cache for build outputs, which are these
action outputs. These outputs consist of a list of output file names and the
hashes of their contents. With a remote cache, you can reuse build outputs
from another user's build rather than building each new output locally.

To use remote caching:

* Set up a server as the cache's backend
* Configure the Bazel build to use the remote cache
* Use Bazel version 0.10.0 or later

The remote cache stores two types of data:

* The action cache, which is a map of action hashes to action result metadata.
* A content-addressable store (CAS) of output files.

Note that the remote cache additionally stores the stdout and stderr for every
action. Inspecting the stdout/stderr of Bazel thus is not a good signal for
[estimating cache hits](/remote/cache-local).

### How a build uses remote caching

Once a server is set up as the remote cache, you use the cache in multiple
ways:

* Read and write to the remote cache
* Read and/or write to the remote cache except for specific targets
* Only read from the remote cache
* Not use the remote cache at all

When you run a Bazel build that can read and write to the remote cache,
the build follows these steps:

1. Bazel creates the graph of targets that need to be built, and then creates
a list of required actions. Each of these actions has declared inputs
and output filenames.
2. Bazel checks your local machine for existing build outputs and reuses any
that it finds.
3. Bazel checks the cache for existing build outputs. If the output is found,
Bazel retrieves the output. This is a cache hit.
4. For required actions where the outputs were not found, Bazel executes the
actions locally and creates the required build outputs.
5. New build outputs are uploaded to the remote cache.

## Setting up a server as the cache's backend

You need to set up a server to act as the cache's backend. A HTTP/1.1
server can treat Bazel's data as opaque bytes and so many existing servers
can be used as a remote caching backend. Bazel's
[HTTP Caching Protocol](#http-caching) is what supports remote
caching.

You are responsible for choosing, setting up, and maintaining the backend
server that will store the cached outputs. When choosing a server, consider:

* Networking speed. For example, if your team is in the same office, you may
want to run your own local server.
* Security. The remote cache will have your binaries and so needs to be secure.
* Ease of management. For example, Google Cloud Storage is a fully managed service.

There are many backends that can be used for a remote cache. Some options
include:

* [nginx](#nginx)
* [bazel-remote](#bazel-remote)
* [Google Cloud Storage](#cloud-storage)

### nginx

nginx is an open source web server. With its [WebDAV module], it can be
used as a remote cache for Bazel. On Debian and Ubuntu you can install the
`nginx-extras` package. On macOS nginx is available via Homebrew:

```posix-terminal
brew tap denji/nginx

brew install nginx-full --with-webdav
```

Below is an example configuration for nginx. Note that you will need to
change `/path/to/cache/dir` to a valid directory where nginx has permission
to write and read. You may need to change `client_max_body_size` option to a
larger value if you have larger output files. The server will require other
configuration such as authentication.


Example configuration for `server` section in `nginx.conf`:

```nginx
location /cache/ {
  # The path to the directory where nginx should store the cache contents.
  root /path/to/cache/dir;
  # Allow PUT
  dav_methods PUT;
  # Allow nginx to create the /ac and /cas subdirectories.
  create_full_put_path on;
  # The maximum size of a single file.
  client_max_body_size 1G;
  allow all;
}
```

### bazel-remote

bazel-remote is an open source remote build cache that you can use on
your infrastructure. It has been successfully used in production at
several companies since early 2018. Note that the Bazel project does
not provide technical support for bazel-remote.

This cache stores contents on disk and also provides garbage collection
to enforce an upper storage limit and clean unused artifacts. The cache is
available as a [docker image] and its code is available on
[GitHub](https://github.com/buchgr/bazel-remote/).
Both the REST and gRPC remote cache APIs are supported.

Refer to the [GitHub](https://github.com/buchgr/bazel-remote/)
page for instructions on how to use it.

### Google Cloud Storage

[Google Cloud Storage] is a fully managed object store which provides an
HTTP API that is compatible with Bazel's remote caching protocol. It requires
that you have a Google Cloud account with billing enabled.

To use Cloud Storage as the cache:

1. [Create a storage bucket](https://cloud.google.com/storage/docs/creating-buckets).
Ensure that you select a bucket location that's closest to you, as network bandwidth
is important for the remote cache.

2. Create a service account for Bazel to authenticate to Cloud Storage. See
[Creating a service account](https://cloud.google.com/iam/docs/creating-managing-service-accounts#creating_a_service_account).

3. Generate a secret JSON key and then pass it to Bazel for authentication. Store
the key securely, as anyone with the key can read and write arbitrary data
to/from your GCS bucket.

4. Connect to Cloud Storage by adding the following flags to your Bazel command:
   * Pass the following URL to Bazel by using the flag:
       `--remote_cache=https://storage.googleapis.com<var>/bucket-name</var>` where `bucket-name` is the name of your storage bucket.
   * Pass the authentication key using the flag: `--google_credentials=<var>/path/to/your/secret-key</var>.json`, or
     `--google_default_credentials` to use [Application Authentication](https://cloud.google.com/docs/authentication/production).

5. You can configure Cloud Storage to automatically delete old files. To do so, see
[Managing Object Lifecycles](https://cloud.google.com/storage/docs/managing-lifecycles).

### Other servers

You can set up any HTTP/1.1 server that supports PUT and GET as the cache's
backend. Users have reported success with caching backends such as [Hazelcast](https://hazelcast.com),
[Apache httpd](http://httpd.apache.org), and [AWS S3](https://aws.amazon.com/s3).

## Authentication

As of version 0.11.0 support for HTTP Basic Authentication was added to Bazel.
You can pass a username and password to Bazel via the remote cache URL. The
syntax is `https://username:password@hostname.com:port/path`. Note that
HTTP Basic Authentication transmits username and password in plaintext over the
network and it's thus critical to always use it with HTTPS.

## HTTP caching protocol

Bazel supports remote caching via HTTP/1.1. The protocol is conceptually simple:
Binary data (BLOB) is uploaded via PUT requests and downloaded via GET requests.
Action result metadata is stored under the path `/ac/` and output files are stored
under the path `/cas/`.

For example, consider a remote cache running under `http://localhost:8080/cache`.
A Bazel request to download action result metadata for an action with the SHA256
hash `01ba4719...` will look as follows:

```http
GET /cache/ac/01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b HTTP/1.1
Host: localhost:8080
Accept: */*
Connection: Keep-Alive
```

A Bazel request to upload an output file with the SHA256 hash `15e2b0d3...` to
the CAS will look as follows:

```http
PUT /cache/cas/15e2b0d3c33891ebb0f1ef609ec419420c20e320ce94c65fbc8c3312448eb225 HTTP/1.1
Host: localhost:8080
Accept: */*
Content-Length: 9
Connection: Keep-Alive

0x310x320x330x340x350x360x370x380x39
```

## Run Bazel using the remote cache

Once a server is set up as the remote cache, to use the remote cache you
need to add flags to your Bazel command. See list of configurations and
their flags below.

You may also need configure authentication, which is specific to your
chosen server.

You may want to add these flags in a `.bazelrc` file so that you don't
need to specify them every time you run Bazel. Depending on your project and
team dynamics, you can add flags to a `.bazelrc` file that is:

* On your local machine
* In your project's workspace, shared with the team
* On the CI system

### Read from and write to the remote cache

Take care in who has the ability to write to the remote cache. You may want
only your CI system to be able to write to the remote cache.

Use the following flag to read from and write to the remote cache:

```posix-terminal
build --remote_cache=http://{{ '<var>' }}your.host:port{{ '</var>' }}
```

Besides `HTTP`, the following protocols are also supported: `HTTPS`, `grpc`, `grpcs`.

Use the following flag in addition to the one above to only read from the
remote cache:

```posix-terminal
build --remote_upload_local_results=false
```

### Exclude specific targets from using the remote cache

To exclude specific targets from using the remote cache, tag the target with
`no-remote-cache`. For example:

```starlark
java_library(
    name = "target",
    tags = ["no-remote-cache"],
)
```

### Delete content from the remote cache

Deleting content from the remote cache is part of managing your server.
How you delete content from the remote cache depends on the server you have
set up as the cache. When deleting outputs, either delete the entire cache,
or delete old outputs.

The cached outputs are stored as a set of names and hashes. When deleting
content, there's no way to distinguish which output belongs to a specific
build.

You may want to delete content from the cache to:

* Create a clean cache after a cache was poisoned
* Reduce the amount of storage used by deleting old outputs

### Unix sockets

The remote HTTP cache supports connecting over unix domain sockets. The behavior
is similar to curl's `--unix-socket` flag. Use the following to configure unix
domain socket:

```posix-terminal
   build --remote_cache=http://{{ '<var>' }}your.host:port{{ '</var>' }}
   build --remote_proxy=unix:/{{ '<var>' }}path/to/socket{{ '</var>' }}
```

This feature is unsupported on Windows.

## Disk cache

Bazel can use a directory on the file system as a remote cache. This is
useful for sharing build artifacts when switching branches and/or working
on multiple workspaces of the same project, such as multiple checkouts.
Enable the disk cache as follows:

```posix-terminal
build --disk_cache={{ '<var>' }}path/to/build/cache{{ '</var>' }}
```

You can pass a user-specific path to the `--disk_cache` flag using the `~` alias
(Bazel will substitute the current user's home directory). This comes in handy
when enabling the disk cache for all developers of a project via the project's
checked in `.bazelrc` file.

### Garbage collection

Starting with Bazel 7.4, you can use `--experimental_disk_cache_gc_max_size` and
`--experimental_disk_cache_gc_max_age` to set a maximum size for the disk cache
or for the age of individual cache entries. Bazel will automatically garbage
collect the disk cache while idling between builds; the idle timer can be set
with `--experimental_disk_cache_gc_idle_delay` (defaulting to 5 minutes).

As an alternative to automatic garbage collection, we also provide a [tool](
https://github.com/bazelbuild/bazel/tree/master/src/tools/diskcache) to run a
garbage collection on demand.

## Known issues

**Input file modification during a build**

When an input file is modified during a build, Bazel might upload invalid
results to the remote cache. You can enable a change detection with
the `--experimental_guard_against_concurrent_changes` flag. There
are no known issues and it will be enabled by default in a future release.
See [issue #3360] for updates. Generally, avoid modifying source files during a
build.

**Environment variables leaking into an action**

An action definition contains environment variables. This can be a problem for
sharing remote cache hits across machines. For example, environments with
different `$PATH` variables won't share cache hits. Only environment variables
explicitly whitelisted via `--action_env` are included in an action
definition. Bazel's Debian/Ubuntu package used to install `/etc/bazel.bazelrc`
with a whitelist of environment variables including `$PATH`. If you are getting
fewer cache hits than expected, check that your environment doesn't have an old
`/etc/bazel.bazelrc` file.

**Bazel does not track tools outside a workspace**

Bazel currently does not track tools outside a workspace. This can be a
problem if, for example, an action uses a compiler from `/usr/bin/`. Then,
two users with different compilers installed will wrongly share cache hits
because the outputs are different but they have the same action hash. See
[issue #4558](https://github.com/bazelbuild/bazel/issues/4558) for updates.

**Incremental in-memory state is lost when running builds inside docker containers**
Bazel uses server/client architecture even when running in single docker container.
On the server side, Bazel maintains an in-memory state which speeds up builds.
When running builds inside docker containers such as in CI, the in-memory state is lost
and Bazel must rebuild it before using the remote cache.

## External links

* **Your Build in a Datacenter:** The Bazel team gave a [talk](https://fosdem.org/2018/schedule/event/datacenter_build/) about remote caching and execution at FOSDEM 2018.

* **Faster Bazel builds with remote caching: a benchmark:** Nicolò Valigi wrote a [blog post](https://nicolovaligi.com/faster-bazel-remote-caching-benchmark.html)
in which he benchmarks remote caching in Bazel.

* [Adapting Rules for Remote Execution](/remote/rules)
* [Troubleshooting Remote Execution](/remote/sandbox)
* [WebDAV module](https://nginx.org/en/docs/http/ngx_http_dav_module.html)
* [Docker image](https://hub.docker.com/r/buchgr/bazel-remote-cache/)
* [bazel-remote](https://github.com/buchgr/bazel-remote/)
* [Google Cloud Storage](https://cloud.google.com/storage)
* [Google Cloud Console](https://cloud.google.com/console)
* [Bucket locations](https://cloud.google.com/storage/docs/bucket-locations)
* [Hazelcast](https://hazelcast.com)
* [Apache httpd](http://httpd.apache.org)
* [AWS S3](https://aws.amazon.com/s3)
* [issue #3360](https://github.com/bazelbuild/bazel/issues/3360)
* [gRPC](https://grpc.io/)
* [gRPC protocol](https://github.com/bazelbuild/remote-apis/blob/main/build/bazel/remote/execution/v2/remote_execution.proto)
* [Buildbarn](https://github.com/buildbarn)
* [Buildfarm](https://github.com/bazelbuild/bazel-buildfarm)
* [BuildGrid](https://gitlab.com/BuildGrid/buildgrid)
* [issue #4558](https://github.com/bazelbuild/bazel/issues/4558)
* [Application Authentication](https://cloud.google.com/docs/authentication/production)
* [NativeLink](https://github.com/TraceMachina/nativelink)

---

## Configuring Bazel CI to Test Rules for Remote Execution
- URL: https://bazel.build/remote/ci
- Source: remote/ci.mdx
- Slug: /remote/ci

This page is for owners and maintainers of Bazel rule repositories. It
describes how to configure the Bazel Continuous Integration (CI) system for
your repository to test your rules for compatibility against a remote execution
scenario. The instructions on this page apply to projects stored in
GitHub repositories.

## Prerequisites

Before completing the steps on this page, ensure the following:

*   Your GitHub repository is part of the
    [Bazel GitHub organization](https://github.com/bazelbuild).
*   You have configured Buildkite for your repository as described in
    [Bazel Continuous Integration](https://github.com/bazelbuild/continuous-integration/tree/master/buildkite).

## Setting up the Bazel CI for testing

1.  In your `.bazelci/presubmit.yml` file, do the following:

    a.  Add a config named `rbe_ubuntu1604`.

    b.  In the `rbe_ubuntu1604` config, add the build and test targets you want to test against remote execution.

2.  Add the[`bazel-toolchains`](https://github.com/bazelbuild/bazel-toolchains)
    GitHub repository to your `WORKSPACE` file, pinned to the
    [latest release](https://releases.bazel.build/bazel-toolchains.html). Also
    add an `rbe_autoconfig` target with name `buildkite_config`. This example
    creates toolchain configuration for remote execution with BuildKite CI
    for `rbe_ubuntu1604`.

```posix-terminal
load("@bazel_toolchains//rules:rbe_repo.bzl", "rbe_autoconfig")

rbe_autoconfig(name = "buildkite_config")
```

3.  Send a pull request with your changes to the `presubmit.yml` file. (See
    [example pull request](https://github.com/bazelbuild/rules_rust/commit/db141526d89d00748404856524cedd7db8939c35).)

4.  To view build results, click **Details** for the RBE (Ubuntu
    16.04) pull request check in GitHub, as shown in the figure below. This link
    becomes available after the pull request has been merged and the CI tests
    have run. (See
    [example results](https://source.cloud.google.com/results/invocations/375e325c-0a05-47af-87bd-fed1363e0333).)

    ![Example results](/docs/images/rbe-ci-1.png "Example results")

5.  (Optional) Set the **bazel test (RBE (Ubuntu 16.04))** check as a test
    required to pass before merging in your branch protection rule. The setting
    is located in GitHub in **Settings > Branches > Branch protection rules**,
    as shown in the following figure.

    ![Branch protection rules settings](/docs/images/rbe-ci-2.png "Branch protection rules")

## Troubleshooting failed builds and tests

If your build or tests fail, it's likely due to the following:

*   **Required build or test tools are not installed in the default container.**
    Builds using the `rbe_ubuntu1604` config run by default inside an
    [`rbe-ubuntu16-04`](https://console.cloud.google.com/marketplace/details/google/rbe-ubuntu16-04)
    container, which includes tools common to many Bazel builds. However, if
    your rules require tools not present in the default container, you must
    create a custom container based on the
    [`rbe-ubuntu16-04`](https://console.cloud.google.com/marketplace/details/google/rbe-ubuntu16-04)
    container and include those tools as described later.

*   **Build or test targets are using rules that are incompatible with remote
    execution.** See
    [Adapting Bazel Rules for Remote Execution](/remote/rules) for
    details about compatibility with remote execution.

## Using a custom container in the rbe_ubuntu1604 CI config

The `rbe-ubuntu16-04` container is publicly available at the following URL:

```
http://gcr.io/cloud-marketplace/google/rbe-ubuntu16-04
```

You can pull it directly from Container Registry or build it from source. The
next sections describe both options.

Before you begin, make sure you have installed `gcloud`, `docker`, and `git`.
If you are building the container from source, you must also install the latest
version of Bazel.

### Pulling the rbe-ubuntu16-04 from Container Registry

To pull the `rbe-ubuntu16-04` container from Container Registry, run the
following command:

```posix-terminal
gcloud docker -- pull gcr.io/cloud-marketplace/google/rbe-ubuntu16-04@sha256:{{ '<var>' }}sha256-checksum{{ '</var>' }}
```

Replace <var>sha256-checksum</var> with the SHA256 checksum value for
[the latest container](https://console.cloud.google.com/gcr/images/cloud-marketplace/GLOBAL/google/rbe-ubuntu16-04).

### Building the rbe-ubuntu16-04 container from source

To build the `rbe-ubuntu16-04` container from source, do the following:

1.  Clone the `bazel-toolchains` repository:

    ```posix-terminal
    git clone https://github.com/bazelbuild/bazel-toolchains
    ```

2.  Set up toolchain container targets and build the container as explained in
    [Toolchain Containers](https://github.com/bazelbuild/bazel-toolchains/tree/master/container).

3.  Pull the freshly built container:

    ```posix-terminal
gcloud docker -- pull gcr.io/<var>project-id</var>/<var>custom-container-name</var><var>sha256-checksum</var>
    ```

### Running the custom container

To run the custom container, do one of the following:

*   If you pulled the container from Container Registry, run the following
    command:

    ```posix-terminal
    docker run -it gcr.io/cloud-marketplace/google/rbe-ubuntu16-04@sha256:<var>sha256-checksum</var>/bin/bash
    ```

    Replace `sha256-checksum` with the SHA256 checksum value for the
    [latest container](https://console.cloud.google.com/gcr/images/cloud-marketplace/GLOBAL/google/rbe-ubuntu16-04).

*   If you built the container from source, run the following command:

    ```posix-terminal
    docker run -it gcr.io/<var>project-id</var>/<var>custom-container-name</var>@sha256:<var>sha256sum</var> /bin/bash
    ```

### Adding resources to the custom container

Use a [`Dockerfile`](https://docs.docker.com/engine/reference/builder/) or
[`rules_docker`](https://github.com/bazelbuild/rules_docker) to add resources or
alternate versions of the original resources to the `rbe-ubuntu16-04` container.
If you are new to Docker, read the following:

*   [Docker for beginners](https://github.com/docker/labs/tree/master/beginner)
*   [Docker Samples](https://docs.docker.com/samples/)

For example, the following `Dockerfile` snippet installs `<var>my_tool_package</var>`:

```
FROM gcr.io/cloud-marketplace/google/rbe-ubuntu16-04@sha256:{{ '<var>' }}sha256-checksum{{ '</var>' }}
RUN apt-get update && yes | apt-get install -y {{ '<var>' }}my_tool_package{{ '</var>' }}
```

### Pushing the custom container to Container Registry

Once you have customized the container, build the container image and push it to
Container Registry as follows:

1. Build the container image:

    ```posix-terminal
    docker build -t <var>custom-container-name</var>.

    docker tag <var>custom-container-name</var> gcr.io/<var>project-id</var>/<var>custom-container-name</var>
    ```

2.  Push the container image to Container Registry:

    ```posix-terminal
    gcloud docker -- push gcr.io/<var>project-id</var>/<var>custom-container-name</var>
    ```

3.  Navigate to the following URL to verify the container has been pushed:

    https://console.cloud.google.com/gcr/images/<var>project-id</var>/GLOBAL/<var>custom-container-name</var>

4.  Take note of the SHA256 checksum of your custom container. You will need to
    provide it in your build platform definition later.

5.  Configure the container for public access as described in  publicly
    accessible as explained in
    [Serving images publicly](https://cloud.google.com/container-registry/docs/access-control#serving_images_publicly).

    For more information, see
    [Pushing and Pulling Images](https://cloud.google.com/container-registry/docs/pushing-and-pulling).


### Specifying the build platform definition

You must include a [Bazel platform](/extending/platforms) configuration in your
custom toolchain configuration, which allows Bazel to select a toolchain
appropriate to the desired hardware/software platform. To generate
automatically a valid platform, you can add  to your `WORKSPACE` an
`rbe_autoconfig` target with name `buildkite_config` which includes additional
attrs to select your custom container. For details on this setup, read
the up-to-date documentation for [`rbe_autoconfig`](https://github.com/bazelbuild/bazel-toolchains/blob/master/rules/rbe_repo.bzl).

---

## Creating Persistent Workers
- URL: https://bazel.build/remote/creating
- Source: remote/creating.mdx
- Slug: /remote/creating

[Persistent workers](/remote/persistent) can make your build faster. If
you have repeated actions in your build that have a high startup cost or would
benefit from cross-action caching, you may want to implement your own persistent
worker to perform these actions.

The Bazel server communicates with the worker using `stdin`/`stdout`. It
supports the use of protocol buffers or JSON strings.

The worker implementation has two parts:

*   The [worker](#making-worker).
*   The [rule that uses the worker](#rule-uses-worker).

## Making the worker

A persistent worker upholds a few requirements:

*   It reads
    [WorkRequests](https://github.com/bazelbuild/bazel/blob/54a547f30fd582933889b961df1d6e37a3e33d85/src/main/protobuf/worker_protocol.proto#L36)
    from its `stdin`.
*   It writes
    [WorkResponses](https://github.com/bazelbuild/bazel/blob/54a547f30fd582933889b961df1d6e37a3e33d85/src/main/protobuf/worker_protocol.proto#L77)
    (and only `WorkResponse`s) to its `stdout`.
*   It accepts the `--persistent_worker` flag. The wrapper must recognize the
    `--persistent_worker` command-line flag and only make itself persistent if
    that flag is passed, otherwise it must do a one-shot compilation and exit.

If your program upholds these requirements, it can be used as a persistent
worker!

### Work requests

A `WorkRequest` contains a list of arguments to the worker, a list of
path-digest pairs representing the inputs the worker can access (this isn’t
enforced, but you can use this info for caching), and a request id, which is 0
for singleplex workers.

NOTE: While the protocol buffer specification uses "snake case" (`request_id`),
the JSON protocol uses "camel case" (`requestId`). This document uses camel case
in the JSON examples, but snake case when talking about the field regardless of
protocol.

```json
{
  "arguments" : ["--some_argument"],
  "inputs" : [
    { "path": "/path/to/my/file/1", "digest": "fdk3e2ml23d"},
    { "path": "/path/to/my/file/2", "digest": "1fwqd4qdd" }
 ],
  "requestId" : 12
}
```

The optional `verbosity` field can be used to request extra debugging output
from the worker. It is entirely up to the worker what and how to output. Higher
values indicate more verbose output. Passing the `--worker_verbose` flag to
Bazel sets the `verbosity` field to 10, but smaller or larger values can be used
manually for different amounts of output.

The optional `sandbox_dir` field is used only by workers that support
[multiplex sandboxing](/remote/multiplex).

### Work responses

A `WorkResponse` contains a request id, a zero or nonzero exit code, and an
output message describing any errors encountered in processing or executing
the request. A worker should capture the `stdout` and `stderr` of any tool it
calls and report them through the `WorkResponse`. Writing it to the `stdout` of
the worker process is unsafe, as it will interfere with the worker protocol.
Writing it to the `stderr` of the worker process is safe, but the result is
collected in a per-worker log file instead of ascribed to individual actions.

```json
{
  "exitCode" : 1,
  "output" : "Action failed with the following message:\nCould not find input
    file \"/path/to/my/file/1\"",
  "requestId" : 12
}
```

As per the norm for protobufs, all fields are optional. However, Bazel requires
the `WorkRequest` and the corresponding `WorkResponse`, to have the same request
id, so the request id must be specified if it is nonzero. This is a valid
`WorkResponse`.

```json
{
  "requestId" : 12,
}
```

A `request_id` of 0 indicates a "singleplex" request, used when this request
cannot be processed in parallel with other requests. The server guarantees that
a given worker receives requests with either only `request_id` 0 or only
`request_id` greater than zero. Singleplex requests are sent in serial, for
example if the server doesn't send another request until it has received a
response (except for cancel requests, see below).

**Notes**

*   Each protocol buffer is preceded by its length in `varint` format (see
    [`MessageLite.writeDelimitedTo()`](https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/MessageLite.html#writeDelimitedTo-java.io.OutputStream-).
*   JSON requests and responses are not preceded by a size indicator.
*   JSON requests uphold the same structure as the protobuf, but use standard
    JSON and use camel case for all field names.
*   In order to maintain the same backward and forward compatibility properties
    as protobuf, JSON workers must tolerate unknown fields in these messages,
    and use the protobuf defaults for missing values.
*   Bazel stores requests as protobufs and converts them to JSON using
    [protobuf's JSON format](https://cs.opensource.google/protobuf/protobuf/+/master:java/util/src/main/java/com/google/protobuf/util/JsonFormat.java)

### Cancellation

Workers can optionally allow work requests to be cancelled before they finish.
This is particularly useful in connection with dynamic execution, where local
execution can regularly be interrupted by a faster remote execution. To allow
cancellation, add `supports-worker-cancellation: 1` to the
`execution-requirements` field (see below) and set the
`--experimental_worker_cancellation` flag.

A **cancel request** is a `WorkRequest` with the `cancel` field set (and
similarly a **cancel response** is a `WorkResponse` with the `was_cancelled`
field set). The only other field that must be in a cancel request or cancel
response is `request_id`, indicating which request to cancel. The `request_id`
field will be 0 for singleplex workers or the non-0 `request_id` of a previously
sent `WorkRequest` for multiplex workers. The server may send cancel requests
for requests that the worker has already responded to, in which case the cancel
request must be ignored.

Each non-cancel `WorkRequest` message must be answered exactly once, whether or
not it was cancelled. Once the server has sent a cancel request, the worker may
respond with a `WorkResponse` with the `request_id` set and the `was_cancelled`
field set to true. Sending a regular `WorkResponse` is also accepted, but the
`output` and `exit_code` fields will be ignored.

Once a response has been sent for a `WorkRequest`, the worker must not touch the
files in its working directory. The server is free to clean up the files,
including temporary files.

## Making the rule that uses the worker

You'll also need to create a rule that generates actions to be performed by the
worker. Making a Starlark rule that uses a worker is just like
[creating any other rule](https://github.com/bazelbuild/examples/tree/master/rules).

In addition, the rule needs to contain a reference to the worker itself, and
there are some requirements for the actions it produces.

### Referring to the worker

The rule that uses the worker needs to contain a field that refers to the worker
itself, so you'll need to create an instance of a `\*\_binary` rule to define
your worker. If your worker is called `MyWorker.Java`, this might be the
associated rule:

```python
java_binary(
    name = "worker",
    srcs = ["MyWorker.Java"],
)
```

This creates the "worker" label, which refers to the worker binary. You'll then
define a rule that *uses* the worker. This rule should define an attribute that
refers to the worker binary.

If the worker binary you built is in a package named "work", which is at the top
level of the build, this might be the attribute definition:

```python
"worker": attr.label(
    default = Label("//work:worker"),
    executable = True,
    cfg = "exec",
)
```

`cfg = "exec"` indicates that the worker should be built to run on your
execution platform rather than on the target platform (i.e., the worker is used
as tool during the build).

### Work action requirements

The rule that uses the worker creates actions for the worker to perform. These
actions have a couple of requirements.

*   The *"arguments"* field. This takes a list of strings, all but the last of
    which are arguments passed to the worker upon startup. The last element in
    the "arguments" list is a `flag-file` (@-preceded) argument. Workers read
    the arguments from the specified flagfile on a per-WorkRequest basis. Your
    rule can write non-startup arguments for the worker to this flagfile.

*   The *"execution-requirements"* field, which takes a dictionary containing
    `"supports-workers" : "1"`, `"supports-multiplex-workers" : "1"`, or both.

    The "arguments" and "execution-requirements" fields are required for all
    actions sent to workers. Additionally, actions that should be executed by
    JSON workers need to include `"requires-worker-protocol" : "json"` in the
    execution requirements field. `"requires-worker-protocol" : "proto"` is also
    a valid execution requirement, though it’s not required for proto workers,
    since they are the default.

    You can also set a `worker-key-mnemonic` in the execution requirements. This
    may be useful if you're reusing the executable for multiple action types and
    want to distinguish actions by this worker.

*   Temporary files generated in the course of the action should be saved to the
    worker's directory. This enables sandboxing.

Note: To pass an argument starting with a literal `@`, start the argument with
`@@` instead. If an argument is also an external repository label, it will not
be considered a flagfile argument.

Assuming a rule definition with "worker" attribute described above, in addition
to a "srcs" attribute representing the inputs, an "output" attribute
representing the outputs, and an "args" attribute representing the worker
startup args, the call to `ctx.actions.run` might be:

```python
ctx.actions.run(
  inputs=ctx.files.srcs,
  outputs=[ctx.outputs.output],
  executable=ctx.executable.worker,
  mnemonic="someMnemonic",
  execution_requirements={
    "supports-workers" : "1",
    "requires-worker-protocol" : "json"},
  arguments=ctx.attr.args + ["@flagfile"]
 )
```

For another example, see
[Implementing persistent workers](/remote/persistent#implementation).

## Examples

The Bazel code base uses
[Java compiler workers](https://github.com/bazelbuild/bazel/blob/a4251eab6988d6cf4f5e35681fbe2c1b0abe48ef/src/java_tools/buildjar/java/com/google/devtools/build/buildjar/BazelJavaBuilder.java),
in addition to an
[example JSON worker](https://github.com/bazelbuild/bazel/blob/c65f768fec9889bbf1ee934c61d0dc061ea54ca2/src/test/java/com/google/devtools/build/lib/worker/ExampleWorker.java)
that is used in our integration tests.

You can use their
[scaffolding](https://github.com/bazelbuild/bazel/blob/a4251eab6988d6cf4f5e35681fbe2c1b0abe48ef/src/main/java/com/google/devtools/build/lib/worker/WorkRequestHandler.java)
to make any Java-based tool into a worker by passing in the correct callback.

For an example of a rule that uses a worker, take a look at Bazel's
[worker integration test](https://github.com/bazelbuild/bazel/blob/22b4dbcaf05756d506de346728db3846da56b775/src/test/shell/integration/bazel_worker_test.sh#L106).

External contributors have implemented workers in a variety of languages; take a
look at
[Polyglot implementations of Bazel persistent workers](https://github.com/Ubehebe/bazel-worker-examples).
You can
[find many more examples on GitHub](https://github.com/search?q=bazel+workrequest&type=Code)!

---

## Multiplex Workers (Experimental Feature)
- URL: https://bazel.build/remote/multiplex
- Source: remote/multiplex.mdx
- Slug: /remote/multiplex

This page describes multiplex workers, how to write multiplex-compatible
rules, and workarounds for certain limitations.

Caution: Experimental features are subject to change at any time.

_Multiplex workers_ allow Bazel to handle multiple requests with a single worker
process. For multi-threaded workers, Bazel can use fewer resources to
achieve the same, or better performance. For example, instead of having one
worker process per worker, Bazel can have four multiplexed workers talking to
the same worker process, which can then handle requests in parallel. For
languages like Java and Scala, this saves JVM warm-up time and JIT compilation
time, and in general it allows using one shared cache between all workers of
the same type.

## Overview

There are two layers between the Bazel server and the worker process. For certain
mnemonics that can run processes in parallel, Bazel gets a `WorkerProxy` from
the worker pool. The `WorkerProxy` forwards requests to the worker process
sequentially along with a `request_id`, the worker process processes the request
and sends responses to the `WorkerMultiplexer`. When the `WorkerMultiplexer`
receives a response, it parses the `request_id` and then forwards the responses
back to the correct `WorkerProxy`. Just as with non-multiplexed workers, all
communication is done over standard in/out, but the tool cannot just use
`stderr` for user-visible output ([see below](#output)).

Each worker has a key. Bazel uses the key's hash code (composed of environment
variables, the execution root, and the mnemonic) to determine which
`WorkerMultiplexer` to use. `WorkerProxy`s communicate with the same
`WorkerMultiplexer` if they have the same hash code. Therefore, assuming
environment variables and the execution root are the same in a single Bazel
invocation, each unique mnemonic can only have one `WorkerMultiplexer` and one
worker process. The total number of workers, including regular workers and
`WorkerProxy`s, is still limited by `--worker_max_instances`.

## Writing multiplex-compatible rules

The rule's worker process should be multi-threaded to take advantage of
multiplex workers. Protobuf allows a ruleset to parse a single request even
though there might be multiple requests piling up in the stream. Whenever the
worker process parses a request from the stream, it should handle the request in
a new thread. Because different thread could complete and write to the stream at
the same time, the worker process needs to make sure the responses are written
atomically (messages don't overlap). Responses must contain the
`request_id` of the request they're handling.

### Handling multiplex output

Multiplex workers need to be more careful about handling their output than
singleplex workers. Anything sent to `stderr` will go into a single log file
shared among all `WorkerProxy`s of the same type,
randomly interleaved between concurrent requests. While redirecting `stdout`
into `stderr` is a good idea, do not collect that output into the `output`
field of `WorkResponse`, as that could show the user mangled pieces of output.
If your tool only sends user-oriented output to `stdout` or `stderr`, you will
need to change that behaviour before you can enable multiplex workers.

## Enabling multiplex workers

Multiplex workers are not enabled by default. A ruleset can turn on multiplex
workers by using the `supports-multiplex-workers` tag in the
`execution_requirements` of an action (just like the `supports-workers` tag
enables regular workers). As is the case when using regular workers, a worker
strategy needs to be specified, either at the ruleset level (for example,
`--strategy=[some_mnemonic]=worker`) or generally at the strategy level (for
example, `--dynamic_local_strategy=worker,standalone`.) No additional flags are
necessary, and `supports-multiplex-workers` takes precedence over
`supports-workers`, if both are set. You can turn off multiplex workers
globally by passing `--noworker_multiplex`.

A ruleset is encouraged to use multiplex workers if possible,  to reduce memory
pressure and improve performance. However, multiplex workers are not currently
compatible with [dynamic execution](/remote/dynamic) unless they
implement multiplex sandboxing. Attempting to run non-sandboxed multiplex
workers with dynamic execution will silently use sandboxed
singleplex workers instead.

## Multiplex sandboxing

Multiplex workers can be sandboxed by adding explicit support for it in the
worker implementations. While singleplex worker sandboxing can be done by
running each worker process in its own sandbox, multiplex workers share the
process working directory between multiple parallel requests. To allow
sandboxing of multiplex workers, the worker must support reading from and
writing to a subdirectory specified in each request, instead of directly in
its working directory.

To support multiplex sandboxing, the worker must use the `sandbox_dir` field
from the `WorkRequest` and use that as a prefix for all file reads and writes.
While the `arguments` and `inputs` fields remain unchanged from an unsandboxed
request, the actual inputs are relative to the `sandbox_dir`. The worker must
translate file paths found in `arguments` and `inputs` to read from this
modified path, and must also write all outputs relative to the `sandbox_dir`.
This includes paths such as '.', as well as paths found in files specified
in the arguments (such as ["argfile"](https://docs.oracle.com/javase/7/docs/technotes/tools/windows/javac.html#commandlineargfile) arguments).

Once a worker supports multiplex sandboxing, the ruleset can declare this
support by adding `supports-multiplex-sandboxing` to the
`execution_requirements` of an action. Bazel will then use multiplex sandboxing
if the `--experimental_worker_multiplex_sandboxing` flag is passed, or if
the worker is used with dynamic execution.

The worker files of a sandboxed multiplex worker are still relative to the
working directory of the worker process. Thus, if a file is
used both for running the worker and as an input, it must be specified both as
an input in the flagfile argument as well as in `tools`, `executable`, or
`runfiles`.

---

## Output Directory Layout
- URL: https://bazel.build/remote/output-directories
- Source: remote/output-directories.mdx
- Slug: /remote/output-directories

This page covers requirements and layout for output directories.

## Requirements

Requirements for an output directory layout:

* Doesn't collide if multiple users are building on the same box.
* Supports building in multiple workspaces at the same time.
* Supports building for multiple target configurations in the same workspace.
* Doesn't collide with any other tools.
* Is easy to access.
* Is easy to clean, even selectively.
* Is unambiguous, even if the user relies on symbolic links when changing into
  their client directory.
* All the build state per user should be underneath one directory ("I'd like to
  clean all the .o files from all my clients.")

## Current layout

The solution that's currently implemented:

* Bazel must be invoked from a directory containing a repo boundary file, or a
  subdirectory thereof. In other words, Bazel must be invoked from inside a
  [repository](../external/overview#repository). Otherwise, an error is
  reported.
* The _outputRoot_ directory defaults to `~/.cache/bazel` on Linux,
  `/private/var/tmp` on macOS, and on Windows it defaults to `%HOME%` if
  set, else `%USERPROFILE%` if set, else the result of calling
  `SHGetKnownFolderPath()` with the `FOLDERID_Profile` flag set. If the
  environment variable `$XDG_CACHE_HOME` is set on either Linux or
  macOS, the value `${XDG_CACHE_HOME}/bazel` will override the default.
  If the environment variable `$TEST_TMPDIR` is set, as in a test of Bazel
  itself, then that value overrides any defaults.
* The Bazel user's build state is located beneath `outputRoot/_bazel_$USER`.
  This is called the _outputUserRoot_ directory.
* Beneath the `outputUserRoot` directory there is an `install` directory, and in
  it is an `installBase` directory whose name is the MD5 hash of the Bazel
  installation manifest.
* Beneath the `outputUserRoot` directory, an `outputBase` directory
  is also created whose name is the MD5 hash of the path name of the workspace
  root. So, for example, if Bazel is running in the workspace root
  `/home/user/src/my-project` (or in a directory symlinked to that one), then
  an output base directory is created called:
  `/home/user/.cache/bazel/_bazel_user/7ffd56a6e4cb724ea575aba15733d113`. You
  can also run `echo -n $(pwd) | md5sum` in the workspace root to get the MD5.
* You can use Bazel's `--output_base` startup option to override the default
  output base directory. For example,
  `bazel --output_base=/tmp/bazel/output build x/y:z`.
* You can also use Bazel's `--output_user_root` startup option to override the
  default install base and output base directories. For example:
  `bazel --output_user_root=/tmp/bazel build x/y:z`.

The symlinks for "bazel-&lt;workspace-name&gt;", "bazel-out", "bazel-testlogs",
and "bazel-bin" are put in the workspace directory; these symlinks point to some
directories inside a target-specific directory inside the output directory.
These symlinks are only for the user's convenience, as Bazel itself does not
use them. Also, this is done only if the workspace root is writable.

## Layout diagram

The directories are laid out as follows:

```
&lt;workspace-name&gt;/                         <== The workspace root
  bazel-my-project => <..._main>          <== Symlink to execRoot
  bazel-out => <...bazel-out>             <== Convenience symlink to outputPath
  bazel-bin => <...bin>                   <== Convenience symlink to most recent written bin dir $(BINDIR)
  bazel-testlogs => <...testlogs>         <== Convenience symlink to the test logs directory

/home/user/.cache/bazel/                  <== Root for all Bazel output on a machine: outputRoot
  _bazel_$USER/                           <== Top level directory for a given user depends on the user name:
                                              outputUserRoot
    install/
      fba9a2c87ee9589d72889caf082f1029/   <== Hash of the Bazel install manifest: installBase
        _embedded_binaries/               <== Contains binaries and scripts unpacked from the data section of
                                              the bazel executable on first run (such as helper scripts and the
                                              main Java file BazelServer_deploy.jar)
    7ffd56a6e4cb724ea575aba15733d113/     <== Hash of the client's workspace root (such as
                                              /home/user/src/my-project): outputBase
      action_cache/                       <== Action cache directory hierarchy
                                              This contains the persistent record of the file
                                              metadata (timestamps, and perhaps eventually also MD5
                                              sums) used by the FilesystemValueChecker.
      command.log                         <== A copy of the stdout/stderr output from the most
                                              recent bazel command.
      external/                           <== The directory that remote repositories are
                                              downloaded/symlinked into.
      server/                             <== The Bazel server puts all server-related files (such
                                              as socket file, logs, etc) here.
        jvm.out                           <== The debugging output for the server.
      execroot/                           <== The working directory for all actions. For special
                                              cases such as sandboxing and remote execution, the
                                              actions run in a directory that mimics execroot.
                                              Implementation details, such as where the directories
                                              are created, are intentionally hidden from the action.
                                              Every action can access its inputs and outputs relative
                                              to the execroot directory.
        _main/                            <== Working tree for the Bazel build & root of symlink forest: execRoot
          _bin/                           <== Helper tools are linked from or copied to here.

          bazel-out/                      <== All actual output of the build is under here: outputPath
            _tmp/actions/                 <== Action output directory. This contains a file with the
                                              stdout/stderr for every action from the most recent
                                              bazel run that produced output.
            local_linux-fastbuild/        <== one subdirectory per unique target BuildConfiguration instance;
                                              this is currently encoded
              bin/                        <== Bazel outputs binaries for target configuration here: $(BINDIR)
                foo/bar/_objs/baz/        <== Object files for a cc_* rule named //foo/bar:baz
                  foo/bar/baz1.o          <== Object files from source //foo/bar:baz1.cc
                  other_package/other.o   <== Object files from source //other_package:other.cc
                foo/bar/baz               <== foo/bar/baz might be the artifact generated by a cc_binary named
                                              //foo/bar:baz
                foo/bar/baz.runfiles/     <== The runfiles symlink farm for the //foo/bar:baz executable.
                  MANIFEST
                  _main/
                    ...
              genfiles/                   <== Bazel puts generated source for the target configuration here:
                                              $(GENDIR)
                foo/bar.h                     such as foo/bar.h might be a headerfile generated by //foo:bargen
              testlogs/                   <== Bazel internal test runner puts test log files here
                foo/bartest.log               such as foo/bar.log might be an output of the //foo:bartest test with
                foo/bartest.status            foo/bartest.status containing exit status of the test (such as
                                              PASSED or FAILED (Exit 1), etc)
            host/                         <== BuildConfiguration for build host (user's workstation), for
                                              building prerequisite tools, that will be used in later stages
                                              of the build (ex: Protocol Compiler)
        &lt;packages&gt;/                       <== Packages referenced in the build appear as if under a regular workspace
```

The layout of the \*.runfiles directories is documented in more detail in the places pointed to by RunfilesSupport.

## `bazel clean`

`bazel clean` does an `rm -rf` on the `outputPath` and the `action_cache`
directory. It also removes the workspace symlinks. The `--expunge` option
will clean the entire outputBase.

---

## Persistent Workers
- URL: https://bazel.build/remote/persistent
- Source: remote/persistent.mdx
- Slug: /remote/persistent

This page covers how to use persistent workers, the benefits, requirements, and
how workers affect sandboxing.

A persistent worker is a long-running process started by the Bazel server, which
functions as a *wrapper* around the actual *tool* (typically a compiler), or is
the *tool* itself. In order to benefit from persistent workers, the tool must
support doing a sequence of compilations, and the wrapper needs to translate
between the tool's API and the request/response format described below. The same
worker might be called with and without the `--persistent_worker` flag in the
same build, and is responsible for appropriately starting and talking to the
tool, as well as shutting down workers on exit. Each worker instance is assigned
(but not chrooted to) a separate working directory under
`<outputBase>/bazel-workers`.

Using persistent workers is an
[execution strategy](/docs/user-manual#execution-strategy) that decreases
start-up overhead, allows more JIT compilation, and enables caching of for
example the abstract syntax trees in the action execution. This strategy
achieves these improvements by sending multiple requests to a long-running
process.

Persistent workers are implemented for multiple languages, including Java,
[Scala](https://github.com/bazelbuild/rules_scala),
[Kotlin](https://github.com/bazelbuild/rules_kotlin), and more.

Programs using a NodeJS runtime can use the
[@bazel/worker](https://www.npmjs.com/package/@bazel/worker) helper library to
implement the worker protocol.

## Using persistent workers

[Bazel 0.27 and higher](https://blog.bazel.build/2019/06/19/list-strategy.html)
uses persistent workers by default when executing builds, though remote
execution takes precedence. For actions that do not support persistent workers,
Bazel falls back to starting a tool instance for each action. You can explicitly
set your build to use persistent workers by setting the `worker`
[strategy](/docs/user-manual#execution-strategy) for the applicable tool
mnemonics. As a best practice, this example includes specifying `local` as a
fallback to the `worker` strategy:

```posix-terminal
bazel build //{{ '<var>' }}my:target{{ '</var>' }} --strategy=Javac=worker,local
```

Using the workers strategy instead of the local strategy can boost compilation
speed significantly, depending on implementation. For Java, builds can be 2–4
times faster, sometimes more for incremental compilation. Compiling Bazel is
about 2.5 times as fast with workers. For more details, see the
"[Choosing number of workers](#number-of-workers)" section.

If you also have a remote build environment that matches your local build
environment, you can use the experimental
[*dynamic* strategy](https://blog.bazel.build/2019/02/01/dynamic-spawn-scheduler.html),
which races a remote execution and a worker execution. To enable the dynamic
strategy, pass the
[--experimental_spawn_scheduler](/reference/command-line-reference#flag--experimental_spawn_scheduler)
flag. This strategy automatically enables workers, so there is no need to
specify the `worker` strategy, but you can still use `local` or `sandboxed` as
fallbacks.

## Choosing number of workers

The default number of worker instances per mnemonic is 4, but can be adjusted
with the
[`worker_max_instances`](/reference/command-line-reference#flag--worker_max_instances)
flag. There is a trade-off between making good use of the available CPUs and the
amount of JIT compilation and cache hits you get. With more workers, more
targets will pay start-up costs of running non-JITted code and hitting cold
caches. If you have a small number of targets to build, a single worker may give
the best trade-off between compilation speed and resource usage (for example,
see [issue #8586](https://github.com/bazelbuild/bazel/issues/8586).
The `worker_max_instances` flag sets the maximum number of worker instances per
mnemonic and flag set (see below), so in a mixed system you could end up using
quite a lot of memory if you keep the default value. For incremental builds the
benefit of multiple worker instances is even smaller.

This graph shows the from-scratch compilation times for Bazel (target
`//src:bazel`) on a 6-core hyper-threaded Intel Xeon 3.5 GHz Linux workstation
with 64 GB of RAM. For each worker configuration, five clean builds are run and
the average of the last four are taken.

![Graph of performance improvements of clean builds](/docs/images/workers-clean-chart.png "Performance improvements of clean builds")

**Figure 1.** Graph of performance improvements of clean builds.

For this configuration, two workers give the fastest compile, though at only 14%
improvement compared to one worker. One worker is a good option if you want to
use less memory.

Incremental compilation typically benefits even more. Clean builds are
relatively rare, but changing a single file between compiles is common, in
particular in test-driven development. The above example also has some non-Java
packaging actions to it that can overshadow the incremental compile time.

Recompiling the Java sources only
(`//src/main/java/com/google/devtools/build/lib/bazel:BazelServer_deploy.jar`)
after changing an internal string constant in
[AbstractContainerizingSandboxedSpawn.java](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/sandbox/AbstractContainerizingSandboxedSpawn.java)
gives a 3x speed-up (average of 20 incremental builds with one warmup build
discarded):

![Graph of performance improvements of incremental builds](/docs/images/workers-incremental-chart.png "Performance improvements of incremental builds")

**Figure 2.** Graph of performance improvements of incremental builds.

The speed-up depends on the change being made. A speed-up of a factor 6 is
measured in the above situation when a commonly used constant is changed.

## Modifying persistent workers

You can pass the
[`--worker_extra_flag`](/reference/command-line-reference#flag--worker_extra_flag)
flag to specify start-up flags to workers, keyed by mnemonic. For instance,
passing `--worker_extra_flag=javac=--debug` turns on debugging for Javac only.
Only one worker flag can be set per use of this flag, and only for one mnemonic.
Workers are not just created separately for each mnemonic, but also for
variations in their start-up flags. Each combination of mnemonic and start-up
flags is combined into a `WorkerKey`, and for each `WorkerKey` up to
`worker_max_instances` workers may be created. See the next section for how the
action configuration can also specify set-up flags.

Passing the
[`--worker_sandboxing`](/reference/command-line-reference#flag--worker_sandboxing)
flag makes each worker request use a separate sandbox directory for all its
inputs. Setting up the [sandbox](/docs/sandboxing) takes some extra time,
especially on macOS, but gives a better correctness guarantee.

The
[`--worker_quit_after_build`](/reference/command-line-reference#flag--worker_quit_after_build)
flag is mainly useful for debugging and profiling. This flag forces all workers
to quit once a build is done. You can also pass
[`--worker_verbose`](/reference/command-line-reference#flag--worker_verbose) to
get more output about what the workers are doing. This flag is reflected in the
`verbosity` field in `WorkRequest`, allowing worker implementations to also be
more verbose.

Workers store their logs in the `<outputBase>/bazel-workers` directory, for
example
`/tmp/_bazel_larsrc/191013354bebe14fdddae77f2679c3ef/bazel-workers/worker-1-Javac.log`.
The file name includes the worker id and the mnemonic. Since there can be more
than one `WorkerKey` per mnemonic, you may see more than `worker_max_instances`
log files for a given mnemonic.

For Android builds, see details at the
[Android Build Performance page](/docs/android-build-performance).

## Implementing persistent workers

See the [creating persistent workers](/remote/creating) page for more
information on how to make a worker.

This example shows a Starlark configuration for a worker that uses JSON:

```python
args_file = ctx.actions.declare_file(ctx.label.name + "_args_file")
ctx.actions.write(
    output = args_file,
    content = "\n".join(["-g", "-source", "1.5"] + ctx.files.srcs),
)
ctx.actions.run(
    mnemonic = "SomeCompiler",
    executable = "bin/some_compiler_wrapper",
    inputs = inputs,
    outputs = outputs,
    arguments = [ "-max_mem=4G",  "@%s" % args_file.path],
    execution_requirements = {
        "supports-workers" : "1", "requires-worker-protocol" : "json" }
)
```

With this definition, the first use of this action would start with executing
the command line `/bin/some_compiler -max_mem=4G --persistent_worker`. A request
to compile `Foo.java` would then look like:

NOTE: While the protocol buffer specification uses "snake case" (`request_id`),
the JSON protocol uses "camel case" (`requestId`). In this document, we will use
camel case in the JSON examples, but snake case when talking about the field
regardless of protocol.

```json
{
  "arguments": [ "-g", "-source", "1.5", "Foo.java" ]
  "inputs": [
    { "path": "symlinkfarm/input1", "digest": "d49a..." },
    { "path": "symlinkfarm/input2", "digest": "093d..." },
  ],
}
```

The worker receives this on `stdin` in newline-delimited JSON format (because
`requires-worker-protocol` is set to JSON). The worker then performs the action,
and sends a JSON-formatted `WorkResponse` to Bazel on its stdout. Bazel then
parses this response and manually converts it to a `WorkResponse` proto. To
communicate with the associated worker using binary-encoded protobuf instead of
JSON, `requires-worker-protocol` would be set to `proto`, like this:

```
  execution_requirements = {
    "supports-workers" : "1" ,
    "requires-worker-protocol" : "proto"
  }
```

If you do not include `requires-worker-protocol` in the execution requirements,
Bazel will default the worker communication to use protobuf.

Bazel derives the `WorkerKey` from the mnemonic and the shared flags, so if this
configuration allowed changing the `max_mem` parameter, a separate worker would
be spawned for each value used. This can lead to excessive memory consumption if
too many variations are used.

Each worker can currently only process one request at a time. The experimental
[multiplex workers](/remote/multiplex) feature allows using multiple
threads, if the underlying tool is multithreaded and the wrapper is set up to
understand this.

In
[this GitHub repo](https://github.com/Ubehebe/bazel-worker-examples),
you can see example worker wrappers written in Java as well as in Python. If you
are working in JavaScript or TypeScript, the
[@bazel/worker package](https://www.npmjs.com/package/@bazel/worker)
and
[nodejs worker example](https://github.com/bazelbuild/rules_nodejs/tree/stable/examples/worker)
might be helpful.

## How do workers affect sandboxing?

Using the `worker` strategy by default does not run the action in a
[sandbox](/docs/sandboxing), similar to the `local` strategy. You can set the
`--worker_sandboxing` flag to run all workers inside sandboxes, making sure each
execution of the tool only sees the input files it's supposed to have. The tool
may still leak information between requests internally, for instance through a
cache. Using `dynamic` strategy
[requires workers to be sandboxed](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/exec/SpawnStrategyRegistry.java).

To allow correct use of compiler caches with workers, a digest is passed along
with each input file. Thus the compiler or the wrapper can check if the input is
still valid without having to read the file.

Even when using the input digests to guard against unwanted caching, sandboxed
workers offer less strict sandboxing than a pure sandbox, because the tool may
keep other internal state that has been affected by previous requests.

Multiplex workers can only be sandboxed if the worker implementation support it,
and this sandboxing must be separately enabled with the
`--experimental_worker_multiplex_sandboxing` flag. See more details in
[the design doc](https://docs.google.com/document/d/1ncLW0hz6uDhNvci1dpzfEoifwTiNTqiBEm1vi-bIIRM/edit)).

## Further reading

For more information on persistent workers, see:

*   [Original persistent workers blog post](https://blog.bazel.build/2015/12/10/java-workers.html)
*   [Haskell implementation description](https://www.tweag.io/blog/2019-09-25-bazel-ghc-persistent-worker-internship/)
*   [Blog post by Mike Morearty](https://medium.com/@mmorearty/how-to-create-a-persistent-worker-for-bazel-7738bba2cabb)
*   [Front End Development with Bazel: Angular/TypeScript and Persistent Workers
    w/ Asana](https://www.youtube.com/watch?v=0pgERydGyqo)
*   [Bazel strategies explained](https://jmmv.dev/2019/12/bazel-strategies.html)
*   [Informative worker strategy discussion on the bazel-discuss mailing list](https://groups.google.com/forum/#!msg/bazel-discuss/oAEnuhYOPm8/ol7hf4KWJgAJ)

---

## Remote Execution Overview
- URL: https://bazel.build/remote/rbe
- Source: remote/rbe.mdx
- Slug: /remote/rbe

This page covers the benefits, requirements, and options for running Bazel
with remote execution.

By default, Bazel executes builds and tests on your local machine. Remote
execution of a Bazel build allows you to distribute build and test actions
across multiple machines, such as a datacenter.

Remote execution provides the following benefits:

*  Faster build and test execution through scaling of nodes available
   for parallel actions
*  A consistent execution environment for a development team
*  Reuse of build outputs across a development team

Bazel uses an open-source
[gRPC protocol](https://github.com/bazelbuild/remote-apis)
to allow for remote execution and remote caching.

For a list of commercially supported remote execution services as well as
self-service tools, see
[Remote Execution Services](https://www.bazel.build/remote-execution-services.html)

## Requirements

Remote execution of Bazel builds imposes a set of mandatory configuration
constraints on the build. For more information, see
[Adapting Bazel Rules for Remote Execution](/remote/rules).

---

## Adapting Bazel Rules for Remote Execution
- URL: https://bazel.build/remote/rules
- Source: remote/rules.mdx
- Slug: /remote/rules

This page is intended for Bazel users writing custom build and test rules
who want to understand the requirements for Bazel rules in the context of
remote execution.

Remote execution allows Bazel to execute actions on a separate platform, such as
a datacenter. Bazel uses a
[gRPC protocol](https://github.com/bazelbuild/remote-apis/blob/main/build/bazel/remote/execution/v2/remote_execution.proto)
for its remote execution. You can try remote execution with
[bazel-buildfarm](https://github.com/bazelbuild/bazel-buildfarm),
an open-source project that aims to provide a distributed remote execution
platform.

This page uses the following terminology when referring to different
environment types or *platforms*:

*   **Host platform** - where Bazel runs.
*   **Execution platform** - where Bazel actions run.
*   **Target platform** - where the build outputs (and some actions) run.

## Overview

When configuring a Bazel build for remote execution, you must follow the
guidelines described in this page to ensure the build executes remotely
error-free. This is due to the nature of remote execution, namely:

*   **Isolated build actions.** Build tools do not retain state and dependencies
    cannot leak between them.

*   **Diverse execution environments.** Local build configuration is not always
    suitable for remote execution environments.

This page describes the issues that can arise when implementing custom Bazel
build and test rules for remote execution and how to avoid them. It covers the
following topics:

*  [Invoking build tools through toolchain rules](#toolchain-rules)
*  [Managing implicit dependencies](#manage-dependencies)
*  [Managing platform-dependent binaries](#manage-binaries)
*  [Managing configure-style WORKSPACE rules](#manage-workspace-rules)

## Invoking build tools through toolchain rules

A Bazel toolchain rule is a configuration provider that tells a build rule what
build tools, such as compilers and linkers, to use and how to configure them
using parameters defined by the rule's creator. A toolchain rule allows build
and test rules to invoke build tools in a predictable, preconfigured manner
that's compatible with remote execution. For example, use a toolchain rule
instead of invoking build tools via the `PATH`, `JAVA_HOME`, or other local
variables that may not be set to equivalent values (or at all) in the remote
execution environment.

Toolchain rules currently exist for Bazel build and test rules for
[Scala](https://github.com/bazelbuild/rules_scala/blob/master/scala/scala_toolch
ain.bzl),
[Rust](https://github.com/bazelbuild/rules_rust/blob/main/rust/toolchain.bzl),
and [Go](https://github.com/bazelbuild/rules_go/blob/master/go/toolchains.rst),
and new toolchain rules are under way for other languages and tools such as
[bash](https://docs.google.com/document/d/e/2PACX-1vRCSB_n3vctL6bKiPkIa_RN_ybzoAccSe0ic8mxdFNZGNBJ3QGhcKjsL7YKf-ngVyjRZwCmhi_5KhcX/pub).
If a toolchain rule does not exist for the tool your rule uses, consider
[creating a toolchain rule](/extending/toolchains#creating-a-toolchain-rule).

## Managing implicit dependencies

If a build tool can access dependencies across build actions, those actions will
fail when remotely executed because each remote build action is executed
separately from others. Some build tools retain state across build actions and
access dependencies that have not been explicitly included in the tool
invocation, which will cause remotely executed build actions to fail.

For example, when Bazel instructs a stateful compiler to locally build _foo_,
the compiler retains references to foo's build outputs. When Bazel then
instructs the compiler to build _bar_, which depends on _foo_, without
explicitly stating that dependency in the BUILD file for inclusion in the
compiler invocation, the action executes successfully as long as the same
compiler instance executes for both actions (as is typical for local execution).
However, since in a remote execution scenario each build action executes a
separate compiler instance, compiler state and _bar_'s implicit dependency on
_foo_ will be lost and the build will fail.

To help detect and eliminate these dependency problems, Bazel 0.14.1 offers the
local Docker sandbox, which has the same restrictions for dependencies as remote
execution. Use the sandbox to prepare your build for remote execution by
identifying and resolving dependency-related build errors. See [Troubleshooting Bazel Remote Execution with Docker Sandbox](/remote/sandbox)
for more information.

## Managing platform-dependent binaries

Typically, a binary built on the host platform cannot safely execute on an
arbitrary remote execution platform due to potentially mismatched dependencies.
For example, the SingleJar binary supplied with Bazel targets the host platform.
However, for remote execution, SingleJar must be compiled as part of the process
of building your code so that it targets the remote execution platform. (See the
[target selection logic](https://github.com/bazelbuild/bazel/blob/130aeadfd660336572c3da397f1f107f0c89aa8d/tools/jdk/BUILD#L115).)

Do not ship binaries of build tools required by your build with your source code
unless you are sure they will safely run in your execution platform. Instead, do
one of the following:

*   Ship or externally reference the source code for the tool so that it can be
    built for the remote execution platform.

*   Pre-install the tool into the remote execution environment (for example, a
    toolchain container) if it's stable enough and use toolchain rules to run it
    in your build.

## Managing configure-style WORKSPACE rules

Bazel's `WORKSPACE` rules can be used for probing the host platform for tools
and libraries required by the build, which, for local builds, is also Bazel's
execution platform. If the build explicitly depends on local build tools and
artifacts, it will fail during remote execution if the remote execution platform
is not identical to the host platform.

The following actions performed by `WORKSPACE` rules are not compatible with
remote execution:

*   **Building binaries.** Executing compilation actions in `WORKSPACE` rules
    results in binaries that are incompatible with the remote execution platform
    if different from the host platform.

*   **Installing `pip` packages.** `pip` packages  installed via `WORKSPACE`
    rules require that their dependencies be pre-installed on the host platform.
    Such packages, built specifically for the host platform, will be
    incompatible with the remote execution platform if different from the host
    platform.

*   **Symlinking to local tools or artifacts.** Symlinks to tools or libraries
    installed on the host platform created via `WORKSPACE` rules will cause the
    build to fail on the remote execution platform as Bazel will not be able to
    locate them. Instead, create symlinks using standard build actions so that
    the symlinked tools and libraries are accessible from Bazel's `runfiles`
    tree. Do not use [`repository_ctx.symlink`](/rules/lib/builtins/repository_ctx#symlink)
    to symlink target files outside of the external repo directory.

*   **Mutating the host platform.** Avoid creating files outside of the Bazel
    `runfiles` tree, creating environment variables, and similar actions, as
     they may behave unexpectedly on the remote execution platform.

To help find potential non-hermetic behavior you can use [Workspace rules log](/remote/workspace).

If an external dependency executes specific operations dependent on the host
platform, you should split those operations between `WORKSPACE` and build
rules as follows:

*   **Platform inspection and dependency enumeration.** These operations are
    safe to execute locally via `WORKSPACE` rules, which can check which
    libraries are installed, download packages that must be built, and prepare
    required artifacts for compilation. For remote execution, these rules must
    also support using pre-checked artifacts to provide the information that
    would normally be obtained during host platform inspection. Pre-checked
    artifacts allow Bazel to describe dependencies as if they were local. Use
    conditional statements or the `--override_repository` flag for this.

*   **Generating or compiling target-specific artifacts and platform mutation**.
    These operations must be executed via regular build rules. Actions that
    produce target-specific artifacts for external dependencies must execute
    during the build.

To more easily generate pre-checked artifacts for remote execution, you can use
`WORKSPACE` rules to emit generated files. You can run those rules on each new
execution environment, such as inside each toolchain container, and check the
outputs of your remote execution build in to your source repo to reference.

For example, for Tensorflow's rules for [`cuda`](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl)
and [`python`](https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl),
the `WORKSPACE` rules produce the following [`BUILD files`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/third_party/toolchains/cpus/py).
For local execution, files produced by checking the host environment are used.
For remote execution, a [conditional statement](https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl#L304)
on an environment variable allows the rule to use files that are checked into
the repo.

The `BUILD` files declare [`genrules`](https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl#L84)
that can run both locally and remotely, and perform the necessary processing
that was previously done via `repository_ctx.symlink` as shown [here](https://github.com/tensorflow/tensorflow/blob/d1ba01f81d8fa1d0171ba9ce871599063d5c7eb9/third_party/gpus/cuda_configure.bzl#L730).

---

## Troubleshooting Bazel Remote Execution with Docker Sandbox
- URL: https://bazel.build/remote/sandbox
- Source: remote/sandbox.mdx
- Slug: /remote/sandbox

Bazel builds that succeed locally may fail when executed remotely due to
restrictions and requirements that do not affect local builds. The most common
causes of such failures are described in [Adapting Bazel Rules for Remote Execution](/remote/rules).

This page describes how to identify and resolve the most common issues that
arise with remote execution using the Docker sandbox feature, which imposes
restrictions upon the build equal to those of remote execution. This allows you
to troubleshoot your build without the need for a remote execution service.

The Docker sandbox feature mimics the restrictions of remote execution as
follows:

*   **Build actions execute in toolchain containers.** You can use the same
    toolchain containers to run your build locally and remotely via a service
    supporting containerized remote execution.

*   **No extraneous data crosses the container boundary.** Only explicitly
    declared inputs and outputs enter and leave the container, and only after
    the associated build action successfully completes.

*   **Each action executes in a fresh container.** A new, unique container is
    created for each spawned build action.

Note: Builds take noticeably more time to complete when the Docker sandbox
feature is enabled. This is normal.

You can troubleshoot these issues using one of the following methods:

*   **[Troubleshooting natively.](#troubleshooting-natively)** With this method,
    Bazel and its build actions run natively on your local machine. The Docker
    sandbox feature imposes restrictions upon the build equal to those of remote
    execution. However, this method will not detect local tools, states, and
    data leaking into your build, which will cause problems with remote execution.

*   **[Troubleshooting in a Docker container.](#troubleshooting-docker-container)**
    With this method, Bazel and its build actions run inside a Docker container,
    which allows you to detect tools, states, and data leaking from the local
    machine into the build in addition to imposing restrictions
    equal to those of remote execution. This method provides insight into your
    build even if portions of the build are failing. This method is experimental
    and not officially supported.

## Prerequisites

Before you begin troubleshooting, do the following if you have not already done so:

*   Install Docker and configure the permissions required to run it.
*   Install Bazel 0.14.1 or later. Earlier versions do not support the Docker
    sandbox feature.
*   Add the [bazel-toolchains](https://releases.bazel.build/bazel-toolchains.html)
    repo, pinned to the latest release version, to your build's `WORKSPACE` file
    as described [here](https://releases.bazel.build/bazel-toolchains.html).
*   Add flags to your `.bazelrc` file to enable the feature. Create the file in
    the root directory of your Bazel project if it does not exist. Flags below
    are a reference sample. Please see the latest
    [`.bazelrc`](https://github.com/bazelbuild/bazel-toolchains/tree/master/bazelrc)
    file in the bazel-toolchains repo and copy the values of the flags defined
    there for config `docker-sandbox`.

```
# Docker Sandbox Mode
build:docker-sandbox --host_javabase=<...>
build:docker-sandbox --javabase=<...>
build:docker-sandbox --crosstool_top=<...>
build:docker-sandbox --experimental_docker_image=<...>
build:docker-sandbox --spawn_strategy=docker --strategy=Javac=docker --genrule_strategy=docker
build:docker-sandbox --experimental_docker_verbose
build:docker-sandbox --experimental_enable_docker_sandbox
```

Note: The flags referenced in the `.bazelrc` file shown above are configured
to run within the [`rbe-ubuntu16-04`](https://console.cloud.google.com/launcher/details/google/rbe-ubuntu16-04)
container.

If your rules require additional tools, do the following:

1.  Create a custom Docker container by installing tools using a [Dockerfile](https://docs.docker.com/engine/reference/builder/)
    and [building](https://docs.docker.com/engine/reference/commandline/build/)
    the image locally.

2.  Replace the value of the `--experimental_docker_image` flag above with the
    name of your custom container image.


## Troubleshooting natively

This method executes Bazel and all of its build actions directly on the local
machine and is a reliable way to confirm whether your build will succeed when
executed remotely.

However, with this method, locally installed tools, binaries, and data may leak
into into your build, especially if it uses [configure-style WORKSPACE rules](/remote/rules#manage-workspace-rules).
Such leaks will cause problems with remote execution; to detect them, [troubleshoot in a Docker container](#troubleshooting-docker-container)
in addition to troubleshooting natively.

### Step 1: Run the build

1.  Add the `--config=docker-sandbox` flag to the Bazel command that executes
    your build. For example:

    ```posix-terminal
    bazel --bazelrc=.bazelrc build --config=docker-sandbox <var>target</var>
    ```

2.  Run the build and wait for it to complete. The build will run up to four
    times slower than normal due to the Docker sandbox feature.

You may encounter the following error:

```none {:.devsite-disable-click-to-copy}
ERROR: 'docker' is an invalid value for docker spawn strategy.
```

If you do, run the build again with the `--experimental_docker_verbose`  flag.
This flag enables verbose error messages. This error is typically caused by a
faulty Docker installation or lack of permissions to execute it under the
current user account. See the [Docker documentation](https://docs.docker.com/install/linux/linux-postinstall/)
for more information. If problems persist, skip ahead to [Troubleshooting in a Docker container](#troubleshooting-docker-container).

### Step 2: Resolve detected issues

The following are the most commonly encountered issues and their workarounds.

*  **A file, tool, binary, or resource referenced by the Bazel runfiles tree is
   missing.**. Confirm that all dependencies of the affected targets have been
   [explicitly declared](/concepts/dependencies). See
   [Managing implicit dependencies](/remote/rules#manage-dependencies)
   for more information.

*  **A file, tool, binary, or resource referenced by an absolute path or the `PATH`
   variable is missing.** Confirm that all required tools are installed within
   the toolchain container and use [toolchain rules](/extending/toolchains) to properly
   declare dependencies pointing to the missing resource. See
   [Invoking build tools through toolchain rules](/remote/rules#invoking-build-tools-through-toolchain-rules)
   for more information.

*  **A binary execution fails.** One of the build rules is referencing a binary
   incompatible with the execution environment (the Docker container). See
   [Managing platform-dependent binaries](/remote/rules#manage-binaries)
   for more information. If you cannot resolve the issue, contact [bazel-discuss@google.com](mailto:bazel-discuss@google.com)
   for help.

*  **A file from `@local-jdk` is missing or causing errors.** The Java binaries
   on your local machine are leaking into the build while being incompatible with
   it. Use [`java_toolchain`](/reference/be/java#java_toolchain)
   in your rules and targets instead of `@local_jdk`. Contact [bazel-discuss@google.com](mailto:bazel-discuss@google.com) if you need further help.

*  **Other errors.** Contact [bazel-discuss@google.com](mailto:bazel-discuss@google.com) for help.

## Troubleshooting in a Docker container

With this method, Bazel runs inside a host Docker container, and Bazel's build
actions execute inside individual toolchain containers spawned by the Docker
sandbox feature. The sandbox spawns a brand new toolchain container for each
build action and only one action executes in each toolchain container.

This method provides more granular control of tools installed in the host
environment. By separating the execution of the build from the execution of its
build actions and keeping the installed tooling to a minimum, you can verify
whether your build has any dependencies on the local execution environment.

### Step 1: Build the container

Note: The commands below are tailored specifically for a  `debian:stretch` base.
For other bases, modify them as necessary.

1.  Create a `Dockerfile` that creates the Docker container and installs Bazel
    with a minimal set of build tools:

    ```
    FROM debian:stretch

    RUN apt-get update && apt-get install -y apt-transport-https curl software-properties-common git gcc gnupg2 g++ openjdk-8-jdk-headless python-dev zip wget vim

    RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -

    RUN add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable"

    RUN apt-get update && apt-get install -y docker-ce

    RUN wget https://releases.bazel.build/<latest Bazel version>/release/bazel-<latest Bazel version>-installer-linux-x86_64.sh -O ./bazel-installer.sh && chmod 755 ./bazel-installer.sh

    RUN ./bazel-installer.sh
    ```

2.  Build the container as `bazel_container`:

    ```posix-terminal
    docker build -t bazel_container - < Dockerfile
    ```

### Step 2: Start the container

Start the Docker container using the command shown below. In the command,
substitute the path to the source code on your host that you want to build.

```posix-terminal
docker run -it \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /tmp:/tmp \
  -v {{ '<var>' }}your source code directory{{ '</var>' }}:/src \
  -w /src \
  bazel_container \
  /bin/bash
```

This command runs the container as root, mapping the docker socket, and mounting
the `/tmp` directory. This allows Bazel to spawn other Docker containers and to
use directories under `/tmp` to share files with those containers. Your source
code is available at `/src` inside the container.

The command intentionally starts from a `debian:stretch` base container that
includes binaries incompatible with the `rbe-ubuntu16-04` container used as a
toolchain container. If binaries from the local environment are leaking into the
toolchain container, they will cause build errors.

### Step 3: Test the container

Run the following commands from inside the Docker container to test it:

```posix-terminal
docker ps

bazel version
```

### Step 4: Run the build

Run the build as shown below. The output user is root so that it corresponds to
a directory that is accessible with the same absolute path from inside the host
container in which Bazel runs, from the toolchain containers spawned by the Docker
sandbox feature in which Bazel's build actions are running, and from the local
machine on which the host and action containers run.

```posix-terminal
bazel --output_user_root=/tmp/bazel_docker_root --bazelrc=.bazelrc \ build --config=docker-sandbox {{ '<var>' }}target{{ '</var>' }}
```

### Step 5: Resolve detected issues

You can resolve build failures as follows:

*   If the build fails with an "out of disk space" error, you  can increase this
    limit by starting the host container with the flag `--memory=XX` where `XX`
    is the allocated disk space in gigabytes. This is experimental and may
    result in unpredictable behavior.

*   If the build fails during the analysis or loading phases, one or more of
    your build rules declared in the WORKSPACE file are not compatible with
    remote execution. See [Adapting Bazel Rules for Remote Execution](/remote/rules)
    for possible causes and workarounds.

*   If the build fails for any other reason, see the troubleshooting steps in [Step 2: Resolve detected issues](#start-container).

---

## Finding Non-Hermetic Behavior in WORKSPACE Rules
- URL: https://bazel.build/remote/workspace
- Source: remote/workspace.mdx
- Slug: /remote/workspace

In the following, a host machine is the machine where Bazel runs.

When using remote execution, the actual build and/or test steps are not
happening on the host machine, but are instead sent off to the remote execution
system. However, the steps involved in resolving workspace rules are happening
on the host machine. If your workspace rules access information about the
host machine for use during execution, your build is likely to break due to
incompatibilities between the environments.

As part of [adapting Bazel rules for remote
execution](/remote/rules), you need to find such workspace rules
and fix them. This page describes how to find potentially problematic workspace
rules using the workspace log.


## Finding non-hermetic rules

[Workspace rules](/reference/be/workspace) allow the developer to add dependencies to
external workspaces, but they are rich enough to allow arbitrary processing to
happen in the process. All related commands are happening locally and can be a
potential source of non-hermeticity. Usually non-hermetic behavior is
introduced through
[`repository_ctx`](/rules/lib/builtins/repository_ctx) which allows interacting
with the host machine.

Starting with Bazel 0.18, you can get a log of some potentially non-hermetic
actions by adding the flag `--experimental_workspace_rules_log_file=[PATH]` to
your Bazel command. Here `[PATH]` is a filename under which the log will be
created.

Things to note:

* the log captures the events as they are executed. If some steps are
  cached, they will not show up in the log, so to get a full result, don't
  forget to run `bazel clean --expunge` beforehand.

* Sometimes functions might be re-executed, in which case the related
  events will show up in the log multiple times.

* Workspace rules  currently only log Starlark events.

  Note: These particular rules do not cause hermiticity concerns as long
  as a hash is specified.

To find what was executed during workspace initialization:

1.  Run `bazel clean --expunge`. This command will clean your local cache and
    any cached repositories, ensuring that all initialization will be re-run.

2.  Add `--experimental_workspace_rules_log_file=/tmp/workspacelog` to your
    Bazel command and run the build.

    This produces a binary proto file listing messages of type
    [WorkspaceEvent](https://source.bazel.build/bazel/+/master:src/main/java/com/google/devtools/build/lib/bazel/debug/workspace_log.proto?q=WorkspaceEvent)

3.  Download the Bazel source code and navigate to the Bazel folder by using
    the command below. You need the source code to be able to parse the
    workspace log with the
    [workspacelog parser](https://source.bazel.build/bazel/+/master:src/tools/workspacelog/).

    ```posix-terminal
    git clone https://github.com/bazelbuild/bazel.git

    cd bazel
    ```

4.  In the Bazel source code repo, convert the whole workspace log to text.

    ```posix-terminal
    bazel build src/tools/workspacelog:parser

    bazel-bin/src/tools/workspacelog/parser --log_path=/tmp/workspacelog > /tmp/workspacelog.txt
    ```

5.  The output may be quite verbose and include output from built in Bazel
    rules.

    To exclude specific rules from the output, use `--exclude_rule` option.
    For example:

    ```posix-terminal
    bazel build src/tools/workspacelog:parser

    bazel-bin/src/tools/workspacelog/parser --log_path=/tmp/workspacelog \
        --exclude_rule "//external:local_config_cc" \
        --exclude_rule "//external:dep" > /tmp/workspacelog.txt
    ```

5.  Open `/tmp/workspacelog.txt` and check for unsafe operations.

The log consists of
[WorkspaceEvent](https://source.bazel.build/bazel/+/master:src/main/java/com/google/devtools/build/lib/bazel/debug/workspace_log.proto?q=WorkspaceEvent)
messages outlining certain potentially non-hermetic actions performed on a
[`repository_ctx`](/rules/lib/builtins/repository_ctx).

The actions that have been highlighted as potentially non-hermetic are as follows:

* `execute`: executes an arbitrary command on the host environment. Check if
  these may introduce any dependencies on the host environment.

* `download`, `download_and_extract`: to ensure hermetic builds, make sure
  that sha256 is specified

* `file`, `template`: this is not non-hermetic in itself, but may be a mechanism
  for introducing dependencies on the host environment into the repository.
  Ensure that you understand where the input comes from, and that it does not
  depend on the host environment.

* `os`: this is not non-hermetic in itself, but an easy way to get dependencies
  on the host environment. A hermetic build would generally not call this.
  In evaluating whether your usage is hermetic, keep in mind that this is
  running on the host and not on the workers. Getting environment specifics
  from the host is generally not a good idea for remote builds.

* `symlink`: this is normally safe, but look for red flags. Any symlinks to
  outside the repository or to an absolute path would cause problems on the
  remote worker. If the symlink is created based on host machine properties
  it would probably be problematic as well.

* `which`: checking for programs installed on the host is usually problematic
  since the workers may have different configurations.

---

## Rules
- URL: https://bazel.build/rules
- Source: rules/index.mdx
- Slug: /rules

The Bazel ecosystem has a growing and evolving set of rules to support popular
languages and packages. Much of Bazel's strength comes from the ability to
[define new rules](/extending/concepts) that can be used by others.

This page describes the recommended, native, and non-native Bazel rules.

## Recommended rules

Here is a selection of recommended rules:

* [Android](/docs/bazel-and-android)
* [C / C++](/docs/bazel-and-cpp)
* [Docker/OCI](https://github.com/bazel-contrib/rules_oci)
* [Go](https://github.com/bazelbuild/rules_go)
* [Haskell](https://github.com/tweag/rules_haskell)
* [Java](/docs/bazel-and-java)
* [JavaScript / NodeJS](https://github.com/bazelbuild/rules_nodejs)
* [Maven dependency management](https://github.com/bazelbuild/rules_jvm_external)
* [Objective-C](/docs/bazel-and-apple)
* [Package building](https://github.com/bazelbuild/rules_pkg)
* [Protocol Buffers](https://github.com/bazelbuild/rules_proto#protobuf-rules-for-bazel)
* [Python](https://github.com/bazelbuild/rules_python)
* [Rust](https://github.com/bazelbuild/rules_rust)
* [Scala](https://github.com/bazelbuild/rules_scala)
* [Shell](/reference/be/shell)
* [Webtesting](https://github.com/bazelbuild/rules_webtesting) (Webdriver)

The repository [Skylib](https://github.com/bazelbuild/bazel-skylib) contains
additional functions that can be useful when writing new rules and new
macros.

The rules above were reviewed and follow our
[requirements for recommended rules](/community/recommended-rules).
Contact the respective rule set's maintainers regarding issues and feature
requests.

To find more Bazel rules, use a search engine, take a look on
[awesomebazel.com](https://awesomebazel.com/), or search on
[GitHub](https://github.com/search?o=desc&q=bazel+rules&s=stars&type=Repositories).

## Native rules that do not apply to a specific programming language

Native rules are shipped with the Bazel binary, they are always available in
BUILD files without a `load` statement.

* Extra actions
  - [`extra_action`](/reference/be/extra-actions#extra_action)
  - [`action_listener`](/reference/be/extra-actions#action_listener)
* General
  - [`filegroup`](/reference/be/general#filegroup)
  - [`genquery`](/reference/be/general#genquery)
  - [`test_suite`](/reference/be/general#test_suite)
  - [`alias`](/reference/be/general#alias)
  - [`config_setting`](/reference/be/general#config_setting)
  - [`genrule`](/reference/be/general#genrule)
* Platform
  - [`constraint_setting`](/reference/be/platforms-and-toolchains#constraint_setting)
  - [`constraint_value`](/reference/be/platforms-and-toolchains#constraint_value)
  - [`platform`](/reference/be/platforms-and-toolchains#platform)
  - [`toolchain`](/reference/be/platforms-and-toolchains#toolchain)
  - [`toolchain_type`](/reference/be/platforms-and-toolchains#toolchain_type)
* Workspace
  - [`bind`](/reference/be/workspace#bind)
  - [`local_repository`](/reference/be/workspace#local_repository)
  - [`new_local_repository`](/reference/be/workspace#new_local_repository)
  - [`xcode_config`](/reference/be/objective-c#xcode_config)
  - [`xcode_version`](/reference/be/objective-c#xcode_version)

## Embedded non-native rules

Bazel also embeds additional rules written in [Starlark](/rules/language). Those can be loaded from
the `@bazel_tools` built-in external repository.

* Repository rules
  - [`git_repository`](/rules/lib/repo/git#git_repository)
  - [`http_archive`](/rules/lib/repo/http#http_archive)
  - [`http_file`](/rules/lib/repo/http#http_archive)
  - [`http_jar`](/rules/lib/repo/http#http_jar)
  - [Utility functions on patching](/rules/lib/repo/utils)

---

## .bzl style guide
- URL: https://bazel.build/rules/bzl-style
- Source: rules/bzl-style.mdx
- Slug: /rules/bzl-style

This page covers basic style guidelines for Starlark and also includes
information on macros and rules.

[Starlark](/rules/language) is a
language that defines how software is built, and as such it is both a
programming and a configuration language.

You will use Starlark to write `BUILD` files, macros, and build rules. Macros and
rules are essentially meta-languages - they define how `BUILD` files are written.
`BUILD` files are intended to be simple and repetitive.

All software is read more often than it is written. This is especially true for
Starlark, as engineers read `BUILD` files to understand dependencies of their
targets and details of their builds. This reading will often happen in passing,
in a hurry, or in parallel to accomplishing some other task. Consequently,
simplicity and readability are very important so that users can parse and
comprehend `BUILD` files quickly.

When a user opens a `BUILD` file, they quickly want to know the list of targets in
the file; or review the list of sources of that C++ library; or remove a
dependency from that Java binary. Each time you add a layer of abstraction, you
make it harder for a user to do these tasks.

`BUILD` files are also analyzed and updated by many different tools. Tools may not
be able to edit your `BUILD` file if it uses abstractions. Keeping your `BUILD`
files simple will allow you to get better tooling. As a code base grows, it
becomes more and more frequent to do changes across many `BUILD` files in order to
update a library or do a cleanup.

Important: Do not create a variable or macro just to avoid some amount of
repetition in `BUILD` files. Your `BUILD` file should be easily readable both by
developers and tools. The
[DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) principle doesn't
really apply here.

## General advice

*   Use [Buildifier](https://github.com/bazelbuild/buildtools/tree/master/buildifier#linter)
    as a formatter and linter.
*   Follow [testing guidelines](/rules/testing).

## Style

### Python style

When in doubt, follow the
[PEP 8 style guide](https://www.python.org/dev/peps/pep-0008/) where possible.
In particular, use four rather than two spaces for indentation to follow the
Python convention.

Since
[Starlark is not Python](/rules/language#differences-with-python),
some aspects of Python style do not apply. For example, PEP 8 advises that
comparisons to singletons be done with `is`, which is not an operator in
Starlark.


### Docstring

Document files and functions using [docstrings](https://github.com/bazelbuild/buildtools/blob/master/WARNINGS.md#function-docstring).
Use a docstring at the top of each `.bzl` file, and a docstring for each public
function.

### Document rules and aspects

Rules and aspects, along with their attributes, as well as providers and their
fields, should be documented using the `doc` argument.

### Naming convention

*   Variables and function names use lowercase with words separated by
    underscores (`[a-z][a-z0-9_]*`), such as `cc_library`.
*   Top-level private values start with one underscore. Bazel enforces that
    private values cannot be used from other files. Local variables should not
    use the underscore prefix.

### Line length

As in `BUILD` files, there is no strict line length limit as labels can be long.
When possible, try to use at most 79 characters per line (following Python's
style guide, [PEP 8](https://www.python.org/dev/peps/pep-0008/)). This guideline
should not be enforced strictly: editors should display more than 80 columns,
automated changes will frequently introduce longer lines, and humans shouldn't
spend time splitting lines that are already readable.

### Keyword arguments

In keyword arguments, spaces around the equal sign are preferred:

```python
def fct(name, srcs):
    filtered_srcs = my_filter(source = srcs)
    native.cc_library(
        name = name,
        srcs = filtered_srcs,
        testonly = True,
    )
```

### Boolean values

Prefer values `True` and `False` (rather than of `1` and `0`) for boolean values
(such as when using a boolean attribute in a rule).

### Use print only for debugging

Do not use the `print()` function in production code; it is only intended for
debugging, and will spam all direct and indirect users of your `.bzl` file. The
only exception is that you may submit code that uses `print()` if it is disabled
by default and can only be enabled by editing the source -- for example, if all
uses of `print()` are guarded by `if DEBUG:` where `DEBUG` is hardcoded to
`False`. Be mindful of whether these statements are useful enough to justify
their impact on readability.

## Macros

A macro is a function which instantiates one or more rules during the loading
phase. In general, use rules whenever possible instead of macros. The build
graph seen by the user is not the same as the one used by Bazel during the
build - macros are expanded *before Bazel does any build graph analysis.*

Because of this, when something goes wrong, the user will need to understand
your macro's implementation to troubleshoot build problems. Additionally, `bazel
query` results can be hard to interpret because targets shown in the results
come from macro expansion. Finally, aspects are not aware of macros, so tooling
depending on aspects (IDEs and others) might fail.

A safe use for macros is for defining additional targets intended to be
referenced directly at the Bazel CLI or in BUILD files: In that case, only the
*end users* of those targets need to know about them, and any build problems
introduced by macros are never far from their usage.

For macros that define generated targets (implementation details of the macro
which are not supposed to be referred to at the CLI or depended on by targets
not instantiated by that macro), follow these best practices:

*   A macro should take a `name` argument and define a target with that name.
    That target becomes that macro's *main target*.
*   Generated targets, that is all other targets defined by a macro, should:
    *   Have their names prefixed by `<name>`. For example, using
        `name = '%s_bar' % (name)`.
    *   Have restricted visibility (`//visibility:private`), and
    *   Have a `manual` tag to avoid expansion in wildcard targets (`:all`,
        `...`, `:*`, etc).
*   The `name` should only be used to derive names of targets defined by the
    macro, and not for anything else. For example, don't use the name to derive
    a dependency or input file that is not generated by the macro itself.
*   All the targets created in the macro should be coupled in some way to the
    main target.
*   Conventionally, `name` should be the first argument when defining a macro.
*   Keep the parameter names in the macro consistent. If a parameter is passed
    as an attribute value to the main target, keep its name the same. If a macro
    parameter serves the same purpose as a common rule attribute, such as
    `deps`, name as you would the attribute (see below).
*   When calling a macro, use only keyword arguments. This is consistent with
    rules, and greatly improves readability.

Engineers often write macros when the Starlark API of relevant rules is
insufficient for their specific use case, regardless of whether the rule is
defined within Bazel in native code, or in Starlark. If you're facing this
problem, ask the rule author if they can extend the API to accomplish your
goals.

As a rule of thumb, the more macros resemble the rules, the better.

See also [macros](/extending/macros#conventions).

## Rules

*   Rules, aspects, and their attributes should use lower_case names ("snake
    case").
*   Rule names are nouns that describe the main kind of artifact produced by the
    rule, from the point of view of its dependencies (or for leaf rules, the
    user). This is not necessarily a file suffix. For instance, a rule that
    produces C++ artifacts meant to be used as Python extensions might be called
    `py_extension`. For most languages, typical rules include:
    *   `*_library` - a compilation unit or "module".
    *   `*_binary` - a target producing an executable or a deployment unit.
    *   `*_test` - a test target. This can include multiple tests. Expect all
        tests in a `*_test` target to be variations on the same theme, for
        example, testing a single library.
    *   `*_import`: a target encapsulating a pre-compiled artifact, such as a
        `.jar`, or a `.dll` that is used during compilation.
*   Use consistent names and types for attributes. Some generally applicable
    attributes include:
    *   `srcs`: `label_list`, allowing files: source files, typically
        human-authored.
    *   `deps`: `label_list`, typically *not* allowing files: compilation
        dependencies.
    *   `data`: `label_list`, allowing files: data files, such as test data etc.
    *   `runtime_deps`: `label_list`: runtime dependencies that are not needed
        for compilation.
*   For any attributes with non-obvious behavior (for example, string templates
    with special substitutions, or tools that are invoked with specific
    requirements), provide documentation using the `doc` keyword argument to the
    attribute's declaration (`attr.label_list()` or similar).
*   Rule implementation functions should almost always be private functions
    (named with a leading underscore). A common style is to give the
    implementation function for `myrule` the name `_myrule_impl`.
*   Pass information between your rules using a well-defined
    [provider](/extending/rules#providers) interface. Declare and document provider
    fields.
*   Design your rule with extensibility in mind. Consider that other rules might
    want to interact with your rule, access your providers, and reuse the
    actions you create.
*   Follow [performance guidelines](/rules/performance) in your rules.

---

## Challenges of Writing Rules
- URL: https://bazel.build/rules/challenges
- Source: rules/challenges.mdx
- Slug: /rules/challenges

This page gives a high-level overview of the specific issues and challenges
of writing efficient Bazel rules.

## Summary Requirements

* Assumption: Aim for Correctness, Throughput, Ease of Use & Latency
* Assumption: Large Scale Repositories
* Assumption: BUILD-like Description Language
* Historic: Hard Separation between Loading, Analysis, and Execution is
  Outdated, but still affects the API
* Intrinsic: Remote Execution and Caching are Hard
* Intrinsic: Using Change Information for Correct and Fast Incremental Builds
  requires Unusual Coding Patterns
* Intrinsic: Avoiding Quadratic Time and Memory Consumption is Hard

## Assumptions

Here are some assumptions made about the build system, such as need for
correctness, ease of use, throughput, and large scale repositories. The
following sections address these assumptions and offer guidelines to ensure
rules are written in an effective manner.

### Aim for correctness, throughput, ease of use & latency

We assume that the build system needs to be first and foremost correct with
respect to incremental builds. For a given source tree, the output of the
same build should always be the same, regardless of what the output tree looks
like. In the first approximation, this means Bazel needs to know every single
input that goes into a given build step, such that it can rerun that step if any
of the inputs change. There are limits to how correct Bazel can get, as it leaks
some information such as date / time of the build, and ignores certain types of
changes such as changes to file attributes. [Sandboxing](/docs/sandboxing)
helps ensure correctness by preventing reads to undeclared input files. Besides
the intrinsic limits of the system, there are a few known correctness issues,
most of which are related to Fileset or the C++ rules, which are both hard
problems. We have long-term efforts to fix these.

The second goal of the build system is to have high throughput; we are
permanently pushing the boundaries of what can be done within the current
machine allocation for a remote execution service. If the remote execution
service gets overloaded, nobody can get work done.

Ease of use comes next. Of multiple correct approaches with the same (or
similar) footprint of the remote execution service, we choose the one that is
easier to use.

Latency denotes the time it takes from starting a build to getting the intended
result, whether that is a test log from a passing or failing test, or an error
message that a `BUILD` file has a typo.

Note that these goals often overlap; latency is as much a function of throughput
of the remote execution service as is correctness relevant for ease of use.

### Large scale repositories

The build system needs to operate at the scale of large repositories where large
scale means that it does not fit on a single hard drive, so it is impossible to
do a full checkout on virtually all developer machines. A medium-sized build
will need to read and parse tens of thousands of `BUILD` files, and evaluate
hundreds of thousands of globs. While it is theoretically possible to read all
`BUILD` files on a single machine, we have not yet been able to do so within a
reasonable amount of time and memory. As such, it is critical that `BUILD` files
can be loaded and parsed independently.

### BUILD-like description language

In this context, we assume a configuration language that is
roughly similar to `BUILD` files in declaration of library and binary rules
and their interdependencies. `BUILD` files can be read and parsed independently,
and we avoid even looking at source files whenever we can (except for
existence).

## Historic

There are differences between Bazel versions that cause challenges and some
of these are outlined in the following sections.

### Hard separation between loading, analysis, and execution is outdated but still affects the API

Technically, it is sufficient for a rule to know the input and output files of
an action just before the action is sent to remote execution. However, the
original Bazel code base had a strict separation of loading packages, then
analyzing rules using a configuration (command-line flags, essentially), and
only then running any actions. This distinction is still part of the rules API
today, even though the core of Bazel no longer requires it (more details below).

That means that the rules API requires a declarative description of the rule
interface (what attributes it has, types of attributes). There are some
exceptions where the API allows custom code to run during the loading phase to
compute implicit names of output files and implicit values of attributes. For
example, a java_library rule named 'foo' implicitly generates an output named
'libfoo.jar', which can be referenced from other rules in the build graph.

Furthermore, the analysis of a rule cannot read any source files or inspect the
output of an action; instead, it needs to generate a partial directed bipartite
graph of build steps and output file names that is only determined from the rule
itself and its dependencies.

## Intrinsic

There are some intrinsic properties that make writing rules challenging and
some of the most common ones are described in the following sections.

### Remote execution and caching are hard

Remote execution and caching improve build times in large repositories by
roughly two orders of magnitude compared to running the build on a single
machine. However, the scale at which it needs to perform is staggering: Google's
remote execution service is designed to handle a huge number of requests per
second, and the protocol carefully avoids unnecessary roundtrips as well as
unnecessary work on the service side.

At this time, the protocol requires that the build system knows all inputs to a
given action ahead of time; the build system then computes a unique action
fingerprint, and asks the scheduler for a cache hit. If a cache hit is found,
the scheduler replies with the digests of the output files; the files itself are
addressed by digest later on. However, this imposes restrictions on the Bazel
rules, which need to declare all input files ahead of time.

### Using change information for correct and fast incremental builds requires unusual coding patterns

Above, we argued that in order to be correct, Bazel needs to know all the input
files that go into a build step in order to detect whether that build step is
still up-to-date. The same is true for package loading and rule analysis, and we
have designed [Skyframe](/reference/skyframe) to handle this
in general. Skyframe is a graph library and evaluation framework that takes a
goal node (such as 'build //foo with these options'), and breaks it down into
its constituent parts, which are then evaluated and combined to yield this
result. As part of this process, Skyframe reads packages, analyzes rules, and
executes actions.

At each node, Skyframe tracks exactly which nodes any given node used to compute
its own output, all the way from the goal node down to the input files (which
are also Skyframe nodes). Having this graph explicitly represented in memory
allows the build system to identify exactly which nodes are affected by a given
change to an input file (including creation or deletion of an input file), doing
the minimal amount of work to restore the output tree to its intended state.

As part of this, each node performs a dependency discovery process. Each
node can declare dependencies, and then use the contents of those dependencies
to declare even further dependencies. In principle, this maps well to a
thread-per-node model. However, medium-sized builds contain hundreds of
thousands of Skyframe nodes, which isn't easily possible with current Java
technology (and for historical reasons, we're currently tied to using Java, so
no lightweight threads and no continuations).

Instead, Bazel uses a fixed-size thread pool. However, that means that if a node
declares a dependency that isn't available yet, we may have to abort that
evaluation and restart it (possibly in another thread), when the dependency is
available. This, in turn, means that nodes should not do this excessively; a
node that declares N dependencies serially can potentially be restarted N times,
costing O(N^2) time. Instead, we aim for up-front bulk declaration of
dependencies, which sometimes requires reorganizing the code, or even splitting
a node into multiple nodes to limit the number of restarts.

Note that this technology isn't currently available in the rules API; instead,
the rules API is still defined using the legacy concepts of loading, analysis,
and execution phases. However, a fundamental restriction is that all accesses to
other nodes have to go through the framework so that it can track the
corresponding dependencies. Regardless of the language in which the build system
is implemented or in which the rules are written (they don't have to be the
same), rule authors must not use standard libraries or patterns that bypass
Skyframe. For Java, that means avoiding java.io.File as well as any form of
reflection, and any library that does either. Libraries that support dependency
injection of these low-level interfaces still need to be setup correctly for
Skyframe.

This strongly suggests to avoid exposing rule authors to a full language runtime
in the first place. The danger of accidental use of such APIs is just too big -
several Bazel bugs in the past were caused by rules using unsafe APIs, even
though the rules were written by the Bazel team or other domain experts.

### Avoiding quadratic time and memory consumption is hard

To make matters worse, apart from the requirements imposed by Skyframe, the
historical constraints of using Java, and the outdatedness of the rules API,
accidentally introducing quadratic time or memory consumption is a fundamental
problem in any build system based on library and binary rules. There are two
very common patterns that introduce quadratic memory consumption (and therefore
quadratic time consumption).

1. Chains of Library Rules -
Consider the case of a chain of library rules A depends on B, depends on C, and
so on. Then, we want to compute some property over the transitive closure of
these rules, such as the Java runtime classpath, or the C++ linker command for
each library. Naively, we might take a standard list implementation; however,
this already introduces quadratic memory consumption: the first library
contains one entry on the classpath, the second two, the third three, and so
on, for a total of 1+2+3+...+N = O(N^2) entries.

2. Binary Rules Depending on the Same Library Rules -
Consider the case where a set of binaries that depend on the same library
rules — such as if you have a number of test rules that test the same
library code. Let's say out of N rules, half the rules are binary rules, and
the other half library rules. Now consider that each binary makes a copy of
some property computed over the transitive closure of library rules, such as
the Java runtime classpath, or the C++ linker command line. For example, it
could expand the command line string representation of the C++ link action. N/2
copies of N/2 elements is O(N^2) memory.

#### Custom collections classes to avoid quadratic complexity

Bazel is heavily affected by both of these scenarios, so we introduced a set of
custom collection classes that effectively compress the information in memory by
avoiding the copy at each step. Almost all of these data structures have set
semantics, so we called it
[depset](/rules/lib/depset)
(also known as `NestedSet` in the internal implementation). The majority of
changes to reduce Bazel's memory consumption over the past several years were
changes to use depsets instead of whatever was previously used.

Unfortunately, usage of depsets does not automatically solve all the issues;
in particular, even just iterating over a depset in each rule re-introduces
quadratic time consumption. Internally, NestedSets also has some helper methods
to facilitate interoperability with normal collections classes; unfortunately,
accidentally passing a NestedSet to one of these methods leads to copying
behavior, and reintroduces quadratic memory consumption.

---

## Deploying Rules
- URL: https://bazel.build/rules/deploying
- Source: rules/deploying.mdx
- Slug: /rules/deploying

This page is for rule writers who are planning to make their rules available
to others.

We recommend you start a new ruleset from the template repository:
https://github.com/bazel-contrib/rules-template
That template follows the recommendations below, and includes API documentation generation
and sets up a CI/CD pipeline to make it trivial to distribute your ruleset.

## Hosting and naming rules

New rules should go into their own GitHub repository under your organization.
Start a thread on [GitHub](https://github.com/bazelbuild/bazel/discussions)
if you feel like your rules belong in the [bazelbuild](https://github.com/bazelbuild)
organization.

Repository names for Bazel rules are standardized on the following format:
`$ORGANIZATION/rules_$NAME`.
See [examples on GitHub](https://github.com/search?q=rules+bazel&type=Repositories).
For consistency, you should follow this same format when publishing your Bazel rules.

Make sure to use a descriptive GitHub repository description and `README.md`
title, example:

* Repository name: `bazelbuild/rules_go`
* Repository description: *Go rules for Bazel*
* Repository tags: `golang`, `bazel`
* `README.md` header: *Go rules for [Bazel](https://bazel.build)*
(note the link to https://bazel.build which will guide users who are unfamiliar
with Bazel to the right place)

Rules can be grouped either by language (such as Scala), runtime platform
(such as Android), or framework (such as Spring).

## Repository content

Every rule repository should have a certain layout so that users can quickly
understand new rules.

For example, when writing new rules for the (make-believe)
`mockascript` language, the rule repository would have the following structure:

```
/
  LICENSE
  README
  MODULE.bazel
  mockascript/
    constraints/
      BUILD
    runfiles/
      BUILD
      runfiles.mocs
    BUILD
    defs.bzl
  tests/
    BUILD
    some_test.sh
    another_test.py
  examples/
    BUILD
    bin.mocs
    lib.mocs
    test.mocs
```

### MODULE.bazel

In the project's `MODULE.bazel`, you should define the name that users will use
to reference your rules. If your rules belong to the
[bazelbuild](https://github.com/bazelbuild) organization, you must use
`rules_<lang>` (such as `rules_mockascript`). Otherwise, you should name your
repository `<org>_rules_<lang>` (such as `build_stack_rules_proto`). Please
start a thread on [GitHub](https://github.com/bazelbuild/bazel/discussions)
if you feel like your rules should follow the convention for rules in the
[bazelbuild](https://github.com/bazelbuild) organization.

In the following sections, assume the repository belongs to the
[bazelbuild](https://github.com/bazelbuild) organization.

```
module(name = "rules_mockascript")
```

### README

At the top level, there should be a `README` that contains a brief description
of your ruleset, and the API users should expect.

### Rules

Often times there will be multiple rules provided by your repository. Create a
directory named by the language and provide an entry point - `defs.bzl` file
exporting all rules (also include a `BUILD` file so the directory is a package).
For `rules_mockascript` that means there will be a directory named
`mockascript`, and a `BUILD` file and a `defs.bzl` file inside:

```
/
  mockascript/
    BUILD
    defs.bzl
```

### Constraints

If your rule defines
[toolchain](/extending/toolchains) rules,
it's possible that you'll need to define custom `constraint_setting`s and/or
`constraint_value`s. Put these into a `//<LANG>/constraints` package. Your
directory structure will look like this:

```
/
  mockascript/
    constraints/
      BUILD
    BUILD
    defs.bzl
```

Please read
[github.com/bazelbuild/platforms](https://github.com/bazelbuild/platforms)
for best practices, and to see what constraints are already present, and
consider contributing your constraints there if they are language independent.
Be mindful of introducing custom constraints, all users of your rules will
use them to perform platform specific logic in their `BUILD` files (for example,
using [selects](/reference/be/functions#select)).
With custom constraints, you define a language that the whole Bazel ecosystem
will speak.

### Runfiles library

If your rule provides a standard library for accessing runfiles, it should be
in the form of a library target located at `//<LANG>/runfiles` (an abbreviation
of `//<LANG>/runfiles:runfiles`). User targets that need to access their data
dependencies will typically add this target to their `deps` attribute.

### Repository rules

#### Dependencies

Your rules might have external dependencies, which you'll need to specify in
your MODULE.bazel file.

#### Registering toolchains

Your rules might also register toolchains, which you can also specify in the
MODULE.bazel file.

Note that in order to resolve toolchains in the analysis phase Bazel needs to
analyze all `toolchain` targets that are registered. Bazel will not need to
analyze all targets referenced by `toolchain.toolchain` attribute. If in order
to register toolchains you need to perform complex computation in the
repository, consider splitting the repository with `toolchain` targets from the
repository with `<LANG>_toolchain` targets. Former will be always fetched, and
the latter will only be fetched when user actually needs to build `<LANG>` code.


#### Release snippet

In your release announcement provide a snippet that your users can copy-paste
into their `MODULE.bazel` file. This snippet in general will look as follows:

```
bazel_dep(name = "rules_<LANG>", version = "<VERSION>")
```


### Tests

There should be tests that verify that the rules are working as expected. This
can either be in the standard location for the language the rules are for or a
`tests/` directory at the top level.

### Examples (optional)

It is useful to users to have an `examples/` directory that shows users a couple
of basic ways that the rules can be used.

## CI/CD

Many rulesets use GitHub Actions. See the configuration used in the [rules-template](https://github.com/bazel-contrib/rules-template/tree/main/.github/workflows) repo, which are simplified using a "reusable workflow" hosted in the bazel-contrib
org. `ci.yaml` runs tests on each PR and `main` comit, and `release.yaml` runs anytime you push a tag to the repository.
See comments in the rules-template repo for more information.

If your repository is under the [bazelbuild organization](https://github.com/bazelbuild),
you can [ask to add](https://github.com/bazelbuild/continuous-integration/issues/new?template=adding-your-project-to-bazel-ci.md&title=Request+to+add+new+project+%5BPROJECT_NAME%5D&labels=new-project)
it to [ci.bazel.build](http://ci.bazel.build).

## Documentation

See the [Stardoc documentation](https://github.com/bazelbuild/stardoc) for
instructions on how to comment your rules so that documentation can be generated
automatically.

The [rules-template docs/ folder](https://github.com/bazel-contrib/rules-template/tree/main/docs)
shows a simple way to ensure the Markdown content in the `docs/` folder is always up-to-date
as Starlark files are updated.

## FAQs

### Why can't we add our rule to the main Bazel GitHub repository?

We want to decouple rules from Bazel releases as much as possible. It's clearer
who owns individual rules, reducing the load on Bazel developers. For our users,
decoupling makes it easier to modify, upgrade, downgrade, and replace rules.
Contributing to rules can be lighter weight than contributing to Bazel -
depending on the rules -, including full submit access to the corresponding
GitHub repository. Getting submit access to Bazel itself is a much more involved
process.

The downside is a more complicated one-time installation process for our users:
they have to add a dependency on your ruleset in their `MODULE.bazel` file.

We used to have all of the rules in the Bazel repository (under
`//tools/build_rules` or `//tools/build_defs`). We still have a couple rules
there, but we are working on moving the remaining rules out.

---

## Error: Variable x is read only
- URL: https://bazel.build/rules/errors/read-only-variable
- Source: rules/errors/read-only-variable.mdx
- Slug: /rules/errors/read-only-variable

A global variable cannot be reassigned. It will always point to the same object.
However, its content might change, if the value is mutable (for example, the
content of a list). Local variables don't have this restriction.

```python
a = [1, 2]

a[1] = 3

b = 3

b = 4  # forbidden
```

`ERROR: /path/ext.bzl:7:1: Variable b is read only`

You will get a similar error if you try to redefine a function (function
overloading is not supported), for example:

```python
def foo(x): return x + 1

def foo(x, y): return x + y  # forbidden
```

---

## Frequently Asked Questions
- URL: https://bazel.build/rules/faq
- Source: rules/faq.mdx
- Slug: /rules/faq

These are some common issues and questions with writing extensions.

## Why is my file not produced / my action never executed?

Bazel only executes the actions needed to produce the *requested* output files.

* If the file you want has a label, you can request it directly:
  `bazel build //pkg:myfile.txt`

* If the file is in an output group of the target, you may need to specify that
  output group on the command line:
  `bazel build //pkg:mytarget --output_groups=foo`

* If you want the file to be built automatically whenever your target is
  mentioned on the command line, add it to your rule's default outputs by
  returning a [`DefaultInfo`](lib/globals#DefaultInfo) provider.

See the [Rules page](/extending/rules#requesting-output-files) for more information.

## Why is my implementation function not executed?

Bazel analyzes only the targets that are requested for the build. You should
either name the target on the command line, or something that depends on the
target.

## A file is missing when my action or binary is executed

Make sure that 1) the file has been registered as an input to the action or
binary, and 2) the script or tool being executed is accessing the file using the
correct path.

For actions, you declare inputs by passing them to the `ctx.actions.*` function
that creates the action. The proper path for the file can be obtained using
[`File.path`](lib/File#path).

For binaries (the executable outputs run by a `bazel run` or `bazel test`
command), you declare inputs by including them in the
[runfiles](/extending/rules#runfiles). Instead of using the `path` field, use
[`File.short_path`](lib/File#short_path), which is file's path relative to
the runfiles directory in which the binary executes.

## How can I control which files are built by `bazel build //pkg:mytarget`?

Use the [`DefaultInfo`](lib/globals#DefaultInfo) provider to
[set the default outputs](/extending/rules#requesting-output-files).

## How can I run a program or do file I/O as part of my build?

A tool can be declared as a target, just like any other part of your build, and
run during the execution phase to help build other targets. To create an action
that runs a tool, use [`ctx.actions.run`](lib/actions#run) and pass in the
tool as the `executable` parameter.

During the loading and analysis phases, a tool *cannot* run, nor can you perform
file I/O. This means that tools and file contents (except the contents of BUILD
and .bzl files) cannot affect how the target and action graphs get created.

## What if I need to access the same structured data both before and during the execution phase?

You can format the structured data as a .bzl file. You can `load()` the file to
access it during the loading and analysis phases. You can pass it as an input or
runfile to actions and executables that need it during the execution phase.

## How should I document Starlark code?

For rules and rule attributes, you can pass a docstring literal (possibly
triple-quoted) to the `doc` parameter of `rule` or `attr.*()`. For helper
functions and macros, use a triple-quoted docstring literal following the format
given [here](https://github.com/bazelbuild/buildtools/blob/master/WARNINGS.md#function-docstring).
Rule implementation functions generally do not need their own docstring.

Using string literals in the expected places makes it easier for automated
tooling to extract documentation. Feel free to use standard non-string comments
wherever it may help the reader of your code.

---

## Starlark Language
- URL: https://bazel.build/rules/language
- Source: rules/language.mdx
- Slug: /rules/language

{/* [TOC] */}

This page is an overview of [Starlark](https://github.com/bazelbuild/starlark),
formerly known as Skylark, the language used in Bazel. For a complete list of
functions and types, see the [Bazel API reference](/rules/lib/overview).

For more information about the language, see [Starlark's GitHub repo](https://github.com/bazelbuild/starlark/).

For the authoritative specification of the Starlark syntax and
behavior, see the [Starlark Language Specification](https://github.com/bazelbuild/starlark/blob/master/spec.md).

## Syntax

Starlark's syntax is inspired by Python3. This is valid syntax in Starlark:

```python
def fizz_buzz(n):
  """Print Fizz Buzz numbers from 1 to n."""
  for i in range(1, n + 1):
    s = ""
    if i % 3 == 0:
      s += "Fizz"
    if i % 5 == 0:
      s += "Buzz"
    print(s if s else i)

fizz_buzz(20)
```

Starlark's semantics can differ from Python, but behavioral differences are
rare, except for cases where Starlark raises an error. The following Python
types are supported:

* [None](lib/globals#None)
* [bool](lib/bool)
* [dict](lib/dict)
* [tuple](lib/tuple)
* [function](lib/function)
* [int](lib/int)
* [list](lib/list)
* [string](lib/string)

## Type annotations

**Experimental**. Type annotations are an experimental feature and may change
at any time. Don't depend on it. It may be enabled in Bazel at HEAD
by using the `--experimental_starlark_types` flag.

Starlark in Bazel at HEAD is incrementally adding support for type annotations
with a syntax inspired by [PEP 484](https://peps.python.org/pep-0484/).

- Starlark type annotations are under active development. The progress is
  tracked on [issue#22935](https://github.com/bazelbuild/bazel/issues/22935).
- The specification is incrementally extended: [starlark-with-types/spec.md](https://github.com/bazelbuild/starlark/blob/starlark-with-types/spec.md)
- Initial proposal: [SEP-001 Bootstrapping Starlark types](https://docs.google.com/document/d/1Sid7EAbBd_w_T7D94Li_f_bK3zMTztFbzIMvcpzo1wY/edit?tab=t.0#heading=h.5mcn15i0e1ch)

## Mutability

Starlark favors immutability. Two mutable data structures are available:
[lists](lib/list) and [dicts](lib/dict). Changes to mutable
data-structures, such as appending a value to a list or deleting an entry in a
dictionary are valid only for objects created in the current context. After a
context finishes, its values become immutable.

This is because Bazel builds use parallel execution. During a build, each `.bzl`
file and each `BUILD` file get their own execution context. Each rule is also
analyzed in its own context.

Let's go through an example with the file `foo.bzl`:

```python
# `foo.bzl`
var = [] # declare a list

def fct(): # declare a function
  var.append(5) # append a value to the list

fct() # execute the fct function
```

Bazel creates `var` when `foo.bzl` loads. `var` is thus part of `foo.bzl`'s
context. When `fct()` runs, it does so within the context of `foo.bzl`. After
evaluation for `foo.bzl` completes, the environment contains an immutable entry,
`var`, with the value `[5]`.

When another `bar.bzl` loads symbols from `foo.bzl`, loaded values remain
immutable. For this reason, the following code in `bar.bzl` is illegal:

```python
# `bar.bzl`
load(":foo.bzl", "var", "fct") # loads `var`, and `fct` from `./foo.bzl`

var.append(6)  # runtime error, the list stored in var is frozen

fct()          # runtime error, fct() attempts to modify a frozen list
```

Global variables defined in `bzl` files cannot be changed outside of the
`bzl` file that defined them. Just like the above example using `bzl` files,
values returned by rules are immutable.

## Differences between BUILD and .bzl files

`BUILD` files register targets via making calls to rules. `.bzl` files provide
definitions for constants, rules, macros, and functions.

[Native functions](/reference/be/functions) and [native rules](
/reference/be/overview#language-specific-native-rules) are global symbols in
`BUILD` files. `bzl` files need to load them using the [`native` module](
/rules/lib/toplevel/native).

There are two syntactic restrictions in `BUILD` files: 1) declaring functions is
illegal, and 2) `*args` and `**kwargs` arguments are not allowed.

## Differences with Python

* Global variables are immutable.

* `for` statements are not allowed at the top-level. Use them within functions
  instead. In `BUILD` files, you may use list comprehensions.

* `if` statements are not allowed at the top-level. However, `if` expressions
  can be used: `first = data[0] if len(data) > 0 else None`.

* Deterministic order for iterating through Dictionaries.

* Recursion is not allowed.

* Int type is limited to 32-bit signed integers. Overflows will throw an error.

* Modifying a collection during iteration is an error.

* Except for equality tests, comparison operators `<`, `<=`, `>=`, `>`, etc. are
not defined across value types. In short: `5 < 'foo'` will throw an error and
`5 == "5"` will return false.

* In tuples, a trailing comma is valid only when the tuple is between
  parentheses — when you write `(1,)` instead of `1,`.

* Dictionary literals cannot have duplicated keys. For example, this is an
  error: `{"a": 4, "b": 7, "a": 1}`.

* Strings are represented with double-quotes (such as when you call
  [repr](lib/globals#repr)).

* Strings aren't iterable.

The following Python features are not supported:

* implicit string concatenation (use explicit `+` operator).
* Chained comparisons (such as `1 < x < 5`).
* `class` (see [`struct`](lib/struct#struct) function).
* `import` (see [`load`](/extending/concepts#loading-an-extension) statement).
* `while`, `yield`.
* float and set types.
* generators and generator expressions.
* `is` (use `==` instead).
* `try`, `raise`, `except`, `finally` (see [`fail`](lib/globals#fail) for fatal errors).
* `global`, `nonlocal`.
* most builtin functions, most methods.

---

## Creating a Legacy Macro
- URL: https://bazel.build/rules/legacy-macro-tutorial
- Source: rules/legacy-macro-tutorial.mdx
- Slug: /rules/legacy-macro-tutorial

IMPORTANT: This tutorial is for [*legacy macros*](/extending/legacy-macros). If
you only need to support Bazel 8 or newer, we recommend using [symbolic
macros](/extending/macros) instead; take a look at [Creating a Symbolic
Macro](macro-tutorial).

Imagine that you need to run a tool as part of your build. For example, you
may want to generate or preprocess a source file, or compress a binary. In this
tutorial, you are going to create a legacy macro that resizes an image.

Macros are suitable for simple tasks. If you want to do anything more
complicated, for example add support for a new programming language, consider
creating a [rule](/extending/rules). Rules give you more control and flexibility.

The easiest way to create a macro that resizes an image is to use a `genrule`:

```starlark
genrule(
    name = "logo_miniature",
    srcs = ["logo.png"],
    outs = ["small_logo.png"],
    cmd = "convert $< -resize 100x100 $@",
)

cc_binary(
    name = "my_app",
    srcs = ["my_app.cc"],
    data = [":logo_miniature"],
)
```

If you need to resize more images, you may want to reuse the code. To do that,
define a function in a separate `.bzl` file, and call the file `miniature.bzl`:

```starlark
def miniature(name, src, size = "100x100", **kwargs):
    """Create a miniature of the src image.

    The generated file is prefixed with 'small_'.
    """
    native.genrule(
        name = name,
        srcs = [src],
        # Note that the line below will fail if `src` is not a filename string
        outs = ["small_" + src],
        cmd = "convert $< -resize " + size + " $@",
        **kwargs
    )
```

A few remarks:

  * By convention, legacy macros have a `name` argument, just like rules.

  * To document the behavior of a legacy macro, use
    [docstring](https://www.python.org/dev/peps/pep-0257/) like in Python.

  * To call a `genrule`, or any other native rule, prefix with `native.`.

  * Use `**kwargs` to forward the extra arguments to the underlying `genrule`
    (it works just like in
    [Python](https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments)).
    This is useful, so that a user can use standard attributes like
    `visibility`, or `tags`.

Now, use the macro from the `BUILD` file:

```starlark
load("//path/to:miniature.bzl", "miniature")

miniature(
    name = "logo_miniature",
    src = "image.png",
)

cc_binary(
    name = "my_app",
    srcs = ["my_app.cc"],
    data = [":logo_miniature"],
)
```

And finally, a **warning note**: the macro assumes that `src` is a filename
string (otherwise, `outs = ["small_" + src]` will fail). So `src = "image.png"`
works; but what happens if the `BUILD` file instead used `src =
"//other/package:image.png"`, or even `src = select(...)`?

You should make sure to declare such assumptions in your macro's documentation.
Unfortunately, legacy macros, especially large ones, tend to be fragile because
it can be hard to notice and document all such assumptions in your code – and,
of course, some users of the macro won't read the documentation. We recommend,
if possible, instead using [symbolic macros](/extending/macros), which have
built\-in checks on attribute types.

---

## Built-in Types
- URL: https://bazel.build/rules/lib/builtins
- Source: rules/lib/builtins.mdx
- Slug: /rules/lib/builtins

This section lists types of Starlark objects. With some exceptions, these type names are not valid Starlark symbols; instances of them may be acquired through different means.

- [Action](/rules/lib/builtins/Action)
- [actions](/rules/lib/builtins/actions)
- [apple\_platform](/rules/lib/builtins/apple_platform)
- [Args](/rules/lib/builtins/Args)
- [Aspect](/rules/lib/builtins/Aspect)
- [Attribute](/rules/lib/builtins/Attribute)
- [bazel\_module](/rules/lib/builtins/bazel_module)
- [bazel\_module\_tags](/rules/lib/builtins/bazel_module_tags)
- [BuildSetting](/rules/lib/builtins/BuildSetting)
- [CcCompilationOutputs](/rules/lib/builtins/CcCompilationOutputs)
- [CcLinkingOutputs](/rules/lib/builtins/CcLinkingOutputs)
- [CompilationContext](/rules/lib/builtins/CompilationContext)
- [configuration](/rules/lib/builtins/configuration)
- [ctx](/rules/lib/builtins/ctx)
- [depset](/rules/lib/builtins/depset)
- [DirectoryExpander](/rules/lib/builtins/DirectoryExpander)
- [DottedVersion](/rules/lib/builtins/DottedVersion)
- [exec\_result](/rules/lib/builtins/exec_result)
- [ExecGroupCollection](/rules/lib/builtins/ExecGroupCollection)
- [ExecGroupContext](/rules/lib/builtins/ExecGroupContext)
- [ExecTransitionFactory](/rules/lib/builtins/ExecTransitionFactory)
- [ExpandedDirectory](/rules/lib/builtins/ExpandedDirectory)
- [extension\_metadata](/rules/lib/builtins/extension_metadata)
- [Facts](/rules/lib/builtins/Facts)
- [FeatureConfiguration](/rules/lib/builtins/FeatureConfiguration)
- [File](/rules/lib/builtins/File)
- [fragments](/rules/lib/builtins/fragments)
- [java\_annotation\_processing](/rules/lib/builtins/java_annotation_processing)
- [Label](/rules/lib/builtins/Label)
- [LateBoundDefault](/rules/lib/builtins/LateBoundDefault)
- [LibraryToLink](/rules/lib/builtins/LibraryToLink)
- [License](/rules/lib/builtins/License)
- [LinkerInput](/rules/lib/builtins/LinkerInput)
- [LinkingContext](/rules/lib/builtins/LinkingContext)
- [macro](/rules/lib/builtins/macro)
- [mapped\_root](/rules/lib/builtins/mapped_root)
- [module\_ctx](/rules/lib/builtins/module_ctx)
- [path](/rules/lib/builtins/path)
- [propagation\_ctx](/rules/lib/builtins/propagation_ctx)
- [Provider](/rules/lib/builtins/Provider)
- [repo\_metadata](/rules/lib/builtins/repo_metadata)
- [repository\_ctx](/rules/lib/builtins/repository_ctx)
- [repository\_os](/rules/lib/builtins/repository_os)
- [repository\_rule](/rules/lib/builtins/repository_rule)
- [root](/rules/lib/builtins/root)
- [rule](/rules/lib/builtins/rule)
- [rule\_attributes](/rules/lib/builtins/rule_attributes)
- [runfiles](/rules/lib/builtins/runfiles)
- [struct](/rules/lib/builtins/struct)
- [Subrule](/rules/lib/builtins/Subrule)
- [subrule\_ctx](/rules/lib/builtins/subrule_ctx)
- [SymlinkEntry](/rules/lib/builtins/SymlinkEntry)
- [tag\_class](/rules/lib/builtins/tag_class)
- [Target](/rules/lib/builtins/Target)
- [template\_ctx](/rules/lib/builtins/template_ctx)
- [TemplateDict](/rules/lib/builtins/TemplateDict)
- [toolchain\_type](/rules/lib/builtins/toolchain_type)
- [ToolchainContext](/rules/lib/builtins/ToolchainContext)
- [transition](/rules/lib/builtins/transition)
- [wasm\_exec\_result](/rules/lib/builtins/wasm_exec_result)
- [wasm\_module](/rules/lib/builtins/wasm_module)

---

## Action
- URL: https://bazel.build/rules/lib/builtins/Action
- Source: rules/lib/builtins/Action.mdx
- Slug: /rules/lib/builtins/Action

An action created during rule analysis.

This object is visible for the purpose of testing, and may be obtained from an `Actions` provider. It is normally not necessary to access `Action` objects or their fields within a rule's implementation function. You may instead want to see the [Rules page](https://bazel.build/extending/rules#actions) for a general discussion of how to use actions when defining custom rules, or the [API reference](../builtins/actions.html) for creating actions.

Some fields of this object are only applicable for certain kinds of actions. Fields that are inapplicable are set to `None`.

## Members

- [args](#args)
- [argv](#argv)
- [content](#content)
- [env](#env)
- [inputs](#inputs)
- [mnemonic](#mnemonic)
- [outputs](#outputs)
- [substitutions](#substitutions)

## args

```
sequence Action.args
```

 A list of frozen [Args](../builtins/Args.html) objects containing information about the action arguments. These objects contain accurate argument information, including arguments involving expanded action output directories. However, [Args](../builtins/Args.html) objects are not readable in the analysis phase. For a less accurate account of arguments which is available in the analysis phase, see [argv](#argv).

Note that some types of actions do not yet support exposure of this field. For such action types, this is `None`.
May return `None`.



## argv

```
sequence Action.argv
```

 For actions created by [ctx.actions.run()](../builtins/actions.html#run) or [ctx.actions.run\_shell()](../builtins/actions.html#run_shell) an immutable list of the arguments for the command line to be executed. Note that for shell actions the first two arguments will be the shell path and `"-c"`.
 May return `None`.



## content

```
string Action.content
```

 For actions created by [ctx.actions.write()](../builtins/actions.html#write) or [ctx.actions.expand\_template()](../builtins/actions.html#expand_template), the contents of the file to be written, if those contents can be computed during the analysis phase. The value is `None` if the contents cannot be determined until the execution phase, such as when a directory in an [Args](../builtins/Args.html) object needs to be expanded.
 May return `None`.



## env

```
dict Action.env
```

 The 'fixed' environment variables for this action. This includes only environment settings which are explicitly set by the action definition, and thus omits settings which are only pre-set in the execution environment.



## inputs

```
depset Action.inputs
```

 A set of the input files of this action.



## mnemonic

```
string Action.mnemonic
```

 The mnemonic for this action.



## outputs

```
depset Action.outputs
```

 A set of the output files of this action.



## substitutions

```
dict Action.substitutions
```

 For actions created by [ctx.actions.expand\_template()](../builtins/actions.html#expand_template), an immutable dict holding the substitution mapping.
 May return `None`.

---

## Args
- URL: https://bazel.build/rules/lib/builtins/Args
- Source: rules/lib/builtins/Args.mdx
- Slug: /rules/lib/builtins/Args

An object that encapsulates, in a memory-efficient way, the data needed to build part or all of a command line.

It often happens that an action requires a large command line containing values accumulated from transitive dependencies. For example, a linker command line might list every object file needed by all of the libraries being linked. It is best practice to store such transitive data in [`depset`](../builtins/depset.html) s, so that they can be shared by multiple targets. However, if the rule author had to convert these depsets into lists of strings in order to construct an action command line, it would defeat this memory-sharing optimization.

For this reason, the action-constructing functions accept `Args` objects in addition to strings. Each `Args` object represents a concatenation of strings and depsets, with optional transformations for manipulating the data. `Args` objects do not process the depsets they encapsulate until the execution phase, when it comes time to calculate the command line. This helps defer any expensive copying until after the analysis phase is complete. See the [Optimizing Performance](https://bazel.build/rules/performance) page for more information.

`Args` are constructed by calling [`ctx.actions.args()`](../builtins/actions.html#args). They can be passed as the `arguments` parameter of [`ctx.actions.run()`](../builtins/actions.html#run) or [`ctx.actions.run_shell()`](../builtins/actions.html#run_shell). Each mutation of an `Args` object appends values to the eventual command line.

The `map_each` feature allows you to customize how items are transformed into strings. If you do not provide a `map_each` function, the standard conversion is as follows:

- Values that are already strings are left as-is.
- [`File`](../builtins/File.html) objects are turned into their `File.path` values.
- [`Label`](../builtins/Label.html) objects are turned into a string representation that resolves back to the same object when resolved in the context of the main repository. If possible, the string representation uses the apparent name of a repository in favor of the repository's canonical name, which makes this representation suited for use in BUILD files. While the exact form of the representation is not guaranteed, typical examples are `//foo:bar`, `@repo//foo:bar` and `@@canonical_name+//foo:bar.bzl`.
- All other types are turned into strings in an _unspecified_ manner. For this reason, you should avoid passing values that are not of string or `File` type to `add()`, and if you pass them to `add_all()` or `add_joined()` then you should provide a `map_each` function.

When using string formatting ( `format`, `format_each`, and `format_joined` params of the `add*()` methods), the format template is interpreted in the same way as `%`-substitution on strings, except that the template must have exactly one substitution placeholder and it must be `%s`. Literal percents may be escaped as `%%`. Formatting is applied after the value is converted to a string as per the above.

Each of the `add*()` methods have an alternate form that accepts an extra positional parameter, an "arg name" string to insert before the rest of the arguments. For `add_all` and `add_joined` the extra string will not be added if the sequence turns out to be empty. For instance, the same usage can add either `--foo val1 val2 val3 --bar` or just `--bar` to the command line, depending on whether the given sequence contains `val1..val3` or is empty.

If the size of the command line can grow longer than the maximum size allowed by the system, the arguments can be spilled over into parameter files. See [`use_param_file()`](#use_param_file) and [`set_param_file_format()`](#set_param_file_format).

Example: Suppose we wanted to generate the command line:

```
--foo foo1.txt foo2.txt ... fooN.txt --bar bar1.txt,bar2.txt,...,barM.txt --baz

```

We could use the following `Args` object:

```
# foo_deps and bar_deps are depsets containing
# File objects for the foo and bar .txt files.
args = ctx.actions.args()
args.add_all("--foo", foo_deps)
args.add_joined("--bar", bar_deps, join_with=",")
args.add("--baz")
ctx.actions.run(
  ...
  arguments = [args],
  ...
)

```

## Members

- [add](#add)
- [add\_all](#add_all)
- [add\_joined](#add_joined)
- [set\_param\_file\_format](#set_param_file_format)
- [use\_param\_file](#use_param_file)

## add

```
Args Args.add(arg_name_or_value, value=unbound, *, format=None)
```

 Appends an argument to this command line.


### Parameters

ParameterDescription`arg_name_or_value`
 required

 If two positional parameters are passed this is interpreted as the arg name. The arg name is added before the value without any processing. If only one positional parameter is passed, it is interpreted as `value` (see below).
 `value`
 default is `unbound`

 The object to append. It will be converted to a string using the standard conversion mentioned above. Since there is no `map_each` parameter for this function, `value` should be either a string or a `File`. A list, tuple, depset, or directory `File` must be passed to [`add_all()` or](#add_all) [`add_joined()`](#add_joined) instead of this method.
 `format`[string](../core/string.html); or `None`;
 default is `None`

 A format string pattern, to be applied to the stringified version of `value`.


## add\_all

```
Args Args.add_all(arg_name_or_values, values=unbound, *, map_each=None, format_each=None, before_each=None, omit_if_empty=True, uniquify=False, expand_directories=True, terminate_with=None, allow_closure=False)
```

 Appends multiple arguments to this command line. The items are processed lazily during the execution phase.

Most of the processing occurs over a list of arguments to be appended, as per the following steps:

1. Each directory `File` item is replaced by all `File` s recursively contained in that directory.
2. If `map_each` is given, it is applied to each item, and the resulting lists of strings are concatenated to form the initial argument list. Otherwise, the initial argument list is the result of applying the standard conversion to each item.
3. Each argument in the list is formatted with `format_each`, if present.
4. If `uniquify` is true, duplicate arguments are removed. The first occurrence is the one that remains.
5. If a `before_each` string is given, it is inserted as a new argument before each existing argument in the list. This effectively doubles the number of arguments to be appended by this point.
6. Except in the case that the list is empty and `omit_if_empty` is true (the default), the arg name and `terminate_with` are inserted as the first and last arguments, respectively, if they are given.

Note that empty strings are valid arguments that are subject to all these processing steps.


### Parameters

ParameterDescription`arg_name_or_values`
 required

 If two positional parameters are passed this is interpreted as the arg name. The arg name is added before the `values` as a separate argument without any processing. This arg name will not be added if `omit_if_empty` is true (the default) and no other items are appended (as happens if `values` is empty or all of its items are filtered). If only one positional parameter is passed, it is interpreted as `values` (see below).
 `values`[sequence](../core/list.html); or [depset](../builtins/depset.html);
 default is `unbound`

 The list, tuple, or depset whose items will be appended.
 `map_each`
 callable; or `None`;
 default is `None`

 A function that converts each item to zero or more strings, which may be further processed before appending. If this param is not provided, the standard conversion is used.

The function is passed either one or two positional arguments: the item to convert, followed by an optional [`DirectoryExpander`](../builtins/DirectoryExpander.html). The second argument will be passed only if the supplied function is user-defined (not built-in) and declares more than one parameter.

The return value's type depends on how many arguments are to be produced for the item:

- In the common case when each item turns into one string, the function should return that string.
- If the item is to be filtered out entirely, the function should return `None`.
- If the item turns into multiple strings, the function returns a list of those strings.

Returning a single string or `None` has the same effect as returning a list of length 1 or length 0 respectively. However, it is more efficient and readable to avoid creating a list where it is not needed.

Ordinarily, items that are directories are automatically expanded to their contents when `expand_directories=True` is set. However, this will not expand directories contained inside other values -- for instance, when the items are structs that have directories as fields. In this situation, the `DirectoryExpander` argument can be applied to manually obtain the files of a given directory.

To avoid unintended retention of large analysis-phase data structures into the execution phase, the `map_each` function must be declared by a top-level `def` statement; it may not be a nested function closure by default.

_Warning:_ [`print()`](../globals/all.html#print) statements that are executed during the call to `map_each` will not produce any visible output.


`format_each`[string](../core/string.html); or `None`;
 default is `None`

 An optional format string pattern, applied to each string returned by the `map_each` function. The format string must have exactly one '%s' placeholder.
 `before_each`[string](../core/string.html); or `None`;
 default is `None`

 An optional argument to append before each argument derived from `values` is appended.
 `omit_if_empty`[bool](../core/bool.html);
 default is `True`

 If true, if there are no arguments derived from `values` to be appended, then all further processing is suppressed and the command line will be unchanged. If false, the arg name and `terminate_with`, if provided, will still be appended regardless of whether or not there are other arguments.
 `uniquify`[bool](../core/bool.html);
 default is `False`

 If true, duplicate arguments that are derived from `values` will be omitted. Only the first occurrence of each argument will remain. Usually this feature is not needed because depsets already omit duplicates, but it can be useful if `map_each` emits the same string for multiple items.
 `expand_directories`[bool](../core/bool.html);
 default is `True`

 If true, any directories in `values` will be expanded to a flat list of files. This happens before `map_each` is applied.
 `terminate_with`[string](../core/string.html); or `None`;
 default is `None`

 An optional argument to append after all other arguments. This argument will not be added if `omit_if_empty` is true (the default) and no other items are appended (as happens if `values` is empty or all of its items are filtered).
 `allow_closure`[bool](../core/bool.html);
 default is `False`

 If true, allows the use of closures in function parameters like `map_each`. Usually this isn't necessary and it risks retaining large analysis-phase data structures into the execution phase.


## add\_joined

```
Args Args.add_joined(arg_name_or_values, values=unbound, *, join_with, map_each=None, format_each=None, format_joined=None, omit_if_empty=True, uniquify=False, expand_directories=True, allow_closure=False)
```

 Appends an argument to this command line by concatenating together multiple values using a separator. The items are processed lazily during the execution phase.

Processing is similar to [`add_all()`](#add_all), but the list of arguments derived from `values` is combined into a single argument as if by `join_with.join(...)`, and then formatted using the given `format_joined` string template. Unlike `add_all()`, there is no `before_each` or `terminate_with` parameter since these are not generally useful when the items are combined into a single argument.

If after filtering there are no strings to join into an argument, and if `omit_if_empty` is true (the default), no processing is done. Otherwise if there are no strings to join but `omit_if_empty` is false, the joined string will be an empty string.


### Parameters

ParameterDescription`arg_name_or_values`
 required

 If two positional parameters are passed this is interpreted as the arg name. The arg name is added before `values` without any processing. This arg will not be added if `omit_if_empty` is true (the default) and there are no strings derived from `values` to join together (which can happen if `values` is empty or all of its items are filtered). If only one positional parameter is passed, it is interpreted as `values` (see below).
 `values`[sequence](../core/list.html); or [depset](../builtins/depset.html);
 default is `unbound`

 The list, tuple, or depset whose items will be joined.
 `join_with`[string](../core/string.html);
 required

 A delimiter string used to join together the strings obtained from applying `map_each` and `format_each`, in the same manner as [`string.join()`](../core/string.html#join).
 `map_each`
 callable; or `None`;
 default is `None`

 Same as for [`add_all`](#add_all.map_each).
 `format_each`[string](../core/string.html); or `None`;
 default is `None`

 Same as for [`add_all`](#add_all.format_each).
 `format_joined`[string](../core/string.html); or `None`;
 default is `None`

 An optional format string pattern applied to the joined string. The format string must have exactly one '%s' placeholder.
 `omit_if_empty`[bool](../core/bool.html);
 default is `True`

 If true, if there are no strings to join together (either because `values` is empty or all its items are filtered), then all further processing is suppressed and the command line will be unchanged. If false, then even if there are no strings to join together, two arguments will be appended: the arg name followed by an empty string (which is the logical join of zero strings).
 `uniquify`[bool](../core/bool.html);
 default is `False`

 Same as for [`add_all`](#add_all.uniquify).
 `expand_directories`[bool](../core/bool.html);
 default is `True`

 Same as for [`add_all`](#add_all.expand_directories).
 `allow_closure`[bool](../core/bool.html);
 default is `False`

 Same as for [`add_all`](#add_all.allow_closure).


## set\_param\_file\_format

```
Args Args.set_param_file_format(format)
```

 Sets the format of the param file, if one is used


### Parameters

ParameterDescription`format`[string](../core/string.html);
 required

 Must be one of:

- "multiline": Each item (argument name or value) is written verbatim to the param file with a newline character following it.
- "shell": Same as "multiline", but the items are shell-quoted
- "flag\_per\_line": Same as "multiline", but (1) only flags (beginning with '--') are written to the param file, and (2) the values of the flags, if any, are written on the same line with a '=' separator. This is the format expected by the Abseil flags library.

The format defaults to "shell" if not called.


## use\_param\_file

```
Args Args.use_param_file(param_file_arg, *, use_always=False)
```

 Spills the args to a params file, replacing them with a pointer to the param file. Use when your args may be too large for the system's command length limits.

Bazel may choose to elide writing the params file to the output tree during execution for efficiency. If you are debugging actions and want to inspect the param file, pass `--materialize_param_files` to your build.


### Parameters

ParameterDescription`param_file_arg`[string](../core/string.html);
 required

 A format string with a single "%s". If the args are spilled to a params file then they are replaced with an argument consisting of this string formatted with the path of the params file.

For example, if the args are spilled to a params file "params.txt", then specifying "--file=%s" would cause the action command line to contain "--file=params.txt".


`use_always`[bool](../core/bool.html);
 default is `False`

 Whether to always spill the args to a params file. If false, bazel will decide whether the arguments need to be spilled based on your system and arg length.

---

## Aspect
- URL: https://bazel.build/rules/lib/builtins/Aspect
- Source: rules/lib/builtins/Aspect.mdx
- Slug: /rules/lib/builtins/Aspect

For more information about Aspects, please consult the
[documentation of the aspect function](../globals/bzl.html#aspect) or the [introduction to Aspects](https://bazel.build/extending/aspects).

---

## Attribute
- URL: https://bazel.build/rules/lib/builtins/Attribute
- Source: rules/lib/builtins/Attribute.mdx
- Slug: /rules/lib/builtins/Attribute

Representation of a definition of an attribute. Use the [attr](../toplevel/attr.html) module to create an Attribute. They are only for use with a [rule](../globals/bzl.html#rule) or an [aspect](../globals/bzl.html#aspect).

---

## BuildSetting
- URL: https://bazel.build/rules/lib/builtins/BuildSetting
- Source: rules/lib/builtins/BuildSetting.mdx
- Slug: /rules/lib/builtins/BuildSetting

The descriptor for a single piece of configuration information. If configuration is a key-value map of settings like \{'cpu': 'ppc', 'copt': '-DFoo'\}, this describes a single entry in that map.

---

## CcCompilationOutputs
- URL: https://bazel.build/rules/lib/builtins/CcCompilationOutputs
- Source: rules/lib/builtins/CcCompilationOutputs.mdx
- Slug: /rules/lib/builtins/CcCompilationOutputs

Helper class containing CC compilation outputs.

## Members

- [objects](#objects)
- [pic\_objects](#pic_objects)

## objects

```
sequence CcCompilationOutputs.objects
```

 Non-PIC object files.



## pic\_objects

```
sequence CcCompilationOutputs.pic_objects
```

 PIC object files.

---

## CcLinkingOutputs
- URL: https://bazel.build/rules/lib/builtins/CcLinkingOutputs
- Source: rules/lib/builtins/CcLinkingOutputs.mdx
- Slug: /rules/lib/builtins/CcLinkingOutputs

Helper class containing CC compilation outputs.

## Members

- [executable](#executable)
- [library\_to\_link](#library_to_link)

## executable

```
File CcLinkingOutputs.executable
```

 Represents the linked executable.
 May return `None`.



## library\_to\_link

```
LibraryToLink CcLinkingOutputs.library_to_link
```

 `LibraryToLink` for including these outputs in further linking.
 May return `None`.

---

## CompilationContext
- URL: https://bazel.build/rules/lib/builtins/CompilationContext
- Source: rules/lib/builtins/CompilationContext.mdx
- Slug: /rules/lib/builtins/CompilationContext

Immutable store of information needed for C++ compilation that is aggregated across dependencies.

## Members

- [defines](#defines)
- [direct\_headers](#direct_headers)
- [direct\_private\_headers](#direct_private_headers)
- [direct\_public\_headers](#direct_public_headers)
- [direct\_textual\_headers](#direct_textual_headers)
- [external\_includes](#external_includes)
- [framework\_includes](#framework_includes)
- [headers](#headers)
- [includes](#includes)
- [local\_defines](#local_defines)
- [quote\_includes](#quote_includes)
- [system\_includes](#system_includes)
- [validation\_artifacts](#validation_artifacts)

## defines

```
depset CompilationContext.defines
```

 Returns the set of defines needed to compile this target. Each define is a string. These values are propagated to the target's transitive dependents, that is, any rules that depend on this target.



## direct\_headers

```
list CompilationContext.direct_headers
```

 Returns the list of modular headers that are declared by this target. This includes both public headers (such as those listed in "hdrs") and private headers (such as those listed in "srcs").



## direct\_private\_headers

```
list CompilationContext.direct_private_headers
```

 Returns the list of modular private headers (those listed in "srcs") that are declared by this target.



## direct\_public\_headers

```
list CompilationContext.direct_public_headers
```

 Returns the list of modular public headers (those listed in "hdrs") that are declared by this target.



## direct\_textual\_headers

```
list CompilationContext.direct_textual_headers
```

 Returns the list of textual headers that are declared by this target.



## external\_includes

```
depset CompilationContext.external_includes
```

 Returns the set of search paths (as strings) for external header files referenced by angle bracket. Usually passed with -isystem.



## framework\_includes

```
depset CompilationContext.framework_includes
```

 Returns the set of search paths (as strings) for framework header files. Usually passed with -F.



## headers

```
depset CompilationContext.headers
```

 Returns the set of headers needed to compile this target.



## includes

```
depset CompilationContext.includes
```

 Returns the set of search paths (as strings) for header files referenced both by angle bracket and quotes. Usually passed with -I.



## local\_defines

```
depset CompilationContext.local_defines
```

 Returns the set of defines needed to compile this target. Each define is a string. These values are not propagated to the target's transitive dependents.



## quote\_includes

```
depset CompilationContext.quote_includes
```

 Returns the set of search paths (as strings) for header files referenced by quotes, e.g. #include "foo/bar/header.h". They can be either relative to the exec root or absolute. Usually passed with -iquote.



## system\_includes

```
depset CompilationContext.system_includes
```

 Returns the set of search paths (as strings) for header files referenced by angle brackets, e.g. #include &lt;foo/bar/header.h&gt;. They can be either relative to the exec root or absolute. Usually passed with -isystem.



## validation\_artifacts

```
depset CompilationContext.validation_artifacts
```

 Returns the set of validation artifacts.

---

## DirectoryExpander
- URL: https://bazel.build/rules/lib/builtins/DirectoryExpander
- Source: rules/lib/builtins/DirectoryExpander.mdx
- Slug: /rules/lib/builtins/DirectoryExpander

Expands directories created by [`ctx.actions.declare_directory`](../builtins/actions.html#declare_directory) during the execution phase. This is useful to expand directories in [`map_each`](../builtins/Args.html#add_all.map_each).

## Members

- [expand](#expand)

## expand

```
list DirectoryExpander.expand(file)
```

 If the given `File` is a directory, this returns a list of `File` s recursively underneath the directory. Otherwise, this returns a list containing just the given `File` itself.


### Parameters

ParameterDescription`file`[File](../builtins/File.html);
 required

 The directory or file to expand.

---

## DottedVersion
- URL: https://bazel.build/rules/lib/builtins/DottedVersion
- Source: rules/lib/builtins/DottedVersion.mdx
- Slug: /rules/lib/builtins/DottedVersion

A value representing a version with multiple components, separated by periods, such as 1.2.3.4.

## Members

- [compare\_to](#compare_to)

## compare\_to

```
int DottedVersion.compare_to(other)
```

 Compares based on most significant (first) not-matching version component. So, for example, 1.2.3 < 1.2.4


### Parameters

ParameterDescription`other`[DottedVersion](../builtins/DottedVersion.html);
 required

 The other dotted version.

---

## ExecGroupCollection
- URL: https://bazel.build/rules/lib/builtins/ExecGroupCollection
- Source: rules/lib/builtins/ExecGroupCollection.mdx
- Slug: /rules/lib/builtins/ExecGroupCollection

Stores exec groups available to a given rule.

---

## ExecGroupContext
- URL: https://bazel.build/rules/lib/builtins/ExecGroupContext
- Source: rules/lib/builtins/ExecGroupContext.mdx
- Slug: /rules/lib/builtins/ExecGroupContext

Stores information about an exec group.

## Members

- [toolchains](#toolchains)

## toolchains

```
ToolchainContext ExecGroupContext.toolchains
```

 Toolchains required for this exec group

---

## ExecTransitionFactory
- URL: https://bazel.build/rules/lib/builtins/ExecTransitionFactory
- Source: rules/lib/builtins/ExecTransitionFactory.mdx
- Slug: /rules/lib/builtins/ExecTransitionFactory

an execution transition.

---

## ExpandedDirectory
- URL: https://bazel.build/rules/lib/builtins/ExpandedDirectory
- Source: rules/lib/builtins/ExpandedDirectory.mdx
- Slug: /rules/lib/builtins/ExpandedDirectory

Represents an expanded directory that makes the files within the it directly accessible.

## Members

- [children](#children)
- [directory](#directory)

## children

```
list ExpandedDirectory.children
```

 Contains the files within the directory.



## directory

```
File ExpandedDirectory.directory
```

 The input directory that was expanded.

---

## Facts
- URL: https://bazel.build/rules/lib/builtins/Facts
- Source: rules/lib/builtins/Facts.mdx
- Slug: /rules/lib/builtins/Facts

User-provided data attached to a module extension that is persisted across reevaluations of
the extension.

This type supports dict-like access (e.g. \`facts\["key"\]\` and \`facts.get("key")\`) as well as
membership tests (e.g. \`"key" in facts\`). It does not support iteration or methods like
\`keys()\`, \`items()\`, or \`len()\`.

## Members

- [get](#get)

## get

```
unknown Facts.get(key, default=None)
```

 Returns the value for `key` if it exists, or `default`.


### Parameters

ParameterDescription`key`[string](../core/string.html);
 required

 The key to look up.
 `default`
 default is `None`

 The value to return if `key` is not present.

---

## FeatureConfiguration
- URL: https://bazel.build/rules/lib/builtins/FeatureConfiguration
- Source: rules/lib/builtins/FeatureConfiguration.mdx
- Slug: /rules/lib/builtins/FeatureConfiguration

Class used to construct command lines from CROSSTOOL features.

---

## File
- URL: https://bazel.build/rules/lib/builtins/File
- Source: rules/lib/builtins/File.mdx
- Slug: /rules/lib/builtins/File

This object is created during the analysis phase to represent a file or directory that will be read or written during the execution phase. It is not an open file handle, and cannot be used to directly read or write file contents. Rather, you use it to construct the action graph in a rule implementation function by passing it to action-creating functions. See the [Rules page](https://bazel.build/extending/rules#files) for more information.

When a `File` is passed to an [`Args`](../builtins/Args.html) object without using a `map_each` function, it is converted to a string by taking the value of its `path` field.

## Members

- [basename](#basename)
- [dirname](#dirname)
- [extension](#extension)
- [is\_directory](#is_directory)
- [is\_source](#is_source)
- [is\_symlink](#is_symlink)
- [owner](#owner)
- [path](#path)
- [root](#root)
- [short\_path](#short_path)
- [tree\_relative\_path](#tree_relative_path)

## basename

```
string File.basename
```

 The base name of this file. This is the name of the file inside the directory.



## dirname

```
string File.dirname
```

 The name of the directory containing this file. It's taken from [path](#path) and is always relative to the execution directory.



## extension

```
string File.extension
```

 The file extension of this file, following (not including) the rightmost period. Empty string if the file's basename includes no periods.



## is\_directory

```
bool File.is_directory
```

 Returns true if this is a directory. This reflects the type the file was declared as (i.e. ctx.actions.declare\_directory), not its type on the filesystem, which might differ.



## is\_source

```
bool File.is_source
```

 Returns true if this is a source file, i.e. it is not generated.



## is\_symlink

```
bool File.is_symlink
```

 Returns true if this was declared as a symlink. This reflects the type the file was declared as (i.e. ctx.actions.declare\_symlink), not its type on the filesystem, which might differ.



## owner

```
Label File.owner
```

 A label of a target that produces this File.
 May return `None`.



## path

```
string File.path
```

 The execution path of this file, relative to the workspace's execution directory. It consists of two parts, an optional first part called the _root_ (see also the [root](../builtins/root.html) module), and the second part which is the `short_path`. The root may be empty, which it usually is for non-generated files. For generated files it usually contains a configuration-specific path fragment that encodes things like the target CPU architecture that was used while building said file. Use the `short_path` for the path under which the file is mapped if it's in the runfiles of a binary.



## root

```
root File.root
```

 The root beneath which this file resides.



## short\_path

```
string File.short_path
```

 The path of this file relative to its root. This excludes the aforementioned _root_, i.e. configuration-specific fragments of the path. This is also the path under which the file is mapped if it's in the runfiles of a binary.



## tree\_relative\_path

```
string File.tree_relative_path
```

 The path of this file relative to the root of the ancestor's tree, if the ancestor's [is\_directory](#is_directory) field is true. `tree_relative_path` is only available for expanded files of a directory in an action command, i.e. [Args.add\_all()](../builtins/Args.html#add_all). For other types of files, it is an error to access this field.

---

## Label
- URL: https://bazel.build/rules/lib/builtins/Label
- Source: rules/lib/builtins/Label.mdx
- Slug: /rules/lib/builtins/Label

A BUILD target identifier.

For every `Label` instance `l`, the string representation `str(l)` has the property that `Label(str(l)) == l`, regardless of where the `Label()` call occurs.

When passed as positional arguments to `print()` or `fail()`, `Label` use a string representation optimized for human readability instead. This representation uses an [apparent repository name](/external/overview#apparent-repo-name) from the perspective of the main repository if possible.

## Members

- [Label](#Label)
- [name](#name)
- [package](#package)
- [relative](#relative)
- [repo\_name](#repo_name)
- [same\_package\_label](#same_package_label)
- [workspace\_name](#workspace_name)
- [workspace\_root](#workspace_root)

## Label

```
Label Label(input)
```

 Converts a label string into a `Label` object, in the context of the package where the calling `.bzl` source file lives. If the given value is already a `Label`, it is returned unchanged.

For macros, a related function, `native.package_relative_label()`, converts the input into a `Label` in the context of the package currently being constructed. Use that function to mimic the string-to-label conversion that is automatically done by label-valued rule attributes.


### Parameters

ParameterDescription`input`[string](../core/string.html); or [Label](../builtins/Label.html);
 required

 The input label string or Label object. If a Label object is passed, it's returned as is.


## name

```
string Label.name
```

 The name of the target referred to by this label. For instance:

```
Label("@@foo//pkg/foo:abc").name == "abc"
```

## package

```
string Label.package
```

 The name of the package containing the target referred to by this label, without the repository name. For instance:

```
Label("@@repo//pkg/foo:abc").package == "pkg/foo"
```

## relative

```
Label Label.relative(relName)
```

 **Experimental**. This API is experimental and may change at any time. Please do not depend on it. It may be enabled on an experimental basis by setting `--+incompatible_enable_deprecated_label_apis`

**Deprecated.** This method behaves surprisingly when used with an argument containing an apparent repo name. Prefer [`Label.same_package_label()`](#same_package_label), [`native.package_relative_label()`](../toplevel/native.html#package_relative_label), or [`Label()`](#Label) instead.

Resolves a label that is either absolute (starts with `//`) or relative to the current package. If this label is in a remote repository, the argument will be resolved relative to that repository. If the argument contains a repository name, the current label is ignored and the argument is returned as-is, except that the repository name is rewritten if it is in the current repository mapping. Reserved labels will also be returned as-is.

For example:

```
Label("//foo/bar:baz").relative(":quux") == Label("//foo/bar:quux")
Label("//foo/bar:baz").relative("//wiz:quux") == Label("//wiz:quux")
Label("@repo//foo/bar:baz").relative("//wiz:quux") == Label("@repo//wiz:quux")
Label("@repo//foo/bar:baz").relative("//visibility:public") == Label("//visibility:public")
Label("@repo//foo/bar:baz").relative("@other//wiz:quux") == Label("@other//wiz:quux")

```

If the repository mapping passed in is `{'@other' : '@remapped'}`, then the following remapping will take place:

```
Label("@repo//foo/bar:baz").relative("@other//wiz:quux") == Label("@remapped//wiz:quux")

```

### Parameters

ParameterDescription`relName`[string](../core/string.html);
 required

 The label that will be resolved relative to this one.


## repo\_name

```
string Label.repo_name
```

 The canonical name of the repository containing the target referred to by this label, without any leading at-signs ( `@`). For instance,

```
Label("@@foo//bar:baz").repo_name == "foo"
```

## same\_package\_label

```
Label Label.same_package_label(target_name)
```

 Creates a label in the same package as this label with the given target name.


### Parameters

ParameterDescription`target_name`[string](../core/string.html);
 required

 The target name of the new label.


## workspace\_name

```
string Label.workspace_name
```

 **Experimental**. This API is experimental and may change at any time. Please do not depend on it. It may be enabled on an experimental basis by setting `--+incompatible_enable_deprecated_label_apis`

**Deprecated.** The field name "workspace name" is a misnomer here; use the identically-behaving [`Label.repo_name`](#repo_name) instead.

The canonical name of the repository containing the target referred to by this label, without any leading at-signs ( `@`). For instance,

```
Label("@@foo//bar:baz").workspace_name == "foo"
```

## workspace\_root

```
string Label.workspace_root
```

 Returns the execution root for the repository containing the target referred to by this label, relative to the execroot. For instance:

```
Label("@repo//pkg/foo:abc").workspace_root == "external/repo"
```

---

## LateBoundDefault
- URL: https://bazel.build/rules/lib/builtins/LateBoundDefault
- Source: rules/lib/builtins/LateBoundDefault.mdx
- Slug: /rules/lib/builtins/LateBoundDefault

Represents a late-bound default attribute value of type 'Label'. The value of a LateBoundDefault is only resolvable in the context of a rule implementation function, and depends on the current build configuration. For example, a LateBoundDefault might represent the Label of the java toolchain in the current build configuration.

See [configuration\_field](../globals/bzl.html#configuration_field) for example usage.

---

## LibraryToLink
- URL: https://bazel.build/rules/lib/builtins/LibraryToLink
- Source: rules/lib/builtins/LibraryToLink.mdx
- Slug: /rules/lib/builtins/LibraryToLink

A library the user can link against.

## Members

- [alwayslink](#alwayslink)
- [dynamic\_library](#dynamic_library)
- [interface\_library](#interface_library)
- [lto\_bitcode\_files](#lto_bitcode_files)
- [objects](#objects)
- [pic\_lto\_bitcode\_files](#pic_lto_bitcode_files)
- [pic\_objects](#pic_objects)
- [pic\_static\_library](#pic_static_library)
- [resolved\_symlink\_dynamic\_library](#resolved_symlink_dynamic_library)
- [resolved\_symlink\_interface\_library](#resolved_symlink_interface_library)
- [static\_library](#static_library)

## alwayslink

```
bool LibraryToLink.alwayslink
```

 Whether to link the static library/objects in the --whole\_archive block.



## dynamic\_library

```
File LibraryToLink.dynamic_library
```

 `Artifact` of dynamic library to be linked. Always used for runtime and used for linking if `interface_library` is not passed.
 May return `None`.



## interface\_library

```
File LibraryToLink.interface_library
```

 `Artifact` of interface library to be linked.
 May return `None`.



## lto\_bitcode\_files

```
sequence LibraryToLink.lto_bitcode_files
```

 `List` of LTO bitcode files in the library.
 May return `None`.



## objects

```
sequence LibraryToLink.objects
```

 `List` of object files in the library.
 May return `None`.



## pic\_lto\_bitcode\_files

```
sequence LibraryToLink.pic_lto_bitcode_files
```

 `List` of pic LTO bitcode files in the library.
 May return `None`.



## pic\_objects

```
sequence LibraryToLink.pic_objects
```

 `List` of pic object files in the library.
 May return `None`.



## pic\_static\_library

```
File LibraryToLink.pic_static_library
```

 `Artifact` of pic static library to be linked.
 May return `None`.



## resolved\_symlink\_dynamic\_library

```
File LibraryToLink.resolved_symlink_dynamic_library
```

 The resolved `Artifact` of the dynamic library to be linked if `dynamic_library` is a symlink, otherwise this is None.
 May return `None`.



## resolved\_symlink\_interface\_library

```
File LibraryToLink.resolved_symlink_interface_library
```

 The resolved `Artifact` of the interface library to be linked if `interface_library` is a symlink, otherwise this is None.
 May return `None`.



## static\_library

```
File LibraryToLink.static_library
```

 `Artifact` of static library to be linked.
 May return `None`.

---

## License
- URL: https://bazel.build/rules/lib/builtins/License
- Source: rules/lib/builtins/License.mdx
- Slug: /rules/lib/builtins/License

This API is deprecated and will be removed. Please do not depend on it. This object represents the value of a license attribute.

---

## LinkerInput
- URL: https://bazel.build/rules/lib/builtins/LinkerInput
- Source: rules/lib/builtins/LinkerInput.mdx
- Slug: /rules/lib/builtins/LinkerInput

Either libraries, flags or other files that may be passed to the linker as inputs.

## Members

- [additional\_inputs](#additional_inputs)
- [libraries](#libraries)
- [owner](#owner)
- [user\_link\_flags](#user_link_flags)

## additional\_inputs

```
sequence LinkerInput.additional_inputs
```

 Returns the depset of additional inputs, e.g.: linker scripts.



## libraries

```
sequence LinkerInput.libraries
```

 Returns the depset of `LibraryToLink`. May return a list but this is deprecated. See #8118.



## owner

```
Label LinkerInput.owner
```

 Returns the owner of this LinkerInput.



## user\_link\_flags

```
sequence LinkerInput.user_link_flags
```

 Returns the list of user link flags passed as strings.

---

## LinkingContext
- URL: https://bazel.build/rules/lib/builtins/LinkingContext
- Source: rules/lib/builtins/LinkingContext.mdx
- Slug: /rules/lib/builtins/LinkingContext

Immutable store of information needed for C++ linking that is aggregated across dependencies.

## Members

- [linker\_inputs](#linker_inputs)

## linker\_inputs

```
depset LinkingContext.linker_inputs
```

 Returns the depset of linker inputs.

---

## Provider
- URL: https://bazel.build/rules/lib/builtins/Provider
- Source: rules/lib/builtins/Provider.mdx
- Slug: /rules/lib/builtins/Provider

A constructor for simple value objects, known as provider instances.

This value has a dual purpose:

- It is a function that can be called to construct 'struct'-like values:

  ```
  DataInfo = provider()
  d = DataInfo(x = 2, y = 3)
  print(d.x + d.y) # prints 5
  ```

   Note: Some providers, defined internally, do not allow instance creation
- It is a _key_ to access a provider instance on a [Target](../builtins/Target.html)

  ```
  DataInfo = provider()
  def _rule_impl(ctx)
    ... ctx.attr.dep[DataInfo]
  ```


Create a new `Provider` using the [provider](../globals/bzl.html#provider) function.

---

## Subrule
- URL: https://bazel.build/rules/lib/builtins/Subrule
- Source: rules/lib/builtins/Subrule.mdx
- Slug: /rules/lib/builtins/Subrule

Experimental: a building block for writing rules with shared code. For more information, please see the subrule proposal: https://docs.google.com/document/d/1RbNC88QieKvBEwir7iV5zZU08AaMlOzxhVkPnmKDedQ

---

## SymlinkEntry
- URL: https://bazel.build/rules/lib/builtins/SymlinkEntry
- Source: rules/lib/builtins/SymlinkEntry.mdx
- Slug: /rules/lib/builtins/SymlinkEntry

A single runfiles symlink represented by a link name and target.

## Members

- [path](#path)
- [target\_file](#target_file)

## path

```
string SymlinkEntry.path
```

 The path of the symlink in the runfiles tree



## target\_file

```
File SymlinkEntry.target_file
```

 Target file of the symlink

---

## Target
- URL: https://bazel.build/rules/lib/builtins/Target
- Source: rules/lib/builtins/Target.mdx
- Slug: /rules/lib/builtins/Target

The BUILD target for a dependency. Appears in the fields of `ctx.attr` corresponding to [dependency attributes](https://bazel.build/extending/rules#dependency_attributes) ( `label` or `label_list`). Has the following fields:

- ### label

   `Label Target.label`


  The identifier of the target.
- ### Providers


  The [providers](https://bazel.build/extending/rules#providers) of a rule target can be accessed by type using index notation ( `target[DefaultInfo]`). The presence of providers can be checked using the `in` operator ( `SomeInfo in target`).

---

## TemplateDict
- URL: https://bazel.build/rules/lib/builtins/TemplateDict
- Source: rules/lib/builtins/TemplateDict.mdx
- Slug: /rules/lib/builtins/TemplateDict

An Args-like structure for use in ctx.actions.expand\_template(), which allows for deferring evaluation of values till the execution phase.

## Members

- [add](#add)
- [add\_joined](#add_joined)

## add

```
TemplateDict TemplateDict.add(key, value)
```

 Add a String value


### Parameters

ParameterDescription`key`[string](../core/string.html);
 required

 A String key
 `value`[string](../core/string.html);
 required

 A String value


## add\_joined

```
TemplateDict TemplateDict.add_joined(key, values, *, join_with, map_each, uniquify=False, format_joined=None, allow_closure=False)
```

 Add depset of values


### Parameters

ParameterDescription`key`[string](../core/string.html);
 required

 A String key
 `values`[depset](../builtins/depset.html);
 required

 The depset whose items will be joined.
 `join_with`[string](../core/string.html);
 required

 A delimiter string used to join together the strings obtained from applying `map_each`, in the same manner as [`string.join()`](../core/string.html#join).
 `map_each`
 callable;
 required

 A Starlark function accepting a single argument and returning either a string, `None`, or a list of strings. This function is applied to each item of the depset specified in the `values` parameter
 `uniquify`[bool](../core/bool.html);
 default is `False`

 If true, duplicate strings derived from `values` will be omitted. Only the first occurrence of each string will remain. Usually this feature is not needed because depsets already omit duplicates, but it can be useful if `map_each` emits the same string for multiple items.
 `format_joined`[string](../core/string.html); or `None`;
 default is `None`

 An optional format string pattern applied to the joined string. The format string must have exactly one '%s' placeholder.
 `allow_closure`[bool](../core/bool.html);
 default is `False`

 If true, allows the use of closures in function parameters like `map_each`. Usually this isn't necessary and it risks retaining large analysis-phase data structures into the execution phase.

---

## ToolchainContext
- URL: https://bazel.build/rules/lib/builtins/ToolchainContext
- Source: rules/lib/builtins/ToolchainContext.mdx
- Slug: /rules/lib/builtins/ToolchainContext

Holds toolchains available for a particular exec group. Toolchain targets are accessed by indexing with the toolchain type, as in `ctx.toolchains["//pkg:my_toolchain_type"]`. If the toolchain was optional and no toolchain was resolved, this will return `None`. Accessing toolchains of an aspect or rule via `ctx.toolchains` returns the indexed toolchain as a `ToolchainInfo` provider. While when using aspects, `ToolchainContext` is also used to hold the toolchains of the base target. It can be accessed by `ctx.rule.toolchains["//pkg:my_toolchain_type"]` and it returns the list of providers resulted from applying the aspects on these toolchain targets.

---

## actions
- URL: https://bazel.build/rules/lib/builtins/actions
- Source: rules/lib/builtins/actions.mdx
- Slug: /rules/lib/builtins/actions

Module providing functions to create actions. Access this module using [`ctx.actions`](../builtins/ctx.html#actions).

## Members

- [args](#args)
- [declare\_directory](#declare_directory)
- [declare\_file](#declare_file)
- [declare\_symlink](#declare_symlink)
- [do\_nothing](#do_nothing)
- [expand\_template](#expand_template)
- [map\_directory](#map_directory)
- [run](#run)
- [run\_shell](#run_shell)
- [symlink](#symlink)
- [template\_dict](#template_dict)
- [write](#write)

## args

```
Args actions.args()
```

 Returns an Args object that can be used to build memory-efficient command lines.



## declare\_directory

```
File actions.declare_directory(filename, *, sibling=None)
```

 Declares that the rule or aspect creates a directory with the given name, in the current package. You must create an action that generates the directory. The contents of the directory are not directly accessible from Starlark, but can be expanded in an action command with [`Args.add_all()`](../builtins/Args.html#add_all). Only regular files and directories can be in the expanded contents of a declare\_directory.


### Parameters

ParameterDescription`filename`[string](../core/string.html);
 required

 If no 'sibling' provided, path of the new directory, relative to the current package. Otherwise a base name for a file ('sibling' defines a directory).
 `sibling`[File](../builtins/File.html); or `None`;
 default is `None`

 A file that lives in the same directory as the newly declared directory. The file must be in the current package.


## declare\_file

```
File actions.declare_file(filename, *, sibling=None)
```

 Declares that the rule or aspect creates a file with the given filename. If `sibling` is not specified, the file name is relative to the package directory, otherwise the file is in the same directory as `sibling`. Files cannot be created outside of the current package.

Remember that in addition to declaring a file, you must separately create an action that emits the file. Creating that action will require passing the returned `File` object to the action's construction function.

Note that [predeclared output files](https://bazel.build/extending/rules#files) do not need to be (and cannot be) declared using this function. You can obtain their `File` objects from [`ctx.outputs`](../builtins/ctx.html#outputs) instead. [See example of use](https://github.com/bazelbuild/examples/tree/main/rules/computed_dependencies/hash.bzl).


### Parameters

ParameterDescription`filename`[string](../core/string.html);
 required

 If no 'sibling' provided, path of the new file, relative to the current package. Otherwise a base name for a file ('sibling' determines a directory).
 `sibling`[File](../builtins/File.html); or `None`;
 default is `None`

 A file that lives in the same directory as the newly created file. The file must be in the current package.


## declare\_symlink

```
File actions.declare_symlink(filename, *, sibling=None)
```

 Declares that the rule or aspect creates a symlink with the given name in the current package. You must create an action that generates this symlink. Bazel will never dereference this symlink and will transfer it verbatim to sandboxes or remote executors. Symlinks inside tree artifacts are not currently supported.


### Parameters

ParameterDescription`filename`[string](../core/string.html);
 required

 If no 'sibling' provided, path of the new symlink, relative to the current package. Otherwise a base name for a file ('sibling' defines a directory).
 `sibling`[File](../builtins/File.html); or `None`;
 default is `None`

 A file that lives in the same directory as the newly declared symlink.


## do\_nothing

```
None actions.do_nothing(*, mnemonic, inputs=[])
```

 Creates an empty action that neither executes a command nor produces any output, but that is useful for inserting 'extra actions'.


### Parameters

ParameterDescription`mnemonic`[string](../core/string.html);
 required

 A one-word description of the action, for example, CppCompile or GoLink.
 `inputs`[sequence](../core/list.html) of [File](../builtins/File.html) s; or [depset](../builtins/depset.html);
 default is `[]`

 List of the input files of the action.


## expand\_template

```
None actions.expand_template(*, template, output, substitutions={}, is_executable=False, computed_substitutions=unbound)
```

 Creates a template expansion action. When the action is executed, it will generate a file based on a template. Parts of the template will be replaced using the `substitutions` dictionary, in the order the substitutions are specified. Whenever a key of the dictionary appears in the template (or a result of a previous substitution), it is replaced with the associated value. There is no special syntax for the keys. You may, for example, use curly braces to avoid conflicts (for example, `{KEY}`). [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/expand_template/hello.bzl).


### Parameters

ParameterDescription`template`[File](../builtins/File.html);
 required

 The template file, which is a UTF-8 encoded text file.
 `output`[File](../builtins/File.html);
 required

 The output file, which is a UTF-8 encoded text file.
 `substitutions`[dict](../core/dict.html);
 default is `{}`

 Substitutions to make when expanding the template.
 `is_executable`[bool](../core/bool.html);
 default is `False`

 Whether the output file should be executable.
 `computed_substitutions`[TemplateDict](../builtins/TemplateDict.html);
 default is `unbound`

 Substitutions to make when expanding the template.


## map\_directory

```
None actions.map_directory(*, input_directories, additional_inputs={}, output_directories, tools, additional_params={}, execution_requirements=None, exec_group=None, toolchain=None, use_default_shell_env=False, env=None, mnemonic=None, implementation)
```

 Creates multiple actions based on the files within one or more input directories, to output one or more output directories.


### Parameters

ParameterDescription`input_directories`[dict](../core/dict.html) of [File](../builtins/File.html) s;
 required

 A dictionary mapping of strings to input directories, as declared by `ctx.actions.declare_directory()` (only directories are allowed as values here). The values specify the directories that we want expanded to access their files in the implementation function. The keys (strings) act as identifiers to easily reference a specific directory in the implementation function.
 `additional_inputs`[dict](../core/dict.html);
 default is `{}`

 A dictionary of mapping of strings to additional inputs (only files, FilesToRunProvider(s) and Depset(s) are allowed here). The values specify any additional inputs that we want to make accessible to actions created by the implementation function. The keys (strings) act as identifiers to easily reference a specific input from within the implementation function.
 `output_directories`[dict](../core/dict.html) of [File](../builtins/File.html) s;
 required

 A dictionary mapping of strings to output directories, as declared by `ctx.actions.declare_directory()`. The values specify the output directories that we want to generate by the actions created by the implementation function. The keys (strings) act as identifiers to easily reference a specific output directory from within the implementation function.
 `tools`[dict](../core/dict.html);
 required

 A dictionary mapping of strings to tools (only files, FilesToRunProvider(s) and Depset(s) are allowed here). The values specify the tools that we want to make accessible to actions created by the implementation function. The keys (strings) act as identifiers to easily reference a specific tool from within the implementation function.
 `additional_params`[dict](../core/dict.html);
 default is `{}`

 A dictionary mapping of strings to additional parameters (only string, boolean and integer values are allowed here). The values specify any additional parameters that we want to make accessible to the implementation function that could be used to influence its behavior. The keys (strings) act as identifiers to easily reference a specific parameter from within the implementation function.
 `execution_requirements`[dict](../core/dict.html); or `None`;
 default is `None`

 Information for scheduling the created actions. See [tags](/reference/be/common-definitions#common.tags) for useful keys.
 `exec_group`[string](../core/string.html); or `None`;
 default is `None`

 Run the created actions on the given exec group's execution platform. If none, uses the target's default execution platform.
 `toolchain`[Label](../builtins/Label.html); or [string](../core/string.html); or `None`;
 default is `None`

Toolchain type of the executable or tools used by the created actions.

If executable and tools are not coming from a toolchain, set this parameter to `None`.

If executable and tools are coming from a toolchain, toolchain type must be set so that the created actions execute on the correct execution platform.

Note that the rule which creates these actions needs to define this toolchain inside its 'rule()' function.

When `toolchain` and `exec_group` parameters are both set, `exec_group` will be used. An error is raised in case the `exec_group` doesn't specify the same toolchain.

`use_default_shell_env`[bool](../core/bool.html);
 default is `False`

 Whether the created actions should use the default shell environment, which consists of a few OS-dependent variables as well as variables set via [`--action_env`](/reference/command-line-reference#flag--action_env).

If both `use_default_shell_env` and `env` are set to `True`, values set in `env` will overwrite the default shell environment.


`env`[dict](../core/dict.html); or `None`;
 default is `None`

 Sets the dictionary of environment variables.

If both `use_default_shell_env` and `env` are set to `True`, values set in `env` will overwrite the default shell environment.


`mnemonic`[string](../core/string.html); or `None`;
 default is `None`

 A one-word description of the created actions, for example, CppCompile or GoLink.
 `implementation`[function](../core/function.html);
 required

 A Starlark function that gets called after input directories have been built to generate actions
that output files to the specified output directories. This function is passed the following
arguments:

- `template_ctx` (positional): A [`template_ctx`](../builtins/template_ctx.html) object that can be used to
   create actions.
- `input_directories` (keyword-only): A dictionary mapping from the string keys of
   the `input_directories` argument of `actions.map_directory()` to their
   values' corresponding [`ExpandedDirectory`](../builtins/File.html)
   objects.
- `output_directories` (keyword-only): The value of the
   `output_directories` argument of `actions.map_directory()`; a
   dictionary mapping from strings to output directories.
- `additional_inputs` (keyword-only): The value of the
   `additional_inputs` argument of `actions.map_directory()`; a
   dictionary mapping from strings to input files.
- `tools` (keyword-only): The value of the `tools` argument of
   `actions.map_directory()`; a dictionary mapping from strings to tools.
- `additional_params` (keyword-only): The value of the
   `additional_params` argument of `actions.map_directory()`; a
   dictionary mapping from strings to strings, booleans, or integers.

This function must be top-level, i.e. lambdas and nested functions are not allowed.



## run

```
None actions.run(*, outputs, inputs=[], unused_inputs_list=None, executable, tools=unbound, arguments=[], mnemonic=None, progress_message=None, use_default_shell_env=False, env=None, execution_requirements=None, input_manifests=None, exec_group=None, shadowed_action=None, resource_set=None, toolchain=unbound)
```

 Creates an action that runs an executable. [See example of use](https://github.com/bazelbuild/examples/tree/main/rules/actions_run/execute.bzl).


### Parameters

ParameterDescription`outputs`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 required

 List of the output files of the action.
 `inputs`[sequence](../core/list.html) of [File](../builtins/File.html) s; or [depset](../builtins/depset.html);
 default is `[]`

 List or depset of the input files of the action.
 `unused_inputs_list`[File](../builtins/File.html); or `None`;
 default is `None`

 File containing list of inputs unused by the action.

The content of this file (generally one of the outputs of the action) corresponds to the list of input files that were not used during the whole action execution. Any change in those files must not affect in any way the outputs of the action.


`executable`[File](../builtins/File.html); or [string](../core/string.html); or [FilesToRunProvider](../providers/FilesToRunProvider.html);
 required

 The executable file to be called by the action.
 `tools`[sequence](../core/list.html); or [depset](../builtins/depset.html);
 default is `unbound`

 List or [`depset`](../builtins/depset.html) of any tools needed by the action. Tools are executable inputs that may have their own runfiles which are automatically made available to the action.

When a list is provided, it can be a heterogenous collection of:

- `File` s
- `FilesToRunProvider` instances
- `depset` s of `File` s

`File` s from [`ctx.executable`](../builtins/ctx#executable) and `FilesToRunProvider` s which are directly in the list will have their runfiles automatically added. All tools are implicitly added as inputs.

`arguments`[sequence](../core/list.html);
 default is `[]`

 Command line arguments of the action. Must be a list of strings or [`actions.args()`](#args) objects.
 `mnemonic`[string](../core/string.html); or `None`;
 default is `None`

 A one-word description of the action, for example, CppCompile or GoLink.
 `progress_message`[string](../core/string.html); or `None`;
 default is `None`

 Progress message to show to the user during the build, for example, "Compiling foo.cc to create foo.o". The message may contain `%{label}`, `%{input}`, or `%{output}` patterns, which are substituted with label string, first input, or output's path, respectively. Prefer to use patterns instead of static strings, because the former are more efficient.
 `use_default_shell_env`[bool](../core/bool.html);
 default is `False`

 Whether the action should use the default shell environment, which consists of a few OS-dependent variables as well as variables set via [`--action_env`](/reference/command-line-reference#flag--action_env).

If both `use_default_shell_env` and `env` are set to `True`, values set in `env` will overwrite the default shell environment.


`env`[dict](../core/dict.html); or `None`;
 default is `None`

 Sets the dictionary of environment variables.

If both `use_default_shell_env` and `env` are set to `True`, values set in `env` will overwrite the default shell environment.


`execution_requirements`[dict](../core/dict.html); or `None`;
 default is `None`

 Information for scheduling the action. See [tags](/reference/be/common-definitions#common.tags) for useful keys.
 `input_manifests`[sequence](../core/list.html); or `None`;
 default is `None`

 Legacy argument. Ignored.
 `exec_group`[string](../core/string.html); or `None`;
 default is `None`

 Runs the action on the given exec group's execution platform. If none, uses the target's default execution platform.
 `shadowed_action`[Action](../builtins/Action.html);
 default is `None`

 Runs the action using the given shadowed action's inputs and environment added to the action's inputs list and environment. The action environment can overwrite any of the shadowed action's environment variables. If none, uses only the action's inputs and given environment.
 `resource_set`
 callable; or `None`;
 default is `None`

 A callback function that returns a resource set dictionary, used to estimate resource usage at execution time if this action is run locally.

The function accepts two positional arguments: a string representing an OS name (e.g. "osx"), and an integer representing the number of inputs to the action. The returned dictionary may contain the following entries, each of which may be a float or an int:

- "cpu": number of CPUs; default 1
- "memory": in MB; default 250
- "local\_test": number of local tests; default 1

If this parameter is set to `None` , the default values are used.

The callback must be top-level (lambda and nested functions aren't allowed).


`toolchain`[Label](../builtins/Label.html); or [string](../core/string.html); or `None`;
 default is `unbound`

Toolchain type of the executable or tools used in this action.

If executable and tools are not coming from a toolchain, set this parameter to \`None\`.

If executable and tools are coming from a toolchain, toolchain type must be set so that the action executes on the correct execution platform.

Note that the rule which creates this action needs to define this toolchain inside its 'rule()' function.

When \`toolchain\` and \`exec\_group\` parameters are both set, \`exec\_group\` will be used. An error is raised in case the \`exec\_group\` doesn't specify the same toolchain.

## run\_shell

```
None actions.run_shell(*, outputs, inputs=[], tools=unbound, arguments=[], mnemonic=None, command, progress_message=None, use_default_shell_env=False, env=None, execution_requirements=None, input_manifests=None, exec_group=None, shadowed_action=None, resource_set=None, toolchain=unbound)
```

 Creates an action that runs a shell command. [See example of use](https://github.com/bazelbuild/examples/tree/main/rules/shell_command/rules.bzl).


### Parameters

ParameterDescription`outputs`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 required

 List of the output files of the action.
 `inputs`[sequence](../core/list.html) of [File](../builtins/File.html) s; or [depset](../builtins/depset.html);
 default is `[]`

 List or depset of the input files of the action.
 `tools`[sequence](../core/list.html) of [File](../builtins/File.html) s; or [depset](../builtins/depset.html);
 default is `unbound`

 List or [`depset`](../builtins/depset.html) of any tools needed by the action. Tools are executable inputs that may have their own runfiles which are automatically made available to the action.

When a list is provided, it can be a heterogenous collection of:

- `File` s
- `FilesToRunProvider` instances
- `depset` s of `File` s

`File` s from [`ctx.executable`](../builtins/ctx#executable) and `FilesToRunProvider` s which are directly in the list will have their runfiles automatically added. All tools are implicitly added as inputs.

`arguments`[sequence](../core/list.html);
 default is `[]`

 Command line arguments of the action. Must be a list of strings or [`actions.args()`](#args) objects.

Bazel passes the elements in this attribute as arguments to the command.The command can access these arguments using shell variable substitutions such as `$1`, `$2`, etc. Note that since Args objects are flattened before indexing, if there is an Args object of unknown size then all subsequent strings will be at unpredictable indices. It may be useful to use `$@` (to retrieve all arguments) in conjunction with Args objects of indeterminate size.

In the case where `command` is a list of strings, this parameter may not be used.


`mnemonic`[string](../core/string.html); or `None`;
 default is `None`

 A one-word description of the action, for example, CppCompile or GoLink.
 `command`[string](../core/string.html); or [sequence](../core/list.html) of [string](../core/string.html) s;
 required

 Shell command to execute. This may either be a string (preferred) or a sequence of strings **(deprecated)**.

If `command` is a string, then it is executed as if by `sh -c <command> "" <arguments>` \-\- that is, the elements in `arguments` are made available to the command as `$1`, `$2` (or `%1`, `%2` if using Windows batch), etc. If `arguments` contains any [`actions.args()`](#args) objects, their contents are appended one by one to the command line, so `$` _i_ can refer to individual strings within an Args object. Note that if an Args object of unknown size is passed as part of `arguments`, then the strings will be at unknown indices; in this case the `$@` shell substitution (retrieve all arguments) may be useful.

**(Deprecated)** If `command` is a sequence of strings, the first item is the executable to run and the remaining items are its arguments. If this form is used, the `arguments` parameter must not be supplied. _Note that this form is deprecated and will soon be removed. It is disabled with \`--incompatible\_run\_shell\_command\_string\`. Use this flag to verify your code is compatible._

Bazel uses the same shell to execute the command as it does for genrules.


`progress_message`[string](../core/string.html); or `None`;
 default is `None`

 Progress message to show to the user during the build, for example, "Compiling foo.cc to create foo.o". The message may contain `%{label}`, `%{input}`, or `%{output}` patterns, which are substituted with label string, first input, or output's path, respectively. Prefer to use patterns instead of static strings, because the former are more efficient.
 `use_default_shell_env`[bool](../core/bool.html);
 default is `False`

 Whether the action should use the default shell environment, which consists of a few OS-dependent variables as well as variables set via [`--action_env`](/reference/command-line-reference#flag--action_env).

If both `use_default_shell_env` and `env` are set to `True`, values set in `env` will overwrite the default shell environment.


`env`[dict](../core/dict.html); or `None`;
 default is `None`

 Sets the dictionary of environment variables.

If both `use_default_shell_env` and `env` are set to `True`, values set in `env` will overwrite the default shell environment.


`execution_requirements`[dict](../core/dict.html); or `None`;
 default is `None`

 Information for scheduling the action. See [tags](/reference/be/common-definitions#common.tags) for useful keys.
 `input_manifests`[sequence](../core/list.html); or `None`;
 default is `None`

 Legacy argument. Ignored.
 `exec_group`[string](../core/string.html); or `None`;
 default is `None`

 Runs the action on the given exec group's execution platform. If none, uses the target's default execution platform.
 `shadowed_action`[Action](../builtins/Action.html);
 default is `None`

 Runs the action using the given shadowed action's discovered inputs added to the action's inputs list. If none, uses only the action's inputs.
 `resource_set`
 callable; or `None`;
 default is `None`

 A callback function for estimating resource usage if run locally. See [`ctx.actions.run()`](#run.resource_set).
 `toolchain`[Label](../builtins/Label.html); or [string](../core/string.html); or `None`;
 default is `unbound`

Toolchain type of the executable or tools used in this action.

If executable and tools are not coming from a toolchain, set this parameter to \`None\`.

If executable and tools are coming from a toolchain, toolchain type must be set so that the action executes on the correct execution platform.

Note that the rule which creates this action needs to define this toolchain inside its 'rule()' function.

When \`toolchain\` and \`exec\_group\` parameters are both set, \`exec\_group\` will be used. An error is raised in case the \`exec\_group\` doesn't specify the same toolchain.

## symlink

```
None actions.symlink(*, output, target_file=None, target_path=None, target_type=None, is_executable=False, progress_message=None)
```

 Creates an action that writes a symlink in the file system.

This function must be called with exactly one of `target_file` or `target_path` specified.

When you use `target_file`, declare `output` with [`declare_file()`](#declare_file) or [`declare_directory()`](#declare_directory) and match the type of `target_file`. This makes the symlink point to `target_file`. Bazel invalidates the output of this action whenever the target of the symlink or its contents change.

Otherwise, when you use `target_path`, declare `output` with [`declare_symlink()`](#declare_symlink)). In this case, the symlink points to `target_path`. Bazel never resolves the symlink and the output of this action is invalidated only when the text contents of the symlink (that is, the value of `readlink()`) changes. In particular, this can be used to create a dangling symlink.

### Parameters

ParameterDescription`output`[File](../builtins/File.html);
 required

 The output of this action.
 `target_file`[File](../builtins/File.html); or `None`;
 default is `None`

 The File that the output symlink will point to.
 `target_path`[string](../core/string.html); or `None`;
 default is `None`

 The exact path that the output symlink will point to. No normalization or other processing is applied.
 `target_type`[string](../core/string.html); or `None`;
 default is `None`

 May only be used with `target_path`, not `target_file`. If specified, it must be one of 'file' or 'directory', indicating the target path's expected type.

On Windows, this determines which kind of filesystem object to create (junction for a directory, symlink for a file). It has no effect on other operating systems.


`is_executable`[bool](../core/bool.html);
 default is `False`

 May only be used with `target_file`, not `target_path`. If true, when the action is executed, the `target_file`'s path is checked to confirm that it is executable, and an error is reported if it is not. Setting `is_executable` to False does not mean the target is not executable, just that no verification is done.

This feature does not make sense for `target_path` because dangling symlinks might not exist at build time.

`progress_message`[string](../core/string.html); or `None`;
 default is `None`

 Progress message to show to the user during the build.


## template\_dict

```
TemplateDict actions.template_dict()
```

 Returns a TemplateDict object for memory-efficient template expansion.



## write

```
None actions.write(output, content, is_executable=False, *, mnemonic=None)
```

 Creates a file write action. When the action is executed, it will write the given content to a file. This is used to generate files using information available in the analysis phase. If the file is large and with a lot of static content, consider using [`expand_template`](#expand_template).


### Parameters

ParameterDescription`output`[File](../builtins/File.html);
 required

 The output file.
 `content`[string](../core/string.html); or [Args](../builtins/Args.html);
 required

 the contents of the file. May be a either a string or an [`actions.args()`](#args) object.
 `is_executable`[bool](../core/bool.html);
 default is `False`

 Whether the output file should be executable.
 `mnemonic`[string](../core/string.html); or `None`;
 default is `None`

 A one-word description of the action, for example, CppCompile or GoLink.

---

## apple\_platform
- URL: https://bazel.build/rules/lib/builtins/apple_platform
- Source: rules/lib/builtins/apple_platform.mdx
- Slug: /rules/lib/builtins/apple_platform

Corresponds to Xcode's notion of a platform as would be found in `Xcode.app/Contents/Developer/Platforms`. Each platform represents an Apple platform type (such as iOS or tvOS) combined with one or more related CPU architectures. For example, the iOS simulator platform supports `x86_64` and `i386` architectures.

Specific instances of this type can be retrieved from the fields of the [apple\_common.platform](../toplevel/apple_common.html#platform) struct:

- `apple_common.platform.ios_device`
- `apple_common.platform.ios_simulator`
- `apple_common.platform.macos`
- `apple_common.platform.tvos_device`
- `apple_common.platform.tvos_simulator`
- `apple_common.platform.watchos_device`
- `apple_common.platform.watchos_simulator`

More commonly, however, the [apple](../fragments/apple.html) configuration fragment has fields/methods that allow rules to determine the platform for which a target is being built.

Example:

```
p = apple_common.platform.ios_device
print(p.name_in_plist)  # 'iPhoneOS'

```

## Members

- [is\_device](#is_device)
- [name](#name)
- [name\_in\_plist](#name_in_plist)
- [platform\_type](#platform_type)

## is\_device

```
bool apple_platform.is_device
```

 Returns `True` if this platform is a device platform or `False` if it is a simulator platform.



## name

```
string apple_platform.name
```

 Returns the name aka starlarkKey of this platform.



## name\_in\_plist

```
string apple_platform.name_in_plist
```

 The name of the platform as it appears in the `CFBundleSupportedPlatforms` entry of an Info.plist file and in Xcode's platforms directory, without the extension (for example, `iPhoneOS` or `iPhoneSimulator`).

This name, when converted to lowercase (e.g., `iphoneos`, `iphonesimulator`), can be passed to Xcode's command-line tools like `ibtool` and `actool` when they expect a platform name.



## platform\_type

```
string apple_platform.platform_type
```

 Returns the platform type of this platform.

---

## bazel\_module
- URL: https://bazel.build/rules/lib/builtins/bazel_module
- Source: rules/lib/builtins/bazel_module.mdx
- Slug: /rules/lib/builtins/bazel_module

Represents a Bazel module in the external dependency graph.

## Members

- [is\_root](#is_root)
- [name](#name)
- [tags](#tags)
- [version](#version)

## is\_root

```
bool bazel_module.is_root
```

 Whether this module is the root module.



## name

```
string bazel_module.name
```

 The name of the module.



## tags

```
bazel_module_tags bazel_module.tags
```

 The tags in the module related to the module extension currently being processed.



## version

```
string bazel_module.version
```

 The version of the module.

---

## bazel\_module\_tags
- URL: https://bazel.build/rules/lib/builtins/bazel_module_tags
- Source: rules/lib/builtins/bazel_module_tags.mdx
- Slug: /rules/lib/builtins/bazel_module_tags

Contains the tags in a module for the module extension currently being processed. This object has a field for each tag class of the extension, and the value of the field is a list containing an object for each tag instance. This "tag instance" object in turn has a field for each attribute of the tag class.

When passed as positional arguments to `print()` or `fail()`, tag instance objects turn into a meaningful string representation of the form "'install' tag at /home/user/workspace/MODULE.bazel:3:4". This can be used to construct error messages that point to the location of the tag in the module file, e.g. `fail("Conflict between", tag1, "and", tag2)`.

---

## configuration
- URL: https://bazel.build/rules/lib/builtins/configuration
- Source: rules/lib/builtins/configuration.mdx
- Slug: /rules/lib/builtins/configuration

This object holds information about the environment in which the build is running. See the [Rules page](https://bazel.build/extending/rules#configurations) for more on the general concept of configurations.

## Members

- [coverage\_enabled](#coverage_enabled)
- [default\_shell\_env](#default_shell_env)
- [host\_path\_separator](#host_path_separator)
- [short\_id](#short_id)
- [test\_env](#test_env)

## coverage\_enabled

```
bool configuration.coverage_enabled
```

 A boolean that tells whether code coverage is enabled for this run. Note that this does not compute whether a specific rule should be instrumented for code coverage data collection. For that, see the [`ctx.coverage_instrumented`](../builtins/ctx.html#coverage_instrumented) function.



## default\_shell\_env

```
dict configuration.default_shell_env
```

 A dictionary representing the static local shell environment. It maps variables to their values (strings).



## host\_path\_separator

```
string configuration.host_path_separator
```

 Returns the separator for PATH environment variable, which is ':' on Unix.



## short\_id

```
string configuration.short_id
```

 A short identifier for this configuration understood by the `config` and query subcommands.

Use this to distinguish different configurations for the same target in a way that is friendly to humans and tool usage, for example in an aspect used by an IDE. Keep in mind the following caveats:

- The value may differ across Bazel versions, including patch releases.
- The value encodes the value of **every** flag, including those that aren't otherwise relevant for the current target and may thus invalidate caches more frequently.




  ## test\_env





  ```
  dict configuration.test_env
  ```





   A dictionary containing user-specified test environment variables and their values, as set by the --test\_env options. DO NOT USE! This is not the complete environment!

---

## ctx
- URL: https://bazel.build/rules/lib/builtins/ctx
- Source: rules/lib/builtins/ctx.mdx
- Slug: /rules/lib/builtins/ctx

A context object that is passed to the implementation function for a rule or aspect. It provides access to the information and methods needed to analyze the current target.

In particular, it lets the implementation function access the current target's label, attributes, configuration, and the providers of its dependencies. It has methods for declaring output files and the actions that produce them.

Context objects essentially live for the duration of the call to the implementation function. It is not useful to access these objects outside of their associated function. See the [Rules page](https://bazel.build/extending/rules#implementation_function) for more information.

## Members

- [actions](#actions)
- [aspect\_ids](#aspect_ids)
- [attr](#attr)
- [bin\_dir](#bin_dir)
- [build\_file\_path](#build_file_path)
- [build\_setting\_value](#build_setting_value)
- [configuration](#configuration)
- [coverage\_instrumented](#coverage_instrumented)
- [created\_actions](#created_actions)
- [disabled\_features](#disabled_features)
- [exec\_groups](#exec_groups)
- [executable](#executable)
- [expand\_location](#expand_location)
- [expand\_make\_variables](#expand_make_variables)
- [features](#features)
- [file](#file)
- [files](#files)
- [fragments](#fragments)
- [genfiles\_dir](#genfiles_dir)
- [info\_file](#info_file)
- [label](#label)
- [outputs](#outputs)
- [resolve\_command](#resolve_command)
- [resolve\_tools](#resolve_tools)
- [rule](#rule)
- [runfiles](#runfiles)
- [split\_attr](#split_attr)
- [super](#super)
- [target\_platform\_has\_constraint](#target_platform_has_constraint)
- [toolchains](#toolchains)
- [var](#var)
- [version\_file](#version_file)
- [workspace\_name](#workspace_name)

## actions

```
actions ctx.actions
```

 Contains methods for declaring output files and the actions that produce them.



## aspect\_ids

```
list ctx.aspect_ids
```

 A list of ids for all aspects applied to the target. Only available in aspect implementation functions.



## attr

```
struct ctx.attr
```

 A struct to access the values of the [attributes](https://bazel.build/extending/rules#attributes). The values are provided by the user (if not, a default value is used). The attributes of the struct and the types of their values correspond to the keys and values of the [`attrs` dict](../globals/bzl.html#rule.attrs) provided to the [`rule` function](../globals/bzl.html#rule). [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/attributes/printer.bzl).



## bin\_dir

```
root ctx.bin_dir
```

 The root corresponding to bin directory.



## build\_file\_path

```
string ctx.build_file_path
```

 Deprecated: Use `ctx.label.package + '/BUILD'`. The path to the BUILD file for this rule, relative to the source root.



## build\_setting\_value

```
unknown ctx.build_setting_value
```

 Value of the build setting represented by the current target. If this isn't the context for an instance of a rule that sets the [`build_setting`](https://bazel.build/extending/config#rule-parameter) attribute, reading this is an error.



## configuration

```
configuration ctx.configuration
```

 The default configuration. See the [configuration](../builtins/configuration.html) type for more details.



## coverage\_instrumented

```
bool ctx.coverage_instrumented(target=None)
```

 Returns whether code coverage instrumentation should be generated when performing compilation actions for this rule or, if `target` is provided, the rule specified by that Target. (If a non-rule or a Starlark rule Target is provided, this returns False.) Checks if the sources of the current rule (if no Target is provided) or the sources of Target should be instrumented based on the --instrumentation\_filter and --instrument\_test\_targets config settings. This differs from `coverage_enabled` in the [configuration](../builtins/configuration.html), which notes whether coverage data collection is enabled for the entire run, but not whether a specific target should be instrumented.


### Parameters

ParameterDescription`target`[Target](../builtins/Target.html); or `None`;
 default is `None`

 A Target specifying a rule. If not provided, defaults to the current rule.


## created\_actions

```
StarlarkValue ctx.created_actions()
```

 For rules with [\_skylark\_testable](../globals/bzl.html#rule._skylark_testable) set to `True`, this returns an `Actions` provider representing all actions created so far for the current rule. For all other rules, returns `None`. Note that the provider is not updated when subsequent actions are created, so you will have to call this function again if you wish to inspect them.

This is intended to help write tests for rule-implementation helper functions, which may take in a `ctx` object and create actions on it.



## disabled\_features

```
list ctx.disabled_features
```

 The set of features that are explicitly disabled by the user for this rule.



## exec\_groups

```
ExecGroupCollection ctx.exec_groups
```

 A collection of the execution groups available for this rule, indexed by their name. Access with `ctx.exec_groups[name_of_group]`.



## executable

```
struct ctx.executable
```

 A `struct` containing executable files defined in [label type attributes](../toplevel/attr.html#label) marked as [`executable=True`](../toplevel/attr.html#label.executable). The struct fields correspond to the attribute names. Each value in the struct is either a [`File`](../builtins/File.html) or `None`. If an optional attribute is not specified in the rule then the corresponding struct value is `None`. If a label type is not marked as `executable=True`, no corresponding struct field is generated. [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/actions_run/execute.bzl).



## expand\_location

```
string ctx.expand_location(input, targets=[])
```

 Expands all `$(location ...)` templates in the given string by replacing `$(location //x)` with the path of the output file of target //x. Expansion only works for labels that point to direct dependencies of this rule or that are explicitly listed in the optional argument `targets`.

`$(location ...)` will cause an error if the referenced target has multiple outputs. In this case, please use `$(locations ...)` since it produces a space-separated list of output paths. It can be safely used for a single output file, too.

This function is useful to let the user specify a command in a BUILD file (like for `genrule`). In other cases, it is often better to manipulate labels directly.


### Parameters

ParameterDescription`input`[string](../core/string.html);
 required

 String to be expanded.
 `targets`[sequence](../core/list.html) of [Target](../builtins/Target.html) s;
 default is `[]`

 List of targets for additional lookup information. These are expanded as follows: A target with a single file in `DefaultInfo.files` expands to that file. Other targets expand to their `DefaultInfo.executable` file if set and if `--incompatible_locations_prefers_executable` is enabled, otherwise they expand to `DefaultInfo.files`.

 May return `None`.



## expand\_make\_variables

```
string ctx.expand_make_variables(attribute_name, command, additional_substitutions)
```

 **Deprecated.** Use [ctx.var](../builtins/ctx.html#var) to access the variables instead.

Returns a string after expanding all references to "Make variables". The variables must have the following format: `$(VAR_NAME)`. Also, `$$VAR_NAME` expands to `$VAR_NAME`. Examples:

```
ctx.expand_make_variables("cmd", "$(MY_VAR)", {"MY_VAR": "Hi"})  # == "Hi"
ctx.expand_make_variables("cmd", "$$PWD", {})  # == "$PWD"

```

Additional variables may come from other places, such as configurations. Note that this function is experimental.


### Parameters

ParameterDescription`attribute_name`[string](../core/string.html);
 required

 The attribute name. Used for error reporting.
 `command`[string](../core/string.html);
 required

 The expression to expand. It can contain references to "Make variables".
 `additional_substitutions`[dict](../core/dict.html);
 required

 Additional substitutions to make beyond the default make variables.


## features

```
list ctx.features
```

 The set of features that are explicitly enabled by the user for this rule. [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/features/rule.bzl).



## file

```
struct ctx.file
```

 A `struct` containing files defined in [label type attributes](../toplevel/attr.html#label) marked as [`allow_single_file`](../toplevel/attr.html#label.allow_single_file). The struct fields correspond to the attribute names. The struct value is always a [`File`](../builtins/File.html) or `None`. If an optional attribute is not specified in the rule then the corresponding struct value is `None`. If a label type is not marked as `allow_single_file`, no corresponding struct field is generated. It is a shortcut for:

```
list(ctx.attr.<ATTR>.files)[0]
```

In other words, use `file` to access the (singular) [default output](https://bazel.build/extending/rules#requesting_output_files) of a dependency. [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/expand_template/hello.bzl).



## files

```
struct ctx.files
```

 A `struct` containing files defined in [label](../toplevel/attr.html#label) or [label list](../toplevel/attr.html#label_list) type attributes. The struct fields correspond to the attribute names. The struct values are `list` of [`File`](../builtins/File.html) s. It is a shortcut for:

```
[f for t in ctx.attr.<ATTR> for f in t.files]
```

 In other words, use `files` to access the [default outputs](https://bazel.build/extending/rules#requesting_output_files) of a dependency. [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/depsets/foo.bzl).



## fragments

```
fragments ctx.fragments
```

 Allows access to configuration fragments in target configuration.



## genfiles\_dir

```
root ctx.genfiles_dir
```

 The root corresponding to genfiles directory.



## info\_file

```
File ctx.info_file
```

 The file that is used to hold the non-volatile workspace status for the current build request. See documentation for --workspace\_status\_command for more information.



## label

```
Label ctx.label
```

 The label of the target currently being analyzed.



## outputs

```
structure ctx.outputs
```

 A pseudo-struct containing all the predeclared output files, represented by [`File`](../builtins/File.html) objects. See the [Rules page](https://bazel.build/extending/rules#files) for more information and examples.

This field does not exist on aspect contexts, since aspects do not have predeclared outputs.

The fields of this object are defined as follows. It is an error if two outputs produce the same field name or have the same label.

- If the rule declares an [`outputs`](../globals/bzl.html#rule.outputs) dict, then for every entry in the dict, there is a field whose name is the key and whose value is the corresponding `File`.
- For every attribute of type [`attr.output`](../toplevel/attr.html#output) that the rule declares, there is a field whose name is the attribute's name. If the target specified a label for that attribute, then the field value is the corresponding `File`; otherwise the field value is `None`.
- For every attribute of type [`attr.output_list`](../toplevel/attr.html#output_list) that the rule declares, there is a field whose name is the attribute's name. The field value is a list of `File` objects corresponding to the labels given for that attribute in the target, or an empty list if the attribute was not specified in the target.
- **(Deprecated)** If the rule is marked [`executable`](../globals/bzl.html#rule.executable) or [`test`](../globals/bzl.html#rule.test), there is a field named `"executable"`, which is the default executable. It is recommended that instead of using this, you pass another file (either predeclared or not) to the `executable` arg of [`DefaultInfo`](../providers/DefaultInfo.html).

## resolve\_command

```
tuple ctx.resolve_command(*, command='', attribute=None, expand_locations=False, make_variables=None, tools=[], label_dict={}, execution_requirements={})
```

 _(Experimental)_ Returns a tuple `(inputs, command, empty list)` of the list of resolved inputs and the argv list for the resolved command both of them suitable for passing as the same-named arguments of the `ctx.action` method.

**Note for Windows users**: this method requires Bash (MSYS2). Consider using `resolve_tools()` instead (if that fits your needs). The empty list is returned as the third member of the tuple for backwards compatibility.


### Parameters

ParameterDescription`command`[string](../core/string.html);
 default is `''`

 Command to resolve.
 `attribute`[string](../core/string.html); or `None`;
 default is `None`

 Name of the associated attribute for which to issue an error, or None.
 `expand_locations`[bool](../core/bool.html);
 default is `False`

 Shall we expand $(location) variables? See [ctx.expand\_location()](#expand_location) for more details.
 `make_variables`[dict](../core/dict.html); or `None`;
 default is `None`

 Make variables to expand, or None.
 `tools`[sequence](../core/list.html) of [Target](../builtins/Target.html) s;
 default is `[]`

 List of tools (list of targets).
 `label_dict`[dict](../core/dict.html);
 default is `{}`

 Dictionary of resolved labels and the corresponding list of Files (a dict of Label : list of Files).
 `execution_requirements`[dict](../core/dict.html);
 default is `{}`

 Information for scheduling the action to resolve this command. See [tags](/reference/be/common-definitions#common.tags) for useful keys.


## resolve\_tools

```
tuple ctx.resolve_tools(*, tools=[])
```

 Returns a tuple `(inputs, empty list)` of the depset of resolved inputs required to run the tools, suitable for passing as the same-named argument of the `ctx.actions.run` and `ctx.actions.run_shell` methods.

In contrast to `ctx.resolve_command`, this method does not require that Bash be installed on the machine, so it's suitable for rules built on Windows. The empty list is returned as part of the tuple for backward compatibility.


### Parameters

ParameterDescription`tools`[sequence](../core/list.html) of [Target](../builtins/Target.html) s;
 default is `[]`

 List of tools (list of targets).


## rule

```
rule_attributes ctx.rule
```

 Rule attributes descriptor for the rule that the aspect is applied to. Only available in aspect implementation functions.



## runfiles

```
runfiles ctx.runfiles(files=[], transitive_files=None, collect_data=False, collect_default=False, symlinks={}, root_symlinks={})
```

 Creates a runfiles object.


### Parameters

ParameterDescription`files`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 The list of files to be added to the runfiles.
 `transitive_files`[depset](../builtins/depset.html) of [File](../builtins/File.html) s; or `None`;
 default is `None`

 The (transitive) set of files to be added to the runfiles. The depset should use the `default` order (which, as the name implies, is the default).
 `collect_data`[bool](../core/bool.html);
 default is `False`

**Use of this parameter is not recommended. See [runfiles guide](https://bazel.build/extending/rules#runfiles)**.

Whether to collect the data runfiles from the dependencies in srcs, data and deps attributes.


`collect_default`[bool](../core/bool.html);
 default is `False`

**Use of this parameter is not recommended. See [runfiles guide](https://bazel.build/extending/rules#runfiles)**.

Whether to collect the default runfiles from the dependencies in srcs, data and deps attributes.


`symlinks`[dict](../core/dict.html); or [depset](../builtins/depset.html) of [SymlinkEntry](../builtins/SymlinkEntry.html) s;
 default is `{}`

 Either a SymlinkEntry depset or the map of symlinks to be added to the runfiles. Symlinks are always added under the main workspace's runfiles directory (e.g. `<runfiles_root>/_main/<symlink_path>`, **not** the directory corresponding to the current target's repository. See [Runfiles symlinks](https://bazel.build/extending/rules#runfiles_symlinks) in the rules guide.
 `root_symlinks`[dict](../core/dict.html); or [depset](../builtins/depset.html) of [SymlinkEntry](../builtins/SymlinkEntry.html) s;
 default is `{}`

 Either a SymlinkEntry depset or a map of symlinks to be added to the runfiles. See [Runfiles symlinks](https://bazel.build/extending/rules#runfiles_symlinks) in the rules guide.


## split\_attr

```
struct ctx.split_attr
```

 A struct to access the values of attributes with split configurations. If the attribute is a label list, the value of split\_attr is a dict of the keys of the split (as strings) to lists of the ConfiguredTargets in that branch of the split. If the attribute is a label, then the value of split\_attr is a dict of the keys of the split (as strings) to single ConfiguredTargets. Attributes with split configurations still appear in the attr struct, but their values will be single lists with all the branches of the split merged together.



## super

```
unknown ctx.super()
```

 Experimental: Calls parent's implementation function and returns its providers



## target\_platform\_has\_constraint

```
bool ctx.target_platform_has_constraint(constraintValue)
```

 Returns true if the given constraint value is part of the current target platform.


### Parameters

ParameterDescription`constraintValue`[ConstraintValueInfo](../providers/ConstraintValueInfo.html);
 required

 The constraint value to check the target platform against.


## toolchains

```
ToolchainContext ctx.toolchains
```

 Toolchains for the default exec group of this rule.



## var

```
dict ctx.var
```

 Dictionary (String to String) of configuration variables.



## version\_file

```
File ctx.version_file
```

 The file that is used to hold the volatile workspace status for the current build request. See documentation for --workspace\_status\_command for more information.



## workspace\_name

```
string ctx.workspace_name
```

 The name of the workspace, which is effectively the execution root name and runfiles prefix for the main repo. If `--enable_bzlmod` is on, this is the fixed string `_main`. Otherwise, this is the workspace name as defined in the WORKSPACE file.

---

## depset
- URL: https://bazel.build/rules/lib/builtins/depset
- Source: rules/lib/builtins/depset.mdx
- Slug: /rules/lib/builtins/depset

A specialized data structure that supports efficient merge operations and has a defined traversal
order. Commonly used for accumulating data from transitive dependencies in rules and aspects. For
more information see [here](/extending/depsets).

The elements of a depset must be hashable and all of the same type (as defined by the built-in
[`type(x)`](../globals/all#type) function), but depsets are not simply hash
sets and do not support fast membership tests. If you need a general set datatype, use the core
[Starlark set](../core/set) type (available since Bazel 8.1); if your .bzl file needs to
be compatible with older Bazel releases, you can simulate a set by using a dictionary where all keys
map to `True`.

When tested for truth (that is, when used in a Boolean context such as `if d:` where
`d` is a depset), a depset is True if and only if it is non-empty; this check is an O(1)
operation.

Depsets are immutable. They should be created using their
[constructor function](../globals/bzl.html#depset) and merged or augmented with other
depsets via the `transitive` argument.

The `order` parameter determines the kind of traversal that is done to convert the
depset to an iterable. There are four possible values:

- `"default"` (formerly `"stable"`): Order is unspecified (but
   deterministic).

- `"postorder"` (formerly `"compile"`): A left-to-right post-ordering.
   Precisely, this recursively traverses all children leftmost-first, then the direct elements
   leftmost-first.

- `"preorder"` (formerly `"naive_link"`): A left-to-right pre-ordering.
   Precisely, this traverses the direct elements leftmost-first, then recursively traverses the
   children leftmost-first.

- `"topological"` (formerly `"link"`): A topological ordering from the root
   down to the leaves. There is no left-to-right guarantee.


Two depsets may only be merged if either both depsets have the same order, or one of them has
`"default"` order. In the latter case the resulting depset's order will be the same as
the other's order.

Depsets may contain duplicate values but these will be suppressed when iterating (using
[`to_list()`](#to_list)). Duplicates may interfere with the ordering
semantics.

## Members

- [to\_list](#to_list)

## to\_list

```
list depset.to_list()
```

 Returns a list of the elements, without duplicates, in the depset's traversal order. Note that order is unspecified (but deterministic) for elements that were added more than once to the depset. Order is also unspecified for `"default"`-ordered depsets, and for elements of child depsets whose order differs from that of the parent depset. The list is a copy; modifying it has no effect on the depset and vice versa.

---

## exec\_result
- URL: https://bazel.build/rules/lib/builtins/exec_result
- Source: rules/lib/builtins/exec_result.mdx
- Slug: /rules/lib/builtins/exec_result

A structure storing result of repository\_ctx.execute() method. It contains the standard output stream content, the standard error stream content and the execution return code.

## Members

- [return\_code](#return_code)
- [stderr](#stderr)
- [stdout](#stdout)

## return\_code

```
int exec_result.return_code
```

 The return code returned after the execution of the program. 256 if the process was terminated by a time out; values larger than 128 indicate termination by a signal.



## stderr

```
string exec_result.stderr
```

 The content of the standard error output returned by the execution.



## stdout

```
string exec_result.stdout
```

 The content of the standard output returned by the execution.

---

## extension\_metadata
- URL: https://bazel.build/rules/lib/builtins/extension_metadata
- Source: rules/lib/builtins/extension_metadata.mdx
- Slug: /rules/lib/builtins/extension_metadata

Return values of this type from a module extension's implementation function to provide metadata about the repositories generated by the extension to Bazel.

---

## fragments
- URL: https://bazel.build/rules/lib/builtins/fragments
- Source: rules/lib/builtins/fragments.mdx
- Slug: /rules/lib/builtins/fragments

A collection of configuration fragments available in the current rule implementation context. Access a specific fragment by its field name. For example, `ctx.fragments.java`

Only configuration fragments which are declared in the rule definition may be accessed in this collection.

See the [configuration fragment reference](../fragments.html) for a list of available fragments and the [rules documentation](https://bazel.build/extending/rules#configuration_fragments) for how to use them.

---

## java\_annotation\_processing
- URL: https://bazel.build/rules/lib/builtins/java_annotation_processing
- Source: rules/lib/builtins/java_annotation_processing.mdx
- Slug: /rules/lib/builtins/java_annotation_processing

Information about jars that are a result of annotation processing for a Java rule.

## Members

- [class\_jar](#class_jar)
- [enabled](#enabled)
- [processor\_classnames](#processor_classnames)
- [processor\_classpath](#processor_classpath)
- [source\_jar](#source_jar)
- [transitive\_class\_jars](#transitive_class_jars)
- [transitive\_source\_jars](#transitive_source_jars)

## class\_jar

```
File java_annotation_processing.class_jar
```

 Deprecated: Please use `JavaInfo.java_outputs.generated_class_jar` instead.
 May return `None`.



## enabled

```
bool java_annotation_processing.enabled
```

 Deprecated. Returns true if annotation processing was applied on this target.



## processor\_classnames

```
list java_annotation_processing.processor_classnames
```

 Deprecated: Please use `JavaInfo.plugins` instead. Returns class names of annotation processors applied to this rule.



## processor\_classpath

```
depset java_annotation_processing.processor_classpath
```

 Deprecated: Please use `JavaInfo.plugins` instead. Returns a classpath of annotation processors applied to this rule.



## source\_jar

```
File java_annotation_processing.source_jar
```

 Deprecated: Please use `JavaInfo.java_outputs.generated_source_jar` instead.
 May return `None`.



## transitive\_class\_jars

```
depset java_annotation_processing.transitive_class_jars
```

 Deprecated. Returns a transitive set of class file jars resulting from annotation processing of this rule and its dependencies.



## transitive\_source\_jars

```
depset java_annotation_processing.transitive_source_jars
```

 Deprecated. Returns a transitive set of source archives resulting from annotation processing of this rule and its dependencies.

---

## macro
- URL: https://bazel.build/rules/lib/builtins/macro
- Source: rules/lib/builtins/macro.mdx
- Slug: /rules/lib/builtins/macro

A callable Starlark value representing a symbolic macro; in other words, the return value of
[`macro()`](../globals/bzl.html#macro). Invoking this value during package
construction time will instantiate the macro, and cause the macro's implementation function to be
evaluated (in a separate context, different from the context in which the macro value was invoked),
in most cases causing targets to be added to the package's target set. For more information, see
[Macros](https://bazel.build/extending/macros).

---

## mapped\_root
- URL: https://bazel.build/rules/lib/builtins/mapped_root
- Source: rules/lib/builtins/mapped_root.mdx
- Slug: /rules/lib/builtins/mapped_root

A root for files that have been subject to path mapping

## Members

- [path](#path)

## path

```
string mapped_root.path
```

 Returns the relative path from the exec root to the actual root.

---

## module\_ctx
- URL: https://bazel.build/rules/lib/builtins/module_ctx
- Source: rules/lib/builtins/module_ctx.mdx
- Slug: /rules/lib/builtins/module_ctx

The context of the module extension containing helper functions and information about pertinent tags across the dependency graph. You get a module\_ctx object as an argument to the `implementation` function when you create a module extension.

## Members

- [download](#download)
- [download\_and\_extract](#download_and_extract)
- [execute](#execute)
- [extension\_metadata](#extension_metadata)
- [extract](#extract)
- [facts](#facts)
- [file](#file)
- [getenv](#getenv)
- [is\_dev\_dependency](#is_dev_dependency)
- [modules](#modules)
- [os](#os)
- [path](#path)
- [read](#read)
- [report\_progress](#report_progress)
- [root\_module\_has\_non\_dev\_dependency](#root_module_has_non_dev_dependency)
- [watch](#watch)
- [which](#which)

## download

```
unknown module_ctx.download(url, output='', sha256='', executable=False, allow_fail=False, canonical_id='', auth={}, headers={}, *, integrity='', block=True)
```

 Downloads a file to the output path for the provided url and returns a struct containing `success`, a flag which is `true` if the download completed successfully, and if successful, a hash of the file with the fields `sha256` and `integrity`. When `sha256` or `integrity` is user specified, setting an explicit `canonical_id` is highly recommended. e.g. [`get_default_canonical_id`](/rules/lib/repo/cache#get_default_canonical_id)

### Parameters

ParameterDescription`url`[string](../core/string.html); or Iterable of [string](../core/string.html) s;
 required

 List of mirror URLs referencing the same file.
 `output`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 default is `''`

 path to the output file, relative to the repository directory.
 `sha256`[string](../core/string.html);
 default is `''`

 The expected SHA-256 hash of the file downloaded. This must match the SHA-256 hash of the file downloaded. It is a security risk to omit the SHA-256 as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given hash; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.

 `executable`[bool](../core/bool.html);
 default is `False`

 Set the executable flag on the created file, false by default.
 `allow_fail`[bool](../core/bool.html);
 default is `False`

 If set, indicate the error in the return value instead of raising an error for failed downloads.

 `canonical_id`[string](../core/string.html);
 default is `''`

 If set, restrict cache hits to those cases where the file was added to the cache with the same canonical id. By default caching uses the checksum ( `sha256` or `integrity`).

 `auth`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying authentication information for some of the URLs.
 `headers`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying http headers for all URLs.
 `integrity`[string](../core/string.html);
 default is `''`

 Expected checksum of the file downloaded, in Subresource Integrity format. This must match the checksum of the file downloaded. It is a security risk to omit the checksum as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given checksum; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.

 `block`[bool](../core/bool.html);
 default is `True`

 If set to false, the call returns immediately and instead of the regular return value, it returns a token with one single method, wait(), which blocks until the download is finished and returns the usual return value or throws as usual.



## download\_and\_extract

```
struct module_ctx.download_and_extract(url, output='', sha256='', type='', strip_prefix='', allow_fail=False, canonical_id='', auth={}, headers={}, *, integrity='', rename_files={})
```

 Downloads a file to the output path for the provided url, extracts it, and returns a struct containing `success`, a flag which is `true` if the download completed successfully, and if successful, a hash of the file with the fields `sha256` and `integrity`. When `sha256` or `integrity` is user specified, setting an explicit `canonical_id` is highly recommended. e.g. [`get_default_canonical_id`](/rules/lib/repo/cache#get_default_canonical_id)

### Parameters

ParameterDescription`url`[string](../core/string.html); or Iterable of [string](../core/string.html) s;
 required

 List of mirror URLs referencing the same file.
 `output`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 default is `''`

 Path to the directory where the archive will be unpacked, relative to the repository directory.

 `sha256`[string](../core/string.html);
 default is `''`

 The expected SHA-256 hash of the file downloaded. This must match the SHA-256 hash of the file downloaded. It is a security risk to omit the SHA-256 as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given hash; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.

 `type`[string](../core/string.html);
 default is `''`

 The archive type of the downloaded file. By default, the archive type is determined from the file extension of the URL. If the file has no extension, you can explicitly specify either "zip", "jar", "war", "aar", "nupkg", "whl", "tar", "tar.gz", "tgz", "tar.xz", "txz", ".tar.zst", ".tzst", "tar.bz2", ".tbz", ".ar", ".deb", or ".7z" here.

 `strip_prefix`[string](../core/string.html);
 default is `''`

 A directory prefix to strip from the extracted files. Many archives contain a
top-level directory that contains all files in the archive. Instead of needing to
specify this prefix over and over in the `build_file`, this field can
be used to strip it from extracted files.

For compatibility, this parameter may also be used under the deprecated name
`stripPrefix`.



`allow_fail`[bool](../core/bool.html);
 default is `False`

 If set, indicate the error in the return value instead of raising an error for failed downloads.

 `canonical_id`[string](../core/string.html);
 default is `''`

 If set, restrict cache hits to those cases where the file was added to the cache with the same canonical id. By default caching uses the checksum
( `sha256` or `integrity`).

 `auth`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying authentication information for some of the URLs.
 `headers`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying http headers for all URLs.
 `integrity`[string](../core/string.html);
 default is `''`

 Expected checksum of the file downloaded, in Subresource Integrity format. This must match the checksum of the file downloaded. It is a security risk to omit the checksum as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given checksum; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.
 `rename_files`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying files to rename during the extraction. Archive entries with names exactly matching a key will be renamed to the value, prior to any directory prefix adjustment. This can be used to extract archives that contain non-Unicode filenames, or which have files that would extract to the same path on case-insensitive filesystems.



## execute

```
exec_result module_ctx.execute(arguments, timeout=600, environment={}, quiet=True, working_directory="")
```

 Executes the command given by the list of arguments. The execution time of the command is limited by `timeout` (in seconds, default 600 seconds). This method returns an `exec_result` structure containing the output of the command. The `environment` map can be used to override some environment variables to be passed to the process.



### Parameters

ParameterDescription`arguments`[sequence](../core/list.html);
 required

 List of arguments, the first element should be the path to the program to execute.

 `timeout`[int](../core/int.html);
 default is `600`

 Maximum duration of the command in seconds (default is 600 seconds).
 `environment`[dict](../core/dict.html);
 default is `{}`

 Force some environment variables to be set to be passed to the process. The value can be `None` to remove the environment variable.

 `quiet`[bool](../core/bool.html);
 default is `True`

 If stdout and stderr should be printed to the terminal.
 `working_directory`[string](../core/string.html);
 default is `""`

 Working directory for command execution.
Can be relative to the repository root or absolute.
The default is the repository root.



## extension\_metadata

```
extension_metadata module_ctx.extension_metadata(*, root_module_direct_deps=None, root_module_direct_dev_deps=None, reproducible=False, facts={})
```

 Constructs an opaque object that can be returned from the module extension's implementation function to provide metadata about the repositories generated by the extension to Bazel.


### Parameters

ParameterDescription`root_module_direct_deps`[sequence](../core/list.html) of [string](../core/string.html) s; or [string](../core/string.html); or `None`;
 default is `None`

 The names of the repositories that the extension considers to be direct dependencies of the root module. If the root module imports additional repositories or does not import all of these repositories via [`use_repo`](../globals/module.html#use_repo), Bazel will print a warning when the extension is evaluated, instructing the user to run `bazel mod tidy` to fix the `use_repo` calls automatically.

If one of `root_module_direct_deps` and will print a warning and a fixup command when the extension is evaluated.

If one of `root_module_direct_deps` and `root_module_direct_dev_deps` is specified, the other has to be as well. The lists specified by these two parameters must be disjoint.

Exactly one of `root_module_direct_deps` and `root_module_direct_dev_deps` can be set to the special value `"all"`, which is treated as if a list with the names of all repositories generated by the extension was specified as the value.


`root_module_direct_dev_deps`[sequence](../core/list.html) of [string](../core/string.html) s; or [string](../core/string.html); or `None`;
 default is `None`

 The names of the repositories that the extension considers to be direct dev dependencies of the root module. If the root module imports additional repositories or does not import all of these repositories via [`use_repo`](../globals/module.html#use_repo) on an extension proxy created with `use_extension(..., dev_dependency = True)`, Bazel will print a warning when the extension is evaluated, instructing the user to run `bazel mod tidy` to fix the `use_repo` calls automatically.

If one of `root_module_direct_deps` and `root_module_direct_dev_deps` is specified, the other has to be as well. The lists specified by these two parameters must be disjoint.

Exactly one of `root_module_direct_deps` and `root_module_direct_dev_deps` can be set to the special value `"all"`, which is treated as if a list with the names of all repositories generated by the extension was specified as the value.


`reproducible`[bool](../core/bool.html);
 default is `False`

 States that this module extension ensures complete reproducibility, thereby it should not be stored in the lockfile.
 `facts`[dict](../core/dict.html) of [string](../core/string.html) s;
 default is `{}`

 A JSON-like dict that is made available to future executions of this extension via
the \`module\_ctx.facts\` property.
This is useful for extensions that want to preserve universally true facts such as
the hashes of artifacts in an immutable repository.

Bazel may shallowly merge multiple facts dicts returned by different versions of the
extension in order to resolve merge conflicts on the MODULE.bazel.lock file, as if
by applying the \`dict.update()\` method or the \`\|\` operator in Starlark. Extensions
should use facts for key-value storage only and ensure that the key uniquely
determines the value, although perhaps only via additional information and network
access. An extension can opt out of this merging by providing a dict with a single,
fixed top-level key and an arbitrary value.

Note that the value provided here may be read back by a different version of the
extension, so either include a version number or use a schema that is unlikely to
result in ambiguities.



## extract

```
None module_ctx.extract(archive, output='', strip_prefix='', *, rename_files={}, watch_archive='auto')
```

 Extract an archive to the repository directory.


### Parameters

ParameterDescription`archive`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 path to the archive that will be unpacked, relative to the repository directory.
 `output`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 default is `''`

 path to the directory where the archive will be unpacked, relative to the repository directory.
 `strip_prefix`[string](../core/string.html);
 default is `''`

 a directory prefix to strip from the extracted files. Many archives contain a
top-level directory that contains all files in the archive. Instead of needing to
specify this prefix over and over in the `build_file`, this field can be
used to strip it from extracted files.

For compatibility, this parameter may also be used under the deprecated name
`stripPrefix`.



`rename_files`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying files to rename during the extraction. Archive entries with names exactly matching a key will be renamed to the value, prior to any directory prefix adjustment. This can be used to extract archives that contain non-Unicode filenames, or which have files that would extract to the same path on case-insensitive filesystems.
 `watch_archive`[string](../core/string.html);
 default is `'auto'`

 whether to [watch](#watch) the archive file. Can be the string 'yes', 'no', or 'auto'. Passing 'yes' is equivalent to immediately invoking the [`watch()`](#watch) method; passing 'no' does not attempt to watch the file; passing 'auto' will only attempt to watch the file when it is legal to do so (see `watch()` docs for more information.


## facts

```
Facts module_ctx.facts
```

 The JSON-like dict returned by a previous execution of this extension in the \`facts\`
parameter of \[\`extension\_metadata\`\](../builtins/module\_ctx#extension\_metadata) or else
\`{}\`.
This is useful for extensions that want to preserve universally true facts such as the
hashes of artifacts in an immutable repository.
Note that the returned value may have been created by a different version of the
extension, which may have used a different schema.



## file

```
None module_ctx.file(path, content='', executable=True, legacy_utf8=False)
```

 Generates a file in the repository directory with the provided content.


### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the file to create, relative to the repository directory.
 `content`[string](../core/string.html);
 default is `''`

 The content of the file to create, empty by default.
 `executable`[bool](../core/bool.html);
 default is `True`

 Set the executable flag on the created file, true by default.
 `legacy_utf8`[bool](../core/bool.html);
 default is `False`

 No-op. This parameter is deprecated and will be removed in a future version of Bazel.



## getenv

```
string module_ctx.getenv(name, default=None)
```

 Returns the value of an environment variable `name` as a string if exists, or `default` if it doesn't.

When building incrementally, any change to the value of the variable named by `name` will cause this repository to be re-fetched.



### Parameters

ParameterDescription`name`[string](../core/string.html);
 required

 Name of desired environment variable.
 `default`[string](../core/string.html); or `None`;
 default is `None`

 Default value to return if `name` is not found.

 May return `None`.



## is\_dev\_dependency

```
bool module_ctx.is_dev_dependency(tag)
```

 Returns whether the given tag was specified on the result of a [use\_extension](../globals/module.html#use_extension) call with `devDependency = True`.


### Parameters

ParameterDescription`tag`
 bazel\_module\_tag;
 required

 A tag obtained from [bazel\_module.tags](../builtins/bazel_module.html#tags).


## modules

```
list module_ctx.modules
```

 A list of all the Bazel modules in the external dependency graph that use this module extension, each of which is a [bazel\_module](../builtins/bazel_module.html) object that exposes all the tags it specified for this extension. The iteration order of this dictionary is guaranteed to be the same as breadth-first search starting from the root module.



## os

```
repository_os module_ctx.os
```

 A struct to access information from the system.



## path

```
path module_ctx.path(path)
```

 Returns a path from a string, label, or path. If this context is a `repository_ctx`, a relative path will resolve relative to the repository directory. If it is a `module_ctx`, a relative path will resolve relative to a temporary working directory for this module extension. If the path is a label, it will resolve to the path of the corresponding file. Note that remote repositories and module extensions are executed during the analysis phase and thus cannot depends on a target result (the label should point to a non-generated file). If path is a path, it will return that path as is.



### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

`string`, `Label` or `path` from which to create a path from.


## read

```
string module_ctx.read(path, *, watch='auto')
```

 Reads the content of a file on the filesystem.


### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the file to read from.
 `watch`[string](../core/string.html);
 default is `'auto'`

 Whether to [watch](#watch) the file. Can be the string 'yes', 'no', or 'auto'. Passing 'yes' is equivalent to immediately invoking the [`watch()`](#watch) method; passing 'no' does not attempt to watch the file; passing 'auto' will only attempt to watch the file when it is legal to do so (see `watch()` docs for more information.



## report\_progress

```
None module_ctx.report_progress(status='')
```

 Updates the progress status for the fetching of this repository or module extension.


### Parameters

ParameterDescription`status`[string](../core/string.html);
 default is `''`

`string` describing the current status of the fetch progress.


## root\_module\_has\_non\_dev\_dependency

```
bool module_ctx.root_module_has_non_dev_dependency
```

 Whether the root module uses this extension as a non-dev dependency.



## watch

```
None module_ctx.watch(path)
```

 Tells Bazel to watch for changes to the given path, whether or not it exists, or whether it's a file or a directory. Any changes to the file or directory will invalidate this repository or module extension, and cause it to be refetched or re-evaluated next time.

"Changes" include changes to the contents of the file (if the path is a file); if the path was a file but is now a directory, or vice versa; and if the path starts or stops existing. Notably, this does _not_ include changes to any files under the directory if the path is a directory. For that, use [`path.readdir()`](path.html#readdir) instead.

Note that attempting to watch paths inside the repo currently being fetched, or inside the working directory of the current module extension, will result in an error. A module extension attempting to watch a path outside the current Bazel workspace will also result in an error.



### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the file to watch.


## which

```
path module_ctx.which(program)
```

 Returns the `path` of the corresponding program or `None` if there is no such program in the path.



### Parameters

ParameterDescription`program`[string](../core/string.html);
 required

 Program to find in the path.

 May return `None`.

---

## path
- URL: https://bazel.build/rules/lib/builtins/path
- Source: rules/lib/builtins/path.mdx
- Slug: /rules/lib/builtins/path

A structure representing a file to be used inside a repository.

## Members

- [basename](#basename)
- [dirname](#dirname)
- [exists](#exists)
- [get\_child](#get_child)
- [is\_dir](#is_dir)
- [readdir](#readdir)
- [realpath](#realpath)

## basename

```
string path.basename
```

 A string giving the basename of the file.



## dirname

```
path path.dirname
```

 The parent directory of this file, or None if this file does not have a parent.
 May return `None`.



## exists

```
bool path.exists
```

 Returns true if the file or directory denoted by this path exists.

Note that accessing this field does _not_ cause the path to be watched. If you'd like the repo rule or module extension to be sensitive to the path's existence, use the `watch()` method on the context object.



## get\_child

```
path path.get_child(*relative_paths)
```

 Returns the path obtained by joining this path with the given relative paths.


### Parameters

ParameterDescription`relative_paths`
 required

 Zero or more relative path strings to append to this path with path separators added as needed.



## is\_dir

```
bool path.is_dir
```

 Returns true if this path points to a directory.

Note that accessing this field does _not_ cause the path to be watched. If you'd like the repo rule or module extension to be sensitive to whether the path is a directory or a file, use the `watch()` method on the context object.



## readdir

```
list path.readdir(*, watch='auto')
```

 Returns the list of entries in the directory denoted by this path. Each entry is a `path` object itself.



### Parameters

ParameterDescription`watch`[string](../core/string.html);
 default is `'auto'`

 whether Bazel should watch the list of entries in this directory and refetch the repository or re-evaluate the module extension next time when any changes are detected. Changes to detect include entry creation, deletion, and renaming. Note that this doesn't watch the _contents_ of any entries in the directory.

Can be the string 'yes', 'no', or 'auto'. If set to 'auto', Bazel will only watch this directory when it is legal to do so (see [`repository_ctx.watch()`](repository_ctx.html#watch) docs for more information).



## realpath

```
path path.realpath
```

 Returns the canonical path for this path by repeatedly replacing all symbolic links with their referents.

---

## propagation\_ctx
- URL: https://bazel.build/rules/lib/builtins/propagation_ctx
- Source: rules/lib/builtins/propagation_ctx.mdx
- Slug: /rules/lib/builtins/propagation_ctx

A context object that is passed to the `propagation_predicate`, `attr_aspects` and `toolchains_aspects` functions of aspects. It provides access to the information needed to determine whether the aspect should be propagated to the target and what attributes or toolchain types it should be propagated to next.

## Members

- [attr](#attr)
- [rule](#rule)

## attr

```
struct propagation_ctx.attr
```

 A struct to access only the public parameters of the aspect. The keys and values of the struct are the parameters names and values.



## rule

```
StarlarkAspectPropagationRuleApi propagation_ctx.rule
```

 Allows access to the details of the rule.

---

## repo\_metadata
- URL: https://bazel.build/rules/lib/builtins/repo_metadata
- Source: rules/lib/builtins/repo_metadata.mdx
- Slug: /rules/lib/builtins/repo_metadata

See [`repository_ctx.repo_metadata`](repository_ctx#repo_metadata).

---

## repository\_ctx
- URL: https://bazel.build/rules/lib/builtins/repository_ctx
- Source: rules/lib/builtins/repository_ctx.mdx
- Slug: /rules/lib/builtins/repository_ctx

The context of the repository rule containing helper functions and information about attributes. You get a repository\_ctx object as an argument to the `implementation` function when you create a repository rule.

## Members

- [attr](#attr)
- [delete](#delete)
- [download](#download)
- [download\_and\_extract](#download_and_extract)
- [execute](#execute)
- [extract](#extract)
- [file](#file)
- [getenv](#getenv)
- [name](#name)
- [original\_name](#original_name)
- [os](#os)
- [patch](#patch)
- [path](#path)
- [read](#read)
- [rename](#rename)
- [repo\_metadata](#repo_metadata)
- [report\_progress](#report_progress)
- [symlink](#symlink)
- [template](#template)
- [watch](#watch)
- [watch\_tree](#watch_tree)
- [which](#which)
- [workspace\_root](#workspace_root)

## attr

```
structure repository_ctx.attr
```

 A struct to access the values of the attributes. The values are provided by the user (if not, a default value is used).



## delete

```
bool repository_ctx.delete(path)
```

 Deletes a file or a directory. Returns a bool, indicating whether the file or directory was actually deleted by this call.



### Parameters

ParameterDescription`path`[string](../core/string.html); or [path](../builtins/path.html);
 required

 Path of the file to delete, relative to the repository directory, or absolute. Can be a path or a string.



## download

```
unknown repository_ctx.download(url, output='', sha256='', executable=False, allow_fail=False, canonical_id='', auth={}, headers={}, *, integrity='', block=True)
```

 Downloads a file to the output path for the provided url and returns a struct containing `success`, a flag which is `true` if the download completed successfully, and if successful, a hash of the file with the fields `sha256` and `integrity`. When `sha256` or `integrity` is user specified, setting an explicit `canonical_id` is highly recommended. e.g. [`get_default_canonical_id`](/rules/lib/repo/cache#get_default_canonical_id)

### Parameters

ParameterDescription`url`[string](../core/string.html); or Iterable of [string](../core/string.html) s;
 required

 List of mirror URLs referencing the same file.
 `output`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 default is `''`

 path to the output file, relative to the repository directory.
 `sha256`[string](../core/string.html);
 default is `''`

 The expected SHA-256 hash of the file downloaded. This must match the SHA-256 hash of the file downloaded. It is a security risk to omit the SHA-256 as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given hash; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.

 `executable`[bool](../core/bool.html);
 default is `False`

 Set the executable flag on the created file, false by default.
 `allow_fail`[bool](../core/bool.html);
 default is `False`

 If set, indicate the error in the return value instead of raising an error for failed downloads.

 `canonical_id`[string](../core/string.html);
 default is `''`

 If set, restrict cache hits to those cases where the file was added to the cache with the same canonical id. By default caching uses the checksum ( `sha256` or `integrity`).

 `auth`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying authentication information for some of the URLs.
 `headers`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying http headers for all URLs.
 `integrity`[string](../core/string.html);
 default is `''`

 Expected checksum of the file downloaded, in Subresource Integrity format. This must match the checksum of the file downloaded. It is a security risk to omit the checksum as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given checksum; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.

 `block`[bool](../core/bool.html);
 default is `True`

 If set to false, the call returns immediately and instead of the regular return value, it returns a token with one single method, wait(), which blocks until the download is finished and returns the usual return value or throws as usual.



## download\_and\_extract

```
struct repository_ctx.download_and_extract(url, output='', sha256='', type='', strip_prefix='', allow_fail=False, canonical_id='', auth={}, headers={}, *, integrity='', rename_files={})
```

 Downloads a file to the output path for the provided url, extracts it, and returns a struct containing `success`, a flag which is `true` if the download completed successfully, and if successful, a hash of the file with the fields `sha256` and `integrity`. When `sha256` or `integrity` is user specified, setting an explicit `canonical_id` is highly recommended. e.g. [`get_default_canonical_id`](/rules/lib/repo/cache#get_default_canonical_id)

### Parameters

ParameterDescription`url`[string](../core/string.html); or Iterable of [string](../core/string.html) s;
 required

 List of mirror URLs referencing the same file.
 `output`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 default is `''`

 Path to the directory where the archive will be unpacked, relative to the repository directory.

 `sha256`[string](../core/string.html);
 default is `''`

 The expected SHA-256 hash of the file downloaded. This must match the SHA-256 hash of the file downloaded. It is a security risk to omit the SHA-256 as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given hash; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.

 `type`[string](../core/string.html);
 default is `''`

 The archive type of the downloaded file. By default, the archive type is determined from the file extension of the URL. If the file has no extension, you can explicitly specify either "zip", "jar", "war", "aar", "nupkg", "whl", "tar", "tar.gz", "tgz", "tar.xz", "txz", ".tar.zst", ".tzst", "tar.bz2", ".tbz", ".ar", ".deb", or ".7z" here.

 `strip_prefix`[string](../core/string.html);
 default is `''`

 A directory prefix to strip from the extracted files. Many archives contain a
top-level directory that contains all files in the archive. Instead of needing to
specify this prefix over and over in the `build_file`, this field can
be used to strip it from extracted files.

For compatibility, this parameter may also be used under the deprecated name
`stripPrefix`.



`allow_fail`[bool](../core/bool.html);
 default is `False`

 If set, indicate the error in the return value instead of raising an error for failed downloads.

 `canonical_id`[string](../core/string.html);
 default is `''`

 If set, restrict cache hits to those cases where the file was added to the cache with the same canonical id. By default caching uses the checksum
( `sha256` or `integrity`).

 `auth`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying authentication information for some of the URLs.
 `headers`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying http headers for all URLs.
 `integrity`[string](../core/string.html);
 default is `''`

 Expected checksum of the file downloaded, in Subresource Integrity format. This must match the checksum of the file downloaded. It is a security risk to omit the checksum as remote files can change. At best omitting this field will make your build non-hermetic. It is optional to make development easier but should be set before shipping. If provided, the repository cache will first be checked for a file with the given checksum; a download will only be attempted if the file was not found in the cache. After a successful download, the file will be added to the cache.
 `rename_files`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying files to rename during the extraction. Archive entries with names exactly matching a key will be renamed to the value, prior to any directory prefix adjustment. This can be used to extract archives that contain non-Unicode filenames, or which have files that would extract to the same path on case-insensitive filesystems.



## execute

```
exec_result repository_ctx.execute(arguments, timeout=600, environment={}, quiet=True, working_directory="")
```

 Executes the command given by the list of arguments. The execution time of the command is limited by `timeout` (in seconds, default 600 seconds). This method returns an `exec_result` structure containing the output of the command. The `environment` map can be used to override some environment variables to be passed to the process.



### Parameters

ParameterDescription`arguments`[sequence](../core/list.html);
 required

 List of arguments, the first element should be the path to the program to execute.

 `timeout`[int](../core/int.html);
 default is `600`

 Maximum duration of the command in seconds (default is 600 seconds).
 `environment`[dict](../core/dict.html);
 default is `{}`

 Force some environment variables to be set to be passed to the process. The value can be `None` to remove the environment variable.

 `quiet`[bool](../core/bool.html);
 default is `True`

 If stdout and stderr should be printed to the terminal.
 `working_directory`[string](../core/string.html);
 default is `""`

 Working directory for command execution.
Can be relative to the repository root or absolute.
The default is the repository root.



## extract

```
None repository_ctx.extract(archive, output='', strip_prefix='', *, rename_files={}, watch_archive='auto')
```

 Extract an archive to the repository directory.


### Parameters

ParameterDescription`archive`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 path to the archive that will be unpacked, relative to the repository directory.
 `output`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 default is `''`

 path to the directory where the archive will be unpacked, relative to the repository directory.
 `strip_prefix`[string](../core/string.html);
 default is `''`

 a directory prefix to strip from the extracted files. Many archives contain a
top-level directory that contains all files in the archive. Instead of needing to
specify this prefix over and over in the `build_file`, this field can be
used to strip it from extracted files.

For compatibility, this parameter may also be used under the deprecated name
`stripPrefix`.



`rename_files`[dict](../core/dict.html);
 default is `{}`

 An optional dict specifying files to rename during the extraction. Archive entries with names exactly matching a key will be renamed to the value, prior to any directory prefix adjustment. This can be used to extract archives that contain non-Unicode filenames, or which have files that would extract to the same path on case-insensitive filesystems.
 `watch_archive`[string](../core/string.html);
 default is `'auto'`

 whether to [watch](#watch) the archive file. Can be the string 'yes', 'no', or 'auto'. Passing 'yes' is equivalent to immediately invoking the [`watch()`](#watch) method; passing 'no' does not attempt to watch the file; passing 'auto' will only attempt to watch the file when it is legal to do so (see `watch()` docs for more information.


## file

```
None repository_ctx.file(path, content='', executable=True, legacy_utf8=False)
```

 Generates a file in the repository directory with the provided content.


### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the file to create, relative to the repository directory.
 `content`[string](../core/string.html);
 default is `''`

 The content of the file to create, empty by default.
 `executable`[bool](../core/bool.html);
 default is `True`

 Set the executable flag on the created file, true by default.
 `legacy_utf8`[bool](../core/bool.html);
 default is `False`

 No-op. This parameter is deprecated and will be removed in a future version of Bazel.



## getenv

```
string repository_ctx.getenv(name, default=None)
```

 Returns the value of an environment variable `name` as a string if exists, or `default` if it doesn't.

When building incrementally, any change to the value of the variable named by `name` will cause this repository to be re-fetched.



### Parameters

ParameterDescription`name`[string](../core/string.html);
 required

 Name of desired environment variable.
 `default`[string](../core/string.html); or `None`;
 default is `None`

 Default value to return if `name` is not found.

 May return `None`.



## name

```
string repository_ctx.name
```

 The canonical name of the external repository created by this rule. This name is guaranteed to be unique among all external repositories, but its exact format is not specified. Use [`original_name`](#original_name) instead to get the name that was originally specified as the `name` when this repository rule was instantiated.



## original\_name

```
string repository_ctx.original_name
```

 The name that was originally specified as the `name` attribute when this repository rule was instantiated. This name is not necessarily unique among external repositories. Use [`name`](#name) instead to get the canonical name of the external repository.



## os

```
repository_os repository_ctx.os
```

 A struct to access information from the system.



## patch

```
None repository_ctx.patch(patch_file, strip=0, *, watch_patch='auto')
```

 Apply a patch file to the root directory of external repository. The patch file should be a standard [unified diff format](https://en.wikipedia.org/wiki/Diff#Unified_format) file. The Bazel-native patch implementation doesn't support fuzz match and binary patch like the patch command line tool.



### Parameters

ParameterDescription`patch_file`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 The patch file to apply, it can be label, relative path or absolute path. If it's a relative path, it will resolve to the repository directory.

 `strip`[int](../core/int.html);
 default is `0`

 Strip the specified number of leading components from file names.
 `watch_patch`[string](../core/string.html);
 default is `'auto'`

 Whether to [watch](#watch) the patch file. Can be the string 'yes', 'no', or 'auto'. Passing 'yes' is equivalent to immediately invoking the [`watch()`](#watch) method; passing 'no' does not attempt to watch the file; passing 'auto' will only attempt to watch the file when it is legal to do so (see `watch()` docs for more information.



## path

```
path repository_ctx.path(path)
```

 Returns a path from a string, label, or path. If this context is a `repository_ctx`, a relative path will resolve relative to the repository directory. If it is a `module_ctx`, a relative path will resolve relative to a temporary working directory for this module extension. If the path is a label, it will resolve to the path of the corresponding file. Note that remote repositories and module extensions are executed during the analysis phase and thus cannot depends on a target result (the label should point to a non-generated file). If path is a path, it will return that path as is.



### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

`string`, `Label` or `path` from which to create a path from.


## read

```
string repository_ctx.read(path, *, watch='auto')
```

 Reads the content of a file on the filesystem.


### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the file to read from.
 `watch`[string](../core/string.html);
 default is `'auto'`

 Whether to [watch](#watch) the file. Can be the string 'yes', 'no', or 'auto'. Passing 'yes' is equivalent to immediately invoking the [`watch()`](#watch) method; passing 'no' does not attempt to watch the file; passing 'auto' will only attempt to watch the file when it is legal to do so (see `watch()` docs for more information.



## rename

```
None repository_ctx.rename(src, dst)
```

 Renames the file or directory from `src` to `dst`. Parent directories are created as needed. Fails if the destination path
already exists. Both paths must be located within the repository.



### Parameters

ParameterDescription`src`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 The path of the existing file or directory to rename, relative
to the repository directory.

 `dst`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 The new name to which the file or directory will be renamed to,
relative to the repository directory.



## repo\_metadata

```
repo_metadata repository_ctx.repo_metadata(*, reproducible=False, attrs_for_reproducibility={})
```

 Constructs an opaque object that can be returned from the repo rule's implementation function to provide metadata about its reproducibility.



### Parameters

ParameterDescription`reproducible`[bool](../core/bool.html);
 default is `False`

 States that this repo can be reproducibly refetched; that is, if it were fetched another time with exactly the same input attributes, repo rule definition, watched files and environment variables, etc., then exactly the same output would be produced. This property needs to hold even if other untracked conditions change, such as information from the internet, the path of the workspace root, output from running arbitrary executables, etc. If set to True, this allows the fetched repo contents to be cached across workspaces.

Note that setting this to True does not guarantee caching in the repo contents cache; for example, local repo rules are never cached.



`attrs_for_reproducibility`[dict](../core/dict.html);
 default is `{}`

 If `reproducible` is False, this can be specified to tell Bazel which attributes of the original repo rule to change to make it reproducible.



## report\_progress

```
None repository_ctx.report_progress(status='')
```

 Updates the progress status for the fetching of this repository or module extension.


### Parameters

ParameterDescription`status`[string](../core/string.html);
 default is `''`

`string` describing the current status of the fetch progress.


## symlink

```
None repository_ctx.symlink(target, link_name)
```

 Creates a symlink on the filesystem.


### Parameters

ParameterDescription`target`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 The path that the symlink should point to.
 `link_name`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 The path of the symlink to create.


## template

```
None repository_ctx.template(path, template, substitutions={}, executable=True, *, watch_template='auto')
```

 Generates a new file using a `template`. Every occurrence in `template` of a key of `substitutions` will be replaced by the corresponding value. The result is written in `path`. An optional `executable` argument (default to true) can be set to turn on or off the executable bit.



### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the file to create, relative to the repository directory.
 `template`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path to the template file.
 `substitutions`[dict](../core/dict.html);
 default is `{}`

 Substitutions to make when expanding the template.
 `executable`[bool](../core/bool.html);
 default is `True`

 Set the executable flag on the created file, true by default.
 `watch_template`[string](../core/string.html);
 default is `'auto'`

 Whether to [watch](#watch) the template file. Can be the string 'yes', 'no', or 'auto'. Passing 'yes' is equivalent to immediately invoking the [`watch()`](#watch) method; passing 'no' does not attempt to watch the file; passing 'auto' will only attempt to watch the file when it is legal to do so (see `watch()` docs for more information.



## watch

```
None repository_ctx.watch(path)
```

 Tells Bazel to watch for changes to the given path, whether or not it exists, or whether it's a file or a directory. Any changes to the file or directory will invalidate this repository or module extension, and cause it to be refetched or re-evaluated next time.

"Changes" include changes to the contents of the file (if the path is a file); if the path was a file but is now a directory, or vice versa; and if the path starts or stops existing. Notably, this does _not_ include changes to any files under the directory if the path is a directory. For that, use [`path.readdir()`](path.html#readdir) instead.

Note that attempting to watch paths inside the repo currently being fetched, or inside the working directory of the current module extension, will result in an error. A module extension attempting to watch a path outside the current Bazel workspace will also result in an error.



### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the file to watch.


## watch\_tree

```
None repository_ctx.watch_tree(path)
```

 Tells Bazel to watch for changes to any files or directories transitively under the given path. Any changes to the contents of files, the existence of files or directories, file names or directory names, will cause this repo to be refetched.

Note that attempting to watch paths inside the repo currently being fetched will result in an error.



### Parameters

ParameterDescription`path`[string](../core/string.html); or [Label](../builtins/Label.html); or [path](../builtins/path.html);
 required

 Path of the directory tree to watch.


## which

```
path repository_ctx.which(program)
```

 Returns the `path` of the corresponding program or `None` if there is no such program in the path.



### Parameters

ParameterDescription`program`[string](../core/string.html);
 required

 Program to find in the path.

 May return `None`.



## workspace\_root

```
path repository_ctx.workspace_root
```

 The path to the root workspace of the bazel invocation.

---

## repository\_os
- URL: https://bazel.build/rules/lib/builtins/repository_os
- Source: rules/lib/builtins/repository_os.mdx
- Slug: /rules/lib/builtins/repository_os

Various data about the current platform Bazel is running on.

## Members

- [arch](#arch)
- [environ](#environ)
- [name](#name)

## arch

```
string repository_os.arch
```

 A string identifying the architecture Bazel is running on (the value of the `"os.arch"` Java property converted to lower case).



## environ

```
dict repository_os.environ
```

 The dictionary of environment variables.

**NOTE**: Retrieving an environment variable from this dictionary does not establish a dependency from a repository rule or module extension to the environment variable. To establish a dependency when looking up an environment variable, use either `repository_ctx.getenv` or `module_ctx.getenv` instead.



## name

```
string repository_os.name
```

 A string identifying the operating system Bazel is running on (the value of the `"os.name"` Java property converted to lower case).

---

## repository\_rule
- URL: https://bazel.build/rules/lib/builtins/repository_rule
- Source: rules/lib/builtins/repository_rule.mdx
- Slug: /rules/lib/builtins/repository_rule

A callable value that may be invoked within the implementation function of a module extension to instantiate and return a repository rule. Created by [`repository_rule()`](../globals/bzl.html#repository_rule).

---

## root
- URL: https://bazel.build/rules/lib/builtins/root
- Source: rules/lib/builtins/root.mdx
- Slug: /rules/lib/builtins/root

A root for files. The roots are the directories containing files, and they are mapped together into a single directory tree to form the execution environment.

## Members

- [path](#path)

## path

```
string root.path
```

 Returns the relative path from the exec root to the actual root.

---

## rule
- URL: https://bazel.build/rules/lib/builtins/rule
- Source: rules/lib/builtins/rule.mdx
- Slug: /rules/lib/builtins/rule

A callable value representing the type of a native or Starlark rule (created by
[`rule()`](../globals/bzl.html#rule)). Calling the value during
evaluation of a package's BUILD file creates an instance of the rule and adds it to the
package's target set. For more information, visit this page about
[Rules](https://bazel.build/extending/rules).

---

## rule\_attributes
- URL: https://bazel.build/rules/lib/builtins/rule_attributes
- Source: rules/lib/builtins/rule_attributes.mdx
- Slug: /rules/lib/builtins/rule_attributes

Information about attributes of a rule an aspect is applied to.

## Members

- [attr](#attr)
- [exec\_groups](#exec_groups)
- [executable](#executable)
- [file](#file)
- [files](#files)
- [kind](#kind)
- [toolchains](#toolchains)
- [var](#var)

## attr

```
struct rule_attributes.attr
```

 A struct to access the values of the [attributes](https://bazel.build/extending/rules#attributes). The values are provided by the user (if not, a default value is used). The attributes of the struct and the types of their values correspond to the keys and values of the [`attrs` dict](../globals/bzl.html#rule.attrs) provided to the [`rule` function](../globals/bzl.html#rule). [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/attributes/printer.bzl).



## exec\_groups

```
ExecGroupCollection rule_attributes.exec_groups
```

 A collection of the execution groups available for the rule the aspect is applied to, indexed by their names.



## executable

```
struct rule_attributes.executable
```

 A `struct` containing executable files defined in [label type attributes](../toplevel/attr.html#label) marked as [`executable=True`](../toplevel/attr.html#label.executable). The struct fields correspond to the attribute names. Each value in the struct is either a [`File`](../builtins/File.html) or `None`. If an optional attribute is not specified in the rule then the corresponding struct value is `None`. If a label type is not marked as `executable=True`, no corresponding struct field is generated. [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/actions_run/execute.bzl).



## file

```
struct rule_attributes.file
```

 A `struct` containing files defined in [label type attributes](../toplevel/attr.html#label) marked as [`allow_single_file`](../toplevel/attr.html#label.allow_single_file). The struct fields correspond to the attribute names. The struct value is always a [`File`](../builtins/File.html) or `None`. If an optional attribute is not specified in the rule then the corresponding struct value is `None`. If a label type is not marked as `allow_single_file`, no corresponding struct field is generated. It is a shortcut for:

```
list(ctx.attr.<ATTR>.files)[0]
```

In other words, use `file` to access the (singular) [default output](https://bazel.build/extending/rules#requesting_output_files) of a dependency. [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/expand_template/hello.bzl).



## files

```
struct rule_attributes.files
```

 A `struct` containing files defined in [label](../toplevel/attr.html#label) or [label list](../toplevel/attr.html#label_list) type attributes. The struct fields correspond to the attribute names. The struct values are `list` of [`File`](../builtins/File.html) s. It is a shortcut for:

```
[f for t in ctx.attr.<ATTR> for f in t.files]
```

 In other words, use `files` to access the [default outputs](https://bazel.build/extending/rules#requesting_output_files) of a dependency. [See example of use](https://github.com/bazelbuild/examples/blob/main/rules/depsets/foo.bzl).



## kind

```
string rule_attributes.kind
```

 The kind of a rule, such as 'cc\_library'



## toolchains

```
ToolchainContext rule_attributes.toolchains
```

 Toolchains for the default exec group of the rule the aspect is applied to.



## var

```
dict rule_attributes.var
```

 Dictionary (String to String) of configuration variables.

---

## runfiles
- URL: https://bazel.build/rules/lib/builtins/runfiles
- Source: rules/lib/builtins/runfiles.mdx
- Slug: /rules/lib/builtins/runfiles

A container of information regarding a set of files required at runtime by an executable. This object should be passed via [`DefaultInfo`](../providers/DefaultInfo.html) in order to tell the build system about the runfiles needed by the outputs produced by the rule.

See [runfiles guide](https://bazel.build/extending/rules#runfiles) for details.

## Members

- [empty\_filenames](#empty_filenames)
- [files](#files)
- [merge](#merge)
- [merge\_all](#merge_all)
- [root\_symlinks](#root_symlinks)
- [symlinks](#symlinks)

## empty\_filenames

```
depset runfiles.empty_filenames
```

 Returns names of empty files to create.



## files

```
depset runfiles.files
```

 Returns the set of runfiles as files.



## merge

```
runfiles runfiles.merge(other)
```

 Returns a new runfiles object that includes all the contents of this one and the argument.

_Note:_ When you have many runfiles objects to merge, use [`merge_all()`](#merge_all) rather than calling `merge` in a loop. This avoids constructing deep depset structures which can cause build failures.

### Parameters

ParameterDescription`other`[runfiles](../builtins/runfiles.html);
 required

 The runfiles object to merge into this.


## merge\_all

```
runfiles runfiles.merge_all(other)
```

 Returns a new runfiles object that includes all the contents of this one and of the runfiles objects in the argument.



### Parameters

ParameterDescription`other`[sequence](../core/list.html) of [runfiles](../builtins/runfiles.html) s;
 required

 The sequence of runfiles objects to merge into this.


## root\_symlinks

```
depset runfiles.root_symlinks
```

 Returns the set of root symlinks.



## symlinks

```
depset runfiles.symlinks
```

 Returns the set of symlinks.

---

## struct
- URL: https://bazel.build/rules/lib/builtins/struct
- Source: rules/lib/builtins/struct.mdx
- Slug: /rules/lib/builtins/struct

A generic object with fields.

Structs fields cannot be reassigned once the struct is created. Two structs are equal if they have the same fields and if corresponding field values are equal.

## Members

- [struct](#struct)

## struct

```
struct struct(**kwargs)
```

 Creates an immutable struct using the keyword arguments as attributes. It is used to group multiple values together. Example:

```
s = struct(x = 2, y = 3)
return s.x + getattr(s, "y")  # returns 5
```

### Parameters

ParameterDescription`kwargs`
 default is `{}`

 Dictionary of arguments.

---

## subrule\_ctx
- URL: https://bazel.build/rules/lib/builtins/subrule_ctx
- Source: rules/lib/builtins/subrule_ctx.mdx
- Slug: /rules/lib/builtins/subrule_ctx

A context object passed to the implementation function of a subrule.

## Members

- [actions](#actions)
- [fragments](#fragments)
- [label](#label)
- [toolchains](#toolchains)

## actions

```
actions subrule_ctx.actions
```

 Contains methods for declaring output files and the actions that produce them



## fragments

```
fragments subrule_ctx.fragments
```

 Allows access to configuration fragments in target configuration.



## label

```
Label subrule_ctx.label
```

 The label of the target currently being analyzed



## toolchains

```
ToolchainContext subrule_ctx.toolchains
```

 Contains methods for declaring output files and the actions that produce them

---

## tag\_class
- URL: https://bazel.build/rules/lib/builtins/tag_class
- Source: rules/lib/builtins/tag_class.mdx
- Slug: /rules/lib/builtins/tag_class

Defines a schema of attributes for a tag, created by [`tag_class()`](../globals/bzl.html#tag_class).

---

## template\_ctx
- URL: https://bazel.build/rules/lib/builtins/template_ctx
- Source: rules/lib/builtins/template_ctx.mdx
- Slug: /rules/lib/builtins/template_ctx

A context object that is passed to the action template expansion function.

## Members

- [args](#args)
- [declare\_file](#declare_file)
- [run](#run)

## args

```
Args template_ctx.args()
```

 Returns an Args object that can be used to build memory-efficient command lines.



## declare\_file

```
File template_ctx.declare_file(filename, *, directory)
```

 Declares that implementation creates a file with the given filename within the specified directory.

Remember that in addition to declaring a file, you must separately create an action that emits the file. Creating that action will require passing the returned `File` object to the action's construction function.


### Parameters

ParameterDescription`filename`[string](../core/string.html);
 required

 The relative path of the file within the directory.
 `directory`[File](../builtins/File.html);
 required

 The directory in which the file should be created.


## run

```
None template_ctx.run(*, outputs, inputs=[], executable, tools=None, arguments=[], progress_message=None)
```

 Creates an action that runs an executable.


### Parameters

ParameterDescription`outputs`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 required

 List of the output files of the action.
 `inputs`[sequence](../core/list.html) of [File](../builtins/File.html) s; or [depset](../builtins/depset.html);
 default is `[]`

 List or depset of the input files of the action.
 `executable`[File](../builtins/File.html); or [string](../core/string.html); or [FilesToRunProvider](../providers/FilesToRunProvider.html);
 required

 The executable file to be called by the action.
 `tools`[sequence](../core/list.html); or [depset](../builtins/depset.html); or `None`;
 default is `None`

 List or [`depset`](../builtins/depset.html) of any tools needed by the action. Tools are executable inputs that may have their own runfiles which are automatically made available to the action.

When a list is provided, it can be a heterogenous collection of:

- `File` s
- `FilesToRunProvider` instances
- `depset` s of `File` s

`File` s from [`ctx.executable`](../builtins/ctx#executable) and `FilesToRunProvider` s which are directly in the list will have their runfiles automatically added. All tools are implicitly added as inputs.

`arguments`[sequence](../core/list.html);
 default is `[]`

 Command line arguments of the action. Must be a list of strings or [`actions.args()`](#args) objects.
 `progress_message`[string](../core/string.html); or `None`;
 default is `None`

 Progress message to show to the user during the build.

---

## toolchain\_type
- URL: https://bazel.build/rules/lib/builtins/toolchain_type
- Source: rules/lib/builtins/toolchain_type.mdx
- Slug: /rules/lib/builtins/toolchain_type

A data type describing a dependency on a specific toolchain type.

## Members

- [mandatory](#mandatory)
- [toolchain\_type](#toolchain_type)

## mandatory

```
bool toolchain_type.mandatory
```

 Whether the toolchain type is mandatory or optional.



## toolchain\_type

```
Label toolchain_type.toolchain_type
```

 The toolchain type that is required.

---

## transition
- URL: https://bazel.build/rules/lib/builtins/transition
- Source: rules/lib/builtins/transition.mdx
- Slug: /rules/lib/builtins/transition

Represents a configuration transition across a dependency edge. For example, if `//package:foo` depends on `//package:bar` with a configuration transition, then the configuration of `//package:bar` (and its dependencies) will be `//package:foo`'s configuration plus the changes specified by the transition function.

## Members

- [transition](#transition)

## transition

```
transition transition(*, implementation, inputs, outputs)
```

 A transition that reads a set of input build settings and writes a set of output build settings.

Example:

```
def _transition_impl(settings, attr):
    # This transition just reads the current CPU value as a demonstration.
    # A real transition could incorporate this into its followup logic.
    current_cpu = settings["//command_line_option:cpu"]
    return {"//command_line_option:compilation_mode": "dbg"}

build_in_debug_mode = transition(
    implementation = _transition_impl,
    inputs = ["//command_line_option:cpu"],
    outputs = ["//command_line_option:compilation_mode"],
)
```

For more details see [here](https://bazel.build/rules/config#user-defined-transitions).

### Parameters

ParameterDescription`implementation`
 callable;
 required

 The function implementing this transition. This function always has two parameters: `settings` and `attr`. The `settings` param is a dictionary whose set of keys is defined by the inputs parameter. So, for each build setting `--//foo=bar`, if `inputs` contains `//foo`, `settings` will have an entry `settings['//foo']='bar'`.

The `attr` param is a reference to `ctx.attr`. This gives the implementation function access to the rule's attributes to make attribute-parameterized transitions possible.

This function must return a `dict` from build setting identifier to build setting value; this represents the configuration transition: for each entry in the returned `dict`, the transition updates that setting to the new value. All other settings are unchanged. This function can also return a `list` of `dict` s or a `dict` of `dict` s in the case of a split transition.


`inputs`[sequence](../core/list.html) of [string](../core/string.html) s;
 required

 List of build settings that can be read by this transition. This becomes the key set of the settings parameter of the implementation function parameter.
 `outputs`[sequence](../core/list.html) of [string](../core/string.html) s;
 required

 List of build settings that can be written by this transition. This must be a superset of the key set of the dictionary returned by this transition.

---

## wasm\_exec\_result
- URL: https://bazel.build/rules/lib/builtins/wasm_exec_result
- Source: rules/lib/builtins/wasm_exec_result.mdx
- Slug: /rules/lib/builtins/wasm_exec_result

The result of executing a WebAssembly function with
`repository_ctx.execute_wasm()`. It contains the function's
return value and output buffer.

If execution failed before the function returned then the return code will be negative
and the `error_message` field will be set.

## Members

- [error\_message](#error_message)
- [output](#output)
- [return\_code](#return_code)

## error\_message

```
string wasm_exec_result.error_message
```

 Contains an error message if execution failed before the function returned.



## output

```
string wasm_exec_result.output
```

 The content of the output buffer returned by the WebAssembly function.



## return\_code

```
long wasm_exec_result.return_code
```

 The return value of the WebAssembly function, or a negative value if execution
was terminated before the function returned.

---

## wasm\_module
- URL: https://bazel.build/rules/lib/builtins/wasm_module
- Source: rules/lib/builtins/wasm_module.mdx
- Slug: /rules/lib/builtins/wasm_module

A WebAssembly module loaded by `repository_ctx.load_wasm()`.

## Members

- [path](#path)

## path

```
unknown wasm_module.path
```

 The path this WebAssembly module was loaded from.

---

## Core Starlark data types
- URL: https://bazel.build/rules/lib/core
- Source: rules/lib/core.mdx
- Slug: /rules/lib/core

This section lists the data types of the [Starlark core language](https://github.com/bazelbuild/starlark/blob/master/spec.md#built-in-constants-and-functions). With some exceptions, these type names are not valid Starlark symbols; instances of them may be acquired through different means.

- [bool](/rules/lib/core/bool)
- [builtin\_function\_or\_method](/rules/lib/core/builtin_function_or_method)
- [dict](/rules/lib/core/dict)
- [float](/rules/lib/core/float)
- [function](/rules/lib/core/function)
- [int](/rules/lib/core/int)
- [json](/rules/lib/core/json)
- [list](/rules/lib/core/list)
- [range](/rules/lib/core/range)
- [set](/rules/lib/core/set)
- [string](/rules/lib/core/string)
- [tuple](/rules/lib/core/tuple)

---

## bool
- URL: https://bazel.build/rules/lib/core/bool
- Source: rules/lib/core/bool.mdx
- Slug: /rules/lib/core/bool

A type to represent booleans. There are only two possible values: True and False. Any value can be converted to a boolean using the [bool](../globals/all.html#bool) function.

---

## builtin\_function\_or\_method
- URL: https://bazel.build/rules/lib/core/builtin_function_or_method
- Source: rules/lib/core/builtin_function_or_method.mdx
- Slug: /rules/lib/core/builtin_function_or_method

The type of a built-in function, defined by Java code.

---

## dict
- URL: https://bazel.build/rules/lib/core/dict
- Source: rules/lib/core/dict.mdx
- Slug: /rules/lib/core/dict

dict is a built-in type representing an associative mapping or _dictionary_. A dictionary supports indexing using `d[k]` and key membership testing using `k in d`; both operations take constant time. Unfrozen dictionaries are mutable, and may be updated by assigning to `d[k]` or by calling certain methods. Dictionaries are iterable; iteration yields the sequence of keys in insertion order. Iteration order is unaffected by updating the value associated with an existing key, but is affected by removing then reinserting a key.

```
d = {0: "x", 2: "z", 1: "y"}
[k for k in d]  # [0, 2, 1]
d.pop(2)
d[0], d[2] = "a", "b"
0 in d, "a" in d  # (True, False)
[(k, v) for k, v in d.items()]  # [(0, "a"), (1, "y"), (2, "b")]

```

There are four ways to construct a dictionary:

1. A dictionary expression `{k: v, ...}` yields a new dictionary with the specified key/value entries, inserted in the order they appear in the expression. Evaluation fails if any two key expressions yield the same value.

2. A dictionary comprehension `{k: v for vars in seq}` yields a new dictionary into which each key/value pair is inserted in loop iteration order. Duplicates are permitted: the first insertion of a given key determines its position in the sequence, and the last determines its associated value.


   ```
   {k: v for k, v in (("a", 0), ("b", 1), ("a", 2))}  # {"a": 2, "b": 1}
   {i: 2*i for i in range(3)}  # {0: 0, 1: 2, 2: 4}

   ```

3. A call to the built-in [dict](../globals/all.html#dict) function returns a dictionary containing the specified entries, which are inserted in argument order, positional arguments before named. As with comprehensions, duplicate keys are permitted.

4. The union expression `x | y` yields a new dictionary by combining two existing dictionaries. If the two dictionaries have a key `k` in common, the right hand side dictionary's value of the key (in other words, `y[k]`) wins. The `|=` variant of the union operator modifies a dictionary in-place. Example:



   ```
   d = \{"foo": "FOO", "bar": "BAR"\} | \{"foo": "FOO2", "baz": "BAZ"\}
   # d == \{"foo": "FOO2", "bar": "BAR", "baz": "BAZ"\}
   d = {"a": 1, "b": 2}
   d |= {"b": 3, "c": 4}
   # d == {"a": 1, "b": 3, "c": 4}
   ```


## Members

- [clear](#clear)
- [get](#get)
- [items](#items)
- [keys](#keys)
- [pop](#pop)
- [popitem](#popitem)
- [setdefault](#setdefault)
- [update](#update)
- [values](#values)

## clear

```
None dict.clear()
```

 Remove all items from the dictionary.



## get

```
unknown dict.get(key, default=None)
```

 Returns the value for `key` if `key` is in the dictionary, else `default`. If `default` is not given, it defaults to `None`, so that this method never throws an error.


### Parameters

ParameterDescription`key`
 required

 The key to look for.
 `default`
 default is `None`

 The default value to use (instead of None) if the key is not found.


## items

```
list dict.items()
```

 Returns the list of key-value tuples:

```
{2: "a", 4: "b", 1: "c"}.items() == [(2, "a"), (4, "b"), (1, "c")]
```

## keys

```
list dict.keys()
```

 Returns the list of keys:

```
{2: "a", 4: "b", 1: "c"}.keys() == [2, 4, 1]
```

## pop

```
unknown dict.pop(key, default=unbound)
```

 Removes a `key` from the dict, and returns the associated value. If no entry with that key was found, remove nothing and return the specified `default` value; if no default value was specified, fail instead.


### Parameters

ParameterDescription`key`
 required

 The key.
 `default`
 default is `unbound`

 a default value if the key is absent.


## popitem

```
tuple dict.popitem()
```

 Remove and return the first `(key, value)` pair from the dictionary. `popitem` is useful to destructively iterate over a dictionary, as often used in set algorithms. If the dictionary is empty, the `popitem` call fails.



## setdefault

```
unknown dict.setdefault(key, default=None)
```

 If `key` is in the dictionary, return its value. If not, insert key with a value of `default` and return `default`. `default` defaults to `None`.


### Parameters

ParameterDescription`key`
 required

 The key.
 `default`
 default is `None`

 a default value if the key is absent.


## update

```
None dict.update(pairs=[], **kwargs)
```

 Updates the dictionary first with the optional positional argument, `pairs`, then with the optional keyword arguments
If the positional argument is present, it must be a dict, iterable, or None.
If it is a dict, then its key/value pairs are inserted into this dict. If it is an iterable, it must provide a sequence of pairs (or other iterables of length 2), each of which is treated as a key/value pair to be inserted.
Each keyword argument `name=value` causes the name/value pair to be inserted into this dict.


### Parameters

ParameterDescription`pairs`
 default is `[]`

 Either a dictionary or a list of entries. Entries must be tuples or lists with exactly two elements: key, value.
 `kwargs`
 required

 Dictionary of additional entries.


## values

```
list dict.values()
```

 Returns the list of values:

```
{2: "a", 4: "b", 1: "c"}.values() == ["a", "b", "c"]
```

---

## float
- URL: https://bazel.build/rules/lib/core/float
- Source: rules/lib/core/float.mdx
- Slug: /rules/lib/core/float

The type of floating-point numbers in Starlark.

---

## function
- URL: https://bazel.build/rules/lib/core/function
- Source: rules/lib/core/function.mdx
- Slug: /rules/lib/core/function

The type of functions declared in Starlark.

---

## int
- URL: https://bazel.build/rules/lib/core/int
- Source: rules/lib/core/int.mdx
- Slug: /rules/lib/core/int

The type of integers in Starlark. Starlark integers may be of any magnitude; arithmetic is exact. Examples of integer expressions:

```
153
0x2A  # hexadecimal literal
0o54  # octal literal
23 * 2 + 5
100 / -7
100 % -7  # -5 (unlike in some other languages)
int("18")

```

---

## json
- URL: https://bazel.build/rules/lib/core/json
- Source: rules/lib/core/json.mdx
- Slug: /rules/lib/core/json

Module json is a Starlark module of JSON-related functions.

## Members

- [decode](#decode)
- [encode](#encode)
- [encode\_indent](#encode_indent)
- [indent](#indent)

## decode

```
unknown json.decode(x, default=unbound)
```

 The decode function has one required positional parameter: a JSON string.
It returns the Starlark value that the string denotes.

- `"null"`, `"true"` and `"false"` are parsed as `None`, `True`, and `False`.

- Numbers are parsed as int, or as a float if they contain a decimal point or an exponent. Although JSON has no syntax for non-finite values, very large values may be decoded as infinity.

- a JSON object is parsed as a new unfrozen Starlark dict. If the same key string occurs more than once in the object, the last value for the key is kept.

- a JSON array is parsed as new unfrozen Starlark list.


If `x` is not a valid JSON encoding and the optional `default` parameter is specified (including specified as `None`), this function returns the `default` value.
If `x` is not a valid JSON encoding and the optional `default` parameter is _not_ specified, this function fails.


### Parameters

ParameterDescription`x`[string](../core/string.html);
 required

 JSON string to decode.
 `default`
 default is `unbound`

 If specified, the value to return when `x` cannot be decoded.


## encode

```
string json.encode(x)
```

The encode function accepts one required positional argument, which it converts to JSON by cases:

- None, True, and False are converted to 'null', 'true', and 'false', respectively.

- An int, no matter how large, is encoded as a decimal integer. Some decoders may not be able to decode very large integers.

- A float is encoded using a decimal point or an exponent or both, even if its numeric value is an integer. It is an error to encode a non-finite floating-point value.

- A string value is encoded as a JSON string literal that denotes the value. Each unpaired surrogate is replaced by U+FFFD.

- A dict is encoded as a JSON object, in lexicographical key order. It is an error if any key is not a string.

- A list or tuple is encoded as a JSON array.

- A struct-like value is encoded as a JSON object, in field name order.


An application-defined type may define its own JSON encoding.
Encoding any other value yields an error.



### Parameters

ParameterDescription`x`
 required

## encode\_indent

```
string json.encode_indent(x, *, prefix='', indent='\t')
```

 The encode\_indent function is equivalent to `json.indent(json.encode(x), ...)`. See `indent` for description of formatting parameters.


### Parameters

ParameterDescription`x`
 required

`prefix`[string](../core/string.html);
 default is `''`

`indent`[string](../core/string.html);
 default is `'\t'`

## indent

```
string json.indent(s, *, prefix='', indent='\t')
```

 The indent function returns the indented form of a valid JSON-encoded string.
Each array element or object field appears on a new line, beginning with the prefix string followed by one or more copies of the indent string, according to its nesting depth.
The function accepts one required positional parameter, the JSON string,
and two optional keyword-only string parameters, prefix and indent,
that specify a prefix of each new line, and the unit of indentation.
If the input is not valid, the function may fail or return invalid output.



### Parameters

ParameterDescription`s`[string](../core/string.html);
 required

`prefix`[string](../core/string.html);
 default is `''`

`indent`[string](../core/string.html);
 default is `'\t'`

---

## list
- URL: https://bazel.build/rules/lib/core/list
- Source: rules/lib/core/list.mdx
- Slug: /rules/lib/core/list

The built-in list type. Example list expressions:

```
x = [1, 2, 3]
```

Accessing elements is possible using indexing (starts from `0`):

```
e = x[1]   # e == 2
```

Lists support the `+` operator to concatenate two lists. Example:

```
x = [1, 2] + [3, 4]   # x == [1, 2, 3, 4]
x = ["a", "b"]
x += ["c"]            # x == ["a", "b", "c"]
```

Similar to strings, lists support slice operations:

```
['a', 'b', 'c', 'd'][1:3]   # ['b', 'c']
['a', 'b', 'c', 'd'][::2]  # ['a', 'c']
['a', 'b', 'c', 'd'][3:0:-1]  # ['d', 'c', 'b']
```

Lists are mutable, as in Python.

## Members

- [append](#append)
- [clear](#clear)
- [extend](#extend)
- [index](#index)
- [insert](#insert)
- [pop](#pop)
- [remove](#remove)

## append

```
None list.append(item)
```

 Adds an item to the end of the list.


### Parameters

ParameterDescription`item`
 required

 Item to add at the end.


## clear

```
None list.clear()
```

 Removes all the elements of the list.



## extend

```
None list.extend(items)
```

 Adds all items to the end of the list.


### Parameters

ParameterDescription`items`
 iterable;
 required

 Items to add at the end.


## index

```
int list.index(x, start=unbound, end=unbound)
```

 Returns the index in the list of the first item whose value is x. It is an error if there is no such item.


### Parameters

ParameterDescription`x`
 required

 The object to search.
 `start`[int](../core/int.html);
 default is `unbound`

 The start index of the list portion to inspect.
 `end`[int](../core/int.html);
 default is `unbound`

 The end index of the list portion to inspect.


## insert

```
None list.insert(index, item)
```

 Inserts an item at a given position.


### Parameters

ParameterDescription`index`[int](../core/int.html);
 required

 The index of the given position.
 `item`
 required

 The item.


## pop

```
unknown list.pop(i=-1)
```

 Removes the item at the given position in the list, and returns it. If no `index` is specified, it removes and returns the last item in the list.


### Parameters

ParameterDescription`i`[int](../core/int.html);
 default is `-1`

 The index of the item.


## remove

```
None list.remove(x)
```

 Removes the first item from the list whose value is x. It is an error if there is no such item.


### Parameters

ParameterDescription`x`
 required

 The object to remove.

---

## range
- URL: https://bazel.build/rules/lib/core/range
- Source: rules/lib/core/range.mdx
- Slug: /rules/lib/core/range

A language built-in type to support ranges. Example of range literal:

```
x = range(1, 10, 3)
```

Accessing elements is possible using indexing (starts from `0`):

```
e = x[1]   # e == 2
```

Ranges do not support the `+` operator for concatenation.Similar to strings, ranges support slice operations:

```
range(10)[1:3]   # range(1, 3)
range(10)[::2]  # range(0, 10, 2)
range(10)[3:0:-1]  # range(3, 0, -1)
```

Ranges are immutable, as in Python 3.

---

## set
- URL: https://bazel.build/rules/lib/core/set
- Source: rules/lib/core/set.mdx
- Slug: /rules/lib/core/set

The built-in set type. A set is a mutable collection of unique values – the set's
_elements_. The [type name](../globals/all#type) of a set is `"set"`.

Sets provide constant-time operations to insert, remove, or check for the presence of a value.
Sets are implemented using a hash table, and therefore, just like keys of a
[dictionary](../dict), elements of a set must be hashable. A value may be used as an
element of a set if and only if it may be used as a key of a dictionary.

Sets may be constructed using the [`set()`](../globals/all#set) built-in
function, which returns a new set containing the unique elements of its optional argument, which
must be an iterable. Calling `set()` without an argument constructs an empty set. Sets
have no literal syntax.

The `in` and `not in` operations check whether a value is (or is not) in a
set:

```
s = set(["a", "b", "c"])
"a" in s  # True
"z" in s  # False

```

A set is iterable, and thus may be used as the operand of a `for` loop, a list
comprehension, and the various built-in functions that operate on iterables. Its length can be
retrieved using the [`len()`](../globals/all#len) built-in function, and the
order of iteration is the order in which elements were first added to the set:

```
s = set(["z", "y", "z", "y"])
len(s)       # prints 2
s.add("x")
len(s)       # prints 3
for e in s:
    print e  # prints "z", "y", "x"

```

A set used in Boolean context is true if and only if it is non-empty.

```
s = set()
"non-empty" if s else "empty"  # "empty"
t = set(["x", "y"])
"non-empty" if t else "empty"  # "non-empty"

```

Sets may be compared for equality or inequality using `==` and `!=`. A set
`s` is equal to `t` if and only if `t` is a set containing the same
elements; iteration order is not significant. In particular, a set is _not_ equal to the list
of its elements. Sets are not ordered with respect to other sets, and an attempt to compare two sets
using `<`, `<=`, `>`, `>=`, or to sort a
sequence of sets, will fail.

```
set() == set()              # True
set() != []                 # True
set([1, 2]) == set([2, 1])  # True
set([1, 2]) != [1, 2]       # True

```

The `|` operation on two sets returns the union of the two sets: a set containing the
elements found in either one or both of the original sets.

```
set([1, 2]) | set([3, 2])  # set([1, 2, 3])

```

The `&` operation on two sets returns the intersection of the two sets: a set
containing only the elements found in both of the original sets.

```
set([1, 2]) & set([2, 3])  # set([2])
set([1, 2]) & set([3, 4])  # set()

```

The `-` operation on two sets returns the difference of the two sets: a set containing
the elements found in the left-hand side set but not the right-hand side set.

```
set([1, 2]) - set([2, 3])  # set([1])
set([1, 2]) - set([3, 4])  # set([1, 2])

```

The `^` operation on two sets returns the symmetric difference of the two sets: a set
containing the elements found in exactly one of the two original sets, but not in both.

```
set([1, 2]) ^ set([2, 3])  # set([1, 3])
set([1, 2]) ^ set([3, 4])  # set([1, 2, 3, 4])

```

In each of the above operations, the elements of the resulting set retain their order from the
two operand sets, with all elements that were drawn from the left-hand side ordered before any
element that was only present in the right-hand side.

The corresponding augmented assignments, `|=`, `&=`, `-=`,
and `^=`, modify the left-hand set in place.

```
s = set([1, 2])
s |= set([2, 3, 4])     # s now equals set([1, 2, 3, 4])
s &= set([0, 1, 2, 3])  # s now equals set([1, 2, 3])
s -= set([0, 1])        # s now equals set([2, 3])
s ^= set([3, 4])        # s now equals set([2, 4])

```

Like all mutable values in Starlark, a set can be frozen, and once frozen, all subsequent
operations that attempt to update it will fail.

## Members

- [add](#add)
- [clear](#clear)
- [difference](#difference)
- [difference\_update](#difference_update)
- [discard](#discard)
- [intersection](#intersection)
- [intersection\_update](#intersection_update)
- [isdisjoint](#isdisjoint)
- [issubset](#issubset)
- [issuperset](#issuperset)
- [pop](#pop)
- [remove](#remove)
- [symmetric\_difference](#symmetric_difference)
- [symmetric\_difference\_update](#symmetric_difference_update)
- [union](#union)
- [update](#update)

## add

```
None set.add(element)
```

 Adds an element to the set.

It is permissible to `add` a value already present in the set; this leaves the set
unchanged.

If you need to add multiple elements to a set, see [`update`](#update) or
the `|=` augmented assignment operation.



### Parameters

ParameterDescription`element`
 required

 Element to add.


## clear

```
None set.clear()
```

 Removes all the elements of the set.



## difference

```
set set.difference(*others)
```

 Returns a new mutable set containing the difference of this set with others.

If `s` and `t` are sets, `s.difference(t)` is equivalent to
`s - t`; however, note that the `-` operation requires both sides to be sets,
while the `difference` method also accepts sequences and dicts.

It is permissible to call `difference` without any arguments; this returns a copy of
the set.

For example,

```
set([1, 2, 3]).difference([2])             # set([1, 3])
set([1, 2, 3]).difference([0, 1], [3, 4])  # set([2])

```

### Parameters

ParameterDescription`others`
 required

 Collections of hashable elements.


## difference\_update

```
None set.difference_update(*others)
```

 Removes any elements found in any others from this set.

If `s` and `t` are sets, `s.difference_update(t)` is equivalent
to `s -= t`; however, note that the `-=` augmented assignment requires both
sides to be sets, while the `difference_update` method also accepts sequences and dicts.

It is permissible to call `difference_update` without any arguments; this leaves the
set unchanged.

For example,

```
s = set([1, 2, 3, 4])
s.difference_update([2])             # None; s is set([1, 3, 4])
s.difference_update([0, 1], [4, 5])  # None; s is set([3])

```

### Parameters

ParameterDescription`others`
 required

 Collections of hashable elements.


## discard

```
None set.discard(element)
```

 Removes an element from the set if it is present.

It is permissible to `discard` a value not present in the set; this leaves the set
unchanged. If you want to fail on an attempt to remove a non-present element, use
[`remove`](#remove) instead. If you need to remove multiple elements from a
set, see [`difference_update`](#difference_update) or the `-=`
augmented assignment operation.

For example,

```
s = set(["x", "y"])
s.discard("y")  # None; s == set(["x"])
s.discard("y")  # None; s == set(["x"])

```

### Parameters

ParameterDescription`element`
 required

 Element to discard. Must be hashable.


## intersection

```
set set.intersection(*others)
```

 Returns a new mutable set containing the intersection of this set with others.

If `s` and `t` are sets, `s.intersection(t)` is equivalent to
`s & t`; however, note that the `&` operation requires both sides to
be sets, while the `intersection` method also accepts sequences and dicts.

It is permissible to call `intersection` without any arguments; this returns a copy of
the set.

For example,

```
set([1, 2]).intersection([2, 3])             # set([2])
set([1, 2, 3]).intersection([0, 1], [1, 2])  # set([1])

```

### Parameters

ParameterDescription`others`
 required

 Collections of hashable elements.


## intersection\_update

```
None set.intersection_update(*others)
```

 Removes any elements not found in all others from this set.

If `s` and `t` are sets, `s.intersection_update(t)` is
equivalent to `s &= t`; however, note that the `&=` augmented
assignment requires both sides to be sets, while the `intersection_update` method also
accepts sequences and dicts.

It is permissible to call `intersection_update` without any arguments; this leaves the
set unchanged.

For example,

```
s = set([1, 2, 3, 4])
s.intersection_update([0, 1, 2])       # None; s is set([1, 2])
s.intersection_update([0, 1], [1, 2])  # None; s is set([1])

```

### Parameters

ParameterDescription`others`
 required

 Collections of hashable elements.


## isdisjoint

```
bool set.isdisjoint(other)
```

 Returns true if this set has no elements in common with another.

For example,

```
set([1, 2]).isdisjoint([3, 4])  # True
set().isdisjoint(set())         # True
set([1, 2]).isdisjoint([2, 3])  # False

```

### Parameters

ParameterDescription`other`
 required

 A collection of hashable elements.


## issubset

```
bool set.issubset(other)
```

 Returns true of this set is a subset of another.

Note that a set is always considered to be a subset of itself.

For example,

```
set([1, 2]).issubset([1, 2, 3])  # True
set([1, 2]).issubset([1, 2])     # True
set([1, 2]).issubset([2, 3])     # False

```

### Parameters

ParameterDescription`other`
 required

 A collection of hashable elements.


## issuperset

```
bool set.issuperset(other)
```

 Returns true of this set is a superset of another.

Note that a set is always considered to be a superset of itself.

For example,

```
set([1, 2, 3]).issuperset([1, 2])     # True
set([1, 2, 3]).issuperset([1, 2, 3])  # True
set([1, 2, 3]).issuperset([2, 3, 4])  # False

```

### Parameters

ParameterDescription`other`
 required

 A collection of hashable elements.


## pop

```
unknown set.pop()
```

 Removes and returns the first element of the set (in iteration order, which is the order in which
elements were first added to the set).

Fails if the set is empty.

For example,

```
s = set([3, 1, 2])
s.pop()  # 3; s == set([1, 2])
s.pop()  # 1; s == set([2])
s.pop()  # 2; s == set()
s.pop()  # error: empty set

```

## remove

```
None set.remove(element)
```

 Removes an element, which must be present in the set, from the set.

`remove` fails if the element was not present in the set. If you don't want to fail on
an attempt to remove a non-present element, use [`discard`](#discard) instead.
If you need to remove multiple elements from a set, see
[`difference_update`](#difference_update) or the `-=` augmented
assignment operation.



### Parameters

ParameterDescription`element`
 required

 Element to remove. Must be an element of the set (and hashable).


## symmetric\_difference

```
set set.symmetric_difference(other)
```

 Returns a new mutable set containing the symmetric difference of this set with another collection of
hashable elements.

If `s` and `t` are sets, `s.symmetric_difference(t)` is
equivalent to `s ^ t`; however, note that the `^` operation requires both
sides to be sets, while the `symmetric_difference` method also accepts a sequence or a
dict.

For example,

```
set([1, 2]).symmetric_difference([2, 3])  # set([1, 3])

```

### Parameters

ParameterDescription`other`
 required

 A collection of hashable elements.


## symmetric\_difference\_update

```
None set.symmetric_difference_update(other)
```

 Returns a new mutable set containing the symmetric difference of this set with another collection of
hashable elements.

If `s` and `t` are sets, `s.symmetric_difference_update(t)` is
equivalent to \`s ^= t `; however, note that the ` ^=\` augmented assignment requires both
sides to be sets, while the `symmetric_difference_update` method also accepts a sequence
or a dict.

For example,

```
s = set([1, 2])
s.symmetric_difference_update([2, 3])  # None; s == set([1, 3])

```

### Parameters

ParameterDescription`other`
 required

 A collection of hashable elements.


## union

```
set set.union(*others)
```

 Returns a new mutable set containing the union of this set with others.

If `s` and `t` are sets, `s.union(t)` is equivalent to
`s | t`; however, note that the `|` operation requires both sides to be sets,
while the `union` method also accepts sequences and dicts.

It is permissible to call `union` without any arguments; this returns a copy of the
set.

For example,

```
set([1, 2]).union([2, 3])                    # set([1, 2, 3])
set([1, 2]).union([2, 3], {3: "a", 4: "b"})  # set([1, 2, 3, 4])

```

### Parameters

ParameterDescription`others`
 required

 Collections of hashable elements.


## update

```
None set.update(*others)
```

 Adds the elements found in others to this set.

For example,

```
s = set()
s.update([1, 2])          # None; s is set([1, 2])
s.update([2, 3], [3, 4])  # None; s is set([1, 2, 3, 4])

```

If `s` and `t` are sets, `s.update(t)` is equivalent to
`s |= t`; however, note that the `|=` augmented assignment requires both sides
to be sets, while the `update` method also accepts sequences and dicts.

It is permissible to call `update` without any arguments; this leaves the set
unchanged.



### Parameters

ParameterDescription`others`
 required

 Collections of hashable elements.

---

## string
- URL: https://bazel.build/rules/lib/core/string
- Source: rules/lib/core/string.mdx
- Slug: /rules/lib/core/string

A language built-in type to support strings. Examples of string literals:

```
a = 'abc\ndef'
b = "ab'cd"
c = """multiline string"""

# Strings support slicing (negative index starts from the end):
x = "hello"[2:4]  # "ll"
y = "hello"[1:-1]  # "ell"
z = "hello"[:4]  # "hell"
# Slice steps can be used, too:
s = "hello"[::2] # "hlo"
t = "hello"[3:0:-1] # "lle"

```

Strings are not directly iterable, use the `.elems()` method to iterate over their characters. Examples:

```
"bc" in "abcd"   # evaluates to True
x = [c for c in "abc".elems()]  # x == ["a", "b", "c"]
```

Implicit concatenation of strings is not allowed; use the `+` operator instead. Comparison operators perform a lexicographical comparison; use `==` to test for equality.

## Members

- [capitalize](#capitalize)
- [count](#count)
- [elems](#elems)
- [endswith](#endswith)
- [find](#find)
- [format](#format)
- [index](#index)
- [isalnum](#isalnum)
- [isalpha](#isalpha)
- [isdigit](#isdigit)
- [islower](#islower)
- [isspace](#isspace)
- [istitle](#istitle)
- [isupper](#isupper)
- [join](#join)
- [lower](#lower)
- [lstrip](#lstrip)
- [partition](#partition)
- [removeprefix](#removeprefix)
- [removesuffix](#removesuffix)
- [replace](#replace)
- [rfind](#rfind)
- [rindex](#rindex)
- [rpartition](#rpartition)
- [rsplit](#rsplit)
- [rstrip](#rstrip)
- [split](#split)
- [splitlines](#splitlines)
- [startswith](#startswith)
- [strip](#strip)
- [title](#title)
- [upper](#upper)

## capitalize

```
string string.capitalize()
```

 Returns a copy of the string with its first character (if any) capitalized and the rest lowercased. This method does not support non-ascii characters.



## count

```
int string.count(sub, start=0, end=None)
```

 Returns the number of (non-overlapping) occurrences of substring `sub` in string, optionally restricting to `[start:end]`, `start` being inclusive and `end` being exclusive.


### Parameters

ParameterDescription`sub`[string](../core/string.html);
 required

 The substring to count.
 `start`[int](../core/int.html); or `None`;
 default is `0`

 Restrict to search from this position.
 `end`[int](../core/int.html); or `None`;
 default is `None`

 optional position before which to restrict to search.


## elems

```
sequence string.elems()
```

 Returns an iterable value containing successive 1-element substrings of the string. Equivalent to `[s[i] for i in range(len(s))]`, except that the returned value might not be a list.



## endswith

```
bool string.endswith(sub, start=0, end=None)
```

 Returns True if the string ends with `sub`, otherwise False, optionally restricting to `[start:end]`, `start` being inclusive and `end` being exclusive.


### Parameters

ParameterDescription`sub`[string](../core/string.html); or [tuple](../core/tuple.html) of [string](../core/string.html) s;
 required

 The suffix (or tuple of alternative suffixes) to match.
 `start`[int](../core/int.html); or `None`;
 default is `0`

 Test beginning at this position.
 `end`[int](../core/int.html); or `None`;
 default is `None`

 optional position at which to stop comparing.


## find

```
int string.find(sub, start=0, end=None)
```

 Returns the first index where `sub` is found, or -1 if no such index exists, optionally restricting to `[start:end]`, `start` being inclusive and `end` being exclusive.


### Parameters

ParameterDescription`sub`[string](../core/string.html);
 required

 The substring to find.
 `start`[int](../core/int.html); or `None`;
 default is `0`

 Restrict to search from this position.
 `end`[int](../core/int.html); or `None`;
 default is `None`

 optional position before which to restrict to search.


## format

```
string string.format(*args, **kwargs)
```

 Perform string interpolation. Format strings contain replacement fields surrounded by curly braces `{}`. Anything that is not contained in braces is considered literal text, which is copied unchanged to the output.If you need to include a brace character in the literal text, it can be escaped by doubling: `{{` and `}}` A replacement field can be either a name, a number, or empty. Values are converted to strings using the [str](../globals/all.html#str) function.

```
# Access in order:
"{} < {}".format(4, 5) == "4 < 5"
# Access by position:
"{1}, {0}".format(2, 1) == "1, 2"
# Access by name:
"x{key}x".format(key = 2) == "x2x"
```

### Parameters

ParameterDescription`args`
 default is `()`

 List of arguments.
 `kwargs`
 default is `{}`

 Dictionary of arguments.


## index

```
int string.index(sub, start=0, end=None)
```

 Returns the first index where `sub` is found, or raises an error if no such index exists, optionally restricting to `[start:end]` `start` being inclusive and `end` being exclusive.


### Parameters

ParameterDescription`sub`[string](../core/string.html);
 required

 The substring to find.
 `start`[int](../core/int.html); or `None`;
 default is `0`

 Restrict to search from this position.
 `end`[int](../core/int.html); or `None`;
 default is `None`

 optional position before which to restrict to search.


## isalnum

```
bool string.isalnum()
```

 Returns True if all characters in the string are alphanumeric (\[a-zA-Z0-9\]) and there is at least one character.



## isalpha

```
bool string.isalpha()
```

 Returns True if all characters in the string are alphabetic (\[a-zA-Z\]) and there is at least one character.



## isdigit

```
bool string.isdigit()
```

 Returns True if all characters in the string are digits (\[0-9\]) and there is at least one character.



## islower

```
bool string.islower()
```

 Returns True if all cased characters in the string are lowercase and there is at least one character.



## isspace

```
bool string.isspace()
```

 Returns True if all characters are white space characters and the string contains at least one character.



## istitle

```
bool string.istitle()
```

 Returns True if the string is in title case and it contains at least one character. This means that every uppercase character must follow an uncased one (e.g. whitespace) and every lowercase character must follow a cased one (e.g. uppercase or lowercase).



## isupper

```
bool string.isupper()
```

 Returns True if all cased characters in the string are uppercase and there is at least one character.



## join

```
string string.join(elements)
```

 Returns a string in which the string elements of the argument have been joined by this string as a separator. Example:

```
"|".join(["a", "b", "c"]) == "a|b|c"
```

### Parameters

ParameterDescription`elements`
 iterable of [string](../core/string.html) s;
 required

 The objects to join.


## lower

```
string string.lower()
```

 Returns the lower case version of this string.



## lstrip

```
string string.lstrip(chars=None)
```

 Returns a copy of the string where leading characters that appear in `chars` are removed. Note that `chars` is not a prefix: all combinations of its value are removed:

```
"abcba".lstrip("ba") == "cba"
```

### Parameters

ParameterDescription`chars`[string](../core/string.html); or `None`;
 default is `None`

 The characters to remove, or all whitespace if None.


## partition

```
tuple string.partition(sep)
```

 Splits the input string at the first occurrence of the separator `sep` and returns the resulting partition as a three-element tuple of the form (before, separator, after). If the input string does not contain the separator, partition returns (self, '', '').


### Parameters

ParameterDescription`sep`[string](../core/string.html);
 required

 The string to split on.


## removeprefix

```
string string.removeprefix(prefix)
```

 If the string starts with `prefix`, returns a new string with the prefix removed. Otherwise, returns the string.


### Parameters

ParameterDescription`prefix`[string](../core/string.html);
 required

 The prefix to remove if present.


## removesuffix

```
string string.removesuffix(suffix)
```

 If the string ends with `suffix`, returns a new string with the suffix removed. Otherwise, returns the string.


### Parameters

ParameterDescription`suffix`[string](../core/string.html);
 required

 The suffix to remove if present.


## replace

```
string string.replace(old, new, count=-1)
```

 Returns a copy of the string in which the occurrences of `old` have been replaced with `new`, optionally restricting the number of replacements to `count`.


### Parameters

ParameterDescription`old`[string](../core/string.html);
 required

 The string to be replaced.
 `new`[string](../core/string.html);
 required

 The string to replace with.
 `count`[int](../core/int.html);
 default is `-1`

 The maximum number of replacements. If omitted, or if the value is negative, there is no limit.


## rfind

```
int string.rfind(sub, start=0, end=None)
```

 Returns the last index where `sub` is found, or -1 if no such index exists, optionally restricting to `[start:end]`, `start` being inclusive and `end` being exclusive.


### Parameters

ParameterDescription`sub`[string](../core/string.html);
 required

 The substring to find.
 `start`[int](../core/int.html); or `None`;
 default is `0`

 Restrict to search from this position.
 `end`[int](../core/int.html); or `None`;
 default is `None`

 optional position before which to restrict to search.


## rindex

```
int string.rindex(sub, start=0, end=None)
```

 Returns the last index where `sub` is found, or raises an error if no such index exists, optionally restricting to `[start:end]`, `start` being inclusive and `end` being exclusive.


### Parameters

ParameterDescription`sub`[string](../core/string.html);
 required

 The substring to find.
 `start`[int](../core/int.html); or `None`;
 default is `0`

 Restrict to search from this position.
 `end`[int](../core/int.html); or `None`;
 default is `None`

 optional position before which to restrict to search.


## rpartition

```
tuple string.rpartition(sep)
```

 Splits the input string at the last occurrence of the separator `sep` and returns the resulting partition as a three-element tuple of the form (before, separator, after). If the input string does not contain the separator, rpartition returns ('', '', self).


### Parameters

ParameterDescription`sep`[string](../core/string.html);
 required

 The string to split on.


## rsplit

```
list string.rsplit(sep, maxsplit=unbound)
```

 Returns a list of all the words in the string, using `sep` as the separator, optionally limiting the number of splits to `maxsplit`. Except for splitting from the right, this method behaves like split().


### Parameters

ParameterDescription`sep`[string](../core/string.html);
 required

 The string to split on.
 `maxsplit`[int](../core/int.html);
 default is `unbound`

 The maximum number of splits.


## rstrip

```
string string.rstrip(chars=None)
```

 Returns a copy of the string where trailing characters that appear in `chars` are removed. Note that `chars` is not a suffix: all combinations of its value are removed:

```
"abcbaa".rstrip("ab") == "abc"
```

### Parameters

ParameterDescription`chars`[string](../core/string.html); or `None`;
 default is `None`

 The characters to remove, or all whitespace if None.


## split

```
list string.split(sep, maxsplit=unbound)
```

 Returns a list of all the words in the string, using `sep` as the separator, optionally limiting the number of splits to `maxsplit`.


### Parameters

ParameterDescription`sep`[string](../core/string.html);
 required

 The string to split on.
 `maxsplit`[int](../core/int.html);
 default is `unbound`

 The maximum number of splits.


## splitlines

```
sequence string.splitlines(keepends=False)
```

 Splits the string at line boundaries ('\\n', '\\r\\n', '\\r') and returns the result as a new mutable list.


### Parameters

ParameterDescription`keepends`[bool](../core/bool.html);
 default is `False`

 Whether the line breaks should be included in the resulting list.


## startswith

```
bool string.startswith(sub, start=0, end=None)
```

 Returns True if the string starts with `sub`, otherwise False, optionally restricting to `[start:end]`, `start` being inclusive and `end` being exclusive.


### Parameters

ParameterDescription`sub`[string](../core/string.html); or [tuple](../core/tuple.html) of [string](../core/string.html) s;
 required

 The prefix (or tuple of alternative prefixes) to match.
 `start`[int](../core/int.html); or `None`;
 default is `0`

 Test beginning at this position.
 `end`[int](../core/int.html); or `None`;
 default is `None`

 Stop comparing at this position.


## strip

```
string string.strip(chars=None)
```

 Returns a copy of the string where leading or trailing characters that appear in `chars` are removed. Note that `chars` is neither a prefix nor a suffix: all combinations of its value are removed:

```
"aabcbcbaa".strip("ab") == "cbc"
```

### Parameters

ParameterDescription`chars`[string](../core/string.html); or `None`;
 default is `None`

 The characters to remove, or all whitespace if None.


## title

```
string string.title()
```

 Converts the input string into title case, i.e. every word starts with an uppercase letter while the remaining letters are lowercase. In this context, a word means strictly a sequence of letters. This method does not support supplementary Unicode characters.



## upper

```
string string.upper()
```

 Returns the upper case version of this string.

---

## tuple
- URL: https://bazel.build/rules/lib/core/tuple
- Source: rules/lib/core/tuple.mdx
- Slug: /rules/lib/core/tuple

The built-in tuple type. Example tuple expressions:

```
x = (1, 2, 3)
```

Accessing elements is possible using indexing (starts from `0`):

```
e = x[1]   # e == 2
```

Lists support the `+` operator to concatenate two tuples. Example:

```
x = (1, 2) + (3, 4)   # x == (1, 2, 3, 4)
x = ("a", "b")
x += ("c",)            # x == ("a", "b", "c")
```

Similar to lists, tuples support slice operations:

```
('a', 'b', 'c', 'd')[1:3]   # ('b', 'c')
('a', 'b', 'c', 'd')[::2]  # ('a', 'c')
('a', 'b', 'c', 'd')[3:0:-1]  # ('d', 'c', 'b')
```

Tuples are immutable, therefore `x[1] = "a"` is not supported.

---

## Configuration Fragments
- URL: https://bazel.build/rules/lib/fragments
- Source: rules/lib/fragments.mdx
- Slug: /rules/lib/fragments

Configuration fragments give rules access to language-specific parts of [configuration](builtins/configuration.html).

Rule implementations can get them using `ctx.fragments.[fragment name]`

- [apple](/rules/lib/fragments/apple)
- [bazel\_android](/rules/lib/fragments/bazel_android)
- [bazel\_py](/rules/lib/fragments/bazel_py)
- [coverage](/rules/lib/fragments/coverage)
- [cpp](/rules/lib/fragments/cpp)
- [j2objc](/rules/lib/fragments/j2objc)
- [java](/rules/lib/fragments/java)
- [objc](/rules/lib/fragments/objc)
- [platform](/rules/lib/fragments/platform)
- [proto](/rules/lib/fragments/proto)
- [py](/rules/lib/fragments/py)

---

## apple
- URL: https://bazel.build/rules/lib/fragments/apple
- Source: rules/lib/fragments/apple.mdx
- Slug: /rules/lib/fragments/apple

A configuration fragment for Apple platforms.

## Members

- [single\_arch\_cpu](#single_arch_cpu)
- [single\_arch\_platform](#single_arch_platform)

## single\_arch\_cpu

```
string apple.single_arch_cpu
```

 The single "effective" architecture for this configuration (e.g., `i386` or `arm64`) in the context of rule logic that is only concerned with a single architecture (such as `objc_library`, which registers single-architecture compile actions).



## single\_arch\_platform

```
apple_platform apple.single_arch_platform
```

 The platform of the current configuration. This should only be invoked in a context where only a single architecture may be supported; consider [multi\_arch\_platform](#multi_arch_platform) for other cases.

---

## bazel\_android
- URL: https://bazel.build/rules/lib/fragments/bazel_android
- Source: rules/lib/fragments/bazel_android.mdx
- Slug: /rules/lib/fragments/bazel_android

## Members

- [merge\_android\_manifest\_permissions](#merge_android_manifest_permissions)

## merge\_android\_manifest\_permissions

```
bool bazel_android.merge_android_manifest_permissions
```

 The value of --merge\_android\_manifest\_permissions flag.

---

## bazel\_py
- URL: https://bazel.build/rules/lib/fragments/bazel_py
- Source: rules/lib/fragments/bazel_py.mdx
- Slug: /rules/lib/fragments/bazel_py

## Members

- [python\_import\_all\_repositories](#python_import_all_repositories)
- [python\_path](#python_path)

## python\_import\_all\_repositories

```
bool bazel_py.python_import_all_repositories
```

 The value of the --experimental\_python\_import\_all\_repositories flag.



## python\_path

```
string bazel_py.python_path
```

 The value of the --python\_path flag.

---

## coverage
- URL: https://bazel.build/rules/lib/fragments/coverage
- Source: rules/lib/fragments/coverage.mdx
- Slug: /rules/lib/fragments/coverage

A configuration fragment representing the coverage configuration.

## Members

- [output\_generator](#output_generator)

## output\_generator

```
Label coverage.output_generator
```

 Returns the label pointed to by the [`--coverage_output_generator`](https://bazel.build/reference/command-line-reference#flag--coverage_output_generator) option if coverage collection is enabled, otherwise returns `None`. Can be accessed with [`configuration_field`](../globals/bzl.html#configuration_field):

```
attr.label(
    default = configuration_field(
        fragment = "coverage",
        name = "output_generator"
    )
)
```

 May return `None`.

---

## cpp
- URL: https://bazel.build/rules/lib/fragments/cpp
- Source: rules/lib/fragments/cpp.mdx
- Slug: /rules/lib/fragments/cpp

A configuration fragment for C++.

## Members

- [apple\_generate\_dsym](#apple_generate_dsym)
- [conlyopts](#conlyopts)
- [copts](#copts)
- [custom\_malloc](#custom_malloc)
- [cxxopts](#cxxopts)
- [linkopts](#linkopts)
- [objc\_generate\_linkmap](#objc_generate_linkmap)
- [objc\_should\_strip\_binary](#objc_should_strip_binary)
- [objccopts](#objccopts)

## apple\_generate\_dsym

```
bool cpp.apple_generate_dsym
```

 Whether to generate Apple debug symbol(.dSYM) artifacts.



## conlyopts

```
list cpp.conlyopts
```

 The flags passed to Bazel by [`--conlyopt`](/docs/user-manual#flag--conlyopt) option.



## copts

```
list cpp.copts
```

 The flags passed to Bazel by [`--copt`](/docs/user-manual#flag--copt) option.



## custom\_malloc

```
Label cpp.custom_malloc
```

 Returns label pointed to by [`--custom_malloc`](/docs/user-manual#flag--custom_malloc) option. Can be accessed with [`configuration_field`](../globals/bzl.html#configuration_field):

```
attr.label(
    default = configuration_field(
        fragment = "cpp",
        name = "custom_malloc"
    )
)
```

 May return `None`.



## cxxopts

```
list cpp.cxxopts
```

 The flags passed to Bazel by [`--cxxopt`](/docs/user-manual#flag--cxxopt) option.



## linkopts

```
list cpp.linkopts
```

 The flags passed to Bazel by [`--linkopt`](/docs/user-manual#flag--linkopt) option.



## objc\_generate\_linkmap

```
bool cpp.objc_generate_linkmap
```

 (Apple-only) Whether to generate linkmap artifacts.



## objc\_should\_strip\_binary

```
bool cpp.objc_should_strip_binary
```

 (Apple-only) whether to perform symbol and dead-code strippings on linked binaries.



## objccopts

```
list cpp.objccopts
```

 The flags passed to Bazel by [`--objccopt`](/docs/user-manual#flag--objccopt) option.

---

## j2objc
- URL: https://bazel.build/rules/lib/fragments/j2objc
- Source: rules/lib/fragments/j2objc.mdx
- Slug: /rules/lib/fragments/j2objc

A configuration fragment for j2Objc.

## Members

- [translation\_flags](#translation_flags)

## translation\_flags

```
list j2objc.translation_flags
```

 The list of flags to be used when the j2objc compiler is invoked.

---

## java
- URL: https://bazel.build/rules/lib/fragments/java
- Source: rules/lib/fragments/java.mdx
- Slug: /rules/lib/fragments/java

A java compiler configuration.

## Members

- [bytecode\_optimization\_pass\_actions](#bytecode_optimization_pass_actions)
- [bytecode\_optimizer\_mnemonic](#bytecode_optimizer_mnemonic)
- [default\_javac\_flags](#default_javac_flags)
- [default\_javac\_flags\_depset](#default_javac_flags_depset)
- [default\_jvm\_opts](#default_jvm_opts)
- [disallow\_java\_import\_exports](#disallow_java_import_exports)
- [multi\_release\_deploy\_jars](#multi_release_deploy_jars)
- [one\_version\_enforcement\_level](#one_version_enforcement_level)
- [plugins](#plugins)
- [run\_android\_lint](#run_android_lint)
- [split\_bytecode\_optimization\_pass](#split_bytecode_optimization_pass)
- [strict\_java\_deps](#strict_java_deps)
- [use\_header\_compilation\_direct\_deps](#use_header_compilation_direct_deps)
- [use\_ijars](#use_ijars)

## bytecode\_optimization\_pass\_actions

```
int java.bytecode_optimization_pass_actions
```

 This specifies the number of actions to divide the OPTIMIZATION stage of the bytecode optimizer into. Note that if split\_bytecode\_optimization\_pass is set, this will only change behavior if it is > 2.



## bytecode\_optimizer\_mnemonic

```
string java.bytecode_optimizer_mnemonic
```

 The mnemonic for the bytecode optimizer.



## default\_javac\_flags

```
list java.default_javac_flags
```

 The default flags for the Java compiler.



## default\_javac\_flags\_depset

```
depset java.default_javac_flags_depset
```

 The default flags for the Java compiler.



## default\_jvm\_opts

```
list java.default_jvm_opts
```

 Additional options to pass to the Java VM for each java\_binary target



## disallow\_java\_import\_exports

```
bool java.disallow_java_import_exports()
```

 Returns true if java\_import exports are not allowed.



## multi\_release\_deploy\_jars

```
bool java.multi_release_deploy_jars
```

 The value of the --incompatible\_multi\_release\_deploy\_jars flag.



## one\_version\_enforcement\_level

```
string java.one_version_enforcement_level
```

 The value of the --experimental\_one\_version\_enforcement flag.



## plugins

```
list java.plugins
```

 A list containing the labels provided with --plugins, if any.



## run\_android\_lint

```
bool java.run_android_lint
```

 The value of the --experimental\_run\_android\_lint\_on\_java\_rules flag.



## split\_bytecode\_optimization\_pass

```
bool java.split_bytecode_optimization_pass
```

 Returns whether the OPTIMIZATION stage of the bytecode optimizer will be split across two actions.



## strict\_java\_deps

```
string java.strict_java_deps
```

 The value of the strict\_java\_deps flag.



## use\_header\_compilation\_direct\_deps

```
bool java.use_header_compilation_direct_deps()
```

 Returns true if Java header compilation should use separate outputs for direct deps.



## use\_ijars

```
bool java.use_ijars()
```

 Returns true iff Java compilation should use ijars.

---

## objc
- URL: https://bazel.build/rules/lib/fragments/objc
- Source: rules/lib/fragments/objc.mdx
- Slug: /rules/lib/fragments/objc

A configuration fragment for Objective-C.

## Members

- [alwayslink\_by\_default](#alwayslink_by_default)
- [builtin\_objc\_strip\_action](#builtin_objc_strip_action)
- [copts\_for\_current\_compilation\_mode](#copts_for_current_compilation_mode)
- [disallow\_sdk\_frameworks\_attributes](#disallow_sdk_frameworks_attributes)
- [ios\_simulator\_device](#ios_simulator_device)
- [ios\_simulator\_version](#ios_simulator_version)
- [run\_memleaks](#run_memleaks)
- [signing\_certificate\_name](#signing_certificate_name)
- [strip\_executable\_safely](#strip_executable_safely)
- [uses\_device\_debug\_entitlements](#uses_device_debug_entitlements)

## alwayslink\_by\_default

```
bool objc.alwayslink_by_default
```

 Returns whether objc\_library and objc\_import should default to alwayslink=True.



## builtin\_objc\_strip\_action

```
bool objc.builtin_objc_strip_action
```

 Returns whether to emit a strip action as part of objc linking.



## copts\_for\_current\_compilation\_mode

```
list objc.copts_for_current_compilation_mode
```

 Returns a list of default options to use for compiling Objective-C in the current mode.



## disallow\_sdk\_frameworks\_attributes

```
bool objc.disallow_sdk_frameworks_attributes
```

 Returns whether sdk\_frameworks and weak\_sdk\_frameworks are disallowed attributes.



## ios\_simulator\_device

```
string objc.ios_simulator_device
```

 The type of device (e.g. 'iPhone 6') to use when running on the simulator.
 May return `None`.



## ios\_simulator\_version

```
DottedVersion objc.ios_simulator_version
```

 The SDK version of the iOS simulator to use when running on the simulator.
 May return `None`.



## run\_memleaks

```
bool objc.run_memleaks
```

 Returns a boolean indicating whether memleaks should be run during tests or not.



## signing\_certificate\_name

```
string objc.signing_certificate_name
```

 Returns the flag-supplied certificate name to be used in signing, or None if no such certificate was specified.
 May return `None`.



## strip\_executable\_safely

```
bool objc.strip_executable_safely
```

 Returns whether executable strip action should use flag -x, which does not break dynamic symbol resolution.



## uses\_device\_debug\_entitlements

```
bool objc.uses_device_debug_entitlements
```

 Returns whether device debug entitlements should be included when signing an application.

---

## platform
- URL: https://bazel.build/rules/lib/fragments/platform
- Source: rules/lib/fragments/platform.mdx
- Slug: /rules/lib/fragments/platform

The platform configuration.

## Members

- [host\_platform](#host_platform)
- [platform](#platform)

## host\_platform

```
Label platform.host_platform
```

 The current host platform



## platform

```
Label platform.platform
```

 The current target platform

---

## proto
- URL: https://bazel.build/rules/lib/fragments/proto
- Source: rules/lib/fragments/proto.mdx
- Slug: /rules/lib/fragments/proto

A configuration fragment representing protocol buffers.

---

## py
- URL: https://bazel.build/rules/lib/fragments/py
- Source: rules/lib/fragments/py.mdx
- Slug: /rules/lib/fragments/py

A configuration fragment for Python.

## Members

- [build\_python\_zip](#build_python_zip)
- [default\_python\_version](#default_python_version)
- [default\_to\_explicit\_init\_py](#default_to_explicit_init_py)
- [disallow\_native\_rules](#disallow_native_rules)
- [include\_label\_in\_linkstamp](#include_label_in_linkstamp)
- [use\_toolchains](#use_toolchains)

## build\_python\_zip

```
bool py.build_python_zip
```

 The effective value of --build\_python\_zip



## default\_python\_version

```
string py.default_python_version
```

 No-op: PY3 is the default Python version.



## default\_to\_explicit\_init\_py

```
bool py.default_to_explicit_init_py
```

 The value from the --incompatible\_default\_to\_explicit\_init\_py flag



## disallow\_native\_rules

```
bool py.disallow_native_rules
```

 The value of the --incompatible\_python\_disallow\_native\_rules flag.



## include\_label\_in\_linkstamp

```
bool py.include_label_in_linkstamp
```

 Whether the build label is included in unstamped builds.



## use\_toolchains

```
bool py.use_toolchains
```

 No-op: Python toolchains are always used.

---

## Global functions
- URL: https://bazel.build/rules/lib/globals
- Source: rules/lib/globals.mdx
- Slug: /rules/lib/globals

This section lists the global functions available in Starlark. The list of available functions differs depending on the file type (whether a BUILD file, or a .bzl file, etc).

- [.bzl files](/rules/lib/globals/bzl)
- [All Bazel files](/rules/lib/globals/all)
- [BUILD files](/rules/lib/globals/build)
- [MODULE.bazel files](/rules/lib/globals/module)
- [REPO.bazel files](/rules/lib/globals/repo)
- [VENDOR.bazel files](/rules/lib/globals/vendor)

---

## All Bazel files
- URL: https://bazel.build/rules/lib/globals/all
- Source: rules/lib/globals/all.mdx
- Slug: /rules/lib/globals/all

Methods available in all Bazel files, including .bzl files, BUILD, MODULE.bazel, VENDOR.bazel, and WORKSPACE.

## Members

- [abs](#abs)
- [all](#all)
- [any](#any)
- [bool](#bool)
- [dict](#dict)
- [dir](#dir)
- [enumerate](#enumerate)
- [fail](#fail)
- [float](#float)
- [getattr](#getattr)
- [hasattr](#hasattr)
- [hash](#hash)
- [int](#int)
- [len](#len)
- [list](#list)
- [max](#max)
- [min](#min)
- [print](#print)
- [range](#range)
- [repr](#repr)
- [reversed](#reversed)
- [set](#set)
- [sorted](#sorted)
- [str](#str)
- [tuple](#tuple)
- [type](#type)
- [zip](#zip)

## abs

```
unknown abs(x)
```

 Returns the absolute value of a number (a non-negative number with the same magnitude).

```
abs(-2.3) == 2.3
```

### Parameters

ParameterDescription`x`[int](../core/int.html); or [float](../core/float.html);
 required

 A number (int or float)


## all

```
bool all(elements)
```

 Returns true if all elements evaluate to True or if the collection is empty. Elements are converted to boolean using the [bool](#bool) function.

```
all(["hello", 3, True]) == True
all([-1, 0, 1]) == False
```

### Parameters

ParameterDescription`elements`
 iterable;
 required

 A collection of elements.


## any

```
bool any(elements)
```

 Returns true if at least one element evaluates to True. Elements are converted to boolean using the [bool](#bool) function.

```
any([-1, 0, 1]) == True
any([False, 0, ""]) == False
```

### Parameters

ParameterDescription`elements`
 iterable;
 required

 A collection of elements.


## bool

```
bool bool(x=False)
```

 Constructor for the bool type. It returns `False` if the object is `None`, `False`, an empty string ( `""`), the number `0`, or an empty collection (e.g. `()`, `[]`). Otherwise, it returns `True`.


### Parameters

ParameterDescription`x`
 default is `False`

 The variable to convert.


## dict

```
dict dict(pairs=[], **kwargs)
```

 Creates a [dictionary](../core/dict.html) from an optional positional argument and an optional set of keyword arguments. In the case where the same key is given multiple times, the last value will be used. Entries supplied via keyword arguments are considered to come after entries supplied via the positional argument.


### Parameters

ParameterDescription`pairs`
 default is `[]`

 A dict, or an iterable whose elements are each of length 2 (key, value).
 `kwargs`
 required

 Dictionary of additional entries.


## dir

```
list dir(x)
```

 Returns a list of strings: the names of the attributes and methods of the parameter object.


### Parameters

ParameterDescription`x`
 required

 The object to check.


## enumerate

```
list enumerate(list, start=0)
```

 Returns a list of pairs (two-element tuples), with the index (int) and the item from the input sequence.

```
enumerate([24, 21, 84]) == [(0, 24), (1, 21), (2, 84)]
```

### Parameters

ParameterDescription`list`
 required

 input sequence.
 `start`[int](../core/int.html);
 default is `0`

 start index.


## fail

```
None fail(*args, msg=None, attr=None, sep=" ")
```

 Causes execution to fail with an error.


### Parameters

ParameterDescription`msg`
 default is `None`

 Deprecated: use positional arguments instead. This argument acts like an implicit leading positional argument.
 `attr`[string](../core/string.html); or `None`;
 default is `None`

 Deprecated. Causes an optional prefix containing this string to be added to the error message.
 `sep`[string](../core/string.html);
 default is `" "`

 The separator string between the objects, default is space (" ").
 `args`
 required

 A list of values, formatted with debugPrint (which is equivalent to str by default) and joined with sep (defaults to " "), that appear in the error message.


## float

```
float float(x=unbound)
```

 Returns x as a float value.

- If `x` is already a float, `float` returns it unchanged.
- If `x` is a bool, `float` returns 1.0 for True and 0.0 for False.
- If `x` is an int, `float` returns the nearest finite floating-point value to x, or an error if the magnitude is too large.
- If `x` is a string, it must be a valid floating-point literal, or be equal (ignoring case) to `NaN`, `Inf`, or `Infinity`, optionally preceded by a `+` or `-` sign.

Any other value causes an error. With no argument, `float()` returns 0.0.


### Parameters

ParameterDescription`x`[string](../core/string.html); or [bool](../core/bool.html); or [int](../core/int.html); or [float](../core/float.html);
 default is `unbound`

 The value to convert.


## getattr

```
unknown getattr(x, name, default=unbound)
```

 Returns the struct's field of the given name if it exists. If not, it either returns `default` (if specified) or raises an error. `getattr(x, "foobar")` is equivalent to `x.foobar`.

```
getattr(ctx.attr, "myattr")
getattr(ctx.attr, "myattr", "mydefault")
```

### Parameters

ParameterDescription`x`
 required

 The struct whose attribute is accessed.
 `name`[string](../core/string.html);
 required

 The name of the struct attribute.
 `default`
 default is `unbound`

 The default value to return in case the struct doesn't have an attribute of the given name.


## hasattr

```
bool hasattr(x, name)
```

 Returns True if the object `x` has an attribute or method of the given `name`, otherwise False. Example:

```
hasattr(ctx.attr, "myattr")
```

### Parameters

ParameterDescription`x`
 required

 The object to check.
 `name`[string](../core/string.html);
 required

 The name of the attribute.


## hash

```
int hash(value)
```

 Return a hash value for a string. This is computed deterministically using the same algorithm as Java's `String.hashCode()`, namely:

```
s[0] * (31^(n-1)) + s[1] * (31^(n-2)) + ... + s[n-1]
```

 Hashing of values besides strings is not currently supported.


### Parameters

ParameterDescription`value`[string](../core/string.html);
 required

 String value to hash.


## int

```
int int(x, base=unbound)
```

 Returns x as an int value.

- If `x` is already an int, `int` returns it unchanged.
- If `x` is a bool, `int` returns 1 for True and 0 for False.
- If `x` is a string, it must have the format `<sign>```<digits>`. `<sign>` is either `"+"`, `"-"`, or empty (interpreted as positive). `<digits>` are a sequence of digits from 0 up to `base` \- 1, where the letters a-z (or equivalently, A-Z) are used as digits for 10-35. In the case where `base` is 2/8/16, ````` is optional and may be 0b/0o/0x (or equivalently, 0B/0O/0X) respectively; if the `base` is any other value besides these bases or the special value 0, the prefix must be empty. In the case where `base` is 0, the string is interpreted as an integer literal, in the sense that one of the bases 2/8/10/16 is chosen depending on which prefix if any is used. If `base` is 0, no prefix is used, and there is more than one digit, the leading digit cannot be 0; this is to avoid confusion between octal and decimal. The magnitude of the number represented by the string must be within the allowed range for the int type.
- If `x` is a float, `int` returns the integer value of the float, rounding towards zero. It is an error if x is non-finite (NaN or infinity).

This function fails if `x` is any other type, or if the value is a string not satisfying the above format. Unlike Python's `int` function, this function does not allow zero arguments, and does not allow extraneous whitespace for string arguments.

Examples:

```
int("123") == 123
int("-123") == -123
int("+123") == 123
int("FF", 16) == 255
int("0xFF", 16) == 255
int("10", 0) == 10
int("-0x10", 0) == -16
int("-0x10", 0) == -16
int("123.456") == 123

```

### Parameters

ParameterDescription`x`[string](../core/string.html); or [bool](../core/bool.html); or [int](../core/int.html); or [float](../core/float.html);
 required

 The string to convert.
 `base`[int](../core/int.html);
 default is `unbound`

 The base used to interpret a string value; defaults to 10. Must be between 2 and 36 (inclusive), or 0 to detect the base as if `x` were an integer literal. This parameter must not be supplied if the value is not a string.


## len

```
int len(x)
```

 Returns the length of a string, sequence (such as a list or tuple), dict, set, or other iterable.


### Parameters

ParameterDescription`x`
 iterable; or [string](../core/string.html);
 required

 The value whose length to report.


## list

```
list list(x=[])
```

 Returns a new list with the same elements as the given iterable value.

```
list([1, 2]) == [1, 2]
list((2, 3, 2)) == [2, 3, 2]
list({5: "a", 2: "b", 4: "c"}) == [5, 2, 4]
```

### Parameters

ParameterDescription`x`
 iterable;
 default is `[]`

 The object to convert.


## max

```
unknown max(*args, key=None)
```

 Returns the largest one of all given arguments. If only one positional argument is provided, it must be a non-empty iterable.It is an error if elements are not comparable (for example int with string), or if no arguments are given.

```
max(2, 5, 4) == 5
max([5, 6, 3]) == 6
max("two", "three", "four", key = len) =="three"  # the longest
max([1, -1, -2, 2], key = abs) == -2  # the first encountered with maximal key value

```

### Parameters

ParameterDescription`key`
 callable; or `None`;
 default is `None`

 An optional function applied to each element before comparison.
 `args`
 required

 The elements to be checked.


## min

```
unknown min(*args, key=None)
```

 Returns the smallest one of all given arguments. If only one positional argument is provided, it must be a non-empty iterable. It is an error if elements are not comparable (for example int with string), or if no arguments are given.

```
min(2, 5, 4) == 2
min([5, 6, 3]) == 3
min("six", "three", "four", key = len) == "six"  # the shortest
min([2, -2, -1, 1], key = abs) == -1  # the first encountered with minimal key value

```

### Parameters

ParameterDescription`key`
 callable; or `None`;
 default is `None`

 An optional function applied to each element before comparison.
 `args`
 required

 The elements to be checked.


## print

```
None print(*args, sep=" ")
```

 Prints `args` as debug output. It will be prefixed with the string `"DEBUG"` and the location (file and line number) of this call. The exact way in which the arguments are converted to strings is unspecified and may change at any time. In particular, it may be different from (and more detailed than) the formatting done by [`str()`](#str) and [`repr()`](#repr).

Using `print` in production code is discouraged due to the spam it creates for users. For deprecations, prefer a hard error using [`fail()`](#fail) whenever possible.


### Parameters

ParameterDescription`sep`[string](../core/string.html);
 default is `" "`

 The separator string between the objects, default is space (" ").
 `args`
 required

 The objects to print.


## range

```
sequence range(start_or_stop, stop=unbound, step=1)
```

 Creates a list where items go from `start` to `stop`, using a `step` increment. If a single argument is provided, items will range from 0 to that element.

```
range(4) == [0, 1, 2, 3]
range(3, 9, 2) == [3, 5, 7]
range(3, 0, -1) == [3, 2, 1]
```

### Parameters

ParameterDescription`start_or_stop`[int](../core/int.html);
 required

 Value of the start element if stop is provided, otherwise value of stop and the actual start is 0
 `stop`[int](../core/int.html);
 default is `unbound`

 optional index of the first item _not_ to be included in the resulting list; generation of the list stops before `stop` is reached.
 `step`[int](../core/int.html);
 default is `1`

 The increment (default is 1). It may be negative.


## repr

```
string repr(x)
```

 Converts any object to a string representation. This is useful for debugging.

```
repr("ab") == '"ab"'
```

### Parameters

ParameterDescription`x`
 required

 The object to convert.


## reversed

```
list reversed(sequence)
```

 Returns a new, unfrozen list that contains the elements of the original iterable sequence in reversed order.

```
reversed([3, 5, 4]) == [4, 5, 3]
```

### Parameters

ParameterDescription`sequence`
 iterable;
 required

 The iterable sequence (e.g. list) to be reversed.


## set

```
set set(elements=[])
```

 Creates a new [set](../core/set.html) containing the unique elements of a given
iterable, preserving iteration order.

If called with no argument, `set()` returns a new empty set.

For example,

```
set()                          # an empty set
set([3, 1, 1, 2])              # set([3, 1, 2]), a set of three elements
set({"k1": "v1", "k2": "v2"})  # set(["k1", "k2"]), a set of two elements

```

### Parameters

ParameterDescription`elements`
 iterable;
 default is `[]`

 An iterable of hashable values.


## sorted

```
list sorted(iterable, key=None, *, reverse=False)
```

 Returns a new sorted list containing all the elements of the supplied iterable sequence. An error may occur if any pair of elements x, y may not be compared using x < y. The elements are sorted into ascending order, unless the reverse argument is True, in which case the order is descending.
 Sorting is stable: elements that compare equal retain their original relative order.

```
sorted([3, 5, 4]) == [3, 4, 5]
sorted([3, 5, 4], reverse = True) == [5, 4, 3]
sorted(["two", "three", "four"], key = len) == ["two", "four", "three"]  # sort by length

```

### Parameters

ParameterDescription`iterable`
 iterable;
 required

 The iterable sequence to sort.
 `key`
 callable; or `None`;
 default is `None`

 An optional function applied to each element before comparison.
 `reverse`[bool](../core/bool.html);
 default is `False`

 Return results in descending order.


## str

```
string str(x)
```

 Converts any object to string. This is useful for debugging.

```
str("ab") == "ab"
str(8) == "8"
```

### Parameters

ParameterDescription`x`
 required

 The object to convert.


## tuple

```
tuple tuple(x=())
```

 Returns a tuple with the same elements as the given iterable value.

```
tuple([1, 2]) == (1, 2)
tuple((2, 3, 2)) == (2, 3, 2)
tuple({5: "a", 2: "b", 4: "c"}) == (5, 2, 4)
```

### Parameters

ParameterDescription`x`
 iterable;
 default is `()`

 The object to convert.


## type

```
string type(x)
```

 Returns the type name of its argument. This is useful for debugging and type-checking. Examples:

```
type(2) == "int"
type([1]) == "list"
type(struct(a = 2)) == "struct"
```

This function might change in the future. To write Python-compatible code and be future-proof, use it only to compare return values:

```
if type(x) == type([]):  # if x is a list
```

### Parameters

ParameterDescription`x`
 required

 The object to check type of.


## zip

```
list zip(*args)
```

 Returns a `list` of `tuple` s, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The list has the size of the shortest input. With a single iterable argument, it returns a list of 1-tuples. With no arguments, it returns an empty list. Examples:

```
zip()  # == []
zip([1, 2])  # == [(1,), (2,)]
zip([1, 2], [3, 4])  # == [(1, 3), (2, 4)]
zip([1, 2], [3, 4, 5])  # == [(1, 3), (2, 4)]
```

### Parameters

ParameterDescription`args`
 required

 lists to zip.

---

## BUILD files
- URL: https://bazel.build/rules/lib/globals/build
- Source: rules/lib/globals/build.mdx
- Slug: /rules/lib/globals/build

Methods available in BUILD files. See also the Build Encyclopedia for extra [functions](/reference/be/functions) and build rules, which can also be used in BUILD files.

## Members

- [depset](#depset)
- [existing\_rule](#existing_rule)
- [existing\_rules](#existing_rules)
- [exports\_files](#exports_files)
- [glob](#glob)
- [module\_name](#module_name)
- [module\_version](#module_version)
- [package](#package)
- [package\_default\_visibility](#package_default_visibility)
- [package\_group](#package_group)
- [package\_name](#package_name)
- [package\_relative\_label](#package_relative_label)
- [repo\_name](#repo_name)
- [repository\_name](#repository_name)
- [select](#select)
- [subpackages](#subpackages)

## depset

```
depset depset(direct=None, order="default", *, transitive=None)
```

 Creates a [depset](../builtins/depset.html). The `direct` parameter is a list of direct elements of the depset, and `transitive` parameter is a list of depsets whose elements become indirect elements of the created depset. The order in which elements are returned when the depset is converted to a list is specified by the `order` parameter. See the [Depsets overview](https://bazel.build/extending/depsets) for more information.

All elements (direct and indirect) of a depset must be of the same type, as obtained by the expression [`type(x)`](../globals/all#type).

Because a hash-based set is used to eliminate duplicates during iteration, all elements of a depset should be hashable. However, this invariant is not currently checked consistently in all constructors. Use the --incompatible\_always\_check\_depset\_elements flag to enable consistent checking; this will be the default behavior in future releases; see [Issue 10313](https://github.com/bazelbuild/bazel/issues/10313).

In addition, elements must currently be immutable, though this restriction will be relaxed in future.

The order of the created depset should be _compatible_ with the order of its `transitive` depsets. `"default"` order is compatible with any other order, all other orders are only compatible with themselves.


### Parameters

ParameterDescription`direct`[sequence](../core/list.html); or `None`;
 default is `None`

 A list of _direct_ elements of a depset.
 `order`[string](../core/string.html);
 default is `"default"`

 The traversal strategy for the new depset. See [here](../builtins/depset.html) for the possible values.
 `transitive`[sequence](../core/list.html) of [depset](../builtins/depset.html) s; or `None`;
 default is `None`

 A list of depsets whose elements will become indirect elements of the depset.


## existing\_rule

```
unknown existing_rule(name)
```

 Returns an immutable dict-like object that describes the attributes of a rule instantiated in this thread's package, or `None` if no rule instance of that name exists.

Here, an _immutable dict-like object_ means a deeply immutable object `x` supporting dict-like iteration, `len(x)`, `name in x`, `x[name]`, `x.get(name)`, `x.items()`, `x.keys()`, and `x.values()`.

The result contains an entry for each attribute, with the exception of private ones (whose names do not start with a letter) and a few unrepresentable legacy attribute types. In addition, the dict contains entries for the rule instance's `name` and `kind` (for example, `'cc_binary'`).

The values of the result represent attribute values as follows:

- Attributes of type str, int, and bool are represented as is.
- Labels are converted to strings of the form `':foo'` for targets in the same package or `'//pkg:name'` for targets in a different package.
- Lists are represented as tuples, and dicts are converted to new, mutable dicts. Their elements are recursively converted in the same fashion.
- `select` values are returned with their contents transformed as described above.
- Attributes for which no value was specified during rule instantiation and whose default value is computed are excluded from the result. (Computed defaults cannot be computed until the analysis phase.).

If possible, use this function only in [implementation functions of rule finalizer symbolic macros](https://bazel.build/extending/macros#finalizers). Use of this function in other contexts is not recommened, and will be disabled in a future Bazel release; it makes `BUILD` files brittle and order-dependent. Also, beware that it differs subtly from the two other conversions of rule attribute values from internal form to Starlark: one used by computed defaults, the other used by `ctx.attr.foo`.


### Parameters

ParameterDescription`name`[string](../core/string.html);
 required

 The name of the target.


## existing\_rules

```
unknown existing_rules()
```

 Returns an immutable dict-like object describing the rules so far instantiated in this thread's package. Each entry of the dict-like object maps the name of the rule instance to the result that would be returned by `existing_rule(name)`.

Here, an _immutable dict-like object_ means a deeply immutable object `x` supporting dict-like iteration, `len(x)`, `name in x`, `x[name]`, `x.get(name)`, `x.items()`, `x.keys()`, and `x.values()`.

If possible, use this function only in [implementation functions of rule finalizer symbolic macros](https://bazel.build/extending/macros#finalizers). Use of this function in other contexts is not recommened, and will be disabled in a future Bazel release; it makes `BUILD` files brittle and order-dependent.



## exports\_files

```
None exports_files(srcs, visibility=None, licenses=None)
```

 Specifies a list of files belonging to this package that are exported to other packages.


### Parameters

ParameterDescription`srcs`[sequence](../core/list.html) of [string](../core/string.html) s;
 required

 The list of files to export.
 `visibility`[sequence](../core/list.html); or `None`;
 default is `None`

 A visibility declaration can to be specified. The files will be visible to the targets specified. If no visibility is specified, the files will be visible to every package.
 `licenses`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Licenses to be specified.


## glob

```
sequence glob(include=[], exclude=[], exclude_directories=1, allow_empty=unbound)
```

 Glob returns a new, mutable, sorted list of every file in the current package that:

- Matches at least one pattern in `include`.
- Does not match any of the patterns in `exclude` (default `[]`).

If the `exclude_directories` argument is enabled (set to `1`), files of type directory will be omitted from the results (default `1`).


### Parameters

ParameterDescription`include`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 The list of glob patterns to include.
 `exclude`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 The list of glob patterns to exclude.
 `exclude_directories`[int](../core/int.html);
 default is `1`

 A flag whether to exclude directories or not.
 `allow_empty`
 default is `unbound`

 Whether we allow glob patterns to match nothing. If \`allow\_empty\` is False, each individual include pattern must match something and also the final result must be non-empty (after the matches of the \`exclude\` patterns are excluded).


## module\_name

```
string module_name()
```

 The name of the Bazel module associated with the repo this package is in. If this package is from a repo defined in WORKSPACE instead of MODULE.bazel, this is empty. For repos generated by module extensions, this is the name of the module hosting the extension. It's the same as the `module.name` field seen in `module_ctx.modules`.
 May return `None`.



## module\_version

```
string module_version()
```

 The version of the Bazel module associated with the repo this package is in. If this package is from a repo defined in WORKSPACE instead of MODULE.bazel, this is empty. For repos generated by module extensions, this is the version of the module hosting the extension. It's the same as the `module.version` field seen in `module_ctx.modules`.
 May return `None`.



## package

```
unknown package(**kwargs)
```

 Declares metadata that applies to every rule in the package. It must be called at most once within a package (BUILD file). If called, it should be the first call in the BUILD file, right after the `load()` statements.


### Parameters

ParameterDescription`kwargs`
 required

 See the [`package()`](/reference/be/functions#package) function in the Build Encyclopedia for applicable arguments.


## package\_default\_visibility

```
List package_default_visibility()
```

 Returns the default visibility of the package being evaluated. This is the value of the `default_visibility` parameter of `package()`, extended to include the package itself.



## package\_group

```
None package_group(*, name, packages=[], includes=[])
```

 This function defines a set of packages and assigns a label to the group. The label can be referenced in `visibility` attributes.


### Parameters

ParameterDescription`name`[string](../core/string.html);
 required

 The unique name for this rule.
 `packages`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 A complete enumeration of packages in this group.
 `includes`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Other package groups that are included in this one.


## package\_name

```
string package_name()
```

 The name of the package being evaluated, without the repository name. For example, in the BUILD file `some/package/BUILD`, its value will be `some/package`. If the BUILD file calls a function defined in a .bzl file, `package_name()` will match the caller BUILD file package. The value will always be an empty string for the root package.



## package\_relative\_label

```
Label package_relative_label(input)
```

 Converts the input string into a [Label](../builtins/Label.html) object, in the context of the package currently being initialized (that is, the `BUILD` file for which the current macro is executing). If the input is already a `Label`, it is returned unchanged.

This function may only be called while evaluating a BUILD file and the macros it directly or indirectly calls; it may not be called in (for instance) a rule implementation function.

The result of this function is the same `Label` value as would be produced by passing the given string to a label-valued attribute of a target declared in the BUILD file.

_Usage note:_ The difference between this function and [Label()](../builtins/Label.html#Label) is that `Label()` uses the context of the package of the `.bzl` file that called it, not the package of the `BUILD` file. Use `Label()` when you need to refer to a fixed target that is hardcoded into the macro, such as a compiler. Use `package_relative_label()` when you need to normalize a label string supplied by the BUILD file to a `Label` object. (There is no way to convert a string to a `Label` in the context of a package other than the BUILD file or the calling .bzl file. For that reason, outer macros should always prefer to pass Label objects to inner macros rather than label strings.)


### Parameters

ParameterDescription`input`[string](../core/string.html); or [Label](../builtins/Label.html);
 required

 The input label string or Label object. If a Label object is passed, it's returned as is.


## repo\_name

```
string repo_name()
```

 The canonical name of the repository containing the package currently being evaluated, with no leading at-signs.



## repository\_name

```
string repository_name()
```

 **Experimental**. This API is experimental and may change at any time. Please do not depend on it. It may be enabled on an experimental basis by setting `--+incompatible_enable_deprecated_label_apis`

**Deprecated.** Prefer to use [`repo_name`](#repo_name) instead, which doesn't contain the spurious leading at-sign, but behaves identically otherwise.

The canonical name of the repository containing the package currently being evaluated, with a single at-sign ( `@`) prefixed. For example, in packages that are called into existence by the WORKSPACE stanza `local_repository(name='local', path=...)` it will be set to `@local`. In packages in the main repository, it will be set to `@`.



## select

```
unknown select(x, no_match_error='')
```

 `select()` is the helper function that makes a rule attribute [configurable](/reference/be/common-definitions#configurable-attributes). See [build encyclopedia](/reference/be/functions#select) for details.


### Parameters

ParameterDescription`x`[dict](../core/dict.html);
 required

 A dict that maps configuration conditions to values. Each key is a [Label](../builtins/Label.html) or a label string that identifies a config\_setting or constraint\_value instance. See the [documentation on macros](https://bazel.build/extending/legacy-macros#label-resolution) for when to use a Label instead of a string. If `--incompatible_resolve_select_keys_eagerly` is enabled, the keys are resolved to `Label` objects relative to the package of the file that contains this call to `select`.
 `no_match_error`[string](../core/string.html);
 default is `''`

 Optional custom error to report if no condition matches.


## subpackages

```
sequence subpackages(*, include, exclude=[], allow_empty=False)
```

 Returns a new mutable list of every direct subpackage of the current package, regardless of file-system directory depth. List returned is sorted and contains the names of subpackages relative to the current package. It is advised to prefer using the methods in bazel\_skylib.subpackages module rather than calling this function directly.


### Parameters

ParameterDescription`include`[sequence](../core/list.html) of [string](../core/string.html) s;
 required

 The list of glob patterns to include in subpackages scan.
 `exclude`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 The list of glob patterns to exclude from subpackages scan.
 `allow_empty`[bool](../core/bool.html);
 default is `False`

 Whether we fail if the call returns an empty list. By default empty list indicates potential error in BUILD file where the call to subpackages() is superflous. Setting to true allows this function to succeed in that case.

---

## REPO.bazel files
- URL: https://bazel.build/rules/lib/globals/repo
- Source: rules/lib/globals/repo.mdx
- Slug: /rules/lib/globals/repo

Methods available in REPO.bazel files.

## Members

- [ignore\_directories](#ignore_directories)
- [repo](#repo)

## ignore\_directories

```
None ignore_directories(dirs)
```

 The list of directories to ignore in this repository.

This function takes a list of strings and a directory is ignored if any of the given strings matches its repository-relative path according to the semantics of the `glob()` function. This function can be used to ignore directories that are implementation details of source control systems, output files of other build systems, etc.


### Parameters

ParameterDescription`dirs`[sequence](../core/list.html) of [string](../core/string.html) s;
 required

## repo

```
None repo(**kwargs)
```

 Declares metadata that applies to every rule in the repository. It must be called at most once per REPO.bazel file. If called, it must be the first call in the REPO.bazel file.


### Parameters

ParameterDescription`kwargs`
 required

 The `repo()` function accepts exactly the same arguments as the [`package()`](/reference/be/functions#package) function in BUILD files.

---

## VENDOR.bazel files
- URL: https://bazel.build/rules/lib/globals/vendor
- Source: rules/lib/globals/vendor.mdx
- Slug: /rules/lib/globals/vendor

Methods available in VENDOR.bazel files.

## Members

- [ignore](#ignore)
- [pin](#pin)

## ignore

```
None ignore(*args)
```

 Ignore this repo from vendoring. Bazel will never vendor it or use the corresponding directory (if exists) while building in vendor mode.


### Parameters

ParameterDescription`args`
 required

 The canonical repo names of the repos to ignore.


## pin

```
None pin(*args)
```

 Pin the contents of this repo under the vendor directory. Bazel will not update this repo while vendoring, and will use the vendored source as if there is a --override\_repository flag when building in vendor mode


### Parameters

ParameterDescription`args`
 required

 The canonical repo names of the repos to pin.

---

## One-Page Overview
- URL: https://bazel.build/rules/lib/overview
- Source: rules/lib/overview.mdx
- Slug: /rules/lib/overview

## [Global functions](/rules/lib/globals)

- [.bzl files](/rules/lib/globals/bzl)
- [All Bazel files](/rules/lib/globals/all)
- [BUILD files](/rules/lib/globals/build)
- [MODULE.bazel files](/rules/lib/globals/module)
- [REPO.bazel files](/rules/lib/globals/repo)
- [VENDOR.bazel files](/rules/lib/globals/vendor)

## [Configuration Fragments](/rules/lib/fragments)

- [apple](/rules/lib/fragments/apple)
- [bazel\_android](/rules/lib/fragments/bazel_android)
- [bazel\_py](/rules/lib/fragments/bazel_py)
- [coverage](/rules/lib/fragments/coverage)
- [cpp](/rules/lib/fragments/cpp)
- [j2objc](/rules/lib/fragments/j2objc)
- [java](/rules/lib/fragments/java)
- [objc](/rules/lib/fragments/objc)
- [platform](/rules/lib/fragments/platform)
- [proto](/rules/lib/fragments/proto)
- [py](/rules/lib/fragments/py)

## [Providers](/rules/lib/providers)

- [AnalysisTestResultInfo](/rules/lib/providers/AnalysisTestResultInfo)
- [CcInfo](/rules/lib/providers/CcInfo)
- [CcToolchainConfigInfo](/rules/lib/providers/CcToolchainConfigInfo)
- [CcToolchainInfo](/rules/lib/providers/CcToolchainInfo)
- [ConstraintCollection](/rules/lib/providers/ConstraintCollection)
- [ConstraintSettingInfo](/rules/lib/providers/ConstraintSettingInfo)
- [ConstraintValueInfo](/rules/lib/providers/ConstraintValueInfo)
- [DebugPackageInfo](/rules/lib/providers/DebugPackageInfo)
- [DefaultInfo](/rules/lib/providers/DefaultInfo)
- [ExecutionInfo](/rules/lib/providers/ExecutionInfo)
- [FeatureFlagInfo](/rules/lib/providers/FeatureFlagInfo)
- [file\_provider](/rules/lib/providers/file_provider)
- [FilesToRunProvider](/rules/lib/providers/FilesToRunProvider)
- [IncompatiblePlatformProvider](/rules/lib/providers/IncompatiblePlatformProvider)
- [InstrumentedFilesInfo](/rules/lib/providers/InstrumentedFilesInfo)
- [java\_compilation\_info](/rules/lib/providers/java_compilation_info)
- [java\_output\_jars](/rules/lib/providers/java_output_jars)
- [JavaRuntimeInfo](/rules/lib/providers/JavaRuntimeInfo)
- [JavaToolchainInfo](/rules/lib/providers/JavaToolchainInfo)
- [MaterializedDepsInfo](/rules/lib/providers/MaterializedDepsInfo)
- [ObjcProvider](/rules/lib/providers/ObjcProvider)
- [OutputGroupInfo](/rules/lib/providers/OutputGroupInfo)
- [PackageSpecificationInfo](/rules/lib/providers/PackageSpecificationInfo)
- [PlatformInfo](/rules/lib/providers/PlatformInfo)
- [RunEnvironmentInfo](/rules/lib/providers/RunEnvironmentInfo)
- [TemplateVariableInfo](/rules/lib/providers/TemplateVariableInfo)
- [ToolchainInfo](/rules/lib/providers/ToolchainInfo)
- [ToolchainTypeInfo](/rules/lib/providers/ToolchainTypeInfo)

## [Built-in Types](/rules/lib/builtins)

- [Action](/rules/lib/builtins/Action)
- [actions](/rules/lib/builtins/actions)
- [apple\_platform](/rules/lib/builtins/apple_platform)
- [Args](/rules/lib/builtins/Args)
- [Aspect](/rules/lib/builtins/Aspect)
- [Attribute](/rules/lib/builtins/Attribute)
- [bazel\_module](/rules/lib/builtins/bazel_module)
- [bazel\_module\_tags](/rules/lib/builtins/bazel_module_tags)
- [BuildSetting](/rules/lib/builtins/BuildSetting)
- [CcCompilationOutputs](/rules/lib/builtins/CcCompilationOutputs)
- [CcLinkingOutputs](/rules/lib/builtins/CcLinkingOutputs)
- [CompilationContext](/rules/lib/builtins/CompilationContext)
- [configuration](/rules/lib/builtins/configuration)
- [ctx](/rules/lib/builtins/ctx)
- [depset](/rules/lib/builtins/depset)
- [DirectoryExpander](/rules/lib/builtins/DirectoryExpander)
- [DottedVersion](/rules/lib/builtins/DottedVersion)
- [exec\_result](/rules/lib/builtins/exec_result)
- [ExecGroupCollection](/rules/lib/builtins/ExecGroupCollection)
- [ExecGroupContext](/rules/lib/builtins/ExecGroupContext)
- [ExecTransitionFactory](/rules/lib/builtins/ExecTransitionFactory)
- [ExpandedDirectory](/rules/lib/builtins/ExpandedDirectory)
- [extension\_metadata](/rules/lib/builtins/extension_metadata)
- [Facts](/rules/lib/builtins/Facts)
- [FeatureConfiguration](/rules/lib/builtins/FeatureConfiguration)
- [File](/rules/lib/builtins/File)
- [fragments](/rules/lib/builtins/fragments)
- [java\_annotation\_processing](/rules/lib/builtins/java_annotation_processing)
- [Label](/rules/lib/builtins/Label)
- [LateBoundDefault](/rules/lib/builtins/LateBoundDefault)
- [LibraryToLink](/rules/lib/builtins/LibraryToLink)
- [License](/rules/lib/builtins/License)
- [LinkerInput](/rules/lib/builtins/LinkerInput)
- [LinkingContext](/rules/lib/builtins/LinkingContext)
- [macro](/rules/lib/builtins/macro)
- [mapped\_root](/rules/lib/builtins/mapped_root)
- [module\_ctx](/rules/lib/builtins/module_ctx)
- [path](/rules/lib/builtins/path)
- [propagation\_ctx](/rules/lib/builtins/propagation_ctx)
- [Provider](/rules/lib/builtins/Provider)
- [repo\_metadata](/rules/lib/builtins/repo_metadata)
- [repository\_ctx](/rules/lib/builtins/repository_ctx)
- [repository\_os](/rules/lib/builtins/repository_os)
- [repository\_rule](/rules/lib/builtins/repository_rule)
- [root](/rules/lib/builtins/root)
- [rule](/rules/lib/builtins/rule)
- [rule\_attributes](/rules/lib/builtins/rule_attributes)
- [runfiles](/rules/lib/builtins/runfiles)
- [struct](/rules/lib/builtins/struct)
- [Subrule](/rules/lib/builtins/Subrule)
- [subrule\_ctx](/rules/lib/builtins/subrule_ctx)
- [SymlinkEntry](/rules/lib/builtins/SymlinkEntry)
- [tag\_class](/rules/lib/builtins/tag_class)
- [Target](/rules/lib/builtins/Target)
- [template\_ctx](/rules/lib/builtins/template_ctx)
- [TemplateDict](/rules/lib/builtins/TemplateDict)
- [toolchain\_type](/rules/lib/builtins/toolchain_type)
- [ToolchainContext](/rules/lib/builtins/ToolchainContext)
- [transition](/rules/lib/builtins/transition)
- [wasm\_exec\_result](/rules/lib/builtins/wasm_exec_result)
- [wasm\_module](/rules/lib/builtins/wasm_module)

## [Top-level Modules](/rules/lib/toplevel)

- [apple\_common](/rules/lib/toplevel/apple_common)
- [attr](/rules/lib/toplevel/attr)
- [cc\_common](/rules/lib/toplevel/cc_common)
- [config](/rules/lib/toplevel/config)
- [config\_common](/rules/lib/toplevel/config_common)
- [coverage\_common](/rules/lib/toplevel/coverage_common)
- [java\_common](/rules/lib/toplevel/java_common)
- [native](/rules/lib/toplevel/native)
- [platform\_common](/rules/lib/toplevel/platform_common)
- [proto](/rules/lib/toplevel/proto)
- [testing](/rules/lib/toplevel/testing)

## [Core Starlark data types](/rules/lib/core)

- [bool](/rules/lib/core/bool)
- [builtin\_function\_or\_method](/rules/lib/core/builtin_function_or_method)
- [dict](/rules/lib/core/dict)
- [float](/rules/lib/core/float)
- [function](/rules/lib/core/function)
- [int](/rules/lib/core/int)
- [json](/rules/lib/core/json)
- [list](/rules/lib/core/list)
- [range](/rules/lib/core/range)
- [set](/rules/lib/core/set)
- [string](/rules/lib/core/string)
- [tuple](/rules/lib/core/tuple)

---

## Providers
- URL: https://bazel.build/rules/lib/providers
- Source: rules/lib/providers.mdx
- Slug: /rules/lib/providers

This section lists providers available on built-in rules. See the [Rules page](https://bazel.build/extending/rules#providers) for more on providers. These symbols are available only in .bzl files.

- [AnalysisTestResultInfo](/rules/lib/providers/AnalysisTestResultInfo)
- [CcInfo](/rules/lib/providers/CcInfo)
- [CcToolchainConfigInfo](/rules/lib/providers/CcToolchainConfigInfo)
- [CcToolchainInfo](/rules/lib/providers/CcToolchainInfo)
- [ConstraintCollection](/rules/lib/providers/ConstraintCollection)
- [ConstraintSettingInfo](/rules/lib/providers/ConstraintSettingInfo)
- [ConstraintValueInfo](/rules/lib/providers/ConstraintValueInfo)
- [DebugPackageInfo](/rules/lib/providers/DebugPackageInfo)
- [DefaultInfo](/rules/lib/providers/DefaultInfo)
- [ExecutionInfo](/rules/lib/providers/ExecutionInfo)
- [FeatureFlagInfo](/rules/lib/providers/FeatureFlagInfo)
- [file\_provider](/rules/lib/providers/file_provider)
- [FilesToRunProvider](/rules/lib/providers/FilesToRunProvider)
- [IncompatiblePlatformProvider](/rules/lib/providers/IncompatiblePlatformProvider)
- [InstrumentedFilesInfo](/rules/lib/providers/InstrumentedFilesInfo)
- [java\_compilation\_info](/rules/lib/providers/java_compilation_info)
- [java\_output\_jars](/rules/lib/providers/java_output_jars)
- [JavaRuntimeInfo](/rules/lib/providers/JavaRuntimeInfo)
- [JavaToolchainInfo](/rules/lib/providers/JavaToolchainInfo)
- [MaterializedDepsInfo](/rules/lib/providers/MaterializedDepsInfo)
- [ObjcProvider](/rules/lib/providers/ObjcProvider)
- [OutputGroupInfo](/rules/lib/providers/OutputGroupInfo)
- [PackageSpecificationInfo](/rules/lib/providers/PackageSpecificationInfo)
- [PlatformInfo](/rules/lib/providers/PlatformInfo)
- [RunEnvironmentInfo](/rules/lib/providers/RunEnvironmentInfo)
- [TemplateVariableInfo](/rules/lib/providers/TemplateVariableInfo)
- [ToolchainInfo](/rules/lib/providers/ToolchainInfo)
- [ToolchainTypeInfo](/rules/lib/providers/ToolchainTypeInfo)

---

## AnalysisTestResultInfo
- URL: https://bazel.build/rules/lib/providers/AnalysisTestResultInfo
- Source: rules/lib/providers/AnalysisTestResultInfo.mdx
- Slug: /rules/lib/providers/AnalysisTestResultInfo

Encapsulates the result of analyis-phase testing. Build targets which return an instance of this provider signal to the build system that it should generate a 'stub' test executable which generates the equivalent test result. Analysis test rules (rules created with `analysis_test=True` **must** return an instance of this provider, and non-analysis-phase test rules **cannot** return this provider.

## Members

- [AnalysisTestResultInfo](#AnalysisTestResultInfo)
- [message](#message)
- [success](#success)

## AnalysisTestResultInfo

```
AnalysisTestResultInfo AnalysisTestResultInfo(success, message)
```

 The `AnalysisTestResultInfo` constructor.


### Parameters

ParameterDescription`success`[bool](../core/bool.html);
 required

 If true, then the analysis-phase test represented by this target should pass. If false, the test should fail.
 `message`[string](../core/string.html);
 required

 A descriptive message containing information about the test and its success/failure.


## message

```
string AnalysisTestResultInfo.message
```

 A descriptive message containing information about the test and its success/failure.



## success

```
bool AnalysisTestResultInfo.success
```

 If true, then the analysis-phase test represented by this target passed. If false, the test failed.

---

## CcInfo
- URL: https://bazel.build/rules/lib/providers/CcInfo
- Source: rules/lib/providers/CcInfo.mdx
- Slug: /rules/lib/providers/CcInfo

A provider for compilation and linking of C++. This is also a marking provider telling C++ rules that they can depend on the rule with this provider. If it is not intended for the rule to be depended on by C++, the rule should wrap the CcInfo in some other provider.

## Members

- [CcInfo](#CcInfo)
- [compilation\_context](#compilation_context)
- [linking\_context](#linking_context)

## CcInfo

```
CcInfo CcInfo(*, compilation_context=None, linking_context=None, debug_context=None)
```

 The `CcInfo` constructor.


### Parameters

ParameterDescription`compilation_context`[CompilationContext](../builtins/CompilationContext.html); or `None`;
 default is `None`

 The `CompilationContext`.
 `linking_context`[struct](../builtins/struct.html); or `None`;
 default is `None`

 The `LinkingContext`.
 `debug_context`[struct](../builtins/struct.html); or `None`;
 default is `None`

 The `DebugContext`.


## compilation\_context

```
CompilationContext CcInfo.compilation_context
```

 Returns the `CompilationContext`

## linking\_context

```
struct CcInfo.linking_context
```

 Returns the `LinkingContext`

---

## CcToolchainConfigInfo
- URL: https://bazel.build/rules/lib/providers/CcToolchainConfigInfo
- Source: rules/lib/providers/CcToolchainConfigInfo.mdx
- Slug: /rules/lib/providers/CcToolchainConfigInfo

Additional layer of configurability for C++ rules. Encapsulates platform-dependent specifics of C++ actions through features and action configs. It is used to configure the C++ toolchain, and later on for command line construction. Replaces the functionality of CROSSTOOL file.

---

## CcToolchainInfo
- URL: https://bazel.build/rules/lib/providers/CcToolchainInfo
- Source: rules/lib/providers/CcToolchainInfo.mdx
- Slug: /rules/lib/providers/CcToolchainInfo

Information about the C++ compiler being used.

## Members

- [all\_files](#all_files)
- [ar\_executable](#ar_executable)
- [built\_in\_include\_directories](#built_in_include_directories)
- [compiler](#compiler)
- [compiler\_executable](#compiler_executable)
- [cpu](#cpu)
- [dynamic\_runtime\_lib](#dynamic_runtime_lib)
- [gcov\_executable](#gcov_executable)
- [ld\_executable](#ld_executable)
- [libc](#libc)
- [needs\_pic\_for\_dynamic\_libraries](#needs_pic_for_dynamic_libraries)
- [nm\_executable](#nm_executable)
- [objcopy\_executable](#objcopy_executable)
- [objdump\_executable](#objdump_executable)
- [preprocessor\_executable](#preprocessor_executable)
- [static\_runtime\_lib](#static_runtime_lib)
- [strip\_executable](#strip_executable)
- [sysroot](#sysroot)
- [target\_gnu\_system\_name](#target_gnu_system_name)

## all\_files

```
None CcToolchainInfo.all_files
```

 Returns all toolchain files (so they can be passed to actions using this toolchain as inputs).



## ar\_executable

```
None CcToolchainInfo.ar_executable
```

 The path to the ar binary.



## built\_in\_include\_directories

```
None CcToolchainInfo.built_in_include_directories
```

 Returns the list of built-in directories of the compiler.



## compiler

```
None CcToolchainInfo.compiler
```

 C++ compiler.



## compiler\_executable

```
None CcToolchainInfo.compiler_executable
```

 The path to the compiler binary.



## cpu

```
None CcToolchainInfo.cpu
```

 Target CPU of the C++ toolchain.



## dynamic\_runtime\_lib

```
None CcToolchainInfo.dynamic_runtime_lib(*, feature_configuration)
```

 Returns the files from \`dynamic\_runtime\_lib\` attribute (so they can be passed to actions using this toolchain as inputs). The caller can check whether the feature\_configuration enables \`static\_link\_cpp\_runtimes\` feature (if not, neither \`static\_runtime\_lib\` nor \`dynamic\_runtime\_lib\` have to be used), and use \`static\_runtime\_lib\` if static linking mode is active.


### Parameters

ParameterDescription`feature_configuration`
 required

 Feature configuration to be queried.


## gcov\_executable

```
None CcToolchainInfo.gcov_executable
```

 The path to the gcov binary.



## ld\_executable

```
None CcToolchainInfo.ld_executable
```

 The path to the ld binary.



## libc

```
None CcToolchainInfo.libc
```

 libc version string.



## needs\_pic\_for\_dynamic\_libraries

```
None CcToolchainInfo.needs_pic_for_dynamic_libraries(*, feature_configuration)
```

 Returns true if this rule's compilations should apply -fPIC, false otherwise. Determines if we should apply -fPIC for this rule's C++ compilations depending on the C++ toolchain and presence of \`--force\_pic\` Bazel option.


### Parameters

ParameterDescription`feature_configuration`
 required

 Feature configuration to be queried.


## nm\_executable

```
None CcToolchainInfo.nm_executable
```

 The path to the nm binary.



## objcopy\_executable

```
None CcToolchainInfo.objcopy_executable
```

 The path to the objcopy binary.



## objdump\_executable

```
None CcToolchainInfo.objdump_executable
```

 The path to the objdump binary.



## preprocessor\_executable

```
None CcToolchainInfo.preprocessor_executable
```

 The path to the preprocessor binary.



## static\_runtime\_lib

```
None CcToolchainInfo.static_runtime_lib(*, feature_configuration)
```

 Returns the files from \`static\_runtime\_lib\` attribute (so they can be passed to actions using this toolchain as inputs). The caller should check whether the feature\_configuration enables \`static\_link\_cpp\_runtimes\` feature (if not, neither \`static\_runtime\_lib\` nor \`dynamic\_runtime\_lib\` should be used), and use \`dynamic\_runtime\_lib\` if dynamic linking mode is active.


### Parameters

ParameterDescription`feature_configuration`
 required

 Feature configuration to be queried.


## strip\_executable

```
None CcToolchainInfo.strip_executable
```

 The path to the strip binary.



## sysroot

```
None CcToolchainInfo.sysroot
```

 Returns the sysroot to be used. If the toolchain compiler does not support different sysroots, or the sysroot is the same as the default sysroot, then this method returns `None`.



## target\_gnu\_system\_name

```
None CcToolchainInfo.target_gnu_system_name
```

 The GNU System Name.

---

## ConstraintCollection
- URL: https://bazel.build/rules/lib/providers/ConstraintCollection
- Source: rules/lib/providers/ConstraintCollection.mdx
- Slug: /rules/lib/providers/ConstraintCollection

Provides access to data about a collection of ConstraintValueInfo providers.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

---

## ConstraintSettingInfo
- URL: https://bazel.build/rules/lib/providers/ConstraintSettingInfo
- Source: rules/lib/providers/ConstraintSettingInfo.mdx
- Slug: /rules/lib/providers/ConstraintSettingInfo

A specific constraint setting that may be used to define a platform. See [Defining Constraints and Platforms](/docs/platforms#constraints-platforms) for more information.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

## Members

- [has\_default\_constraint\_value](#has_default_constraint_value)

## has\_default\_constraint\_value

```
bool ConstraintSettingInfo.has_default_constraint_value
```

 Whether there is a default constraint\_value for this setting.

---

## ConstraintValueInfo
- URL: https://bazel.build/rules/lib/providers/ConstraintValueInfo
- Source: rules/lib/providers/ConstraintValueInfo.mdx
- Slug: /rules/lib/providers/ConstraintValueInfo

A value for a constraint setting that can be used to define a platform. See [Defining Constraints and Platforms](/docs/platforms#constraints-platforms) for more information.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

---

## DebugPackageInfo
- URL: https://bazel.build/rules/lib/providers/DebugPackageInfo
- Source: rules/lib/providers/DebugPackageInfo.mdx
- Slug: /rules/lib/providers/DebugPackageInfo

A provider for the binary file and its associated .dwp files, if fission is enabled.If Fission ({@url https://gcc.gnu.org/wiki/DebugFission}) is not enabled, the dwp file will be null.

## Members

- [DebugPackageInfo](#DebugPackageInfo)
- [dwp\_file](#dwp_file)
- [stripped\_file](#stripped_file)
- [target\_label](#target_label)
- [unstripped\_file](#unstripped_file)

## DebugPackageInfo

```
DebugPackageInfo DebugPackageInfo(*, target_label, stripped_file=None, unstripped_file, dwp_file=None)
```

 The `DebugPackageInfo` constructor.


### Parameters

ParameterDescription`target_label`[Label](../builtins/Label.html);
 required

 The label for the \*\_binary target
 `stripped_file`[File](../builtins/File.html); or `None`;
 default is `None`

 The stripped file (the explicit ".stripped" target)
 `unstripped_file`[File](../builtins/File.html);
 required

 The unstripped file (the default executable target).
 `dwp_file`[File](../builtins/File.html); or `None`;
 default is `None`

 The .dwp file (for fission builds) or null if --fission=no.


## dwp\_file

```
File DebugPackageInfo.dwp_file
```

 Returns the .dwp file (for fission builds) or null if --fission=no.
 May return `None`.



## stripped\_file

```
File DebugPackageInfo.stripped_file
```

 Returns the stripped file (the explicit ".stripped" target).
 May return `None`.



## target\_label

```
Label DebugPackageInfo.target_label
```

 Returns the label for the \*\_binary target



## unstripped\_file

```
File DebugPackageInfo.unstripped_file
```

 Returns the unstripped file (the default executable target)

---

## DefaultInfo
- URL: https://bazel.build/rules/lib/providers/DefaultInfo
- Source: rules/lib/providers/DefaultInfo.mdx
- Slug: /rules/lib/providers/DefaultInfo

A provider that gives general information about a target's direct and transitive files. Every rule type has this provider, even if it is not returned explicitly by the rule's implementation function.

See the [rules](https://bazel.build/extending/rules) page for extensive guides on how to use this provider.

## Members

- [DefaultInfo](#DefaultInfo)
- [data\_runfiles](#data_runfiles)
- [default\_runfiles](#default_runfiles)
- [files](#files)
- [files\_to\_run](#files_to_run)

## DefaultInfo

```
DefaultInfo DefaultInfo(*, files=None, runfiles=None, data_runfiles=None, default_runfiles=None, executable=None)
```

 The `DefaultInfo` constructor.


### Parameters

ParameterDescription`files`[depset](../builtins/depset.html); or `None`;
 default is `None`

 A [`depset`](../builtins/depset.html) of [`File`](../builtins/File.html) objects representing the default outputs to build when this target is specified on the bazel command line. By default it is all predeclared outputs.
 `runfiles`[runfiles](../builtins/runfiles.html); or `None`;
 default is `None`

[`runfiles`](../builtins/runfiles.html) descriptor describing the files that this target needs when run (e.g. via the `run` command or as a tool dependency for an action).

 `data_runfiles`[runfiles](../builtins/runfiles.html); or `None`;
 default is `None`

**It is recommended that you avoid using this parameter (see ["runfiles features to avoid"](https://bazel.build/extending/rules#runfiles_features_to_avoid))**

 runfiles descriptor describing the runfiles this target needs to run when it is a dependency via the `data` attribute.
 `default_runfiles`[runfiles](../builtins/runfiles.html); or `None`;
 default is `None`

**It is recommended that you avoid using this parameter (see ["runfiles features to avoid"](https://bazel.build/extending/rules#runfiles_features_to_avoid))**

 runfiles descriptor describing the runfiles this target needs to run when it is a dependency via any attribute other than the `data` attribute.
 `executable`[File](../builtins/File.html); or `None`;
 default is `None`

 If this rule is marked [`executable`](../globals/bzl.html#rule.executable) or [`test`](../globals/bzl.html#rule.test), this is a [`File`](../builtins/File.html) object representing the file that should be executed to run the target. By default it is the predeclared output `ctx.outputs.executable` but it is recommended to pass another file (either predeclared or not) explicitly.


## data\_runfiles

```
runfiles DefaultInfo.data_runfiles
```

 runfiles descriptor describing the files that this target needs when run in the condition that it is a `data` dependency attribute. Under most circumstances, use the `default_runfiles` parameter instead. See ["runfiles features to avoid"](https://bazel.build/extending/rules#runfiles_features_to_avoid) for details.
 May return `None`.



## default\_runfiles

```
runfiles DefaultInfo.default_runfiles
```

 runfiles descriptor describing the files that this target needs when run (via the `run` command or as a tool dependency).
 May return `None`.



## files

```
depset DefaultInfo.files
```

 A [`depset`](../builtins/depset.html) of [`File`](../builtins/File.html) objects representing the default outputs to build when this target is specified on the bazel command line. By default it is all predeclared outputs.
 May return `None`.



## files\_to\_run

```
FilesToRunProvider DefaultInfo.files_to_run
```

 A [`FilesToRunProvider`](../providers/FilesToRunProvider.html) object containing information about the executable and runfiles of the target.
 May return `None`.

---

## ExecutionInfo
- URL: https://bazel.build/rules/lib/providers/ExecutionInfo
- Source: rules/lib/providers/ExecutionInfo.mdx
- Slug: /rules/lib/providers/ExecutionInfo

Use this provider to specify special environment requirements needed to run tests.

## Members

- [ExecutionInfo](#ExecutionInfo)
- [exec\_group](#exec_group)
- [requirements](#requirements)

## ExecutionInfo

```
ExecutionInfo ExecutionInfo(requirements={}, exec_group='test')
```

 Creates an instance.


### Parameters

ParameterDescription`requirements`[dict](../core/dict.html);
 default is `{}`

 A dict indicating special execution requirements, such as hardware platforms.
 `exec_group`[string](../core/string.html);
 default is `'test'`

 The name of the exec group that is used to execute the test.


## exec\_group

```
string ExecutionInfo.exec_group
```

 The name of the exec group that is used to execute the test.



## requirements

```
dict ExecutionInfo.requirements
```

 A dict indicating special execution requirements, such as hardware platforms.

---

## FeatureFlagInfo
- URL: https://bazel.build/rules/lib/providers/FeatureFlagInfo
- Source: rules/lib/providers/FeatureFlagInfo.mdx
- Slug: /rules/lib/providers/FeatureFlagInfo

A provider used to access information about config\_feature\_flag rules.

## Members

- [error](#error)
- [is\_valid\_value](#is_valid_value)
- [value](#value)

## error

```
string FeatureFlagInfo.error
```

 If non-None, this error was generated when trying to compute current value of flag.
 May return `None`.



## is\_valid\_value

```
bool FeatureFlagInfo.is_valid_value(value)
```

 The value of the flag in the configuration used by the flag rule.


### Parameters

ParameterDescription`value`[string](../core/string.html);
 required

 String, the value to check for validity for this flag.


## value

```
string FeatureFlagInfo.value
```

 The current value of the flag in the flag's current configuration. None if there is an error.
 May return `None`.

---

## FilesToRunProvider
- URL: https://bazel.build/rules/lib/providers/FilesToRunProvider
- Source: rules/lib/providers/FilesToRunProvider.mdx
- Slug: /rules/lib/providers/FilesToRunProvider

Contains information about executables produced by a target and the files needed to run it. This provider can not be created directly, it is an implicit output of executable targets accessible via [`DefaultInfo.files_to_run`](../providers/DefaultInfo.html#files_to_run).

## Members

- [executable](#executable)
- [repo\_mapping\_manifest](#repo_mapping_manifest)
- [runfiles\_manifest](#runfiles_manifest)

## executable

```
File FilesToRunProvider.executable
```

 The main executable or None if it does not exist.
 May return `None`.



## repo\_mapping\_manifest

```
File FilesToRunProvider.repo_mapping_manifest
```

 The repo mapping manifest or None if it does not exist.
 May return `None`.



## runfiles\_manifest

```
File FilesToRunProvider.runfiles_manifest
```

 The runfiles manifest or None if it does not exist.
 May return `None`.

---

## IncompatiblePlatformProvider
- URL: https://bazel.build/rules/lib/providers/IncompatiblePlatformProvider
- Source: rules/lib/providers/IncompatiblePlatformProvider.mdx
- Slug: /rules/lib/providers/IncompatiblePlatformProvider

A provider for targets that are incompatible with the target platform. See [Detecting incompatible targets using `bazel cquery`](/docs/platforms#detecting-incompatible-targets-using-bazel-cquery) for more information.

---

## InstrumentedFilesInfo
- URL: https://bazel.build/rules/lib/providers/InstrumentedFilesInfo
- Source: rules/lib/providers/InstrumentedFilesInfo.mdx
- Slug: /rules/lib/providers/InstrumentedFilesInfo

Contains information about source files and instrumentation metadata files for rule targets matched by [`--instrumentation_filter`](https://bazel.build/reference/command-line-reference#flag--instrumentation_filter) for purposes of [code coverage data collection](https://bazel.build/extending/rules#code_coverage). When coverage data collection is enabled, a manifest containing the combined paths in [`instrumented_files`](#instrumented_files) and [`metadata_files`](#metadata_files) are passed to the test action as inputs, with the manifest's path noted in the environment variable `COVERAGE_MANIFEST`. The metadata files, but not the source files, are also passed to the test action as inputs. When `InstrumentedFilesInfo` is returned by an [aspect](https://bazel.build/extending/aspects)'s implementation function, any `InstrumentedFilesInfo` from the base rule target is ignored.

## Members

- [instrumented\_files](#instrumented_files)
- [metadata\_files](#metadata_files)

## instrumented\_files

```
depset InstrumentedFilesInfo.instrumented_files
```

 [`depset`](../builtins/depset.html) of [`File`](../builtins/File.html) objects representing instrumented source files for this target and its dependencies.



## metadata\_files

```
depset InstrumentedFilesInfo.metadata_files
```

 [`depset`](../builtins/depset.html) of [`File`](../builtins/File.html) objects representing coverage metadata files for this target and its dependencies. These files contain additional information required to generate LCOV-format coverage output after the code is executed, e.g. the `.gcno` files generated when `gcc` is run with `-ftest-coverage`.

---

## JavaRuntimeInfo
- URL: https://bazel.build/rules/lib/providers/JavaRuntimeInfo
- Source: rules/lib/providers/JavaRuntimeInfo.mdx
- Slug: /rules/lib/providers/JavaRuntimeInfo

Information about the Java runtime being used.

## Members

- [default\_cds](#default_cds)
- [files](#files)
- [hermetic\_files](#hermetic_files)
- [hermetic\_static\_libs](#hermetic_static_libs)
- [java\_executable\_exec\_path](#java_executable_exec_path)
- [java\_executable\_runfiles\_path](#java_executable_runfiles_path)
- [java\_home](#java_home)
- [java\_home\_runfiles\_path](#java_home_runfiles_path)
- [lib\_ct\_sym](#lib_ct_sym)
- [lib\_modules](#lib_modules)
- [version](#version)

## default\_cds

```
File JavaRuntimeInfo.default_cds
```

 Returns the JDK default CDS archive.
 May return `None`.



## files

```
depset JavaRuntimeInfo.files
```

 Returns the files in the Java runtime.



## hermetic\_files

```
depset JavaRuntimeInfo.hermetic_files
```

 Returns the files in the Java runtime needed for hermetic deployments.



## hermetic\_static\_libs

```
sequence JavaRuntimeInfo.hermetic_static_libs
```

 Returns the JDK static libraries.



## java\_executable\_exec\_path

```
string JavaRuntimeInfo.java_executable_exec_path
```

 Returns the execpath of the Java executable.



## java\_executable\_runfiles\_path

```
string JavaRuntimeInfo.java_executable_runfiles_path
```

 Returns the path of the Java executable in runfiles trees. This should only be used when one needs to access the JVM during the execution of a binary or a test built by Bazel. In particular, when one needs to invoke the JVM during an action, java\_executable\_exec\_path should be used instead.



## java\_home

```
string JavaRuntimeInfo.java_home
```

 Returns the execpath of the root of the Java installation.



## java\_home\_runfiles\_path

```
string JavaRuntimeInfo.java_home_runfiles_path
```

 Returns the path of the Java installation in runfiles trees. This should only be used when one needs to access the JDK during the execution of a binary or a test built by Bazel. In particular, when one needs the JDK during an action, java\_home should be used instead.



## lib\_ct\_sym

```
File JavaRuntimeInfo.lib_ct_sym
```

 Returns the lib/ct.sym file.
 May return `None`.



## lib\_modules

```
File JavaRuntimeInfo.lib_modules
```

 Returns the lib/modules file.
 May return `None`.



## version

```
int JavaRuntimeInfo.version
```

 The Java feature version of the runtime. This is 0 if the version is unknown.

---

## JavaToolchainInfo
- URL: https://bazel.build/rules/lib/providers/JavaToolchainInfo
- Source: rules/lib/providers/JavaToolchainInfo.mdx
- Slug: /rules/lib/providers/JavaToolchainInfo

Provides access to information about the Java toolchain rule. Accessible as a 'java\_toolchain' field on a Target struct.

## Members

- [bootclasspath](#bootclasspath)
- [ijar](#ijar)
- [jacocorunner](#jacocorunner)
- [java\_runtime](#java_runtime)
- [jvm\_opt](#jvm_opt)
- [label](#label)
- [proguard\_allowlister](#proguard_allowlister)
- [single\_jar](#single_jar)
- [source\_version](#source_version)
- [target\_version](#target_version)
- [tools](#tools)

## bootclasspath

```
depset JavaToolchainInfo.bootclasspath
```

 The Java target bootclasspath entries. Corresponds to javac's -bootclasspath flag.



## ijar

```
FilesToRunProvider JavaToolchainInfo.ijar
```

 A FilesToRunProvider representing the ijar executable.



## jacocorunner

```
FilesToRunProvider JavaToolchainInfo.jacocorunner
```

 The jacocorunner used by the toolchain.
 May return `None`.



## java\_runtime

```
JavaRuntimeInfo JavaToolchainInfo.java_runtime
```

 The java runtime information.



## jvm\_opt

```
depset JavaToolchainInfo.jvm_opt
```

 The default options for the JVM running the java compiler and associated tools.



## label

```
Label JavaToolchainInfo.label
```

 The toolchain label.



## proguard\_allowlister

```
FilesToRunProvider JavaToolchainInfo.proguard_allowlister
```

 Return the binary to validate proguard configuration
 May return `None`.



## single\_jar

```
FilesToRunProvider JavaToolchainInfo.single_jar
```

 The SingleJar deploy jar.



## source\_version

```
string JavaToolchainInfo.source_version
```

 The java source version.



## target\_version

```
string JavaToolchainInfo.target_version
```

 The java target version.



## tools

```
depset JavaToolchainInfo.tools
```

 The compilation tools.

---

## MaterializedDepsInfo
- URL: https://bazel.build/rules/lib/providers/MaterializedDepsInfo
- Source: rules/lib/providers/MaterializedDepsInfo.mdx
- Slug: /rules/lib/providers/MaterializedDepsInfo

The provider returned from materializer rules to materialize dependencies.

## Members

- [deps](#deps)

## deps

```
list MaterializedDepsInfo.deps
```

 The list of dependencies. These may be ConfiguredTarget or DormantDependency objects.

---

## ObjcProvider
- URL: https://bazel.build/rules/lib/providers/ObjcProvider
- Source: rules/lib/providers/ObjcProvider.mdx
- Slug: /rules/lib/providers/ObjcProvider

A provider for compilation and linking of objc.

## Members

- [direct\_module\_maps](#direct_module_maps)
- [direct\_sources](#direct_sources)
- [j2objc\_library](#j2objc_library)
- [module\_map](#module_map)
- [source](#source)
- [strict\_include](#strict_include)
- [umbrella\_header](#umbrella_header)

## direct\_module\_maps

```
sequence ObjcProvider.direct_module_maps
```

 Module map files from this target directly (no transitive module maps). Used to enforce proper use of private header files and for Swift compilation.



## direct\_sources

```
sequence ObjcProvider.direct_sources
```

 All direct source files from this target (no transitive files), including any headers in the 'srcs' attribute.



## j2objc\_library

```
depset ObjcProvider.j2objc_library
```

 Static libraries that are built from J2ObjC-translated Java code.



## module\_map

```
depset ObjcProvider.module_map
```

 Clang module maps, used to enforce proper use of private header files.



## source

```
depset ObjcProvider.source
```

 All transitive source files.



## strict\_include

```
depset ObjcProvider.strict_include
```

 Non-propagated include search paths specified with '-I' on the command line. Also known as header search paths (and distinct from _user_ header search paths).



## umbrella\_header

```
depset ObjcProvider.umbrella_header
```

 Clang umbrella header. Public headers are #included in umbrella headers to be compatible with J2ObjC segmented headers.

---

## OutputGroupInfo
- URL: https://bazel.build/rules/lib/providers/OutputGroupInfo
- Source: rules/lib/providers/OutputGroupInfo.mdx
- Slug: /rules/lib/providers/OutputGroupInfo

A provider that indicates what output groups a rule has.

See [Requesting output files](https://bazel.build/extending/rules#requesting_output_files) for more information.

## Members

- [OutputGroupInfo](#OutputGroupInfo)

## OutputGroupInfo

```
OutputGroupInfo OutputGroupInfo(**kwargs)
```

 Instantiate this provider with

```
OutputGroupInfo(group1 = <files>, group2 = <files>...)
```

See [Requesting output files](https://bazel.build/extending/rules#requesting_output_files) for more information.


### Parameters

ParameterDescription`kwargs`
 default is `{}`

 Dictionary of arguments.

---

## PackageSpecificationInfo
- URL: https://bazel.build/rules/lib/providers/PackageSpecificationInfo
- Source: rules/lib/providers/PackageSpecificationInfo.mdx
- Slug: /rules/lib/providers/PackageSpecificationInfo

Information about transitive package specifications used in package groups.

## Members

- [contains](#contains)

## contains

```
bool PackageSpecificationInfo.contains(target)
```

 Checks if a target exists in a package group.


### Parameters

ParameterDescription`target`[Label](../builtins/Label.html); or [string](../core/string.html);
 required

 A target which is checked if it exists inside the package group.

---

## PlatformInfo
- URL: https://bazel.build/rules/lib/providers/PlatformInfo
- Source: rules/lib/providers/PlatformInfo.mdx
- Slug: /rules/lib/providers/PlatformInfo

Provides access to data about a specific platform. See [Defining Constraints and Platforms](/docs/platforms#constraints-platforms) for more information.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

---

## RunEnvironmentInfo
- URL: https://bazel.build/rules/lib/providers/RunEnvironmentInfo
- Source: rules/lib/providers/RunEnvironmentInfo.mdx
- Slug: /rules/lib/providers/RunEnvironmentInfo

A provider that can be returned from executable rules to control the environment in which their executable is executed.

## Members

- [environment](#environment)
- [inherited\_environment](#inherited_environment)

## environment

```
dict RunEnvironmentInfo.environment
```

 A map of string keys and values that represent environment variables and their values. These will be made available when the target that returns this provider is executed, either as a test or via the run command.



## inherited\_environment

```
List RunEnvironmentInfo.inherited_environment
```

 A sequence of names of environment variables. These variables are made available with their current value taken from the shell environment when the target that returns this provider is executed, either as a test or via the run command. If a variable is contained in both `environment` and `inherited_environment`, the value inherited from the shell environment will take precedence if set. This is most useful for test rules, which run with a hermetic environment under `bazel test` and can use this mechanism to non-hermetically include a variable from the outer environment. By contrast, `bazel run` already forwards the outer environment. Note, though, that it may be surprising for an otherwise hermetic test to hardcode a non-hermetic dependency on the environment, and that this may even accidentally expose sensitive information. Prefer setting the test environment explicitly with the `--test_env` flag, and even then prefer to avoid using this flag and instead populate the environment explicitly.

---

## TemplateVariableInfo
- URL: https://bazel.build/rules/lib/providers/TemplateVariableInfo
- Source: rules/lib/providers/TemplateVariableInfo.mdx
- Slug: /rules/lib/providers/TemplateVariableInfo

Encapsulates template variables, that is, variables that can be referenced by strings like `$(VARIABLE)` in BUILD files and expanded by `ctx.expand_make_variables` and implicitly in certain attributes of built-in rules.

`TemplateVariableInfo` can be created by calling its eponymous constructor with a string-to-string dict as an argument that specifies the variables provided.

Example: `platform_common.TemplateVariableInfo({'FOO': 'bar'})`

## Members

- [variables](#variables)

## variables

```
dict TemplateVariableInfo.variables
```

 Returns the make variables defined by this target as a dictionary with string keys and string values

---

## ToolchainInfo
- URL: https://bazel.build/rules/lib/providers/ToolchainInfo
- Source: rules/lib/providers/ToolchainInfo.mdx
- Slug: /rules/lib/providers/ToolchainInfo

Provider returned by [toolchain rules](/docs/toolchains#defining-toolchains) to share data with [rules which depend on toolchains](/docs/toolchains#writing-rules-that-use-toolchains). Read about [toolchains](/docs/toolchains) for more information.

---

## ToolchainTypeInfo
- URL: https://bazel.build/rules/lib/providers/ToolchainTypeInfo
- Source: rules/lib/providers/ToolchainTypeInfo.mdx
- Slug: /rules/lib/providers/ToolchainTypeInfo

Provides access to data about a specific toolchain type.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

## Members

- [type\_label](#type_label)

## type\_label

```
Label ToolchainTypeInfo.type_label
```

 The label uniquely identifying this toolchain type.

---

## file\_provider
- URL: https://bazel.build/rules/lib/providers/file_provider
- Source: rules/lib/providers/file_provider.mdx
- Slug: /rules/lib/providers/file_provider

An interface for rules that provide files.

---

## java\_compilation\_info
- URL: https://bazel.build/rules/lib/providers/java_compilation_info
- Source: rules/lib/providers/java_compilation_info.mdx
- Slug: /rules/lib/providers/java_compilation_info

Provides access to compilation information for Java rules.

## Members

- [boot\_classpath](#boot_classpath)
- [compilation\_classpath](#compilation_classpath)
- [javac\_options](#javac_options)
- [runtime\_classpath](#runtime_classpath)

## boot\_classpath

```
list java_compilation_info.boot_classpath
```

 Boot classpath for this Java target.



## compilation\_classpath

```
depset java_compilation_info.compilation_classpath
```

 Compilation classpath for this Java target.



## javac\_options

```
depset java_compilation_info.javac_options
```

 A depset of options to java compiler. To get the exact list of options passed to javac in the correct order, use the tokenize\_javacopts utility in rules\_java



## runtime\_classpath

```
depset java_compilation_info.runtime_classpath
```

 Run-time classpath for this Java target.

---

## java\_output\_jars
- URL: https://bazel.build/rules/lib/providers/java_output_jars
- Source: rules/lib/providers/java_output_jars.mdx
- Slug: /rules/lib/providers/java_output_jars

Information about outputs of a Java rule. Deprecated: use java\_info.java\_outputs.

## Members

- [jars](#jars)
- [jdeps](#jdeps)
- [native\_headers](#native_headers)

## jars

```
list java_output_jars.jars
```

 Returns information about outputs of this Java/Java-like target. Deprecated: Use java\_info.java\_outputs.



## jdeps

```
File java_output_jars.jdeps
```

 A manifest proto file. The protobuf file containing the manifest generated from JavaBuilder. This function returns a value when exactly one manifest proto file is present in the outputs. Deprecated: Use java\_info.java\_outputs\[i\].jdeps.
 May return `None`.



## native\_headers

```
File java_output_jars.native_headers
```

 A jar containing CC header files supporting native method implementation. This function returns a value when exactly one native headers jar file is present in the outputs. Deprecated: Use java\_info.java\_outputs\[i\].native\_headers\_jar.
 May return `None`.

---

## Repository Rules
- URL: https://bazel.build/rules/lib/repo
- Source: rules/lib/repo/index.mdx
- Slug: /rules/lib/repo

* [Rules related to git](git)
* [Rules related to http](http)
* [Rules related to local directories](local)

# Generic functions for repository rule authors

* [Utility functions on patching](utils)

---

## Top-level Modules
- URL: https://bazel.build/rules/lib/toplevel
- Source: rules/lib/toplevel.mdx
- Slug: /rules/lib/toplevel

This section lists top-level modules. These symbols are available only in .bzl files.

- [apple\_common](/rules/lib/toplevel/apple_common)
- [attr](/rules/lib/toplevel/attr)
- [cc\_common](/rules/lib/toplevel/cc_common)
- [config](/rules/lib/toplevel/config)
- [config\_common](/rules/lib/toplevel/config_common)
- [coverage\_common](/rules/lib/toplevel/coverage_common)
- [java\_common](/rules/lib/toplevel/java_common)
- [native](/rules/lib/toplevel/native)
- [platform\_common](/rules/lib/toplevel/platform_common)
- [proto](/rules/lib/toplevel/proto)
- [testing](/rules/lib/toplevel/testing)

---

## apple\_common
- URL: https://bazel.build/rules/lib/toplevel/apple_common
- Source: rules/lib/toplevel/apple_common.mdx
- Slug: /rules/lib/toplevel/apple_common

Functions for Starlark to access internals of the apple rule implementations.

## Members

- [apple\_host\_system\_env](#apple_host_system_env)
- [apple\_toolchain](#apple_toolchain)
- [dotted\_version](#dotted_version)
- [platform](#platform)
- [platform\_type](#platform_type)
- [target\_apple\_env](#target_apple_env)
- [XcodeProperties](#XcodeProperties)
- [XcodeVersionConfig](#XcodeVersionConfig)

## apple\_host\_system\_env

```
dict apple_common.apple_host_system_env(xcode_config)
```

 Returns a [dict](../core/dict.html) of environment variables that should be set for actions that need to run build tools on an Apple host system, such as the version of Xcode that should be used. The keys are variable names and the values are their corresponding values.


### Parameters

ParameterDescription`xcode_config`
 required

 A provider containing information about the Xcode configuration.


## apple\_toolchain

```
unknown apple_common.apple_toolchain()
```

 Utilities for resolving items from the apple toolchain.



## dotted\_version

```
DottedVersion apple_common.dotted_version(version)
```

 Creates a new [DottedVersion](../builtins/DottedVersion.html) instance.


### Parameters

ParameterDescription`version`[string](../core/string.html);
 required

 The string representation of the DottedVersion.


## platform

```
struct apple_common.platform
```

 An enum-like struct that contains the following fields corresponding to Apple platforms:

- `ios_device`
- `ios_simulator`
- `macos`
- `tvos_device`
- `tvos_simulator`
- `visionos_device`
- `visionos_simulator`
- `watchos_device`
- `watchos_simulator`

These values can be passed to methods that expect a platform, like [XcodeVersionConfig.sdk\_version\_for\_platform](../providers/XcodeVersionConfig.html#sdk_version_for_platform).



## platform\_type

```
struct apple_common.platform_type
```

 An enum-like struct that contains the following fields corresponding to Apple platform types:

- `ios`
- `macos`
- `tvos`
- `visionos`
- `watchos`

These values can be passed to methods that expect a platform type.



## target\_apple\_env

```
dict apple_common.target_apple_env(xcode_config, platform)
```

 Returns a `dict` of environment variables that should be set for actions that build targets of the given Apple platform type. For example, this dictionary contains variables that denote the platform name and SDK version with which to build. The keys are variable names and the values are their corresponding values.


### Parameters

ParameterDescription`xcode_config`
 required

 A provider containing information about the Xcode configuration.
 `platform`
 required

 The apple platform.


## XcodeProperties

```
Provider apple_common.XcodeProperties
```

 The constructor/key for the `XcodeVersionProperties` provider.

If a target propagates the `XcodeVersionProperties` provider, use this as the key with which to retrieve it. Example:

```
dep = ctx.attr.deps[0]
p = dep[apple_common.XcodeVersionProperties]

```

## XcodeVersionConfig

```
Provider apple_common.XcodeVersionConfig
```

 The constructor/key for the `XcodeVersionConfig` provider.

---

## attr
- URL: https://bazel.build/rules/lib/toplevel/attr
- Source: rules/lib/toplevel/attr.mdx
- Slug: /rules/lib/toplevel/attr

This is a top-level module for defining the attribute schemas of a rule or aspect. Each function returns an object representing the schema of a single attribute. These objects are used as the values of the `attrs` dictionary argument of [`rule()`](../globals/bzl.html#rule), [`aspect()`](../globals/bzl.html#aspect), [`repository_rule()`](../globals/bzl.html#repository_rule) and [`tag_class()`](../globals/bzl.html#tag_class).

See the Rules page for more on [defining](https://bazel.build/extending/rules#attributes)
and [using](https://bazel.build/extending/rules#implementation_function) attributes.

## Members

- [bool](#bool)
- [int](#int)
- [int\_list](#int_list)
- [label](#label)
- [label\_keyed\_string\_dict](#label_keyed_string_dict)
- [label\_list](#label_list)
- [label\_list\_dict](#label_list_dict)
- [output](#output)
- [output\_list](#output_list)
- [string](#string)
- [string\_dict](#string_dict)
- [string\_keyed\_label\_dict](#string_keyed_label_dict)
- [string\_list](#string_list)
- [string\_list\_dict](#string_list_dict)

## bool

```
Attribute attr.bool(*, configurable=unbound, default=False, doc=None, mandatory=False)
```

 Creates a schema for a boolean attribute. The corresponding [`ctx.attr`](../builtins/ctx.html#attr) attribute will be of type [`bool`](../core/bool.html).


### Parameters

ParameterDescription`configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[bool](../core/bool.html);
 default is `False`

 A default value to use if no value for this attribute is given when instantiating the rule.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).


## int

```
Attribute attr.int(*, configurable=unbound, default=0, doc=None, mandatory=False, values=[])
```

 Creates a schema for an integer attribute. The value must be in the signed 32-bit range. The corresponding [`ctx.attr`](../builtins/ctx.html#attr) attribute will be of type [`int`](../core/int.html).


### Parameters

ParameterDescription`configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[int](../core/int.html);
 default is `0`

 A default value to use if no value for this attribute is given when instantiating the rule.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `values`[sequence](../core/list.html) of [int](../core/int.html) s;
 default is `[]`

 The list of allowed values for the attribute. An error is raised if any other value is given.


## int\_list

```
Attribute attr.int_list(mandatory=False, allow_empty=True, *, configurable=unbound, default=[], doc=None)
```

 Creates a schema for a list-of-integers attribute. Each element must be in the signed 32-bit range.


### Parameters

ParameterDescription`mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[sequence](../core/list.html) of [int](../core/int.html) s;
 default is `[]`

 A default value to use if no value for this attribute is given when instantiating the rule.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.


## label

```
Attribute attr.label(*, configurable=unbound, default=None, materializer=None, doc=None, executable=False, allow_files=None, allow_single_file=None, mandatory=False, skip_validations=False, providers=[], for_dependency_resolution=unbound, allow_rules=None, cfg=None, aspects=[], flags=[])
```

Creates a schema for a label attribute. This is a dependency attribute.

This attribute contains unique [`Label`](../builtins/Label.html) values. If a string is supplied in place of a `Label`, it will be converted using the [label constructor](../builtins/Label.html#Label). The relative parts of the label path, including the (possibly renamed) repository, are resolved with respect to the instantiated target's package.

At analysis time (within the rule's implementation function), when retrieving the attribute value from `ctx.attr`, labels are replaced by the corresponding [`Target`](../builtins/Target.html) s. This allows you to access the providers of the current target's dependencies.

In addition to ordinary source files, this kind of attribute is often used to refer to a tool -- for example, a compiler. Such tools are considered to be dependencies, just like source files. To avoid requiring users to specify the tool's label every time they use the rule in their BUILD files, you can hard-code the label of a canonical tool as the `default` value of this attribute. If you also want to prevent users from overriding this default, you can make the attribute private by giving it a name that starts with an underscore. See the [Rules](https://bazel.build/extending/rules#private-attributes) page for more information.


### Parameters

ParameterDescription`configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[Label](../builtins/Label.html); or [string](../core/string.html); or [LateBoundDefault](../builtins/LateBoundDefault.html); or NativeComputedDefault; or [function](../core/function.html); or `None`;
 default is `None`

 A default value to use if no value for this attribute is given when instantiating the rule.Use a string or the [`Label`](../builtins/Label.html#Label) function to specify a default value, for example, `attr.label(default = "//a:b")`.
 `materializer`[function](../core/function.html);
 default is `None`

**Experimental**. This parameter is experimental and may change at any time. Please do not depend on it. It may be enabled on an experimental basis by setting `--experimental_dormant_deps`

If set, the attribute materializes dormant dependencies from the transitive closure. The value of this parameter must be a functon that gets access to the values of the attributes of the rule that either are not dependencies or are marked as available for dependency resolution. It must return either a dormant dependency or a list of them depending on the type of the attribute
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `executable`[bool](../core/bool.html);
 default is `False`

 True if the dependency has to be executable. This means the label must refer to an executable file, or to a rule that outputs an executable file. Access the label with `ctx.executable.<attribute_name>`.
 `allow_files`[bool](../core/bool.html); or [sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Whether `File` targets are allowed. Can be `True`, `False` (default), or a list of file extensions that are allowed (for example, `[".cc", ".cpp"]`).
 `allow_single_file`
 default is `None`

 This is similar to `allow_files`, with the restriction that the label must correspond to a single [File](../builtins/File.html). Access it through `ctx.file.<attribute_name>`.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `skip_validations`[bool](../core/bool.html);
 default is `False`

 If true, validation actions of transitive dependencies from this attribute will not run. This is a temporary mitigation and WILL be removed in the future.
 `providers`[sequence](../core/list.html);
 default is `[]`

 The providers that must be given by any dependency appearing in this attribute.

The format of this argument is a list of lists of providers -- `*Info` objects returned by [`provider()`](../globals/bzl.html#provider) (or in the case of a legacy provider, its string name). The dependency must return ALL providers mentioned in at least ONE of the inner lists. As a convenience, this argument may also be a single-level list of providers, in which case it is wrapped in an outer list with one element (i.e. `[A, B]` means `[[A, B]]`). It is NOT required that the rule of the dependency advertises those providers in its `provides` parameter, however, it is considered best practice.


`for_dependency_resolution`
 default is `unbound`

 If this is set, the attribute is available for materializers. Only rules marked with the flag of the same name are allowed to be referenced through such attributes.
 `allow_rules`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Which rule targets (name of the classes) are allowed. This is deprecated (kept only for compatibility), use providers instead.
 `cfg`
 default is `None`

[Configuration](https://bazel.build/extending/rules#configurations) of the attribute. It can be either `"exec"`, which indicates that the dependency is built for the `execution platform`, or `"target"`, which indicates that the dependency is build for the `target platform`. A typical example of the difference is when building mobile apps, where the `target platform` is `Android` or `iOS` while the `execution platform` is `Linux`, `macOS`, or `Windows`. This parameter is required if `executable` is True to guard against accidentally building host tools in the target configuration. `"target"` has no semantic effect, so don't set it when `executable` is False unless it really helps clarify your intentions.
 `aspects`[sequence](../core/list.html) of [Aspect](../builtins/Aspect.html) s;
 default is `[]`

 Aspects that should be applied to the dependency or dependencies specified by this attribute.
 `flags`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Deprecated, will be removed.


## label\_keyed\_string\_dict

```
Attribute attr.label_keyed_string_dict(allow_empty=True, *, configurable=unbound, default={}, doc=None, allow_files=None, allow_rules=None, providers=[], for_dependency_resolution=unbound, flags=[], mandatory=False, skip_validations=False, cfg=None, aspects=[])
```

Creates a schema for an attribute holding a dictionary, where the keys are labels and the values are strings. This is a dependency attribute.

This attribute contains unique [`Label`](../builtins/Label.html) values. If a string is supplied in place of a `Label`, it will be converted using the [label constructor](../builtins/Label.html#Label). The relative parts of the label path, including the (possibly renamed) repository, are resolved with respect to the instantiated target's package.

At analysis time (within the rule's implementation function), when retrieving the attribute value from `ctx.attr`, labels are replaced by the corresponding [`Target`](../builtins/Target.html) s. This allows you to access the providers of the current target's dependencies.


### Parameters

ParameterDescription`allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[dict](../core/dict.html); or [function](../core/function.html);
 default is `{}`

 A default value to use if no value for this attribute is given when instantiating the rule.Use strings or the [`Label`](../builtins/Label.html#Label) function to specify default values, for example, `attr.label_keyed_string_dict(default = {"//a:b": "value", "//a:c": "string"})`.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `allow_files`[bool](../core/bool.html); or [sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Whether `File` targets are allowed. Can be `True`, `False` (default), or a list of file extensions that are allowed (for example, `[".cc", ".cpp"]`).
 `allow_rules`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Which rule targets (name of the classes) are allowed. This is deprecated (kept only for compatibility), use providers instead.
 `providers`[sequence](../core/list.html);
 default is `[]`

 The providers that must be given by any dependency appearing in this attribute.

The format of this argument is a list of lists of providers -- `*Info` objects returned by [`provider()`](../globals/bzl.html#provider) (or in the case of a legacy provider, its string name). The dependency must return ALL providers mentioned in at least ONE of the inner lists. As a convenience, this argument may also be a single-level list of providers, in which case it is wrapped in an outer list with one element (i.e. `[A, B]` means `[[A, B]]`). It is NOT required that the rule of the dependency advertises those providers in its `provides` parameter, however, it is considered best practice.


`for_dependency_resolution`
 default is `unbound`

 If this is set, the attribute is available for materializers. Only rules marked with the flag of the same name are allowed to be referenced through such attributes.
 `flags`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Deprecated, will be removed.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `skip_validations`[bool](../core/bool.html);
 default is `False`

 If true, validation actions of transitive dependencies from this attribute will not run. This is a temporary mitigation and WILL be removed in the future.
 `cfg`
 default is `None`

[Configuration](https://bazel.build/extending/rules#configurations) of the attribute. It can be either `"exec"`, which indicates that the dependency is built for the `execution platform`, or `"target"`, which indicates that the dependency is build for the `target platform`. A typical example of the difference is when building mobile apps, where the `target platform` is `Android` or `iOS` while the `execution platform` is `Linux`, `macOS`, or `Windows`.
 `aspects`[sequence](../core/list.html) of [Aspect](../builtins/Aspect.html) s;
 default is `[]`

 Aspects that should be applied to the dependency or dependencies specified by this attribute.


## label\_list

```
Attribute attr.label_list(allow_empty=True, *, configurable=unbound, default=[], materializer=None, doc=None, allow_files=None, allow_rules=None, providers=[], for_dependency_resolution=unbound, flags=[], mandatory=False, skip_validations=False, cfg=None, aspects=[])
```

Creates a schema for a list-of-labels attribute. This is a dependency attribute. The corresponding [`ctx.attr`](../builtins/ctx.html#attr) attribute will be of type [list](../core/list.html) of [`Target` s](../builtins/Target.html).

This attribute contains unique [`Label`](../builtins/Label.html) values. If a string is supplied in place of a `Label`, it will be converted using the [label constructor](../builtins/Label.html#Label). The relative parts of the label path, including the (possibly renamed) repository, are resolved with respect to the instantiated target's package.

At analysis time (within the rule's implementation function), when retrieving the attribute value from `ctx.attr`, labels are replaced by the corresponding [`Target`](../builtins/Target.html) s. This allows you to access the providers of the current target's dependencies.


### Parameters

ParameterDescription`allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[sequence](../core/list.html) of [Label](../builtins/Label.html) s; or [function](../core/function.html);
 default is `[]`

 A default value to use if no value for this attribute is given when instantiating the rule.Use strings or the [`Label`](../builtins/Label.html#Label) function to specify default values, for example, `attr.label_list(default = ["//a:b", "//a:c"])`.
 `materializer`[function](../core/function.html);
 default is `None`

**Experimental**. This parameter is experimental and may change at any time. Please do not depend on it. It may be enabled on an experimental basis by setting `--experimental_dormant_deps`

If set, the attribute materializes dormant dependencies from the transitive closure. The value of this parameter must be a functon that gets access to the values of the attributes of the rule that either are not dependencies or are marked as available for dependency resolution. It must return either a dormant dependency or a list of them depending on the type of the attribute
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `allow_files`[bool](../core/bool.html); or [sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Whether `File` targets are allowed. Can be `True`, `False` (default), or a list of file extensions that are allowed (for example, `[".cc", ".cpp"]`).
 `allow_rules`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Which rule targets (name of the classes) are allowed. This is deprecated (kept only for compatibility), use providers instead.
 `providers`[sequence](../core/list.html);
 default is `[]`

 The providers that must be given by any dependency appearing in this attribute.

The format of this argument is a list of lists of providers -- `*Info` objects returned by [`provider()`](../globals/bzl.html#provider) (or in the case of a legacy provider, its string name). The dependency must return ALL providers mentioned in at least ONE of the inner lists. As a convenience, this argument may also be a single-level list of providers, in which case it is wrapped in an outer list with one element (i.e. `[A, B]` means `[[A, B]]`). It is NOT required that the rule of the dependency advertises those providers in its `provides` parameter, however, it is considered best practice.


`for_dependency_resolution`
 default is `unbound`

 If this is set, the attribute is available for materializers. Only rules marked with the flag of the same name are allowed to be referenced through such attributes.
 `flags`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Deprecated, will be removed.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `skip_validations`[bool](../core/bool.html);
 default is `False`

 If true, validation actions of transitive dependencies from this attribute will not run. This is a temporary mitigation and WILL be removed in the future.
 `cfg`
 default is `None`

[Configuration](https://bazel.build/extending/rules#configurations) of the attribute. It can be either `"exec"`, which indicates that the dependency is built for the `execution platform`, or `"target"`, which indicates that the dependency is build for the `target platform`. A typical example of the difference is when building mobile apps, where the `target platform` is `Android` or `iOS` while the `execution platform` is `Linux`, `macOS`, or `Windows`.
 `aspects`[sequence](../core/list.html) of [Aspect](../builtins/Aspect.html) s;
 default is `[]`

 Aspects that should be applied to the dependency or dependencies specified by this attribute.


## label\_list\_dict

```
Attribute attr.label_list_dict(allow_empty=True, *, configurable=unbound, default={}, doc=None, allow_files=None, allow_rules=None, providers=[], for_dependency_resolution=unbound, flags=[], mandatory=False, skip_validations=False, cfg=None, aspects=[])
```

Creates a schema for an attribute holding a dictionary, where the keys are strings and the values are list of labels. This is a dependency attribute.

This attribute contains unique [`Label`](../builtins/Label.html) values. If a string is supplied in place of a `Label`, it will be converted using the [label constructor](../builtins/Label.html#Label). The relative parts of the label path, including the (possibly renamed) repository, are resolved with respect to the instantiated target's package.

At analysis time (within the rule's implementation function), when retrieving the attribute value from `ctx.attr`, labels are replaced by the corresponding [`Target`](../builtins/Target.html) s. This allows you to access the providers of the current target's dependencies.


### Parameters

ParameterDescription`allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[dict](../core/dict.html);
 default is `{}`

 A default value to use if no value for this attribute is given when instantiating the rule.Use strings or the [`Label`](../builtins/Label.html#Label) function to specify default values, for example,
`attr.label_list_dict(default = {"key1": ["//a:b", "//a:c"], "key2":
[Label("@my_repo//d:e")]})`.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `allow_files`[bool](../core/bool.html); or [sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Whether `File` targets are allowed. Can be `True`, `False` (default), or a list of file extensions that are allowed (for example, `[".cc", ".cpp"]`).
 `allow_rules`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Which rule targets (name of the classes) are allowed. This is deprecated (kept only for compatibility), use providers instead.
 `providers`[sequence](../core/list.html);
 default is `[]`

 The providers that must be given by any dependency appearing in this attribute.

The format of this argument is a list of lists of providers -- `*Info` objects returned by [`provider()`](../globals/bzl.html#provider) (or in the case of a legacy provider, its string name). The dependency must return ALL providers mentioned in at least ONE of the inner lists. As a convenience, this argument may also be a single-level list of providers, in which case it is wrapped in an outer list with one element (i.e. `[A, B]` means `[[A, B]]`). It is NOT required that the rule of the dependency advertises those providers in its `provides` parameter, however, it is considered best practice.


`for_dependency_resolution`
 default is `unbound`

 If this is set, the attribute is available for materializers. Only rules marked with the flag of the same name are allowed to be referenced through such attributes.
 `flags`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Deprecated, will be removed.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `skip_validations`[bool](../core/bool.html);
 default is `False`

 If true, validation actions of transitive dependencies from this attribute will not run. This is a temporary mitigation and WILL be removed in the future.
 `cfg`
 default is `None`

[Configuration](https://bazel.build/extending/rules#configurations) of the attribute. It can be either `"exec"`, which indicates that the dependency is built for the `execution platform`, or `"target"`, which indicates that the dependency is build for the `target platform`. A typical example of the difference is when building mobile apps, where the `target platform` is `Android` or `iOS` while the `execution platform` is `Linux`, `macOS`, or `Windows`.
 `aspects`[sequence](../core/list.html) of [Aspect](../builtins/Aspect.html) s;
 default is `[]`

 Aspects that should be applied to the dependency or dependencies specified by this attribute.


## output

```
Attribute attr.output(*, doc=None, mandatory=False)
```

Creates a schema for an output (label) attribute.

This attribute contains unique [`Label`](../builtins/Label.html) values. If a string is supplied in place of a `Label`, it will be converted using the [label constructor](../builtins/Label.html#Label). The relative parts of the label path, including the (possibly renamed) repository, are resolved with respect to the instantiated target's package.

At analysis time, the corresponding [`File`](../builtins/File.html) can be retrieved using [`ctx.outputs`](../builtins/ctx.html#outputs).


### Parameters

ParameterDescription`doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).


## output\_list

```
Attribute attr.output_list(allow_empty=True, *, doc=None, mandatory=False)
```

 Creates a schema for a list-of-outputs attribute.

This attribute contains unique [`Label`](../builtins/Label.html) values. If a string is supplied in place of a `Label`, it will be converted using the [label constructor](../builtins/Label.html#Label). The relative parts of the label path, including the (possibly renamed) repository, are resolved with respect to the instantiated target's package.

At analysis time, the corresponding [`File`](../builtins/File.html) can be retrieved using [`ctx.outputs`](../builtins/ctx.html#outputs).


### Parameters

ParameterDescription`allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).


## string

```
Attribute attr.string(*, configurable=unbound, default='', doc=None, mandatory=False, values=[])
```

 Creates a schema for a [string](../core/string.html#attr) attribute.


### Parameters

ParameterDescription`configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[string](../core/string.html); or NativeComputedDefault;
 default is `''`

 A default value to use if no value for this attribute is given when instantiating the rule.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `values`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 The list of allowed values for the attribute. An error is raised if any other value is given.


## string\_dict

```
Attribute attr.string_dict(allow_empty=True, *, configurable=unbound, default={}, doc=None, mandatory=False)
```

 Creates a schema for an attribute holding a dictionary, where the keys and values are strings.


### Parameters

ParameterDescription`allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[dict](../core/dict.html);
 default is `{}`

 A default value to use if no value for this attribute is given when instantiating the rule.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).


## string\_keyed\_label\_dict

```
Attribute attr.string_keyed_label_dict(allow_empty=True, *, configurable=unbound, default={}, doc=None, allow_files=None, allow_rules=None, providers=[], for_dependency_resolution=unbound, flags=[], mandatory=False, cfg=None, aspects=[])
```

Creates a schema for an attribute whose value is a dictionary where the keys are strings and the values are labels. This is a dependency attribute.

This attribute contains unique [`Label`](../builtins/Label.html) values. If a string is supplied in place of a `Label`, it will be converted using the [label constructor](../builtins/Label.html#Label). The relative parts of the label path, including the (possibly renamed) repository, are resolved with respect to the instantiated target's package.

At analysis time (within the rule's implementation function), when retrieving the attribute value from `ctx.attr`, labels are replaced by the corresponding [`Target`](../builtins/Target.html) s. This allows you to access the providers of the current target's dependencies.


### Parameters

ParameterDescription`allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[dict](../core/dict.html); or [function](../core/function.html);
 default is `{}`

 A default value to use if no value for this attribute is given when instantiating the rule.Use strings or the [`Label`](../builtins/Label.html#Label) function to specify default values, for example, `attr.string_keyed_label_dict(default = {"foo": "//a:b", "bar": "//a:c"})`.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `allow_files`[bool](../core/bool.html); or [sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Whether `File` targets are allowed. Can be `True`, `False` (default), or a list of file extensions that are allowed (for example, `[".cc", ".cpp"]`).
 `allow_rules`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Which rule targets (name of the classes) are allowed. This is deprecated (kept only for compatibility), use providers instead.
 `providers`[sequence](../core/list.html);
 default is `[]`

 The providers that must be given by any dependency appearing in this attribute.

The format of this argument is a list of lists of providers -- `*Info` objects returned by [`provider()`](../globals/bzl.html#provider) (or in the case of a legacy provider, its string name). The dependency must return ALL providers mentioned in at least ONE of the inner lists. As a convenience, this argument may also be a single-level list of providers, in which case it is wrapped in an outer list with one element (i.e. `[A, B]` means `[[A, B]]`). It is NOT required that the rule of the dependency advertises those providers in its `provides` parameter, however, it is considered best practice.


`for_dependency_resolution`
 default is `unbound`

 If this is set, the attribute is available for materializers. Only rules marked with the flag of the same name are allowed to be referenced through such attributes.
 `flags`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Deprecated, will be removed.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `cfg`
 default is `None`

[Configuration](https://bazel.build/extending/rules#configurations) of the attribute. It can be either `"exec"`, which indicates that the dependency is built for the `execution platform`, or `"target"`, which indicates that the dependency is build for the `target platform`. A typical example of the difference is when building mobile apps, where the `target platform` is `Android` or `iOS` while the `execution platform` is `Linux`, `macOS`, or `Windows`.
 `aspects`[sequence](../core/list.html) of [Aspect](../builtins/Aspect.html) s;
 default is `[]`

 Aspects that should be applied to the dependency or dependencies specified by this attribute.


## string\_list

```
Attribute attr.string_list(mandatory=False, allow_empty=True, *, configurable=unbound, default=[], doc=None)
```

 Creates a schema for a list-of-strings attribute.


### Parameters

ParameterDescription`mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).
 `allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[sequence](../core/list.html) of [string](../core/string.html) s; or NativeComputedDefault;
 default is `[]`

 A default value to use if no value for this attribute is given when instantiating the rule.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.


## string\_list\_dict

```
Attribute attr.string_list_dict(allow_empty=True, *, configurable=unbound, default={}, doc=None, mandatory=False)
```

 Creates a schema for an attribute holding a dictionary, where the keys are strings and the values are lists of strings.


### Parameters

ParameterDescription`allow_empty`[bool](../core/bool.html);
 default is `True`

 True if the attribute can be empty.
 `configurable`[bool](../core/bool.html); or unbound;
 default is `unbound`

 This argument can only be specified for an attribute of a symbolic macro.

If `configurable` is explicitly set to `False`, the symbolic macro attribute is non-configurable - in other words, it cannot take a `select()` value. If the `configurable` is either unbound or explicitly set to `True`, the attribute is configurable and can take a `select()` value.

For an attribute of a rule or aspect, `configurable` must be left unbound. Most Starlark rule attributes are always configurable, with the exception of `attr.output()`, `attr.output_list()`, and `attr.license()` rule attributes, which are always non-configurable.


`default`[dict](../core/dict.html);
 default is `{}`

 A default value to use if no value for this attribute is given when instantiating the rule.
 `doc`[string](../core/string.html); or `None`;
 default is `None`

 A description of the attribute that can be extracted by documentation generating tools.
 `mandatory`[bool](../core/bool.html);
 default is `False`

 If true, the value must be specified explicitly (even if it has a `default`).

---

## cc\_common
- URL: https://bazel.build/rules/lib/toplevel/cc_common
- Source: rules/lib/toplevel/cc_common.mdx
- Slug: /rules/lib/toplevel/cc_common

Utilities for C++ compilation, linking, and command line generation.

## Members

- [action\_is\_enabled](#action_is_enabled)
- [CcToolchainInfo](#CcToolchainInfo)
- [compile](#compile)
- [configure\_features](#configure_features)
- [create\_cc\_toolchain\_config\_info](#create_cc_toolchain_config_info)
- [create\_compilation\_context](#create_compilation_context)
- [create\_compilation\_outputs](#create_compilation_outputs)
- [create\_compile\_variables](#create_compile_variables)
- [create\_library\_to\_link](#create_library_to_link)
- [create\_link\_variables](#create_link_variables)
- [create\_linker\_input](#create_linker_input)
- [create\_linking\_context](#create_linking_context)
- [create\_linking\_context\_from\_compilation\_outputs](#create_linking_context_from_compilation_outputs)
- [create\_lto\_compilation\_context](#create_lto_compilation_context)
- [do\_not\_use\_tools\_cpp\_compiler\_present](#do_not_use_tools_cpp_compiler_present)
- [get\_environment\_variables](#get_environment_variables)
- [get\_execution\_requirements](#get_execution_requirements)
- [get\_memory\_inefficient\_command\_line](#get_memory_inefficient_command_line)
- [get\_tool\_for\_action](#get_tool_for_action)
- [is\_enabled](#is_enabled)
- [link](#link)
- [merge\_cc\_infos](#merge_cc_infos)
- [merge\_compilation\_contexts](#merge_compilation_contexts)
- [merge\_compilation\_outputs](#merge_compilation_outputs)

## action\_is\_enabled

```
bool cc_common.action_is_enabled(*, feature_configuration, action_name)
```

 Returns True if given action\_config is enabled in the feature configuration.


### Parameters

ParameterDescription`feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `action_name`[string](../core/string.html);
 required

 Name of the action\_config.


## CcToolchainInfo

```
Provider cc_common.CcToolchainInfo
```

 The key used to retrieve the provider that contains information about the C++ toolchain being used



## compile

```
tuple cc_common.compile(*, actions, feature_configuration, cc_toolchain, srcs=[], public_hdrs=[], private_hdrs=[], includes=[], quote_includes=[], system_includes=[], framework_includes=[], defines=[], local_defines=[], include_prefix='', strip_include_prefix='', user_compile_flags=[], conly_flags=[], cxx_flags=[], compilation_contexts=[], name, disallow_pic_outputs=False, disallow_nopic_outputs=False, additional_inputs=[], module_interfaces=unbound)
```

 Should be used for C++ compilation. Returns tuple of ( `CompilationContext`, `CcCompilationOutputs`).


### Parameters

ParameterDescription`actions`[actions](../builtins/actions.html);
 required

`actions` object.
 `feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

`feature_configuration` to be queried.
 `cc_toolchain`
 Info;
 required

`CcToolchainInfo` provider to be used.
 `srcs`[sequence](../core/list.html);
 default is `[]`

 The list of source files to be compiled.
 `public_hdrs`[sequence](../core/list.html);
 default is `[]`

 List of headers needed for compilation of srcs and may be included by dependent rules transitively.
 `private_hdrs`[sequence](../core/list.html);
 default is `[]`

 List of headers needed for compilation of srcs and NOT to be included by dependent rules.
 `includes`[sequence](../core/list.html); or [depset](../builtins/depset.html);
 default is `[]`

 Search paths for header files referenced both by angle bracket and quotes. Usually passed with -I. Propagated to dependents transitively.
 `quote_includes`[sequence](../core/list.html);
 default is `[]`

 Search paths for header files referenced by quotes, e.g. #include "foo/bar/header.h". They can be either relative to the exec root or absolute. Usually passed with -iquote. Propagated to dependents transitively.
 `system_includes`[sequence](../core/list.html);
 default is `[]`

 Search paths for header files referenced by angle brackets, e.g. #include &lt;foo/bar/header.h&gt;. They can be either relative to the exec root or absolute. Usually passed with -isystem. Propagated to dependents transitively.
 `framework_includes`[sequence](../core/list.html);
 default is `[]`

 Search paths for header files from Apple frameworks. They can be either relative to the exec root or absolute. Usually passed with -F. Propagated to dependents transitively.
 `defines`[sequence](../core/list.html);
 default is `[]`

 Set of defines needed to compile this target. Each define is a string. Propagated to dependents transitively.
 `local_defines`[sequence](../core/list.html);
 default is `[]`

 Set of defines needed to compile this target. Each define is a string. Not propagated to dependents transitively.
 `include_prefix`[string](../core/string.html);
 default is `''`

 The prefix to add to the paths of the headers of this rule. When set, the headers in the hdrs attribute of this rule are accessible at is the value of this attribute prepended to their repository-relative path. The prefix in the strip\_include\_prefix attribute is removed before this prefix is added.
 `strip_include_prefix`[string](../core/string.html);
 default is `''`

 The prefix to strip from the paths of the headers of this rule. When set, the headers in the hdrs attribute of this rule are accessible at their path with this prefix cut off. If it's a relative path, it's taken as a package-relative one. If it's an absolute one, it's understood as a repository-relative path. The prefix in the include\_prefix attribute is added after this prefix is stripped.
 `user_compile_flags`[sequence](../core/list.html);
 default is `[]`

 Additional list of compilation options.
 `conly_flags`[sequence](../core/list.html);
 default is `[]`

 Additional list of compilation options for C compiles.
 `cxx_flags`[sequence](../core/list.html);
 default is `[]`

 Additional list of compilation options for C++ compiles.
 `compilation_contexts`[sequence](../core/list.html);
 default is `[]`

 Headers from dependencies used for compilation.
 `name`[string](../core/string.html);
 required

 This is used for naming the output artifacts of actions created by this method. See also the \`main\_output\` arg.
 `disallow_pic_outputs`[bool](../core/bool.html);
 default is `False`

 Whether PIC outputs should be created.
 `disallow_nopic_outputs`[bool](../core/bool.html);
 default is `False`

 Whether NOPIC outputs should be created.
 `additional_inputs`[sequence](../core/list.html);
 default is `[]`

 List of additional files needed for compilation of srcs
 `module_interfaces`[sequence](../core/list.html);
 default is `unbound`

 The list of module interfaces source files to be compiled. Note: this is an experimental feature, only enabled with --experimental\_cpp\_modules


## configure\_features

```
FeatureConfiguration cc_common.configure_features(*, ctx, cc_toolchain, language=None, requested_features=[], unsupported_features=[])
```

 Creates a feature\_configuration instance. Requires the cpp configuration fragment.


### Parameters

ParameterDescription`ctx`[ctx](../builtins/ctx.html);
 required

 The rule context.
 `cc_toolchain`
 Info;
 required

 cc\_toolchain for which we configure features.
 `language`[string](../core/string.html); or `None`;
 default is `None`

 The language to configure for: either c++ or objc (default c++)
 `requested_features`[sequence](../core/list.html);
 default is `[]`

 List of features to be enabled.
 `unsupported_features`[sequence](../core/list.html);
 default is `[]`

 List of features that are unsupported by the current rule.


## create\_cc\_toolchain\_config\_info

```
None cc_common.create_cc_toolchain_config_info(*, ctx, features=[], action_configs=[], artifact_name_patterns=[], cxx_builtin_include_directories=[], toolchain_identifier, host_system_name=None, target_system_name=None, target_cpu=None, target_libc=None, compiler, abi_version=None, abi_libc_version=None, tool_paths=[], make_variables=[], builtin_sysroot=None)
```

 Creates a `CcToolchainConfigInfo` provider


### Parameters

ParameterDescription`ctx`[ctx](../builtins/ctx.html);
 required

 The rule context.
 `features`[sequence](../core/list.html);
 default is `[]`

 Contains all flag specifications for one feature.

Arguments:

`name`: The feature's name. It is possible to introduce a feature without a change to Bazel by adding a 'feature' section to the toolchain and adding the corresponding string as feature in the `BUILD` file.

`enabled`: If 'True', this feature is enabled unless a rule type explicitly marks it as unsupported.

`flag_sets`: A FlagSet list. If the given feature is enabled, the flag sets will be applied for the actions are specified for.

`env_sets`: an EnvSet list. If the given feature is enabled, the env sets will be applied for the actions they are specified for.

`requires`: A list of feature sets defining when this feature is supported by the toolchain. The feature is supported if any of the feature sets fully apply, that is, when all features of a feature set are enabled. If `requires` is omitted, the feature is supported independently of which other features are enabled. Use this for example to filter flags depending on the build mode enabled (opt / fastbuild / dbg).

`implies`: A string list of features or action configs that are automatically enabled when this feature is enabled. If any of the implied features or action configs cannot be enabled, this feature will (silently) not be enabled either.

`provides`: A list of names this feature conflicts with.

A feature cannot be enabled if:

\- `provides` contains the name of a different feature or action config that we want to enable.

\- `provides` contains the same value as a 'provides' in a different feature or action config that we want to enable. Use this in order to ensure that incompatible features cannot be accidentally activated at the same time, leading to hard to diagnose compiler errors.
 `action_configs`[sequence](../core/list.html);
 default is `[]`

 An action config corresponds to a Bazel action, and allows selection of a tool based on activated features. Action config activation occurs by the same semantics as features: a feature can 'require' or 'imply' an action config in the same way that it would another feature.

Arguments:

`action_name`: The name of the Bazel action that this config applies to, e.g. 'c-compile' or 'c-module-compile'.

`enabled`: If 'True', this action is enabled unless a rule type explicitly marks it as unsupported.

`tools`: The tool applied to the action will be the first tool with a feature set that matches the feature configuration. An error will be thrown if no tool matches a provided feature configuration - for that reason, it's a good idea to provide a default tool with an empty feature set.

`flag_sets`: If the given action config is enabled, the flag sets will be applied to the corresponding action.

`implies`: A list of features or action configs that are automatically enabled when this action config is enabled. If any of the implied features or action configs cannot be enabled, this action config will (silently) not be enabled either.

`artifact_name_patterns`[sequence](../core/list.html);
 default is `[]`

 The name for an artifact of a given category of input or output artifacts to an action.

Arguments:

`category_name`: The category of artifacts that this selection applies to. This field is compared against a list of categories defined in Bazel. Example categories include "linked\_output" or the artifact for this selection. Together with the extension it is used to create an artifact name based on the target name.

`extension`: The extension for creating the artifact for this selection. Together with the prefix it is used to create an artifact name based on the target name.

`cxx_builtin_include_directories`[sequence](../core/list.html);
 default is `[]`

Built-in include directories for C++ compilation. These should be the exact paths used by the compiler, and are generally relative to the exec root.

The paths used by the compiler can be determined by 'gcc -E -xc++ - -v'.

We currently use the C++ paths also for C compilation, which is safe as long as there are no name clashes between C++ and C header files.

Relative paths are resolved relative to the configuration file directory.

If the compiler has --sysroot support, then these paths should use %sysroot% rather than the include path, and specify the sysroot attribute in order to give blaze the information necessary to make the correct replacements.

`toolchain_identifier`[string](../core/string.html);
 required

The unique identifier of the toolchain within the crosstool release. It must be possible to use this as a directory name in a path.

It has to match the following regex: \[a-zA-Z\_\]\[\\.\\- \\w\]\*

`host_system_name`[string](../core/string.html); or `None`;
 default is `None`

 Ignored.
 `target_system_name`[string](../core/string.html); or `None`;
 default is `None`

 Deprecated. The GNU System Name. The string is exposed to CcToolchainInfo.target\_gnu\_system\_name.
 `target_cpu`[string](../core/string.html); or `None`;
 default is `None`

 Deprecated: Use cpu based constraints instead. If the string is "k8", \`target\_cpu\` will be omitted from the filename of raw FDO profile data.
 `target_libc`[string](../core/string.html); or `None`;
 default is `None`

 Deprecated: Use OS based constraints instead. The libc version string (e.g. "glibc-2.2.2"). If the string is "macosx", platform is assumed to be MacOS. Otherwise, Linux. The string is exposed to CcToolchainInfo.libc.
 `compiler`[string](../core/string.html);
 required

 The compiler string (e.g. "gcc"). The current toolchain's compiler is exposed to \`@bazel\_tools//tools/cpp:compiler (compiler\_flag)\` as a flag value. Targets that require compiler-specific flags can use the config\_settings in https://github.com/bazelbuild/rules\_cc/blob/main/cc/compiler/BUILD in select() statements or create custom config\_setting if the existing settings don't suffice.
 `abi_version`[string](../core/string.html); or `None`;
 default is `None`

 The abi in use, which is a gcc version. E.g.: "gcc-3.4". The string is set to C++ toolchain variable ABI.
 `abi_libc_version`[string](../core/string.html); or `None`;
 default is `None`

 The glibc version used by the abi we're using. The string is set to C++ toolchain variable ABI\_LIBC\_VERSION.
 `tool_paths`[sequence](../core/list.html);
 default is `[]`

 Tool locations.

Arguments:

`name`: Name of the tool.

`path`: Location of the tool; Can be absolute path (in case of non hermetic toolchain), or path relative to the cc\_toolchain's package.

`make_variables`[sequence](../core/list.html);
 default is `[]`

 A make variable that is made accessible to rules.
 `builtin_sysroot`[string](../core/string.html); or `None`;
 default is `None`

 The built-in sysroot. If this attribute is not present, Bazel does not allow using a different sysroot, i.e. through the --grte\_top option.


## create\_compilation\_context

```
CompilationContext cc_common.create_compilation_context(*, headers=unbound, system_includes=unbound, includes=unbound, quote_includes=unbound, framework_includes=unbound, defines=unbound, local_defines=unbound)
```

 Creates a `CompilationContext`.


### Parameters

ParameterDescription`headers`
 default is `unbound`

 Set of headers needed to compile this target
 `system_includes`
 default is `unbound`

 Set of search paths for header files referenced by angle brackets, i.e. #include &lt;foo/bar/header.h&gt;. They can be either relative to the exec root or absolute. Usually passed with -isystem
 `includes`
 default is `unbound`

 Set of search paths for header files referenced both by angle bracket and quotes.Usually passed with -I
 `quote_includes`
 default is `unbound`

 Set of search paths for header files referenced by quotes, i.e. #include "foo/bar/header.h". They can be either relative to the exec root or absolute. Usually passed with -iquote
 `framework_includes`
 default is `unbound`

 Set of framework search paths for header files (Apple platform only)
 `defines`
 default is `unbound`

 Set of defines needed to compile this target. Each define is a string. Propagated transitively to dependents.
 `local_defines`
 default is `unbound`

 Set of defines needed to compile this target. Each define is a string. Not propagated transitively to dependents.


## create\_compilation\_outputs

```
CcCompilationOutputs cc_common.create_compilation_outputs(*, objects=None, pic_objects=None)
```

 Create compilation outputs object.


### Parameters

ParameterDescription`objects`[depset](../builtins/depset.html); or `None`;
 default is `None`

 List of object files.
 `pic_objects`[depset](../builtins/depset.html); or `None`;
 default is `None`

 List of pic object files.


## create\_compile\_variables

```
Variables cc_common.create_compile_variables(*, cc_toolchain, feature_configuration, source_file=None, output_file=None, user_compile_flags=None, include_directories=None, quote_include_directories=None, system_include_directories=None, framework_include_directories=None, preprocessor_defines=None, thinlto_index=None, thinlto_input_bitcode_file=None, thinlto_output_object_file=None, use_pic=False, add_legacy_cxx_options=False, variables_extension=unbound)
```

 Returns variables used for compilation actions.


### Parameters

ParameterDescription`cc_toolchain`
 Info;
 required

 cc\_toolchain for which we are creating build variables.
 `feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `source_file`[File](../builtins/File.html); or [string](../core/string.html); or `None`;
 default is `None`

 Optional source file path for the compilation. Please prefer passing source\_file here over appending it to the end of the command line generated from cc\_common.get\_memory\_inefficient\_command\_line, as then it's in the power of the toolchain author to properly specify and position compiler flags.
 `output_file`[File](../builtins/File.html); or [string](../core/string.html); or `None`;
 default is `None`

 Optional output file path of the compilation. Please prefer passing output\_file here over appending it to the end of the command line generated from cc\_common.get\_memory\_inefficient\_command\_line, as then it's in the power of the toolchain author to properly specify and position compiler flags.
 `user_compile_flags`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 List of additional compilation flags (copts).
 `include_directories`[depset](../builtins/depset.html); or `None`;
 default is `None`

 Depset of include directories.
 `quote_include_directories`[depset](../builtins/depset.html); or `None`;
 default is `None`

 Depset of quote include directories.
 `system_include_directories`[depset](../builtins/depset.html); or `None`;
 default is `None`

 Depset of system include directories.
 `framework_include_directories`[depset](../builtins/depset.html); or `None`;
 default is `None`

 Depset of framework include directories.
 `preprocessor_defines`[depset](../builtins/depset.html); or `None`;
 default is `None`

 Depset of preprocessor defines.
 `thinlto_index`[string](../core/string.html); or `None`;
 default is `None`

 LTO index file path.
 `thinlto_input_bitcode_file`[string](../core/string.html); or `None`;
 default is `None`

 Bitcode file that is input to LTO backend.
 `thinlto_output_object_file`[string](../core/string.html); or `None`;
 default is `None`

 Object file that is output by LTO backend.
 `use_pic`[bool](../core/bool.html);
 default is `False`

 When true the compilation will generate position independent code.
 `add_legacy_cxx_options`[bool](../core/bool.html);
 default is `False`

 Unused.
 `variables_extension`[dict](../core/dict.html);
 default is `unbound`

 A dictionary of additional variables used by compile actions.


## create\_library\_to\_link

```
LibraryToLink cc_common.create_library_to_link(*, actions, feature_configuration=None, cc_toolchain=None, static_library=None, pic_static_library=None, dynamic_library=None, interface_library=None, pic_objects=unbound, objects=unbound, alwayslink=False, dynamic_library_symlink_path='', interface_library_symlink_path='')
```

 Creates `LibraryToLink`

### Parameters

ParameterDescription`actions`
 required

`actions` object.
 `feature_configuration`
 default is `None`

`feature_configuration` to be queried.
 `cc_toolchain`
 default is `None`

`CcToolchainInfo` provider to be used.
 `static_library`[File](../builtins/File.html); or `None`;
 default is `None`

`File` of static library to be linked.
 `pic_static_library`[File](../builtins/File.html); or `None`;
 default is `None`

`File` of pic static library to be linked.
 `dynamic_library`[File](../builtins/File.html); or `None`;
 default is `None`

`File` of dynamic library to be linked. Always used for runtime and used for linking if `interface_library` is not passed.
 `interface_library`[File](../builtins/File.html); or `None`;
 default is `None`

`File` of interface library to be linked.
 `pic_objects`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `unbound`

 Experimental, do not use
 `objects`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `unbound`

 Experimental, do not use
 `alwayslink`[bool](../core/bool.html);
 default is `False`

 Whether to link the static library/objects in the --whole\_archive block.
 `dynamic_library_symlink_path`[string](../core/string.html);
 default is `''`

 Override the default path of the dynamic library link in the solib directory. Empty string to use the default.
 `interface_library_symlink_path`[string](../core/string.html);
 default is `''`

 Override the default path of the interface library link in the solib directory. Empty string to use the default.


## create\_link\_variables

```
Variables cc_common.create_link_variables(*, cc_toolchain, feature_configuration, library_search_directories=[], runtime_library_search_directories=[], user_link_flags=[], output_file=None, param_file=None, is_using_linker=True, is_linking_dynamic_library=False, must_keep_debug=True, use_test_only_flags=False, is_static_linking_mode=True)
```

 Returns link variables used for linking actions.


### Parameters

ParameterDescription`cc_toolchain`
 Info;
 required

 cc\_toolchain for which we are creating build variables.
 `feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `library_search_directories`[depset](../builtins/depset.html);
 default is `[]`

 Depset of directories where linker will look for libraries at link time.
 `runtime_library_search_directories`[depset](../builtins/depset.html);
 default is `[]`

 Depset of directories where loader will look for libraries at runtime.
 `user_link_flags`[sequence](../core/list.html);
 default is `[]`

 List of additional link flags (linkopts).
 `output_file`
 default is `None`

 Optional output file path.
 `param_file`
 default is `None`

 Optional param file path.
 `is_using_linker`[bool](../core/bool.html);
 default is `True`

 True when using linker, False when archiver. Caller is responsible for keeping this in sync with action name used (is\_using\_linker = True for linking executable or dynamic library, is\_using\_linker = False for archiving static library).
 `is_linking_dynamic_library`[bool](../core/bool.html);
 default is `False`

 True when creating dynamic library, False when executable or static library. Caller is responsible for keeping this in sync with action name used. This field will be removed once b/65151735 is fixed.
 `must_keep_debug`[bool](../core/bool.html);
 default is `True`

 When set to False, bazel will expose 'strip\_debug\_symbols' variable, which is usually used to use the linker to strip debug symbols from the output file.
 `use_test_only_flags`[bool](../core/bool.html);
 default is `False`

 When set to true, 'is\_cc\_test' variable will be set.
 `is_static_linking_mode`[bool](../core/bool.html);
 default is `True`

 Unused.


## create\_linker\_input

```
LinkerInput cc_common.create_linker_input(*, owner, libraries=None, user_link_flags=None, additional_inputs=None)
```

 Creates a `LinkerInput`.


### Parameters

ParameterDescription`owner`[Label](../builtins/Label.html);
 required

 The label of the target that produced all files used in this input.
 `libraries``None`; or [depset](../builtins/depset.html);
 default is `None`

 List of `LibraryToLink`.
 `user_link_flags``None`; or [depset](../builtins/depset.html) of [string](../core/string.html) s; or [sequence](../core/list.html) of [string](../core/string.html) s;
 default is `None`

 User link flags passed as strings. Accepts either \[String\], \[\[String\]\] or depset(String). The latter is discouraged as it's only kept for compatibility purposes, the depset is flattened. If you want to propagate user\_link\_flags via unflattened depsets() wrap them in a LinkerInput so that they are not flattened till the end.
 `additional_inputs``None`; or [depset](../builtins/depset.html);
 default is `None`

 For additional inputs to the linking action, e.g.: linking scripts.


## create\_linking\_context

```
LinkingContext cc_common.create_linking_context(*, linker_inputs)
```

 Creates a `LinkingContext`.


### Parameters

ParameterDescription`linker_inputs`[depset](../builtins/depset.html);
 required

 Depset of `LinkerInput`.


## create\_linking\_context\_from\_compilation\_outputs

```
tuple cc_common.create_linking_context_from_compilation_outputs(*, actions, name, feature_configuration, cc_toolchain, language='c++', disallow_static_libraries=False, disallow_dynamic_library=False, compilation_outputs, linking_contexts=[], user_link_flags=[], alwayslink=False, additional_inputs=[], variables_extension=unbound)
```

 Should be used for creating library rules that can propagate information downstream in order to be linked later by a top level rule that does transitive linking to create an executable or a dynamic library. Returns tuple of ( `CcLinkingContext`, `CcLinkingOutputs`).


### Parameters

ParameterDescription`actions`[actions](../builtins/actions.html);
 required

`actions` object.
 `name`[string](../core/string.html);
 required

 This is used for naming the output artifacts of actions created by this method.
 `feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

`feature_configuration` to be queried.
 `cc_toolchain`
 Info;
 required

`CcToolchainInfo` provider to be used.
 `language`[string](../core/string.html);
 default is `'c++'`

 Only C++ supported for now. Do not use this parameter.
 `disallow_static_libraries`[bool](../core/bool.html);
 default is `False`

 Whether static libraries should be created.
 `disallow_dynamic_library`[bool](../core/bool.html);
 default is `False`

 Whether a dynamic library should be created.
 `compilation_outputs`[CcCompilationOutputs](../builtins/CcCompilationOutputs.html);
 required

 Compilation outputs containing object files to link.
 `linking_contexts`[sequence](../core/list.html);
 default is `[]`

 Libraries from dependencies. These libraries will be linked into the output artifact of the link() call, be it a binary or a library.
 `user_link_flags`[sequence](../core/list.html);
 default is `[]`

 Additional list of linking options.
 `alwayslink`[bool](../core/bool.html);
 default is `False`

 Whether this library should always be linked.
 `additional_inputs`[sequence](../core/list.html);
 default is `[]`

 For additional inputs to the linking action, e.g.: linking scripts.
 `variables_extension`[dict](../core/dict.html);
 default is `unbound`

 Additional variables to pass to the toolchain configuration when creating link command line.


## create\_lto\_compilation\_context

```
LtoCompilationContext cc_common.create_lto_compilation_context(*, objects={})
```

 Create LTO compilation context


### Parameters

ParameterDescription`objects`[dict](../core/dict.html);
 default is `{}`

 map of full object to index object


## do\_not\_use\_tools\_cpp\_compiler\_present

```
None cc_common.do_not_use_tools_cpp_compiler_present
```

 Do not use this field, its only purpose is to help with migration from config\_setting.values\{'compiler') to config\_settings.flag\_values\{'@bazel\_tools//tools/cpp:compiler'\}



## get\_environment\_variables

```
dict cc_common.get_environment_variables(*, feature_configuration, action_name, variables)
```

 Returns environment variables to be set for given action.


### Parameters

ParameterDescription`feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `action_name`[string](../core/string.html);
 required

 Name of the action. Has to be one of the names in @bazel\_tools//tools/build\_defs/cc:action\_names.bzl (https://github.com/bazelbuild/bazel/blob/master/tools/build\_defs/cc/action\_names.bzl)
 `variables`
 Variables;
 required

 Build variables to be used for template expansion.


## get\_execution\_requirements

```
sequence cc_common.get_execution_requirements(*, feature_configuration, action_name)
```

 Returns execution requirements for given action.


### Parameters

ParameterDescription`feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `action_name`[string](../core/string.html);
 required

 Name of the action. Has to be one of the names in @bazel\_tools//tools/build\_defs/cc:action\_names.bzl (https://github.com/bazelbuild/bazel/blob/master/tools/build\_defs/cc/action\_names.bzl)


## get\_memory\_inefficient\_command\_line

```
sequence cc_common.get_memory_inefficient_command_line(*, feature_configuration, action_name, variables)
```

 Returns flattened command line flags for given action, using given variables for expansion. Flattens nested sets and ideally should not be used, or at least should not outlive analysis. Work on memory efficient function returning Args is ongoing.


### Parameters

ParameterDescription`feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `action_name`[string](../core/string.html);
 required

 Name of the action. Has to be one of the names in @bazel\_tools//tools/build\_defs/cc:action\_names.bzl (https://github.com/bazelbuild/bazel/blob/master/tools/build\_defs/cc/action\_names.bzl)
 `variables`
 Variables;
 required

 Build variables to be used for template expansions.


## get\_tool\_for\_action

```
string cc_common.get_tool_for_action(*, feature_configuration, action_name)
```

 Returns tool path for given action.


### Parameters

ParameterDescription`feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `action_name`[string](../core/string.html);
 required

 Name of the action. Has to be one of the names in @bazel\_tools//tools/build\_defs/cc:action\_names.bzl (https://github.com/bazelbuild/bazel/blob/master/tools/build\_defs/cc/action\_names.bzl)


## is\_enabled

```
bool cc_common.is_enabled(*, feature_configuration, feature_name)
```

 Returns True if given feature is enabled in the feature configuration.


### Parameters

ParameterDescription`feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

 Feature configuration to be queried.
 `feature_name`[string](../core/string.html);
 required

 Name of the feature.


## link

```
CcLinkingOutputs cc_common.link(*, actions, name, feature_configuration, cc_toolchain, language='c++', output_type='executable', link_deps_statically=True, compilation_outputs=None, linking_contexts=[], user_link_flags=[], stamp=0, additional_inputs=[], additional_outputs=[], variables_extension={})
```

 Should be used for C++ transitive linking.


### Parameters

ParameterDescription`actions`[actions](../builtins/actions.html);
 required

`actions` object.
 `name`[string](../core/string.html);
 required

 This is used for naming the output artifacts of actions created by this method.
 `feature_configuration`[FeatureConfiguration](../builtins/FeatureConfiguration.html);
 required

`feature_configuration` to be queried.
 `cc_toolchain`
 Info;
 required

`CcToolchainInfo` provider to be used.
 `language`[string](../core/string.html);
 default is `'c++'`

 Only C++ supported for now. Do not use this parameter.
 `output_type`[string](../core/string.html);
 default is `'executable'`

 Can be either 'executable' or 'dynamic\_library'.
 `link_deps_statically`[bool](../core/bool.html);
 default is `True`

 True to link dependencies statically, False dynamically.
 `compilation_outputs`[CcCompilationOutputs](../builtins/CcCompilationOutputs.html); or `None`;
 default is `None`

 Compilation outputs containing object files to link.
 `linking_contexts`[sequence](../core/list.html);
 default is `[]`

 Linking contexts from dependencies to be linked into the linking context generated by this rule.
 `user_link_flags`[sequence](../core/list.html);
 default is `[]`

 Additional list of linker options.
 `stamp`[int](../core/int.html);
 default is `0`

 Whether to include build information in the linked executable, if output\_type is 'executable'. If 1, build information is always included. If 0 (the default build information is always excluded. If -1, uses the default behavior, which may be overridden by the --\[no\]stamp flag. This should be unset (or set to 0) when generating the executable output for test rules.
 `additional_inputs`[sequence](../core/list.html); or [depset](../builtins/depset.html);
 default is `[]`

 For additional inputs to the linking action, e.g.: linking scripts.
 `additional_outputs`[sequence](../core/list.html);
 default is `[]`

 For additional outputs to the linking action, e.g.: map files.
 `variables_extension`[dict](../core/dict.html);
 default is `{}`

 Additional variables to pass to the toolchain configuration when create link command line.


## merge\_cc\_infos

```
unknown cc_common.merge_cc_infos(*, direct_cc_infos=[], cc_infos=[])
```

 Merges multiple `CcInfo` s into one.


### Parameters

ParameterDescription`direct_cc_infos`[sequence](../core/list.html);
 default is `[]`

 List of `CcInfo` s to be merged, whose headers will be exported by the direct fields in the returned provider.
 `cc_infos`[sequence](../core/list.html);
 default is `[]`

 List of `CcInfo` s to be merged, whose headers will not be exported by the direct fields in the returned provider.


## merge\_compilation\_contexts

```
CompilationContext cc_common.merge_compilation_contexts(*, compilation_contexts=[])
```

 Merges multiple `CompilationContexts` s into one.


### Parameters

ParameterDescription`compilation_contexts`[sequence](../core/list.html);
 default is `[]`

 List of `CompilationContexts` s to be merged. The headers of each context will be exported by the direct fields in the returned provider.


## merge\_compilation\_outputs

```
CcCompilationOutputs cc_common.merge_compilation_outputs(*, compilation_outputs=[])
```

 Merge compilation outputs.


### Parameters

ParameterDescription`compilation_outputs`[sequence](../core/list.html);
 default is `[]`

---

## config
- URL: https://bazel.build/rules/lib/toplevel/config
- Source: rules/lib/toplevel/config.mdx
- Slug: /rules/lib/toplevel/config

This is a top-level module for creating configuration transitions and build setting descriptors which describe what kind of build setting (if any) a rule is.

ex: the following rule is marked as a build setting by setting the `build_setting` parameter of the `rule()` function. Specifically it is a build setting of type `int` and is a `flag` which means this build setting is callable on the command line.

```
  my_rule = rule(
    implementation = _impl,
    build_setting = config.int(flag = True),
    ...
  )
```

## Members

- [bool](#bool)
- [exec](#exec)
- [int](#int)
- [none](#none)
- [string](#string)
- [string\_list](#string_list)
- [string\_set](#string_set)
- [target](#target)

## bool

```
BuildSetting config.bool(*, flag=False)
```

 A bool-typed build setting


### Parameters

ParameterDescription`flag`[bool](../core/bool.html);
 default is `False`

 Whether or not this build setting is callable on the command line.


## exec

```
ExecTransitionFactory config.exec(exec_group=None)
```

 Creates an execution transition.


### Parameters

ParameterDescription`exec_group`[string](../core/string.html); or `None`;
 default is `None`

 The name of the exec group whose execution platform this transition will use. If not provided, this exec transition will use the target's default execution platform.


## int

```
BuildSetting config.int(*, flag=False)
```

 An integer-typed build setting


### Parameters

ParameterDescription`flag`[bool](../core/bool.html);
 default is `False`

 Whether or not this build setting is callable on the command line.


## none

```
transition config.none()
```

 Creates a transition which removes all configuration, unsetting all flags. Intended for the case where a dependency is data-only and contains no code that needs to be built, but should only be analyzed once.



## string

```
BuildSetting config.string(*, flag=False, allow_multiple=False)
```

 A string-typed build setting


### Parameters

ParameterDescription`flag`[bool](../core/bool.html);
 default is `False`

 Whether or not this build setting is callable on the command line.
 `allow_multiple`[bool](../core/bool.html);
 default is `False`

 Deprecated, use a `string_list` setting with `repeatable = True` instead. If set, this flag is allowed to be set multiple times on the command line. The Value of the flag as accessed in transitions and build setting implementation function will be a list of strings. Insertion order and repeated values are both maintained. This list can be post-processed in the build setting implementation function if different behavior is desired.


## string\_list

```
BuildSetting config.string_list(*, flag=False, repeatable=False)
```

 A string list-typed build setting. On the command line pass a list using comma-separated value like `--//my/setting=foo,bar`.


### Parameters

ParameterDescription`flag`[bool](../core/bool.html);
 default is `False`

 Whether or not this build setting is callable on the command line.
 `repeatable`[bool](../core/bool.html);
 default is `False`

 If set, instead of expecting a comma-separated value, this flag is allowed to be set multiple times on the command line with each individual value treated as a single string to add to the list value. Insertion order and repeated values are both maintained. This list can be post-processed in the build setting implementation function if different behavior is desired.


## string\_set

```
BuildSetting config.string_set(*, flag=False, repeatable=False)
```

 A string set-typed build setting. The value of this setting will be a [set](https://bazel.build/rules/lib/core/set) of strings in Starlark. On the command line, pass a set using a comma-separated value like `--//my/setting=foo,bar`.

Unlike with a `string_list`, the order of the elements doesn't matter and only a single instance of each element is maintained. This is recommended over `string_list` for flags where these properties are not needed as it can improve build performance by avoiding unnecessary configurations forking.


### Parameters

ParameterDescription`flag`[bool](../core/bool.html);
 default is `False`

 Whether or not this build setting is callable on the command line.
 `repeatable`[bool](../core/bool.html);
 default is `False`

 If set, instead of expecting a comma-separated value, this flag is allowed to be set multiple times on the command line with each individual value treated as a single string to add to the set value. Only a single instance of repeated values is maintained and the insertion order does not matter.


## target

```
transition config.target()
```

 Creates a target transition. This is a no-op transition intended for the case where a transition object is needed, but doesn't want to actually change anything. Equivalent to `cfg = "target"` in `attr.label()`.

---

## config\_common
- URL: https://bazel.build/rules/lib/toplevel/config_common
- Source: rules/lib/toplevel/config_common.mdx
- Slug: /rules/lib/toplevel/config_common

Functions for Starlark to interact with Blaze's configurability APIs.

## Members

- [FeatureFlagInfo](#FeatureFlagInfo)
- [toolchain\_type](#toolchain_type)

## FeatureFlagInfo

```
Provider config_common.FeatureFlagInfo
```

 The key used to retrieve the provider containing config\_feature\_flag's value.



## toolchain\_type

```
toolchain_type config_common.toolchain_type(name, *, mandatory=True)
```

 Declare a rule's dependency on a toolchain type.


### Parameters

ParameterDescription`name`[string](../core/string.html); or [Label](../builtins/Label.html);
 required

 The toolchain type that is required.
 `mandatory`[bool](../core/bool.html);
 default is `True`

 Whether the toolchain type is mandatory or optional.

---

## coverage\_common
- URL: https://bazel.build/rules/lib/toplevel/coverage_common
- Source: rules/lib/toplevel/coverage_common.mdx
- Slug: /rules/lib/toplevel/coverage_common

Helper functions to access coverage-related infrastructure.

## Members

- [instrumented\_files\_info](#instrumented_files_info)

## instrumented\_files\_info

```
InstrumentedFilesInfo coverage_common.instrumented_files_info(ctx, *, source_attributes=[], dependency_attributes=[], extensions=None, metadata_files=[], baseline_coverage_files=None)
```

 Creates a new [InstrumentedFilesInfo](../providers/InstrumentedFilesInfo.html) instance. Use this provider to communicate coverage-related attributes of the current build rule.


### Parameters

ParameterDescription`ctx`[ctx](../builtins/ctx.html);
 required

 The rule context.
 `source_attributes`[sequence](../core/list.html);
 default is `[]`

 A list of attribute names which contain source files processed by this rule.
 `dependency_attributes`[sequence](../core/list.html);
 default is `[]`

 A list of attribute names which might provide runtime dependencies (either code dependencies or runfiles).
 `extensions`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 File extensions used to filter files from source\_attributes. For example, 'js'. If not provided (or None), then all files from source\_attributes will be added to instrumented files, if an empty list is provided, then no files from source attributes will be added.
 `metadata_files`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 Additional files required to generate coverage LCOV files after code execution. e.g. .gcno files for C++.
 `baseline_coverage_files`[sequence](../core/list.html) of [File](../builtins/File.html) s; or `None`;
 default is `None`

---

## java\_common
- URL: https://bazel.build/rules/lib/toplevel/java_common
- Source: rules/lib/toplevel/java_common.mdx
- Slug: /rules/lib/toplevel/java_common

Utilities for Java compilation support in Starlark.

## Members

- [BootClassPathInfo](#BootClassPathInfo)
- [compile](#compile)
- [JavaRuntimeInfo](#JavaRuntimeInfo)
- [JavaToolchainInfo](#JavaToolchainInfo)
- [merge](#merge)
- [pack\_sources](#pack_sources)
- [run\_ijar](#run_ijar)
- [stamp\_jar](#stamp_jar)

## BootClassPathInfo

```
Provider java_common.BootClassPathInfo
```

 The provider used to supply bootclasspath information



## compile

```
struct java_common.compile(ctx, *, source_jars=[], source_files=[], output, output_source_jar=None, javac_opts=[], deps=[], runtime_deps=[], exports=[], plugins=[], exported_plugins=[], native_libraries=[], annotation_processor_additional_inputs=[], annotation_processor_additional_outputs=[], strict_deps='ERROR', java_toolchain, bootclasspath=None, sourcepath=[], resources=[], resource_jars=[], classpath_resources=[], neverlink=False, enable_annotation_processing=True, enable_compile_jar_action=True, add_exports=[], add_opens=[])
```

 Compiles Java source files/jars from the implementation of a Starlark rule and returns a provider that represents the results of the compilation and can be added to the set of providers emitted by this rule.


### Parameters

ParameterDescription`ctx`[ctx](../builtins/ctx.html);
 required

 The rule context.
 `source_jars`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 A list of the jars to be compiled. At least one of source\_jars or source\_files should be specified.
 `source_files`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 A list of the Java source files to be compiled. At least one of source\_jars or source\_files should be specified.
 `output`[File](../builtins/File.html);
 required

`output_source_jar`[File](../builtins/File.html); or `None`;
 default is `None`

 The output source jar. Defaults to \`{output\_jar}-src.jar\` if unset.
 `javac_opts`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 A list of the desired javac options.
 `deps`[sequence](../core/list.html) of [struct](../builtins/struct.html) s;
 default is `[]`

 A list of dependencies.
 `runtime_deps`[sequence](../core/list.html) of [struct](../builtins/struct.html) s;
 default is `[]`

 A list of runtime dependencies.
 `exports`[sequence](../core/list.html) of [struct](../builtins/struct.html) s;
 default is `[]`

 A list of exports.
 `plugins`[sequence](../core/list.html) of [struct](../builtins/struct.html) s; or [sequence](../core/list.html) of [struct](../builtins/struct.html) s;
 default is `[]`

 A list of plugins.
 `exported_plugins`[sequence](../core/list.html) of [struct](../builtins/struct.html) s; or [sequence](../core/list.html) of [struct](../builtins/struct.html) s;
 default is `[]`

 A list of exported plugins.
 `native_libraries`[sequence](../core/list.html) of [CcInfo](../providers/CcInfo.html) s;
 default is `[]`

 CC native library dependencies that are needed for this library.
 `annotation_processor_additional_inputs`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 A list of inputs that the Java compilation action will take in addition to the Java sources for annotation processing.
 `annotation_processor_additional_outputs`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 A list of outputs that the Java compilation action will output in addition to the class jar from annotation processing.
 `strict_deps`[string](../core/string.html);
 default is `'ERROR'`

 A string that specifies how to handle strict deps. Possible values: 'OFF', 'ERROR', 'WARN' and 'DEFAULT'. For more details see [`--strict_java_deps flag`](/docs/user-manual#flag--strict_java_deps) `. By default 'ERROR'.
            ``java_toolchain`
 Info;
 required

 A JavaToolchainInfo to be used for this compilation. Mandatory.
 `bootclasspath`
 default is `None`

 A BootClassPathInfo to be used for this compilation. If present, overrides the bootclasspath associated with the provided java\_toolchain.
 `sourcepath`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

`resources`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

`resource_jars`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

`classpath_resources`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

`neverlink`[bool](../core/bool.html);
 default is `False`

`enable_annotation_processing`[bool](../core/bool.html);
 default is `True`

 Disables annotation processing in this compilation, causing any annotation processors provided in plugins or in exported\_plugins of deps to be ignored.
 `enable_compile_jar_action`[bool](../core/bool.html);
 default is `True`

 Enables header compilation or ijar creation. If set to False, it forces use of the full class jar in the compilation classpaths of any dependants. Doing so is intended for use by non-library targets such as binaries that do not have dependants.
 `add_exports`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Allow this library to access the given /.
 `add_opens`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Allow this library to reflectively access the given /.


## JavaRuntimeInfo

```
Provider java_common.JavaRuntimeInfo
```

 The key used to retrieve the provider that contains information about the Java runtime being used.



## JavaToolchainInfo

```
Provider java_common.JavaToolchainInfo
```

 The key used to retrieve the provider that contains information about the Java toolchain being used.



## merge

```
struct java_common.merge(providers)
```

 Merges the given providers into a single JavaInfo.


### Parameters

ParameterDescription`providers`[sequence](../core/list.html) of [struct](../builtins/struct.html) s;
 required

 The list of providers to merge.


## pack\_sources

```
File java_common.pack_sources(actions, *, output_source_jar=None, sources=[], source_jars=[], java_toolchain)
```

 Packs sources and source jars into a single source jar file. The return value is typically passed to

`JavaInfo#source_jar`

.At least one of parameters output\_jar or output\_source\_jar is required.


### Parameters

ParameterDescription`actions`[actions](../builtins/actions.html);
 required

 ctx.actions
 `output_source_jar`[File](../builtins/File.html); or `None`;
 default is `None`

 The output source jar.
 `sources`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 A list of Java source files to be packed into the source jar.
 `source_jars`[sequence](../core/list.html) of [File](../builtins/File.html) s;
 default is `[]`

 A list of source jars to be packed into the source jar.
 `java_toolchain`
 Info;
 required

 A JavaToolchainInfo to used to find the ijar tool.


## run\_ijar

```
File java_common.run_ijar(actions, *, jar, target_label=None, java_toolchain)
```

 Runs ijar on a jar, stripping it of its method bodies. This helps reduce rebuilding of dependent jars during any recompiles consisting only of simple changes to method implementations. The return value is typically passed to `JavaInfo#compile_jar`.


### Parameters

ParameterDescription`actions`[actions](../builtins/actions.html);
 required

 ctx.actions
 `jar`[File](../builtins/File.html);
 required

 The jar to run ijar on.
 `target_label`[Label](../builtins/Label.html); or `None`;
 default is `None`

 A target label to stamp the jar with. Used for `add_dep` support. Typically, you would pass `ctx.label` to stamp the jar with the current rule's label.
 `java_toolchain`
 Info;
 required

 A JavaToolchainInfo to used to find the ijar tool.


## stamp\_jar

```
File java_common.stamp_jar(actions, *, jar, target_label, java_toolchain)
```

 Stamps a jar with a target label for `add_dep` support. The return value is typically passed to `JavaInfo#compile_jar`. Prefer to use `run_ijar` when possible.


### Parameters

ParameterDescription`actions`[actions](../builtins/actions.html);
 required

 ctx.actions
 `jar`[File](../builtins/File.html);
 required

 The jar to run stamp\_jar on.
 `target_label`[Label](../builtins/Label.html);
 required

 A target label to stamp the jar with. Used for `add_dep` support. Typically, you would pass `ctx.label` to stamp the jar with the current rule's label.
 `java_toolchain`
 Info;
 required

 A JavaToolchainInfo to used to find the stamp\_jar tool.

---

## native
- URL: https://bazel.build/rules/lib/toplevel/native
- Source: rules/lib/toplevel/native.mdx
- Slug: /rules/lib/toplevel/native

A built-in module to support native rules and other package helper functions. All native rules appear as functions in this module, e.g. `native.cc_library`. Note that the native module is only available in the loading phase (i.e. for macros, not for rule implementations). Attributes will ignore `None` values, and treat them as if the attribute was unset.

The following functions are also available:

## Members

- [existing\_rule](#existing_rule)
- [existing\_rules](#existing_rules)
- [exports\_files](#exports_files)
- [glob](#glob)
- [module\_name](#module_name)
- [module\_version](#module_version)
- [package\_default\_visibility](#package_default_visibility)
- [package\_group](#package_group)
- [package\_name](#package_name)
- [package\_relative\_label](#package_relative_label)
- [repo\_name](#repo_name)
- [repository\_name](#repository_name)
- [subpackages](#subpackages)

## existing\_rule

```
unknown native.existing_rule(name)
```

 Returns an immutable dict-like object that describes the attributes of a rule instantiated in this thread's package, or `None` if no rule instance of that name exists.

Here, an _immutable dict-like object_ means a deeply immutable object `x` supporting dict-like iteration, `len(x)`, `name in x`, `x[name]`, `x.get(name)`, `x.items()`, `x.keys()`, and `x.values()`.

The result contains an entry for each attribute, with the exception of private ones (whose names do not start with a letter) and a few unrepresentable legacy attribute types. In addition, the dict contains entries for the rule instance's `name` and `kind` (for example, `'cc_binary'`).

The values of the result represent attribute values as follows:

- Attributes of type str, int, and bool are represented as is.
- Labels are converted to strings of the form `':foo'` for targets in the same package or `'//pkg:name'` for targets in a different package.
- Lists are represented as tuples, and dicts are converted to new, mutable dicts. Their elements are recursively converted in the same fashion.
- `select` values are returned with their contents transformed as described above.
- Attributes for which no value was specified during rule instantiation and whose default value is computed are excluded from the result. (Computed defaults cannot be computed until the analysis phase.).

If possible, use this function only in [implementation functions of rule finalizer symbolic macros](https://bazel.build/extending/macros#finalizers). Use of this function in other contexts is not recommened, and will be disabled in a future Bazel release; it makes `BUILD` files brittle and order-dependent. Also, beware that it differs subtly from the two other conversions of rule attribute values from internal form to Starlark: one used by computed defaults, the other used by `ctx.attr.foo`.


### Parameters

ParameterDescription`name`[string](../core/string.html);
 required

 The name of the target.


## existing\_rules

```
unknown native.existing_rules()
```

 Returns an immutable dict-like object describing the rules so far instantiated in this thread's package. Each entry of the dict-like object maps the name of the rule instance to the result that would be returned by `existing_rule(name)`.

Here, an _immutable dict-like object_ means a deeply immutable object `x` supporting dict-like iteration, `len(x)`, `name in x`, `x[name]`, `x.get(name)`, `x.items()`, `x.keys()`, and `x.values()`.

If possible, use this function only in [implementation functions of rule finalizer symbolic macros](https://bazel.build/extending/macros#finalizers). Use of this function in other contexts is not recommened, and will be disabled in a future Bazel release; it makes `BUILD` files brittle and order-dependent.



## exports\_files

```
None native.exports_files(srcs, visibility=None, licenses=None)
```

 Specifies a list of files belonging to this package that are exported to other packages.


### Parameters

ParameterDescription`srcs`[sequence](../core/list.html) of [string](../core/string.html) s;
 required

 The list of files to export.
 `visibility`[sequence](../core/list.html); or `None`;
 default is `None`

 A visibility declaration can to be specified. The files will be visible to the targets specified. If no visibility is specified, the files will be visible to every package.
 `licenses`[sequence](../core/list.html) of [string](../core/string.html) s; or `None`;
 default is `None`

 Licenses to be specified.


## glob

```
sequence native.glob(include=[], exclude=[], exclude_directories=1, allow_empty=unbound)
```

 Glob returns a new, mutable, sorted list of every file in the current package that:

- Matches at least one pattern in `include`.
- Does not match any of the patterns in `exclude` (default `[]`).

If the `exclude_directories` argument is enabled (set to `1`), files of type directory will be omitted from the results (default `1`).


### Parameters

ParameterDescription`include`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 The list of glob patterns to include.
 `exclude`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 The list of glob patterns to exclude.
 `exclude_directories`[int](../core/int.html);
 default is `1`

 A flag whether to exclude directories or not.
 `allow_empty`
 default is `unbound`

 Whether we allow glob patterns to match nothing. If \`allow\_empty\` is False, each individual include pattern must match something and also the final result must be non-empty (after the matches of the \`exclude\` patterns are excluded).


## module\_name

```
string native.module_name()
```

 The name of the Bazel module associated with the repo this package is in. If this package is from a repo defined in WORKSPACE instead of MODULE.bazel, this is empty. For repos generated by module extensions, this is the name of the module hosting the extension. It's the same as the `module.name` field seen in `module_ctx.modules`.
 May return `None`.



## module\_version

```
string native.module_version()
```

 The version of the Bazel module associated with the repo this package is in. If this package is from a repo defined in WORKSPACE instead of MODULE.bazel, this is empty. For repos generated by module extensions, this is the version of the module hosting the extension. It's the same as the `module.version` field seen in `module_ctx.modules`.
 May return `None`.



## package\_default\_visibility

```
List native.package_default_visibility()
```

 Returns the default visibility of the package being evaluated. This is the value of the `default_visibility` parameter of `package()`, extended to include the package itself.



## package\_group

```
None native.package_group(*, name, packages=[], includes=[])
```

 This function defines a set of packages and assigns a label to the group. The label can be referenced in `visibility` attributes.


### Parameters

ParameterDescription`name`[string](../core/string.html);
 required

 The unique name for this rule.
 `packages`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 A complete enumeration of packages in this group.
 `includes`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 Other package groups that are included in this one.


## package\_name

```
string native.package_name()
```

 The name of the package being evaluated, without the repository name. For example, in the BUILD file `some/package/BUILD`, its value will be `some/package`. If the BUILD file calls a function defined in a .bzl file, `package_name()` will match the caller BUILD file package. The value will always be an empty string for the root package.



## package\_relative\_label

```
Label native.package_relative_label(input)
```

 Converts the input string into a [Label](../builtins/Label.html) object, in the context of the package currently being initialized (that is, the `BUILD` file for which the current macro is executing). If the input is already a `Label`, it is returned unchanged.

This function may only be called while evaluating a BUILD file and the macros it directly or indirectly calls; it may not be called in (for instance) a rule implementation function.

The result of this function is the same `Label` value as would be produced by passing the given string to a label-valued attribute of a target declared in the BUILD file.

_Usage note:_ The difference between this function and [Label()](../builtins/Label.html#Label) is that `Label()` uses the context of the package of the `.bzl` file that called it, not the package of the `BUILD` file. Use `Label()` when you need to refer to a fixed target that is hardcoded into the macro, such as a compiler. Use `package_relative_label()` when you need to normalize a label string supplied by the BUILD file to a `Label` object. (There is no way to convert a string to a `Label` in the context of a package other than the BUILD file or the calling .bzl file. For that reason, outer macros should always prefer to pass Label objects to inner macros rather than label strings.)


### Parameters

ParameterDescription`input`[string](../core/string.html); or [Label](../builtins/Label.html);
 required

 The input label string or Label object. If a Label object is passed, it's returned as is.


## repo\_name

```
string native.repo_name()
```

 The canonical name of the repository containing the package currently being evaluated, with no leading at-signs.



## repository\_name

```
string native.repository_name()
```

 **Experimental**. This API is experimental and may change at any time. Please do not depend on it. It may be enabled on an experimental basis by setting `--+incompatible_enable_deprecated_label_apis`

**Deprecated.** Prefer to use [`repo_name`](#repo_name) instead, which doesn't contain the spurious leading at-sign, but behaves identically otherwise.

The canonical name of the repository containing the package currently being evaluated, with a single at-sign ( `@`) prefixed. For example, in packages that are called into existence by the WORKSPACE stanza `local_repository(name='local', path=...)` it will be set to `@local`. In packages in the main repository, it will be set to `@`.



## subpackages

```
sequence native.subpackages(*, include, exclude=[], allow_empty=False)
```

 Returns a new mutable list of every direct subpackage of the current package, regardless of file-system directory depth. List returned is sorted and contains the names of subpackages relative to the current package. It is advised to prefer using the methods in bazel\_skylib.subpackages module rather than calling this function directly.


### Parameters

ParameterDescription`include`[sequence](../core/list.html) of [string](../core/string.html) s;
 required

 The list of glob patterns to include in subpackages scan.
 `exclude`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 The list of glob patterns to exclude from subpackages scan.
 `allow_empty`[bool](../core/bool.html);
 default is `False`

 Whether we fail if the call returns an empty list. By default empty list indicates potential error in BUILD file where the call to subpackages() is superflous. Setting to true allows this function to succeed in that case.

---

## platform\_common
- URL: https://bazel.build/rules/lib/toplevel/platform_common
- Source: rules/lib/toplevel/platform_common.mdx
- Slug: /rules/lib/toplevel/platform_common

Functions for Starlark to interact with the platform APIs.

## Members

- [ConstraintSettingInfo](#ConstraintSettingInfo)
- [ConstraintValueInfo](#ConstraintValueInfo)
- [PlatformInfo](#PlatformInfo)
- [TemplateVariableInfo](#TemplateVariableInfo)
- [ToolchainInfo](#ToolchainInfo)

## ConstraintSettingInfo

```
Provider platform_common.ConstraintSettingInfo
```

 The constructor/key for the [ConstraintSettingInfo](../providers/ConstraintSettingInfo.html) provider.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

## ConstraintValueInfo

```
Provider platform_common.ConstraintValueInfo
```

 The constructor/key for the [ConstraintValueInfo](../providers/ConstraintValueInfo.html) provider.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

## PlatformInfo

```
Provider platform_common.PlatformInfo
```

 The constructor/key for the [PlatformInfo](../providers/PlatformInfo.html) provider.

_Note: This API is experimental and may change at any time. It is disabled by default, but may be enabled with `--experimental_platforms_api`_

## TemplateVariableInfo

```
Provider platform_common.TemplateVariableInfo
```

 The constructor/key for the [TemplateVariableInfo](../providers/TemplateVariableInfo.html) provider.



## ToolchainInfo

```
Provider platform_common.ToolchainInfo
```

 The constructor/key for the [ToolchainInfo](../providers/ToolchainInfo.html) provider.

---

## proto
- URL: https://bazel.build/rules/lib/toplevel/proto
- Source: rules/lib/toplevel/proto.mdx
- Slug: /rules/lib/toplevel/proto

A module for protocol message processing.

## Members

- [encode\_text](#encode_text)

## encode\_text

```
string proto.encode_text(x)
```

 Returns the struct argument's encoding as a text-format protocol message.
The data structure must be recursively composed of strings, ints, floats, or bools, or structs, sequences, and dicts of these types.

A struct is converted to a message. Fields are emitted in name order.
Each struct field whose value is None is ignored.

A sequence (such as a list or tuple) is converted to a repeated field.
Its elements must not be sequences or dicts.

A dict is converted to a repeated field of messages with fields named 'key' and 'value'.
Entries are emitted in iteration (insertion) order.
The dict's keys must be strings or ints, and its values must not be sequences or dicts.
Examples:

```
proto.encode_text(struct(field=123))
# field: 123

proto.encode_text(struct(field=True))
# field: true

proto.encode_text(struct(field=[1, 2, 3]))
# field: 1
# field: 2
# field: 3

proto.encode_text(struct(field='text', ignored_field=None))
# field: "text"

proto.encode_text(struct(field=struct(inner_field='text', ignored_field=None)))
# field {
#   inner_field: "text"
# }

proto.encode_text(struct(field=[struct(inner_field=1), struct(inner_field=2)]))
# field {
#   inner_field: 1
# }
# field {
#   inner_field: 2
# }

proto.encode_text(struct(field=struct(inner_field=struct(inner_inner_field='text'))))
# field {
#    inner_field {
#     inner_inner_field: "text"
#   }
# }

proto.encode_text(struct(foo={4: 3, 2: 1}))
# foo: {
#   key: 4
#   value: 3
# }
# foo: {
#   key: 2
#   value: 1
# }

```

### Parameters

ParameterDescription`x`
 structure; or NativeInfo;
 required

---

## testing
- URL: https://bazel.build/rules/lib/toplevel/testing
- Source: rules/lib/toplevel/testing.mdx
- Slug: /rules/lib/toplevel/testing

Helper methods for Starlark to access testing infrastructure.

## Members

- [analysis\_test](#analysis_test)
- [ExecutionInfo](#ExecutionInfo)
- [TestEnvironment](#TestEnvironment)

## analysis\_test

```
None testing.analysis_test(name, implementation, attrs={}, fragments=[], toolchains=[], attr_values={})
```

 Creates a new analysis test target.

The number of transitive dependencies of the test are limited. The limit is controlled by `--analysis_testing_deps_limit` flag.


### Parameters

ParameterDescription`name`[string](../core/string.html);
 required

 Name of the target. It should be a Starlark identifier, matching pattern '\[A-Za-z\_\]\[A-Za-z0-9\_\]\*'.
 `implementation`[function](../core/function.html);
 required

 The Starlark function implementing this analysis test. It must have exactly one parameter: [ctx](../builtins/ctx.html). The function is called during the analysis phase. It can access the attributes declared by `attrs` and populated via `attr_values`. The implementation function may not register actions. Instead, it must register a pass/fail result via providing [AnalysisTestResultInfo](../providers/AnalysisTestResultInfo.html).
 `attrs`[dict](../core/dict.html);
 default is `{}`

 Dictionary declaring the attributes. See the [rule](../globals/bzl.html#rule) call. Attributes are allowed to use configuration transitions defined using [analysis\_test\_transition](../globals/bzl.html#analysis_test_transition).
 `fragments`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 List of configuration fragments that are available to the implementation of the analysis test.
 `toolchains`[sequence](../core/list.html);
 default is `[]`

 The set of toolchains the test requires. See the [rule](../globals/bzl.html#rule) call.
 `attr_values`[dict](../core/dict.html) of [string](../core/string.html) s;
 default is `{}`

 Dictionary of attribute values to pass to the implementation.


## ExecutionInfo

```
ExecutionInfo testing.ExecutionInfo
```

 [testing.ExecutionInfo](../providers/ExecutionInfo.html) provider key/constructor



## TestEnvironment

```
RunEnvironmentInfo testing.TestEnvironment(environment, inherited_environment=[])
```

 **Deprecated: Use RunEnvironmentInfo instead.** Creates a new test environment provider. Use this provider to specify extra environment variables to be made available during test execution.


### Parameters

ParameterDescription`environment`[dict](../core/dict.html);
 required

 A map of string keys and values that represent environment variables and their values. These will be made available during the test execution.
 `inherited_environment`[sequence](../core/list.html) of [string](../core/string.html) s;
 default is `[]`

 A sequence of names of environment variables. These variables are made available during the test execution with their current value taken from the shell environment. If a variable is contained in both `environment` and `inherited_environment`, the value inherited from the shell environment will take precedence if set.

---

## Creating a Symbolic Macro
- URL: https://bazel.build/rules/macro-tutorial
- Source: rules/macro-tutorial.mdx
- Slug: /rules/macro-tutorial

IMPORTANT: This tutorial is for [*symbolic macros*](/extending/macros) – the new
macro system introduced in Bazel 8. If you need to support older Bazel versions,
you will want to write a [legacy macro](/extending/legacy-macros) instead; take
a look at [Creating a Legacy Macro](legacy-macro-tutorial).

Imagine that you need to run a tool as part of your build. For example, you
may want to generate or preprocess a source file, or compress a binary. In this
tutorial, you are going to create a symbolic macro that resizes an image.

Macros are suitable for simple tasks. If you want to do anything more
complicated, for example add support for a new programming language, consider
creating a [rule](/extending/rules). Rules give you more control and flexibility.

The easiest way to create a macro that resizes an image is to use a `genrule`:

```starlark
genrule(
    name = "logo_miniature",
    srcs = ["logo.png"],
    outs = ["small_logo.png"],
    cmd = "convert $< -resize 100x100 $@",
)

cc_binary(
    name = "my_app",
    srcs = ["my_app.cc"],
    data = [":logo_miniature"],
)
```

If you need to resize more images, you may want to reuse the code. To do that,
define an *implementation function* and a *macro declaration* in a separate
`.bzl` file, and call the file `miniature.bzl`:

```starlark
# Implementation function
def _miniature_impl(name, visibility, src, size, **kwargs):
    native.genrule(
        name = name,
        visibility = visibility,
        srcs = [src],
        outs = [name + "_small_" + src.name],
        cmd = "convert $< -resize " + size + " $@",
        **kwargs,
    )

# Macro declaration
miniature = macro(
    doc = """Create a miniature of the src image.

    The generated file name will be prefixed with `name + "_small_"`.
    """,
    implementation = _miniature_impl,
    # Inherit most of genrule's attributes (such as tags and testonly)
    inherit_attrs = native.genrule,
    attrs = {
        "src": attr.label(
            doc = "Image file",
            allow_single_file = True,
            # Non-configurable because our genrule's output filename is
            # suffixed with src's name. (We want to suffix the output file with
            # srcs's name because some tools that operate on image files expect
            # the files to have the right file extension.)
            configurable = False,
        ),
        "size": attr.string(
            doc = "Output size in WxH format",
            default = "100x100",
        ),
        # Do not allow callers of miniature() to set srcs, cmd, or outs -
        # _miniature_impl overrides their values when calling native.genrule()
        "srcs": None,
        "cmd": None,
        "outs": None,
    },
)
```

A few remarks:

  * Symbolic macro implementation functions must have `name` and `visibility`
    parameters. They should used for the macro's main target.

  * To document the behavior of a symbolic macro, use `doc` parameters for
    `macro()` and its attributes.

  * To call a `genrule`, or any other native rule, use `native.`.

  * Use `**kwargs` to forward the extra inherited arguments to the underlying
    `genrule` (it works just like in
    [Python](https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments)).
    This is useful so that a user can set standard attributes like `tags` or
    `testonly`.

Now, use the macro from the `BUILD` file:

```starlark
load("//path/to:miniature.bzl", "miniature")

miniature(
    name = "logo_miniature",
    src = "image.png",
)

cc_binary(
    name = "my_app",
    srcs = ["my_app.cc"],
    data = [":logo_miniature"],
)
```

---

## Optimizing Performance
- URL: https://bazel.build/rules/performance
- Source: rules/performance.mdx
- Slug: /rules/performance

When writing rules, the most common performance pitfall is to traverse or copy
data that is accumulated from dependencies. When aggregated over the whole
build, these operations can easily take O(N^2) time or space. To avoid this, it
is crucial to understand how to use depsets effectively.

This can be hard to get right, so Bazel also provides a memory profiler that
assists you in finding spots where you might have made a mistake. Be warned:
The cost of writing an inefficient rule may not be evident until it is in
widespread use.

## Use depsets

Whenever you are rolling up information from rule dependencies you should use
[depsets](lib/depset). Only use plain lists or dicts to publish information
local to the current rule.

A depset represents information as a nested graph which enables sharing.

Consider the following graph:

```
C -> B -> A
D ---^
```

Each node publishes a single string. With depsets the data looks like this:

```
a = depset(direct=['a'])
b = depset(direct=['b'], transitive=[a])
c = depset(direct=['c'], transitive=[b])
d = depset(direct=['d'], transitive=[b])
```

Note that each item is only mentioned once. With lists you would get this:

```
a = ['a']
b = ['b', 'a']
c = ['c', 'b', 'a']
d = ['d', 'b', 'a']
```

Note that in this case `'a'` is mentioned four times! With larger graphs this
problem will only get worse.

Here is an example of a rule implementation that uses depsets correctly to
publish transitive information. Note that it is OK to publish rule-local
information using lists if you want since this is not O(N^2).

```
MyProvider = provider()

def _impl(ctx):
  my_things = ctx.attr.things
  all_things = depset(
      direct=my_things,
      transitive=[dep[MyProvider].all_things for dep in ctx.attr.deps]
  )
  ...
  return [MyProvider(
    my_things=my_things,  # OK, a flat list of rule-local things only
    all_things=all_things,  # OK, a depset containing dependencies
  )]
```

See the [depset overview](/extending/depsets) page for more information.

### Avoid calling `depset.to_list()`

You can coerce a depset to a flat list using
[`to_list()`](lib/depset#to_list), but doing so usually results in O(N^2)
cost. If at all possible, avoid any flattening of depsets except for debugging
purposes.

A common misconception is that you can freely flatten depsets if you only do it
at top-level targets, such as an `<xx>_binary` rule, since then the cost is not
accumulated over each level of the build graph. But this is *still* O(N^2) when
you build a set of targets with overlapping dependencies. This happens when
building your tests `//foo/tests/...`, or when importing an IDE project.

### Reduce the number of calls to `depset`

Calling `depset` inside a loop is often a mistake. It can lead to depsets with
very deep nesting, which perform poorly. For example:

```python
x = depset()
for i in inputs:
    # Do not do that.
    x = depset(transitive = [x, i.deps])
```

This code can be replaced easily. First, collect the transitive depsets and
merge them all at once:

```python
transitive = []

for i in inputs:
    transitive.append(i.deps)

x = depset(transitive = transitive)
```

This can sometimes be reduced using a list comprehension:

```python
x = depset(transitive = [i.deps for i in inputs])
```

## Use ctx.actions.args() for command lines

When building command lines you should use [ctx.actions.args()](lib/Args).
This defers expansion of any depsets to the execution phase.

Apart from being strictly faster, this will reduce the memory consumption of
your rules -- sometimes by 90% or more.

Here are some tricks:

* Pass depsets and lists directly as arguments, instead of flattening them
yourself. They will get expanded by `ctx.actions.args()` for you.
If you need any transformations on the depset contents, look at
[ctx.actions.args#add](lib/Args#add) to see if anything fits the bill.

* Are you passing `File#path` as arguments? No need. Any
[File](lib/File) is automatically turned into its
[path](lib/File#path), deferred to expansion time.

* Avoid constructing strings by concatenating them together.
The best string argument is a constant as its memory will be shared between
all instances of your rule.

* If the args are too long for the command line an `ctx.actions.args()` object
can be conditionally or unconditionally written to a param file using
[`ctx.actions.args#use_param_file`](lib/Args#use_param_file). This is
done behind the scenes when the action is executed. If you need to explicitly
control the params file you can write it manually using
[`ctx.actions.write`](lib/actions#write).

Example:

```
def _impl(ctx):
  ...
  args = ctx.actions.args()
  file = ctx.declare_file(...)
  files = depset(...)

  # Bad, constructs a full string "--foo=<file path>" for each rule instance
  args.add("--foo=" + file.path)

  # Good, shares "--foo" among all rule instances, and defers file.path to later
  # It will however pass ["--foo", <file path>] to the action command line,
  # instead of ["--foo=<file_path>"]
  args.add("--foo", file)

  # Use format if you prefer ["--foo=<file path>"] to ["--foo", <file path>]
  args.add(file, format="--foo=%s")

  # Bad, makes a giant string of a whole depset
  args.add(" ".join(["-I%s" % file.short_path for file in files.to_list()])

  # Good, only stores a reference to the depset
  args.add_all(files, format_each="-I%s", map_each=_to_short_path)

# Function passed to map_each above
def _to_short_path(f):
  return f.short_path
```

## Transitive action inputs should be depsets

When building an action using [ctx.actions.run](lib/actions?#run), do not
forget that the `inputs` field accepts a depset. Use this whenever inputs are
collected from dependencies transitively.

```
inputs = depset(...)
ctx.actions.run(
  inputs = inputs,  # Do *not* turn inputs into a list
  ...
)
```

## Hanging

If Bazel appears to be hung, you can hit <kbd>Ctrl-&#92;</kbd> or send
Bazel a `SIGQUIT` signal (`kill -3 $(bazel info server_pid)`) to get a thread
dump in the file `$(bazel info output_base)/server/jvm.out`.

Since you may not be able to run `bazel info` if bazel is hung, the
`output_base` directory is usually the parent of the `bazel-<workspace>`
symlink in your workspace directory.

## Performance profiling

The [JSON trace profile](/advanced/performance/json-trace-profile) can be very
useful to quickly understand what Bazel spent time on during the invocation.

The [`--experimental_command_profile`](https://bazel.build/reference/command-line-reference#flag--experimental_command_profile)
flag may be used to capture Java Flight Recorder profiles of various kinds
(cpu time, wall time, memory allocations and lock contention).

The [`--starlark_cpu_profile`](https://bazel.build/reference/command-line-reference#flag--starlark_cpu_profile)
flag may be used to write a pprof profile of CPU usage by all Starlark threads.

## Memory profiling

Bazel comes with a built-in memory profiler that can help you check your rule’s
memory use. If there is a problem you can dump the heap to find the
exact line of code that is causing the problem.

### Enabling memory tracking

You must pass these two startup flags to *every* Bazel invocation:

  ```
  STARTUP_FLAGS=\
  --host_jvm_args=-javaagent:<path to java-allocation-instrumenter-3.3.4.jar> \</p>
  --host_jvm_args=-DRULE_MEMORY_TRACKER=1
  ```
Note: You can download the allocation instrumenter jar file from [Maven Central
Repository][allocation-instrumenter-link].

[allocation-instrumenter-link]: https://repo1.maven.org/maven2/com/google/code/java-allocation-instrumenter/java-allocation-instrumenter/3.3.4

These start the server in memory tracking mode. If you forget these for even
one Bazel invocation the server will restart and you will have to start over.

### Using the Memory Tracker

As an example, look at the target `foo` and see what it does. To only
run the analysis and not run the build execution phase, add the
`--nobuild` flag.

```
$ bazel $(STARTUP_FLAGS) build --nobuild //foo:foo
```

Next, see how much memory the whole Bazel instance consumes:

```
$ bazel $(STARTUP_FLAGS) info used-heap-size-after-gc
> 2594MB
```

Break it down by rule class by using `bazel dump --rules`:

```
$ bazel $(STARTUP_FLAGS) dump --rules
>

RULE                                 COUNT     ACTIONS          BYTES         EACH
genrule                             33,762      33,801    291,538,824        8,635
config_setting                      25,374           0     24,897,336          981
filegroup                           25,369      25,369     97,496,272        3,843
cc_library                           5,372      73,235    182,214,456       33,919
proto_library                        4,140     110,409    186,776,864       45,115
android_library                      2,621      36,921    218,504,848       83,366
java_library                         2,371      12,459     38,841,000       16,381
_gen_source                            719       2,157      9,195,312       12,789
_check_proto_library_deps              719         668      1,835,288        2,552
... (more output)
```

Look at where the memory is going by producing a `pprof` file
using `bazel dump --skylark_memory`:

```
$ bazel $(STARTUP_FLAGS) dump --skylark_memory=$HOME/prof.gz
> Dumping Starlark heap to: /usr/local/google/home/$USER/prof.gz
```

Use the `pprof` tool to investigate the heap. A good starting point is
getting a flame graph by using `pprof -flame $HOME/prof.gz`.

Get `pprof` from [https://github.com/google/pprof](https://github.com/google/pprof).

Get a text dump of the hottest call sites annotated with lines:

```
$ pprof -text -lines $HOME/prof.gz
>
      flat  flat%   sum%        cum   cum%
  146.11MB 19.64% 19.64%   146.11MB 19.64%  android_library <native>:-1
  113.02MB 15.19% 34.83%   113.02MB 15.19%  genrule <native>:-1
   74.11MB  9.96% 44.80%    74.11MB  9.96%  glob <native>:-1
   55.98MB  7.53% 52.32%    55.98MB  7.53%  filegroup <native>:-1
   53.44MB  7.18% 59.51%    53.44MB  7.18%  sh_test <native>:-1
   26.55MB  3.57% 63.07%    26.55MB  3.57%  _generate_foo_files /foo/tc/tc.bzl:491
   26.01MB  3.50% 66.57%    26.01MB  3.50%  _build_foo_impl /foo/build_test.bzl:78
   22.01MB  2.96% 69.53%    22.01MB  2.96%  _build_foo_impl /foo/build_test.bzl:73
   ... (more output)
```

---

## Rules Tutorial
- URL: https://bazel.build/rules/rules-tutorial
- Source: rules/rules-tutorial.mdx
- Slug: /rules/rules-tutorial

{/* [TOC] */}

[Starlark](https://github.com/bazelbuild/starlark) is a Python-like
configuration language originally developed for use in Bazel and since adopted
by other tools. Bazel's `BUILD` and `.bzl` files are written in a dialect of
Starlark properly known as the "Build Language", though it is often simply
referred to as "Starlark", especially when emphasizing that a feature is
expressed in the Build Language as opposed to being a built-in or "native" part
of Bazel. Bazel augments the core language with numerous build-related functions
such as `glob`, `genrule`, `java_binary`, and so on.

See the
[Bazel](/start/) and [Starlark](/extending/concepts) documentation for
more details, and the
[Rules SIG template](https://github.com/bazel-contrib/rules-template) as a
starting point for new rulesets.

## The empty rule

To create your first rule, create the file `foo.bzl`:

```python
def _foo_binary_impl(ctx):
    pass

foo_binary = rule(
    implementation = _foo_binary_impl,
)
```

When you call the [`rule`](lib/globals#rule) function, you
must define a callback function. The logic will go there, but you
can leave the function empty for now. The [`ctx`](lib/ctx) argument
provides information about the target.

You can load the rule and use it from a `BUILD` file.

Create a `BUILD` file in the same directory:

```python
load(":foo.bzl", "foo_binary")

foo_binary(name = "bin")
```

Now, the target can be built:

```
$ bazel build bin
INFO: Analyzed target //:bin (2 packages loaded, 17 targets configured).
INFO: Found 1 target...
Target //:bin up-to-date (nothing to build)
```

Even though the rule does nothing, it already behaves like other rules: it has a
mandatory name, it supports common attributes like `visibility`, `testonly`, and
`tags`.

## Evaluation model

Before going further, it's important to understand how the code is evaluated.

Update `foo.bzl` with some print statements:

```python
def _foo_binary_impl(ctx):
    print("analyzing", ctx.label)

foo_binary = rule(
    implementation = _foo_binary_impl,
)

print("bzl file evaluation")
```

and BUILD:

```python
load(":foo.bzl", "foo_binary")

print("BUILD file")
foo_binary(name = "bin1")
foo_binary(name = "bin2")
```

[`ctx.label`](lib/ctx#label)
corresponds to the label of the target being analyzed. The `ctx` object has
many useful fields and methods; you can find an exhaustive list in the
[API reference](lib/ctx).

Query the code:

```
$ bazel query :all
DEBUG: /usr/home/bazel-codelab/foo.bzl:8:1: bzl file evaluation
DEBUG: /usr/home/bazel-codelab/BUILD:2:1: BUILD file
//:bin2
//:bin1
```

Make a few observations:

* "bzl file evaluation" is printed first. Before evaluating the `BUILD` file,
  Bazel evaluates all the files it loads. If multiple `BUILD` files are loading
  foo.bzl, you would see only one occurrence of "bzl file evaluation" because
  Bazel caches the result of the evaluation.
* The callback function `_foo_binary_impl` is not called. Bazel query loads
  `BUILD` files, but doesn't analyze targets.

To analyze the targets, use the [`cquery`](/query/cquery) ("configured
query") or the `build` command:

```
$ bazel build :all
DEBUG: /usr/home/bazel-codelab/foo.bzl:2:5: analyzing //:bin1
DEBUG: /usr/home/bazel-codelab/foo.bzl:2:5: analyzing //:bin2
INFO: Analyzed 2 targets (0 packages loaded, 0 targets configured).
INFO: Found 2 targets...
```

As you can see, `_foo_binary_impl` is now called twice - once for each target.

Notice that neither "bzl file evaluation" nor "BUILD file" are printed again,
because the evaluation of `foo.bzl` is cached after the call to `bazel query`.
Bazel only emits `print` statements when they are actually executed.

## Creating a file

To make your rule more useful, update it to generate a file. First, declare the
file and give it a name. In this example, create a file with the same name as
the target:

```python
ctx.actions.declare_file(ctx.label.name)
```

If you run `bazel build :all` now, you will get an error:

```
The following files have no generating action:
bin2
```

Whenever you declare a file, you have to tell Bazel how to generate it by
creating an action. Use [`ctx.actions.write`](lib/actions#write),
to create a file with the given content.

```python
def _foo_binary_impl(ctx):
    out = ctx.actions.declare_file(ctx.label.name)
    ctx.actions.write(
        output = out,
        content = "Hello\n",
    )
```

The code is valid, but it won't do anything:

```
$ bazel build bin1
Target //:bin1 up-to-date (nothing to build)
```

The `ctx.actions.write` function registered an action, which taught Bazel
how to generate the file. But Bazel won't create the file until it is
actually requested. So the last thing to do is tell Bazel that the file
is an output of the rule, and not a temporary file used within the rule
implementation.

```python
def _foo_binary_impl(ctx):
    out = ctx.actions.declare_file(ctx.label.name)
    ctx.actions.write(
        output = out,
        content = "Hello!\n",
    )
    return [DefaultInfo(files = depset([out]))]
```

Look at the `DefaultInfo` and `depset` functions later. For now,
assume that the last line is the way to choose the outputs of a rule.

Now, run Bazel:

```
$ bazel build bin1
INFO: Found 1 target...
Target //:bin1 up-to-date:
  bazel-bin/bin1

$ cat bazel-bin/bin1
Hello!
```

You have successfully generated a file!

## Attributes

To make the rule more useful, add new attributes using
[the `attr` module](lib/attr) and update the rule definition.

Add a string attribute called `username`:

```python
foo_binary = rule(
    implementation = _foo_binary_impl,
    attrs = {
        "username": attr.string(),
    },
)
```

Next, set it in the `BUILD` file:

```python
foo_binary(
    name = "bin",
    username = "Alice",
)
```

To access the value in the callback function, use `ctx.attr.username`. For
example:

```python
def _foo_binary_impl(ctx):
    out = ctx.actions.declare_file(ctx.label.name)
    ctx.actions.write(
        output = out,
        content = "Hello {}!\n".format(ctx.attr.username),
    )
    return [DefaultInfo(files = depset([out]))]
```

Note that you can make the attribute mandatory or set a default value. Look at
the documentation of [`attr.string`](lib/attr#string).
You may also use other types of attributes, such as [boolean](lib/attr#bool)
or [list of integers](lib/attr#int_list).

## Dependencies

Dependency attributes, such as [`attr.label`](lib/attr#label)
and [`attr.label_list`](lib/attr#label_list),
declare a dependency from the target that owns the attribute to the target whose
label appears in the attribute's value. This kind of attribute forms the basis
of the target graph.

In the `BUILD` file, the target label appears as a string object, such as
`//pkg:name`. In the implementation function, the target will be accessible as a
[`Target`](lib/Target) object. For example, view the files returned
by the target using [`Target.files`](lib/Target#modules.Target.files).

### Multiple files

By default, only targets created by rules may appear as dependencies (such as a
`foo_library()` target). If you want the attribute to accept targets that are
input files (such as source files in the repository), you can do it with
`allow_files` and specify the list of accepted file extensions (or `True` to
allow any file extension):

```python
"srcs": attr.label_list(allow_files = [".java"]),
```

The list of files can be accessed with `ctx.files.<attribute name>`. For
example, the list of files in the `srcs` attribute can be accessed through

```python
ctx.files.srcs
```

### Single file

If you need only one file, use `allow_single_file`:

```python
"src": attr.label(allow_single_file = [".java"])
```

This file is then accessible under `ctx.file.<attribute name>`:

```python
ctx.file.src
```

## Create a file with a template

You can create a rule that generates a .cc file based on a template. Also, you
can use `ctx.actions.write` to output a string constructed in the rule
implementation function, but this has two problems. First, as the template gets
bigger, it becomes more memory efficient to put it in a separate file and avoid
constructing large strings during the analysis phase. Second, using a separate
file is more convenient for the user. Instead, use
[`ctx.actions.expand_template`](lib/actions#expand_template),
which performs substitutions on a template file.

Create a `template` attribute to declare a dependency on the template
file:

```python
def _hello_world_impl(ctx):
    out = ctx.actions.declare_file(ctx.label.name + ".cc")
    ctx.actions.expand_template(
        output = out,
        template = ctx.file.template,
        substitutions = {"{NAME}": ctx.attr.username},
    )
    return [DefaultInfo(files = depset([out]))]

hello_world = rule(
    implementation = _hello_world_impl,
    attrs = {
        "username": attr.string(default = "unknown person"),
        "template": attr.label(
            allow_single_file = [".cc.tpl"],
            mandatory = True,
        ),
    },
)
```

Users can use the rule like this:

```python
hello_world(
    name = "hello",
    username = "Alice",
    template = "file.cc.tpl",
)

cc_binary(
    name = "hello_bin",
    srcs = [":hello"],
)
```

If you don't want to expose the template to the end-user and always use the
same one, you can set a default value and make the attribute private:

```python
    "_template": attr.label(
        allow_single_file = True,
        default = "file.cc.tpl",
    ),
```

Attributes that start with an underscore are private and cannot be set in a
`BUILD` file. The template is now an _implicit dependency_: Every `hello_world`
target has a dependency on this file. Don't forget to make this file visible
to other packages by updating the `BUILD` file and using
[`exports_files`](/reference/be/functions#exports_files):

```python
exports_files(["file.cc.tpl"])
```

## Going further

*   Take a look at the [reference documentation for rules](/extending/rules#contents).
*   Get familiar with [depsets](/extending/depsets).
*   Check out the [examples repository](https://github.com/bazelbuild/examples/tree/master/rules)
    which includes additional examples of rules.

---

## Testing
- URL: https://bazel.build/rules/testing
- Source: rules/testing.mdx
- Slug: /rules/testing

There are several different approaches to testing Starlark code in Bazel. This
page gathers the current best practices and frameworks by use case.

## Testing rules

[Skylib](https://github.com/bazelbuild/bazel-skylib) has a test framework called
[`unittest.bzl`](https://github.com/bazelbuild/bazel-skylib/blob/main/lib/unittest.bzl)
for checking the analysis-time behavior of rules, such as their actions and
providers. Such tests are called "analysis tests" and are currently the best
option for testing the inner workings of rules.

Some caveats:

*   Test assertions occur within the build, not a separate test runner process.
    Targets that are created by the test must be named such that they do not
    collide with targets from other tests or from the build. An error that
    occurs during the test is seen by Bazel as a build breakage rather than a
    test failure.

*   It requires a fair amount of boilerplate to set up the rules under test and
    the rules containing test assertions. This boilerplate may seem daunting at
    first. It helps to [keep in mind](/extending/concepts#evaluation-model) that macros
    are evaluated and targets generated during the loading phase, while rule
    implementation functions don't run until later, during the analysis phase.

*   Analysis tests are intended to be fairly small and lightweight. Certain
    features of the analysis testing framework are restricted to verifying
    targets with a maximum number of transitive dependencies (currently 500).
    This is due to performance implications of using these features with larger
    tests.

The basic principle is to define a testing rule that depends on the
rule-under-test. This gives the testing rule access to the rule-under-test's
providers.

The testing rule's implementation function carries out assertions. If there are
any failures, these are not raised immediately by calling `fail()` (which would
trigger an analysis-time build error), but rather by storing the errors in a
generated script that fails at test execution time.

See below for a minimal toy example, followed by an example that checks actions.

### Minimal example

`//mypkg/myrules.bzl`:

```python
MyInfo = provider(fields = {
    "val": "string value",
    "out": "output File",
})

def _myrule_impl(ctx):
    """Rule that just generates a file and returns a provider."""
    out = ctx.actions.declare_file(ctx.label.name + ".out")
    ctx.actions.write(out, "abc")
    return [MyInfo(val="some value", out=out)]

myrule = rule(
    implementation = _myrule_impl,
)
```

`//mypkg/myrules_test.bzl`:


```python
load("@bazel_skylib//lib:unittest.bzl", "asserts", "analysistest")
load(":myrules.bzl", "myrule", "MyInfo")

# ==== Check the provider contents ====

def _provider_contents_test_impl(ctx):
    env = analysistest.begin(ctx)

    target_under_test = analysistest.target_under_test(env)
    # If preferred, could pass these values as "expected" and "actual" keyword
    # arguments.
    asserts.equals(env, "some value", target_under_test[MyInfo].val)

    # If you forget to return end(), you will get an error about an analysis
    # test needing to return an instance of AnalysisTestResultInfo.
    return analysistest.end(env)

# Create the testing rule to wrap the test logic. This must be bound to a global
# variable, not called in a macro's body, since macros get evaluated at loading
# time but the rule gets evaluated later, at analysis time. Since this is a test
# rule, its name must end with "_test".
provider_contents_test = analysistest.make(_provider_contents_test_impl)

# Macro to setup the test.
def _test_provider_contents():
    # Rule under test. Be sure to tag 'manual', as this target should not be
    # built using `:all` except as a dependency of the test.
    myrule(name = "provider_contents_subject", tags = ["manual"])
    # Testing rule.
    provider_contents_test(name = "provider_contents_test",
                           target_under_test = ":provider_contents_subject")
    # Note the target_under_test attribute is how the test rule depends on
    # the real rule target.

# Entry point from the BUILD file; macro for running each test case's macro and
# declaring a test suite that wraps them together.
def myrules_test_suite(name):
    # Call all test functions and wrap their targets in a suite.
    _test_provider_contents()
    # ...

    native.test_suite(
        name = name,
        tests = [
            ":provider_contents_test",
            # ...
        ],
    )
```

`//mypkg/BUILD`:

```python
load(":myrules.bzl", "myrule")
load(":myrules_test.bzl", "myrules_test_suite")

# Production use of the rule.
myrule(
    name = "mytarget",
)

# Call a macro that defines targets that perform the tests at analysis time,
# and that can be executed with "bazel test" to return the result.
myrules_test_suite(name = "myrules_test")
```

The test can be run with `bazel test //mypkg:myrules_test`.

Aside from the initial `load()` statements, there are two main parts to the
file:

*   The tests themselves, each of which consists of 1) an analysis-time
    implementation function for the testing rule, 2) a declaration of the
    testing rule via `analysistest.make()`, and 3) a loading-time function
    (macro) for declaring the rule-under-test (and its dependencies) and testing
    rule. If the assertions do not change between test cases, 1) and 2) may be
    shared by multiple test cases.

*   The test suite function, which calls the loading-time functions for each
    test, and declares a `test_suite` target bundling all tests together.

For consistency, follow the recommended naming convention: Let `foo` stand for
the part of the test name that describes what the test is checking
(`provider_contents` in the above example). For example, a JUnit test method
would be named `testFoo`.

Then:

*   the macro which generates the test and target under test should should be
    named `_test_foo` (`_test_provider_contents`)

*   its test rule type should be named `foo_test` (`provider_contents_test`)

*   the label of the target of this rule type should be `foo_test`
    (`provider_contents_test`)

*   the implementation function for the testing rule should be named
    `_foo_test_impl` (`_provider_contents_test_impl`)

*   the labels of the targets of the rules under test and their dependencies
    should be prefixed with `foo_` (`provider_contents_`)

Note that the labels of all targets can conflict with other labels in the same
BUILD package, so it's helpful to use a unique name for the test.

### Failure testing

It may be useful to verify that a rule fails given certain inputs or in certain
state. This can be done using the analysis test framework:

The test rule created with `analysistest.make` should specify `expect_failure`:

```python
failure_testing_test = analysistest.make(
    _failure_testing_test_impl,
    expect_failure = True,
)
```

The test rule implementation should make assertions on the nature of the failure
that took place (specifically, the failure message):

```python
def _failure_testing_test_impl(ctx):
    env = analysistest.begin(ctx)
    asserts.expect_failure(env, "This rule should never work")
    return analysistest.end(env)
```

Also make sure that your target under test is specifically tagged 'manual'.
Without this, building all targets in your package using `:all` will result in a
build of the intentionally-failing target and will exhibit a build failure. With
'manual', your target under test will build only if explicitly specified, or as
a dependency of a non-manual target (such as your test rule):

```python
def _test_failure():
    myrule(name = "this_should_fail", tags = ["manual"])

    failure_testing_test(name = "failure_testing_test",
                         target_under_test = ":this_should_fail")

# Then call _test_failure() in the macro which generates the test suite and add
# ":failure_testing_test" to the suite's test targets.
```

### Verifying registered actions

You may want to write tests which make assertions about the actions that your
rule registers, for example, using `ctx.actions.run()`. This can be done in your
analysis test rule implementation function. An example:

```python
def _inspect_actions_test_impl(ctx):
    env = analysistest.begin(ctx)

    target_under_test = analysistest.target_under_test(env)
    actions = analysistest.target_actions(env)
    asserts.equals(env, 1, len(actions))
    action_output = actions[0].outputs.to_list()[0]
    asserts.equals(
        env, target_under_test.label.name + ".out", action_output.basename)
    return analysistest.end(env)
```

Note that `analysistest.target_actions(env)` returns a list of
[`Action`](lib/Action) objects which represent actions registered by the
target under test.

### Verifying rule behavior under different flags

You may want to verify your real rule behaves a certain way given certain build
flags. For example, your rule may behave differently if a user specifies:

```shell
bazel build //mypkg:real_target -c opt
```

versus

```shell
bazel build //mypkg:real_target -c dbg
```

At first glance, this could be done by testing the target under test using the
desired build flags:

```shell
bazel test //mypkg:myrules_test -c opt
```

But then it becomes impossible for your test suite to simultaneously contain a
test which verifies the rule behavior under `-c opt` and another test which
verifies the rule behavior under `-c dbg`. Both tests would not be able to run
in the same build!

This can be solved by specifying the desired build flags when defining the test
rule:

```python
myrule_c_opt_test = analysistest.make(
    _myrule_c_opt_test_impl,
    config_settings = {
        "//command_line_option:compilation_mode": "opt",
    },
)
```

Normally, a target under test is analyzed given the current build flags.
Specifying `config_settings` overrides the values of the specified command line
options. (Any unspecified options will retain their values from the actual
command line).

In the specified `config_settings` dictionary, command line flags must be
prefixed with a special placeholder value `//command_line_option:`, as is shown
above.


## Validating artifacts

The main ways to check that your generated files are correct are:

*   You can write a test script in shell, Python, or another language, and
    create a target of the appropriate `*_test` rule type.

*   You can use a specialized rule for the kind of test you want to perform.

### Using a test target

The most straightforward way to validate an artifact is to write a script and
add a `*_test` target to your BUILD file. The specific artifacts you want to
check should be data dependencies of this target. If your validation logic is
reusable for multiple tests, it should be a script that takes command line
arguments that are controlled by the test target's `args` attribute. Here's an
example that validates that the output of `myrule` from above is `"abc"`.

`//mypkg/myrule_validator.sh`:

```shell
if [ "$(cat $1)" = "abc" ]; then
  echo "Passed"
  exit 0
else
  echo "Failed"
  exit 1
fi
```

`//mypkg/BUILD`:

```python
...

myrule(
    name = "mytarget",
)

...

# Needed for each target whose artifacts are to be checked.
sh_test(
    name = "validate_mytarget",
    srcs = [":myrule_validator.sh"],
    args = ["$(location :mytarget.out)"],
    data = [":mytarget.out"],
)
```

### Using a custom rule

A more complicated alternative is to write the shell script as a template that
gets instantiated by a new rule. This involves more indirection and Starlark
logic, but leads to cleaner BUILD files. As a side-benefit, any argument
preprocessing can be done in Starlark instead of the script, and the script is
slightly more self-documenting since it uses symbolic placeholders (for
substitutions) instead of numeric ones (for arguments).

`//mypkg/myrule_validator.sh.template`:

```shell
if [ "$(cat %TARGET%)" = "abc" ]; then
  echo "Passed"
  exit 0
else
  echo "Failed"
  exit 1
fi
```

`//mypkg/myrule_validation.bzl`:

```python
def _myrule_validation_test_impl(ctx):
  """Rule for instantiating myrule_validator.sh.template for a given target."""
  exe = ctx.outputs.executable
  target = ctx.file.target
  ctx.actions.expand_template(output = exe,
                              template = ctx.file._script,
                              is_executable = True,
                              substitutions = {
                                "%TARGET%": target.short_path,
                              })
  # This is needed to make sure the output file of myrule is visible to the
  # resulting instantiated script.
  return [DefaultInfo(runfiles=ctx.runfiles(files=[target]))]

myrule_validation_test = rule(
    implementation = _myrule_validation_test_impl,
    attrs = {"target": attr.label(allow_single_file=True),
             # You need an implicit dependency in order to access the template.
             # A target could potentially override this attribute to modify
             # the test logic.
             "_script": attr.label(allow_single_file=True,
                                   default=Label("//mypkg:myrule_validator"))},
    test = True,
)
```

`//mypkg/BUILD`:

```python
...

myrule(
    name = "mytarget",
)

...

# Needed just once, to expose the template. Could have also used export_files(),
# and made the _script attribute set allow_files=True.
filegroup(
    name = "myrule_validator",
    srcs = [":myrule_validator.sh.template"],
)

# Needed for each target whose artifacts are to be checked. Notice that you no
# longer have to specify the output file name in a data attribute, or its
# $(location) expansion in an args attribute, or the label for the script
# (unless you want to override it).
myrule_validation_test(
    name = "validate_mytarget",
    target = ":mytarget",
)
```

Alternatively, instead of using a template expansion action, you could have
inlined the template into the .bzl file as a string and expanded it during the
analysis phase using the `str.format` method or `%`-formatting.

## Testing Starlark utilities

[Skylib](https://github.com/bazelbuild/bazel-skylib)'s
[`unittest.bzl`](https://github.com/bazelbuild/bazel-skylib/blob/main/lib/unittest.bzl)
framework can be used to test utility functions (that is, functions that are
neither macros nor rule implementations). Instead of using `unittest.bzl`'s
`analysistest` library, `unittest` may be used. For such test suites, the
convenience function `unittest.suite()` can be used to reduce boilerplate.

`//mypkg/myhelpers.bzl`:

```python
def myhelper():
    return "abc"
```

`//mypkg/myhelpers_test.bzl`:


```python
load("@bazel_skylib//lib:unittest.bzl", "asserts", "unittest")
load(":myhelpers.bzl", "myhelper")

def _myhelper_test_impl(ctx):
  env = unittest.begin(ctx)
  asserts.equals(env, "abc", myhelper())
  return unittest.end(env)

myhelper_test = unittest.make(_myhelper_test_impl)

# No need for a test_myhelper() setup function.

def myhelpers_test_suite(name):
  # unittest.suite() takes care of instantiating the testing rules and creating
  # a test_suite.
  unittest.suite(
    name,
    myhelper_test,
    # ...
  )
```

`//mypkg/BUILD`:

```python
load(":myhelpers_test.bzl", "myhelpers_test_suite")

myhelpers_test_suite(name = "myhelpers_tests")
```

For more examples, see Skylib's own [tests](https://github.com/bazelbuild/bazel-skylib/blob/main/tests/BUILD).

---

## Using Macros to Create Custom Verbs
- URL: https://bazel.build/rules/verbs-tutorial
- Source: rules/verbs-tutorial.mdx
- Slug: /rules/verbs-tutorial

Day-to-day interaction with Bazel happens primarily through a few commands:
`build`, `test`, and `run`. At times, though, these can feel limited: you may
want to push packages to a repository, publish documentation for end-users, or
deploy an application with Kubernetes. But Bazel doesn't have a `publish` or
`deploy` command – where do these actions fit in?

## The bazel run command

Bazel's focus on hermeticity, reproducibility, and incrementality means the
`build` and `test` commands aren't helpful for the above tasks. These actions
may run in a sandbox, with limited network access, and aren't guaranteed to be
re-run with every `bazel build`.

Instead, rely on `bazel run`: the workhorse for tasks that you *want* to have
side effects. Bazel users are accustomed to rules that create executables, and
rule authors can follow a common set of patterns to extend this to
"custom verbs".

### In the wild: rules_k8s
For example, consider [`rules_k8s`](https://github.com/bazelbuild/rules_k8s),
the Kubernetes rules for Bazel. Suppose you have the following target:

```python
# BUILD file in //application/k8s
k8s_object(
    name = "staging",
    kind = "deployment",
    cluster = "testing",
    template = "deployment.yaml",
)
```

The [`k8s_object` rule](https://github.com/bazelbuild/rules_k8s#usage) builds a
standard Kubernetes YAML file when `bazel build` is used on the `staging`
target. However, the additional targets are also created by the `k8s_object`
macro with names like `staging.apply` and `:staging.delete`. These build
scripts to perform those actions, and when executed with `bazel run
staging.apply`, these behave like our own `bazel k8s-apply` or `bazel
k8s-delete` commands.

### Another example: ts_api_guardian_test

This pattern can also be seen in the Angular project. The
[`ts_api_guardian_test` macro](https://github.com/angular/angular/blob/16ac611a8410e6bcef8ffc779f488ca4fa102155/tools/ts-api-guardian/index.bzl#L22)
produces two targets. The first is a standard `nodejs_test` target which compares
some generated output against a "golden" file (that is, a file containing the
expected output). This can be built and run with a normal `bazel
test` invocation. In `angular-cli`, you can run [one such
target](https://github.com/angular/angular-cli/blob/e1269cb520871ee29b1a4eec6e6c0e4a94f0b5fc/etc/api/BUILD)
with `bazel test //etc/api:angular_devkit_core_api`.

Over time, this golden file may need to be updated for legitimate reasons.
Updating this manually is tedious and error-prone, so this macro also provides
a `nodejs_binary` target that updates the golden file, instead of comparing
against it. Effectively, the same test script can be written to run in "verify"
or "accept" mode, based on how it's invoked. This follows the same pattern
you've learned already: there is no native `bazel test-accept` command, but the
same effect can be achieved with
`bazel run //etc/api:angular_devkit_core_api.accept`.

This pattern can be quite powerful, and turns out to be quite common once you
learn to recognize it.

## Adapting your own rules

[Macros](/extending/macros) are the heart of this pattern. Macros are used like
rules, but they can create several targets. Typically, they will create a
target with the specified name which performs the primary build action: perhaps
it builds a normal binary, a Docker image, or an archive of source code. In
this pattern, additional targets are created to produce scripts performing side
effects based on the output of the primary target, like publishing the
resulting binary or updating the expected test output.

To illustrate this, wrap an imaginary rule that generates a website with
[Sphinx](https://www.sphinx-doc.org) with a macro to create an additional
target that allows the user to publish it when ready. Consider the following
existing rule for generating a website with Sphinx:

```python
_sphinx_site = rule(
     implementation = _sphinx_impl,
     attrs = {"srcs": attr.label_list(allow_files = [".rst"])},
)
```

Next, consider a rule like the following, which builds a script that, when run,
publishes the generated pages:

```python
_sphinx_publisher = rule(
    implementation = _publish_impl,
    attrs = {
        "site": attr.label(),
        "_publisher": attr.label(
            default = "//internal/sphinx:publisher",
            executable = True,
        ),
    },
    executable = True,
)
```

Finally, define the following symbolic macro (available in Bazel 8 or newer) to
create targets for both of the above rules together:

```starlark
def _sphinx_site_impl(name, visibility, srcs, **kwargs):
    # This creates the primary target, producing the Sphinx-generated HTML. We
    # set `visibility = visibility` to make it visible to callers of the
    # macro.
    _sphinx_site(name = name, visibility = visibility, srcs = srcs, **kwargs)
    # This creates the secondary target, which produces a script for publishing
    # the site generated above. We don't want it to be visible to callers of
    # our macro, so we omit visibility for it.
    _sphinx_publisher(name = "%s.publish" % name, site = name, **kwargs)

sphinx_site = macro(
    implementation = _sphinx_site_impl,
    attrs = {"srcs": attr.label_list(allow_files = [".rst"])},
    # Inherit common attributes like tags and testonly
    inherit_attrs = "common",
)
```

Or, if you need to support Bazel releases older than Bazel 8, you would instead
define a legacy macro:

```starlark
def sphinx_site(name, srcs = [], **kwargs):
    # This creates the primary target, producing the Sphinx-generated HTML.
    _sphinx_site(name = name, srcs = srcs, **kwargs)
    # This creates the secondary target, which produces a script for publishing
    # the site generated above.
    _sphinx_publisher(name = "%s.publish" % name, site = name, **kwargs)
```

In the `BUILD` files, use the macro as though it just creates the primary
target:

```python
sphinx_site(
    name = "docs",
    srcs = ["index.md", "providers.md"],
)
```

In this example, a "docs" target is created, just as though the macro were a
standard, single Bazel rule. When built, the rule generates some configuration
and runs Sphinx to produce an HTML site, ready for manual inspection. However,
an additional "docs.publish" target is also created, which builds a script for
publishing the site. Once you check the output of the primary target, you can
use `bazel run :docs.publish` to publish it for public consumption, just like
an imaginary `bazel publish` command.

It's not immediately obvious what the implementation of the `_sphinx_publisher`
rule might look like. Often, actions like this write a _launcher_ shell script.
This method typically involves using
[`ctx.actions.expand_template`](lib/actions#expand_template)
to write a very simple shell script, in this case invoking the publisher binary
with a path to the output of the primary target. This way, the publisher
implementation can remain generic, the `_sphinx_site` rule can just produce
HTML, and this small script is all that's necessary to combine the two
together.

In `rules_k8s`, this is indeed what `.apply` does:
[`expand_template`](https://github.com/bazelbuild/rules_k8s/blob/f10e7025df7651f47a76abf1db5ade1ffeb0c6ac/k8s/object.bzl#L213-L241)
writes a very simple Bash script, based on
[`apply.sh.tpl`](https://github.com/bazelbuild/rules_k8s/blob/f10e7025df7651f47a76abf1db5ade1ffeb0c6ac/k8s/apply.sh.tpl),
which runs `kubectl` with the output of the primary target. This script can
then be build and run with `bazel run :staging.apply`, effectively providing a
`k8s-apply` command for `k8s_object` targets.

---

## Write bazelrc configuration files
- URL: https://bazel.build/run/bazelrc
- Source: run/bazelrc.mdx
- Slug: /run/bazelrc

Bazel accepts many options. Some options are varied frequently (for example,
`--subcommands`) while others stay the same across several builds (such as
`--package_path`). To avoid specifying these unchanged options for every build
(and other commands), you can specify options in a configuration file, called
`.bazelrc`.

### Where are the `.bazelrc` files?

Bazel looks for optional configuration files in the following locations,
in the order shown below. The options are interpreted in this order, so
options in later files can override a value from an earlier file if a
conflict arises. All options that control which of these files are loaded are
startup options, which means they must occur after `bazel` and
before the command (`build`, `test`, etc).

1.  **The system RC file**, unless `--nosystem_rc` is present.

    Path:

    - On Linux/macOS/Unixes: `/etc/bazel.bazelrc`
    - On Windows: `%ProgramData%\bazel.bazelrc`

    It is not an error if this file does not exist.

    If another system-specified location is required, you must build a custom
    Bazel binary, overriding the `BAZEL_SYSTEM_BAZELRC_PATH` value in
    [`//src/main/cpp:option_processor`](https://github.com/bazelbuild/bazel/blob/0.28.0/src/main/cpp/BUILD#L141).
    The system-specified location may contain environment variable references,
    such as `${VAR_NAME}` on Unix or `%VAR_NAME%` on Windows.

2.  **The workspace RC file**, unless `--noworkspace_rc` is present.

    Path: `.bazelrc` in your workspace directory (next to the main
    `MODULE.bazel` file).

    It is not an error if this file does not exist.

3.  **The home RC file**, unless `--nohome_rc` is present.

    Path:

    - On Linux/macOS/Unixes: `$HOME/.bazelrc`
    - On Windows: `%USERPROFILE%\.bazelrc` if exists, or `%HOME%/.bazelrc`

    It is not an error if this file does not exist.

4.  **The environment variable RC file**, if its path is set with the `BAZELRC`
    environment variable.

    The environment variable can include multiple comma-separated paths.

5.  **The user-specified RC file**, if specified with
    <code>--bazelrc=<var>file</var></code>

    This flag is optional but can also be specified multiple times.

    `/dev/null` indicates that all further `--bazelrc`s will be ignored, which
     is useful to disable the search for a user rc file, such as in release
     builds.

    For example:

    ```
    --bazelrc=x.rc --bazelrc=y.rc --bazelrc=/dev/null --bazelrc=z.rc
    ```

    - `x.rc` and `y.rc` are read.
    - `z.rc` is ignored due to the prior `/dev/null`.

In addition to this optional configuration file, Bazel looks for a global rc
file. For more details, see the [global bazelrc section](#global-bazelrc).


### `.bazelrc` syntax and semantics

Like all UNIX "rc" files, the `.bazelrc` file is a text file with a line-based
grammar. Empty lines and lines starting with `#` (comments) are ignored. Each
line contains a sequence of words, which are tokenized according to the same
rules as the Bourne shell.

#### Imports

Lines that start with `import` or `try-import` are special: use these to load
other "rc" files. To specify a path that is relative to the workspace root,
write `import %workspace%/path/to/bazelrc`.

The difference between `import` and `try-import` is that Bazel fails if the
`import`'ed file is missing (or can't be read), but not so for a `try-import`'ed
file.

Import precedence:

-   Options in the imported file take precedence over options specified before
    the import statement.
-   Options specified after the import statement take precedence over the
    options in the imported file.
-   Options in files imported later take precedence over files imported earlier.

#### Option defaults

Most lines of a bazelrc define default option values. The first word on each
line specifies when these defaults are applied:

-   `startup`: startup options, which go before the command, and are described
    in `bazel help startup_options`.
-   `common`: options that should be applied to all Bazel commands that support
    them. If a command does not support an option specified in this way, the
    option is ignored so long as it is valid for *some* other Bazel command.
    Note that this only applies to option names: If the current command accepts
    an option with the specified name, but doesn't support the specified value,
    it will fail.
-   `always`: options that apply to all Bazel commands. If a command does not
    support an option specified in this way, it will fail.
-   _`command`_: Bazel command, such as `build` or `query` to which the options
    apply. These options also apply to all commands that inherit from the
    specified command. (For example, `test` inherits from `build`.)

Each of these lines may be used more than once and the arguments that follow the
first word are combined as if they had appeared on a single line. (Users of CVS,
another tool with a "Swiss army knife" command-line interface, will find the
syntax similar to that of `.cvsrc`.) For example, the lines:

```posix-terminal
build --test_tmpdir=/tmp/foo --verbose_failures

build --test_tmpdir=/tmp/bar
```

are combined as:

```posix-terminal
build --test_tmpdir=/tmp/foo --verbose_failures --test_tmpdir=/tmp/bar
```

so the effective flags are `--verbose_failures` and `--test_tmpdir=/tmp/bar`.

Option precedence:

-   Options on the command line always take precedence over those in rc files.
    For example, if a rc file says `build -c opt` but the command line flag is
    `-c dbg`, the command line flag takes precedence.
-   Within the rc file, precedence is governed by specificity: lines for a more
    specific command take precedence over lines for a less specific command.

    Specificity is defined by inheritance. Some commands inherit options from
    other commands, making the inheriting command more specific than the base
    command. For example `test` inherits from the `build` command, so all `bazel
    build` flags are valid for `bazel test`, and all `build` lines apply also to
    `bazel test` unless there's a `test` line for the same option. If the rc
    file says:

    ```posix-terminal
    test -c dbg --test_env=PATH

    build -c opt --verbose_failures
    ```

    then `bazel build //foo` will use `-c opt --verbose_failures`, and `bazel
    test //foo` will use `--verbose_failures -c dbg --test_env=PATH`.

    The inheritance (specificity) graph is:

    *   Every command inherits from `common`
    *   The following commands inherit from (and are more specific than)
        `build`: `test`, `run`, `clean`, `mobile-install`, `info`,
        `print_action`, `config`, `cquery`, and `aquery`
    *   `coverage`, `fetch`, and `vendor` inherit from `test`

-   Two lines specifying options for the same command at equal specificity are
    parsed in the order in which they appear within the file.

-   Because this precedence rule does not match the file order, it helps
    readability if you follow the precedence order within rc files: start with
    `common` options at the top, and end with the most-specific commands at the
    bottom of the file. This way, the order in which the options are read is the
    same as the order in which they are applied, which is more intuitive.

The arguments specified on a line of an rc file may include arguments that are
not options, such as the names of build targets, and so on. These, like the
options specified in the same files, have lower precedence than their siblings
on the command line, and are always prepended to the explicit list of non-
option arguments.

#### `--config`

In addition to setting option defaults, the rc file can be used to group options
and provide a shorthand for common groupings. This is done by adding a `:name`
suffix to the command. These options are ignored by default, but will be
included when the option <code>--config=<var>name</var></code> is present,
either on the command line or in a `.bazelrc` file, recursively, even inside of
another config definition. The options specified by `command:name` will only be
expanded for applicable commands, in the precedence order described above.

Note: Configs can be defined in any `.bazelrc` file, and that all lines of
the form `command:name` (for applicable commands) will be expanded, across the
different rc files. In order to avoid name conflicts, we suggest that configs
defined in personal rc files start with an underscore (`_`) to avoid
unintentional name sharing.

`--config=foo` expands to the options defined in
[the rc files](#bazelrc-file-locations) "in-place" so that the options
specified for the config have the same precedence that the `--config=foo` option
had.

This syntax does not extend to the use of `startup` to set
[startup options](#option-defaults). Setting
`startup:config-name --some_startup_option` in the .bazelrc will be ignored.

#### `--enable_platform_specific_config`

In the `.bazelrc` you can use platform specific configs that will be
automatically enabled based on the host OS. For example, if the host OS
is Linux and the `build` command is run, the `build:linux` configuration
will be automatically enabled. Supported OS identifiers are `linux`,
`macos`, `windows`, `freebsd`, and `openbsd`.

This is equivalent to using `--config=linux` on Linux,
`--config=windows` on Windows, and so on. This can be disabled with
`--enable_platform_specific_config=false`.

See [--enable_platform_specific_config](/reference/command-line-reference#flag--enable_platform_specific_config).

#### Example

Here's an example `~/.bazelrc` file:

```
# Bob's Bazel option defaults

startup --host_jvm_args=-XX:-UseParallelGC
import /home/bobs_project/bazelrc
build --show_timestamps --keep_going --jobs 600
build --color=yes
query --keep_going

# Definition of --config=memcheck
build:memcheck --strip=never --test_timeout=3600
```

### Other files governing Bazel's behavior

#### `.bazelignore`

You can specify directories within the workspace
that you want Bazel to ignore, such as related projects
that use other build systems. Place a file called
`.bazelignore` at the root of the workspace
and add the directories you want Bazel to ignore, one per
line. Entries are relative to the workspace root.

The `.bazelignore` file does not permit glob semantics.
Bazel 8 introduces the `REPO.bazel` file which allows another directive, `ignore_directories()`.
It takes a list of directories to ignore just like .bazelignore does, but with glob semantics.
See [#24203](https://github.com/bazelbuild/bazel/pull/24203).

### The global bazelrc file

Bazel reads optional bazelrc files in this order:

1.  System rc-file located at `/etc/bazel.bazelrc`.
2.  Workspace rc-file located at `$workspace/tools/bazel.rc`.
3.  Home rc-file located at `$HOME/.bazelrc`

Each bazelrc file listed here has a corresponding flag which can be used to
disable them (e.g. `--nosystem_rc`, `--noworkspace_rc`, `--nohome_rc`). You can
also make Bazel ignore all bazelrcs by passing the `--ignore_all_rc_files`
startup option.

---

## Client/server implementation
- URL: https://bazel.build/run/client-server
- Source: run/client-server.mdx
- Slug: /run/client-server

The Bazel system is implemented as a long-lived server process. This allows it
to perform many optimizations not possible with a batch-oriented implementation,
such as caching of BUILD files, dependency graphs, and other metadata from one
build to the next. This improves the speed of incremental builds, and allows
different commands, such as `build` and `query` to share the same cache of
loaded packages, making queries very fast. Each server can handle at most one
invocation at a time; further concurrent invocations will either block or
fail-fast (see `--block_for_lock`).

When you run `bazel`, you're running the client. The client finds the server
based on the [output base](/run/scripts#output-base-option), which by default is
determined by the path of the base workspace directory and your userid, so if
you build in multiple workspaces, you'll have multiple output bases and thus
multiple Bazel server processes. Multiple users on the same workstation can
build concurrently in the same workspace because their output bases will differ
(different userids).

If the client cannot find a running server instance, it starts a new one. It
does this by checking if the output base already exists, implying the blaze
archive has already been unpacked. Otherwise if the output base doesn't exist,
the client unzips the archive's files and sets their `mtime`s to a date 9 years
in the future. Once installed, the client confirms that the `mtime`s of the
unzipped files are equal to the far off date to ensure no installation tampering
has occurred.

The server process will stop after a period of inactivity (3 hours, by default,
which can be modified using the startup option `--max_idle_secs`). For the most
part, the fact that there is a server running is invisible to the user, but
sometimes it helps to bear this in mind. For example, if you're running scripts
that perform a lot of automated builds in different directories, it's important
to ensure that you don't accumulate a lot of idle servers; you can do this by
explicitly shutting them down when you're finished with them, or by specifying
a short timeout period.

The name of a Bazel server process appears in the output of `ps x` or `ps -e f`
as <code>bazel(<i>dirname</i>)</code>, where _dirname_ is the basename of the
directory enclosing the root of your workspace directory. For example:

```posix-terminal
ps -e f
16143 ?        Sl     3:00 bazel(src-johndoe2) -server -Djava.library.path=...
```

This makes it easier to find out which server process belongs to a given
workspace. (Beware that with certain other options to `ps`, Bazel server
processes may be named just `java`.) Bazel servers can be stopped using the
[shutdown](/docs/user-manual#shutdown) command.

When running `bazel`, the client first checks that the server is the appropriate
version; if not, the server is stopped and a new one started. This ensures that
the use of a long-running server process doesn't interfere with proper
versioning.

---

## Calling Bazel from scripts
- URL: https://bazel.build/run/scripts
- Source: run/scripts.mdx
- Slug: /run/scripts

You can call Bazel from scripts to perform a build, run tests, or query
the dependency graph. Bazel has been designed to enable effective scripting, but
this section lists some details to bear in mind to make your scripts more
robust.

### Choosing the output base

The `--output_base` option controls where the Bazel process should write the
outputs of a build to, as well as various working files used internally by
Bazel, one of which is a lock that guards against concurrent mutation of the
output base by multiple Bazel processes.

Choosing the correct output base directory for your script depends on several
factors. If you need to put the build outputs in a specific location, this will
dictate the output base you need to use. If you are making a "read only" call to
Bazel (such as `bazel query`), the locking factors will be more important. In
particular, if you need to run multiple instances of your script concurrently,
you should be mindful that each Blaze server process can handle at most one
invocation [at a time](/run/client-server#clientserver-implementation).
Depending on your situation it may make sense for each instance of your script
to wait its turn, or it may make sense to use `--output_base` to run multiple
Blaze servers and use those.

If you use the default output base value, you will be contending for the same
lock used by the user's interactive Bazel commands. If the user issues
long-running commands such as builds, your script will have to wait for those
commands to complete before it can continue.

### Notes about server mode

By default, Bazel uses a long-running [server process](/run/client-server) as an
optimization. When running Bazel in a script, don't forget to call `shutdown`
when you're finished with the server, or, specify `--max_idle_secs=5` so that
idle servers shut themselves down promptly.

### What exit code will I get?

Bazel attempts to differentiate failures due to the source code under
consideration from external errors that prevent Bazel from executing properly.
Bazel execution can result in following exit codes:

**Exit Codes common to all commands:**

-   `0` - Success
-   `2` - Command Line Problem, Bad or Illegal flags or command combination, or
    Bad Environment Variables. Your command line must be modified.
-   `8` - Build Interrupted but we terminated with an orderly shutdown.
-   `9` - The server lock is held and `--noblock_for_lock` was passed.
-   `32` - External Environment Failure not on this machine.

-   `33` - Bazel ran out of memory and crashed. You need to modify your command line.
-   `34` - Reserved for Google-internal use.
-   `35` - Reserved for Google-internal use.
-   `36` - Local Environmental Issue, suspected permanent.
-   `37` - Unhandled Exception / Internal Bazel Error.
-   `38` - Transient error publishing results to the Build Event Service.
-   `39` - Blobs required by Bazel are evicted from Remote Cache.
-   `41-44` - Reserved for Google-internal use.
-   `45` - Persistent error publishing results to the Build Event Service.
-   `47` - Reserved for Google-internal use.
-   `49` - Reserved for Google-internal use.

**Return codes for commands `bazel build`, `bazel test`:**

-   `1` - Build failed.
-   `3` - Build OK, but some tests failed or timed out.
-   `4` - Build successful but no tests were found even though testing was
    requested.


**For `bazel run`:**

-   `1` - Build failed.
-   If the build succeeds but the executed subprocess returns a non-zero exit
    code it will be the exit code of the command as well.

**For `bazel query`:**

-   `3` - Partial success, but the query encountered 1 or more errors in the
    input BUILD file set and therefore the results of the operation are not 100%
    reliable. This is likely due to a `--keep_going` option on the command line.
-   `7` - Command failure.

Future Bazel versions may add additional exit codes, replacing generic failure
exit code `1` with a different non-zero value with a particular meaning.
However, all non-zero exit values will always constitute an error.


### Reading the .bazelrc file

By default, Bazel reads the [`.bazelrc` file](/run/bazelrc) from the base
workspace directory or the user's home directory. Whether or not this is
desirable is a choice for your script; if your script needs to be perfectly
hermetic (such as when doing release builds), you should disable reading the
.bazelrc file by using the option `--bazelrc=/dev/null`. If you want to perform
a build using the user's preferred settings, the default behavior is better.

### Command log

The Bazel output is also available in a command log file which you can find with
the following command:

```posix-terminal
bazel info command_log
```

The command log file contains the interleaved stdout and stderr streams of the
most recent Bazel command. Note that running `bazel info` will overwrite the
contents of this file, since it then becomes the most recent Bazel command.
However, the location of the command log file will not change unless you change
the setting of the `--output_base` or `--output_user_root` options.

### Parsing output

The Bazel output is quite easy to parse for many purposes. Two options that may
be helpful for your script are `--noshow_progress` which suppresses progress
messages, and <code>--show_result <var>n</var></code>, which controls whether or
not "build up-to-date" messages are printed; these messages may be parsed to
discover which targets were successfully built, and the location of the output
files they created. Be sure to specify a very large value of _n_ if you rely on
these messages.

## Troubleshooting performance by profiling

See the [Performance Profiling](/rules/performance#performance-profiling) section.

---

## Bazel Tutorial: Build an Android App
- URL: https://bazel.build/start/android-app
- Source: start/android-app.mdx
- Slug: /start/android-app

**Note:** There are known limitations on using Bazel for building Android apps.
Visit the [rules_android issues page](https://github.com/bazelbuild/rules_android/issues)
to see the list of known issues. While the Bazel team and Open Source Software
(OSS) contributors work actively to address known issues, users should be aware
that Android Studio does not officially support Bazel projects.

This tutorial covers how to build a simple Android app using Bazel.

Bazel supports building Android apps using the
[Android rules](/reference/be/android).

This tutorial is intended for Windows, macOS and Linux users and does not
require experience with Bazel or Android app development. You do not need to
write any Android code in this tutorial.

## What you'll learn

In this tutorial you learn how to:

*   Set up your environment by installing Bazel and Android Studio, and
    downloading the sample project.
*   Set up a Bazel workspace that contains the source code
    for the app and a `MODULE.bazel` file that identifies the top level of the
    workspace directory.
*   Update the `MODULE.bazel` file to contain references to the required
    external dependencies, like the Android SDK.
*   Create a `BUILD` file.
*   Build the app with Bazel.
*   Deploy and run the app on an Android emulator or physical device.

## Before you begin

### Install Bazel

Before you begin the tutorial, install the following software:

* **Bazel.** To install, follow the [installation instructions](/install).
* **Android Studio.** To install, follow the steps to [download Android
  Studio](https://developer.android.com/sdk/index.html).
  Execute the setup wizard to download the SDK and configure your environment.
* (Optional) **Git.** Use `git` to download the Android app project.

### Get the sample project

For the sample project, use the tutorial Android app project in
[Bazel's examples repository](https://github.com/bazelbuild/examples/tree/main/android/tutorial).

This app has a single button that prints a greeting when clicked:

![Button greeting](/docs/images/android_tutorial_app.png "Tutorial app button greeting")

**Figure 1.** Android app button greeting.

Clone the repository with `git` (or [download the ZIP file
directly](https://github.com/bazelbuild/examples/archive/master.zip)):

```posix-terminal
git clone https://github.com/bazelbuild/examples
```

The sample project for this tutorial is in `examples/android/tutorial`. For
the rest of the tutorial, you will be executing commands in this directory.

### Review the source files

Take a look at the source files for the app.

```
.
├── README.md
└── src
    └── main
        ├── AndroidManifest.xml
        └── java
            └── com
                └── example
                    └── bazel
                        ├── AndroidManifest.xml
                        ├── Greeter.java
                        ├── MainActivity.java
                        └── res
                            ├── layout
                            │   └── activity_main.xml
                            └── values
                                ├── colors.xml
                                └── strings.xml
```

The key files and directories are:

| Name                    | Location                                                                                 |
| ----------------------- | ---------------------------------------------------------------------------------------- |
| Android manifest files  | `src/main/AndroidManifest.xml` and `src/main/java/com/example/bazel/AndroidManifest.xml` |
| Android source files    | `src/main/java/com/example/bazel/MainActivity.java` and `Greeter.java`                   |
| Resource file directory | `src/main/java/com/example/bazel/res/`                                                   |


## Build with Bazel

### Set up the workspace

A [workspace](/concepts/build-ref#workspace) is a directory that contains the
source files for one or more software projects, and has a `MODULE.bazel` file at
its root.

The `MODULE.bazel` file may be empty or may contain references to [external
dependencies](/external/overview) required to build your project.

First, run the following command to create an empty `MODULE.bazel` file:

|          OS              |              Command                |
| ------------------------ | ----------------------------------- |
| Linux, macOS             | `touch MODULE.bazel`                   |
| Windows (Command Prompt) | `type nul > MODULE.bazel`              |
| Windows (PowerShell)     | `New-Item MODULE.bazel -ItemType file` |

### Running Bazel

You can now check if Bazel is running correctly with the command:

```posix-terminal
bazel info workspace
```

If Bazel prints the path of the current directory, you're good to go! If the
`MODULE.bazel` file does not exist, you may see an error message like:

```
ERROR: The 'info' command is only supported from within a workspace.
```

### Integrate with the Android SDK

Bazel needs to run the Android SDK
[build tools](https://developer.android.com/tools/revisions/build-tools.html)
to build the app. This means that you need to add some information to your
`MODULE.bazel` file so that Bazel knows where to find them.

Add the following line to your `MODULE.bazel` file:

```python
bazel_dep(name = "rules_android", version = "0.6.6")

remote_android_extensions = use_extension(
    "@rules_android//bzlmod_extensions:android_extensions.bzl",
    "remote_android_tools_extensions")
use_repo(remote_android_extensions, "android_tools")

android_sdk_repository_extension = use_extension("@rules_android//rules/android_sdk_repository:rule.bzl", "android_sdk_repository_extension")
use_repo(android_sdk_repository_extension, "androidsdk")
```

This will use the Android SDK at the path referenced by the `ANDROID_HOME`
environment variable, and automatically detect the highest API level and the
latest version of build tools installed within that location.

You can set the `ANDROID_HOME` variable to the location of the Android SDK. Find
the path to the installed SDK using Android Studio's [SDK
Manager](https://developer.android.com/studio/intro/update#sdk-manager).
Assuming the SDK is installed to default locations, you can use the following
commands to set the `ANDROID_HOME` variable:

|          OS              |               Command                               |
| ------------------------ | --------------------------------------------------- |
| Linux                    | `export ANDROID_HOME=$HOME/Android/Sdk/`            |
| macOS                    | `export ANDROID_HOME=$HOME/Library/Android/sdk`     |
| Windows (Command Prompt) | `set ANDROID_HOME=%LOCALAPPDATA%\Android\Sdk`       |
| Windows (PowerShell)     | `$env:ANDROID_HOME="$env:LOCALAPPDATA\Android\Sdk"` |

The above commands set the variable only for the current shell session. To make
them permanent, run the following commands:

|          OS              |               Command                               |
| ------------------------ | --------------------------------------------------- |
| Linux                    | `echo "export ANDROID_HOME=$HOME/Android/Sdk/" >> ~/.bashrc`                                                                              |
| macOS                    | `echo "export ANDROID_HOME=$HOME/Library/Android/Sdk/" >> ~/.bashrc`                                                                              |
| Windows (Command Prompt) | `setx ANDROID_HOME "%LOCALAPPDATA%\Android\Sdk"`                                                                                          |
| Windows (PowerShell)     | `[System.Environment]::SetEnvironmentVariable('ANDROID_HOME', "$env:LOCALAPPDATA\Android\Sdk", [System.EnvironmentVariableTarget]::User)` |


**Optional:** If you want to compile native code into your Android app, you
also need to download the [Android
NDK](https://developer.android.com/ndk/downloads/index.html)
and use `rules_android_ndk` by adding the following line to your `MODULE.bazel` file:

```python
bazel_dep(name = "rules_android_ndk", version = "0.1.3")
```


For more information, read [Using the Android Native Development Kit with
Bazel](/docs/android-ndk).

It's not necessary to set the API levels to the same value for the SDK and NDK.
[This page](https://developer.android.com/ndk/guides/stable_apis.html)
contains a map from Android releases to NDK-supported API levels.

### Create a BUILD file

A [`BUILD` file](/concepts/build-files) describes the relationship
between a set of build outputs, like compiled Android resources from `aapt` or
class files from `javac`, and their dependencies. These dependencies may be
source files (Java, C++) in your workspace or other build outputs. `BUILD` files
are written in a language called **Starlark**.

`BUILD` files are part of a concept in Bazel known as the *package hierarchy*.
The package hierarchy is a logical structure that overlays the directory
structure in your workspace. Each [package](/concepts/build-ref#packages) is a
directory (and its subdirectories) that contains a related set of source files
and a `BUILD` file. The package also includes any subdirectories, excluding
those that contain their own `BUILD` file. The *package name* is the path to the
`BUILD` file relative to the `MODULE.bazel` file.

Note that Bazel's package hierarchy is conceptually different from the Java
package hierarchy of your Android App directory where the `BUILD` file is
located, although the directories may be organized identically.

For the simple Android app in this tutorial, the source files in `src/main/`
comprise a single Bazel package. A more complex project may have many nested
packages.

#### Add an android_library rule

A `BUILD` file contains several different types of declarations for Bazel. The
most important type is the
[build rule](/concepts/build-files#types-of-build-rules), which tells
Bazel how to build an intermediate or final software output from a set of source
files or other dependencies. Bazel provides two build rules,
[`android_library`](/reference/be/android#android_library) and
[`android_binary`](/reference/be/android#android_binary), that you can use to
build an Android app.

For this tutorial, you'll first use the
`android_library` rule to tell Bazel to build an [Android library
module](http://developer.android.com/tools/projects/index.html#LibraryProjects)
from the app source code and resource files. You'll then use the
`android_binary` rule to tell Bazel how to build the Android application package.

Create a new `BUILD` file in the `src/main/java/com/example/bazel` directory,
and declare a new `android_library` target:

`src/main/java/com/example/bazel/BUILD`:

```python
load("@rules_android//rules:rules.bzl", "android_library")

package(
    default_visibility = ["//src:__subpackages__"],
)

android_library(
    name = "greeter_activity",
    srcs = [
        "Greeter.java",
        "MainActivity.java",
    ],
    manifest = "AndroidManifest.xml",
    resource_files = glob(["res/**"]),
)
```

The `android_library` build rule contains a set of attributes that specify the
information that Bazel needs to build a library module from the source files.
Note also that the name of the rule is `greeter_activity`. You'll reference the
rule using this name as a dependency in the `android_binary` rule.

#### Add an android_binary rule

The [`android_binary`](/reference/be/android#android_binary) rule builds
the Android application package (`.apk` file) for your app.

Create a new `BUILD` file in the `src/main/` directory,
and declare a new `android_binary` target:

`src/main/BUILD`:

```python
load("@rules_android//rules:rules.bzl", "android_binary")

android_binary(
    name = "app",
    manifest = "//src/main/java/com/example/bazel:AndroidManifest.xml",
    deps = ["//src/main/java/com/example/bazel:greeter_activity"],
)
```

Here, the `deps` attribute references the output of the `greeter_activity` rule
you added to the `BUILD` file above. This means that when Bazel builds the
output of this rule it checks first to see if the output of the
`greeter_activity` library rule has been built and is up-to-date. If not, Bazel
builds it and then uses that output to build the application package file.

Now, save and close the file.

### Build the app

Try building the app! Run the following command to build the
`android_binary` target:

```posix-terminal
bazel build //src/main:app
```

The [`build`](/docs/user-manual#build) subcommand instructs Bazel to build the
target that follows. The target is specified as the name of a build rule inside
a `BUILD` file, with along with the package path relative to your workspace
directory. For this example, the target is `app` and the package path is
`//src/main/`.

Note that you can sometimes omit the package path or target name, depending on
your current working directory at the command line and the name of the target.
For more details about target labels and paths, see [Labels](/concepts/labels).

Bazel will start to build the sample app. During the build process, its output
will appear similar to the following:

```bash
INFO: Analysed target //src/main:app (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //src/main:app up-to-date:
  bazel-bin/src/main/app_deploy.jar
  bazel-bin/src/main/app_unsigned.apk
  bazel-bin/src/main/app.apk
```

#### Locate the build outputs

Bazel puts the outputs of both intermediate and final build operations in a set
of per-user, per-workspace output directories. These directories are symlinked
from the following locations at the top-level of the project directory, where
the `MODULE.bazel` file is:

* `bazel-bin` stores binary executables and other runnable build outputs
* `bazel-genfiles` stores intermediary source files that are generated by
   Bazel rules
* `bazel-out` stores other types of build outputs

Bazel stores the Android `.apk` file generated using the `android_binary` rule
in the `bazel-bin/src/main` directory, where the subdirectory name `src/main` is
derived from the name of the Bazel package.

At a command prompt, list the contents of this directory and find the `app.apk`
file:

|          OS              |          Command         |
| ------------------------ | ------------------------ |
| Linux, macOS             | `ls bazel-bin/src/main`  |
| Windows (Command Prompt) | `dir bazel-bin\src\main` |
| Windows (PowerShell)     | `ls bazel-bin\src\main`  |


### Run the app

You can now deploy the app to a connected Android device or emulator from the
command line using `bazel mobile-install`.
This command uses the Android Debug Bridge (`adb`) to communicate with the
device. You must set up your device to use `adb` following the instructions in
[Android Debug Bridge](http://developer.android.com/tools/help/adb.html)
before deployment. You can also choose to install the app on the Android emulator
included in Android Studio. Make sure the emulator is running before executing
the command below.

Enter the following:

```posix-terminal
bazel mobile-install //src/main:app \
  --mode=skylark \
  --mobile_install_aspect=@rules_android//mobile_install:mi.bzl \
  --mobile_install_supported_rules=android_binary \
  --java_runtime_version=17 \
  --java_language_version=17 \
  --tool_java_runtime_version=17 \
  --tool_java_language_version=17
```

Note that the extra flags required for mobile-install can be added to your
project's [bazelrc file](/run/bazelrc). The mobile-install-specific flags
(`--mode`, `--mobile_install*`) will no longer be required starting from
Bazel 8.4.0 and onwards. The various Java flags for language and runtime version
may be required depending on your workspace's Java configuration.
_Mobile-install sub-tools require a language and runtime level of 17 or higher._

Now the "Bazel Tutorial App" should install and launch automatically:

![Bazel tutorial app](/docs/images/android_tutorial_before.png "Bazel tutorial app")

**Figure 2.** Bazel tutorial app.

**Congratulations! You have just installed your first Bazel-built Android app.**

## Further reading

For more details, see these pages:

* Open issues on [rules_android GitHub](https://github.com/bazelbuild/rules_android/issues)
* More information on [mobile-install](/docs/mobile-install)
* Integrate external dependencies like AppCompat, Guava and JUnit from Maven
  repositories using [rules_jvm_external](https://github.com/bazelbuild/rules_jvm_external)
* Run Robolectric tests with the [robolectric-bazel](https://github.com/robolectric/robolectric-bazel)
  integration.
* Integrating C and C++ code into your Android app with the [NDK](/docs/android-ndk)
* See more Bazel example projects of:
  * [a Kotlin app](https://github.com/bazelbuild/rules_jvm_external/tree/master/examples/android_kotlin_app)
  * [Robolectric testing](https://github.com/bazelbuild/rules_jvm_external/tree/master/examples/android_local_test)
  * [Espresso testing](https://github.com/bazelbuild/rules_jvm_external/tree/master/examples/android_instrumentation_test)

Happy building!

---

## Bazel Tutorial: Build a C++ Project
- URL: https://bazel.build/start/cpp
- Source: start/cpp.mdx
- Slug: /start/cpp

## Introduction

New to Bazel? You're in the right place. Follow this First Build tutorial for a
simplified introduction to using Bazel. This tutorial defines key terms as they
are used in Bazel's context and walks you through the basics of the Bazel
workflow. Starting with the tools you need, you will build and run three
projects with increasing complexity and learn how and why they get more complex.

While Bazel is a [build system](https://bazel.build/basics/build-systems) that
supports multi-language builds, this tutorial uses a C++ project as an example
and provides the general guidelines and flow that apply to most languages.

Estimated completion time: 30 minutes.

### Prerequisites

Start by [installing Bazel](https://bazel.build/install), if you haven't
already. This tutorial uses Git for source control, so for best results [install
Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) as well.

Next, retrieve the sample project from Bazel's GitHub repository by running the
following in your command-line tool of choice:

```posix-terminal
git clone https://github.com/bazelbuild/examples
```

The sample project for this tutorial is in the `examples/cpp-tutorial`
directory.

Take a look at how it's structured:

```none
examples
└── cpp-tutorial
    ├──stage1
    │  ├── main
    │  │   ├── BUILD
    │  │   └── hello-world.cc
    │  └── MODULE.bazel
    ├──stage2
    │  ├── main
    │  │   ├── BUILD
    │  │   ├── hello-world.cc
    │  │   ├── hello-greet.cc
    │  │   └── hello-greet.h
    │  └── MODULE.bazel
    └──stage3
       ├── main
       │   ├── BUILD
       │   ├── hello-world.cc
       │   ├── hello-greet.cc
       │   └── hello-greet.h
       ├── lib
       │   ├── BUILD
       │   ├── hello-time.cc
       │   └── hello-time.h
       └── MODULE.bazel
```

There are three sets of files, each set representing a stage in this tutorial.
In the first stage, you will build a single [target]
(https://bazel.build/reference/glossary#target) residing in a single [package]
(https://bazel.build/reference/glossary#package). In the second stage, you will
build both a binary and a library from a single package. In the third and final
stage, you will build a project with multiple packages and build it with
multiple targets.

### Summary: Introduction

By installing Bazel (and Git) and cloning the repository for this tutorial, you
have laid the foundation for your first build with Bazel. Continue to the next
section to define some terms and set up your
[workspace](https://bazel.build/reference/glossary#workspace).

## Getting started

Before you can build a project, you need to set up its workspace. A workspace
is a directory that holds your project's source files and Bazel's build outputs.
It also contains these significant files:

*   The `MODULE.bazel` file, which identifies the directory and its contents as
    a Bazel workspace and lives at the root of the project's directory
    structure. It's also where you specify your external dependencies.
*   One or more [`BUILD`
    files](https://bazel.build/reference/glossary#build-file), which tell Bazel
    how to build different parts of the project. A directory within the
    workspace that contains a `BUILD` file is a
    [package](https://bazel.build/reference/glossary#package). (More on packages
    later in this tutorial.)

In future projects, to designate a directory as a Bazel workspace, create an
empty file named `MODULE.bazel` in that directory. For the purposes of this
tutorial, a `MODULE.bazel` file is already present in each stage.

### Understand the BUILD file

A `BUILD` file contains several different types of instructions for Bazel. Each
`BUILD` file requires at least one
[rule](https://bazel.build/reference/glossary#rule) as a set of instructions,
which tells Bazel how to build the outputs you want, such as executable binaries
or libraries. Each instance of a build rule in the `BUILD` file is called a
[target](https://bazel.build/reference/glossary#target) and points to a specific
set of source files and
[dependencies](https://bazel.build/reference/glossary#dependency). A target can
also point to other targets.

Take a look at the `BUILD` file in the `cpp-tutorial/stage1/main` directory:

```bazel
cc_binary(
    name = "hello-world",
    srcs = ["hello-world.cc"],
)
```

In our example, the `hello-world` target instantiates Bazel's built-in
[`cc_binary` rule](https://bazel.build/reference/be/c-cpp#cc_binary). The rule
tells Bazel to build a self-contained executable binary from the
`hello-world.cc`> source file with no dependencies.

### Summary: getting started

Now you are familiar with some key terms, and what they mean in the context of
this project and Bazel in general. In the next section, you will build and test
Stage 1 of the project.

## Stage 1: single target, single package

It's time to build the first part of the project. For a visual reference, the
structure of the Stage 1 section of the project is:

```none
examples
└── cpp-tutorial
    └──stage1
       ├── main
       │   ├── BUILD
       │   └── hello-world.cc
       └── MODULE.bazel
```

Run the following to move to the `cpp-tutorial/stage1` directory:

```posix-terminal
cd cpp-tutorial/stage1
```

Next, run:

```posix-terminal
bazel build //main:hello-world
```

In the target label, the `//main:` part is the location of the `BUILD` file
relative to the root of the workspace, and `hello-world` is the target name in
the `BUILD` file.

Bazel produces something that looks like this:

```none
INFO: Found 1 target...
Target //main:hello-world up-to-date:
  bazel-bin/main/hello-world
INFO: Elapsed time: 2.267s, Critical Path: 0.25s
```

You just built your first Bazel target. Bazel places build outputs in the
`bazel-bin` directory at the root of the workspace.

Now test your freshly built binary, which is:

```posix-terminal
bazel-bin/main/hello-world
```

This results in a printed "`Hello world`" message.

Here's the dependency graph of Stage 1:

![Dependency graph for hello-world displays a single target with a single source
file.](/docs/images/cpp-tutorial-stage1.png "Dependency graph for hello-world
displays a single target with a single source file.")

### Summary: stage 1

Now that you have completed your first build, you have a basic idea of how a
build is structured. In the next stage, you will add complexity by adding
another target.

## Stage 2: multiple build targets

While a single target is sufficient for small projects, you may want to split
larger projects into multiple targets and packages. This allows for fast
incremental builds – that is, Bazel only rebuilds what's changed – and speeds up
your builds by building multiple parts of a project at once. This stage of the
tutorial adds a target, and the next adds a package.

This is the directory you are working with for Stage 2:

```none
    ├──stage2
    │  ├── main
    │  │   ├── BUILD
    │  │   ├── hello-world.cc
    │  │   ├── hello-greet.cc
    │  │   └── hello-greet.h
    │  └── MODULE.bazel
```

Take a look at the `BUILD` file in the `cpp-tutorial/stage2/main` directory:

```bazel
cc_library(
    name = "hello-greet",
    srcs = ["hello-greet.cc"],
    hdrs = ["hello-greet.h"],
)

cc_binary(
    name = "hello-world",
    srcs = ["hello-world.cc"],
    deps = [
        ":hello-greet",
    ],
)
```

With this `BUILD` file, Bazel first builds the `hello-greet` library (using
Bazel's built-in [`cc_library`
rule](https://bazel.build/reference/be/c-cpp#cc_library)), then the
`hello-world` binary. The `deps` attribute in the `hello-world` target tells
Bazel that the `hello-greet` library is required to build the `hello-world`
binary.

Before you can build this new version of the project, you need to change
directories, switching to the `cpp-tutorial/stage2` directory by running:

```posix-terminal
cd ../stage2
```

Now you can build the new binary using the following familiar command:

```posix-terminal
bazel build //main:hello-world
```

Once again, Bazel produces something that looks like this:

```none
INFO: Found 1 target...
Target //main:hello-world up-to-date:
  bazel-bin/main/hello-world
INFO: Elapsed time: 2.399s, Critical Path: 0.30s
```

Now you can test your freshly built binary, which returns another "`Hello
world`":

```posix-terminal
bazel-bin/main/hello-world
```

If you now modify `hello-greet.cc` and rebuild the project, Bazel only
recompiles that file.

Looking at the dependency graph, you can see that `hello-world` depends on an
extra input named `hello-greet`:

![Dependency graph for `hello-world` displays dependency changes after
modification to the file.](/docs/images/cpp-tutorial-stage2.png "Dependency
graph for `hello-world` displays dependency changes after modification to the
file.")

### Summary: stage 2

You've now built the project with two targets. The `hello-world` target builds
one source file and depends on one other target (`//main:hello-greet`), which
builds two additional source files. In the next section, take it a step further
and add another package.

## Stage 3: multiple packages

This next stage adds another layer of complication and builds a project with
multiple packages. Take a look at the structure and contents of the
`cpp-tutorial/stage3` directory:

```none
└──stage3
   ├── main
   │   ├── BUILD
   │   ├── hello-world.cc
   │   ├── hello-greet.cc
   │   └── hello-greet.h
   ├── lib
   │   ├── BUILD
   │   ├── hello-time.cc
   │   └── hello-time.h
   └── MODULE.bazel
```

You can see that now there are two sub-directories, and each contains a `BUILD`
file. Therefore, to Bazel, the workspace now contains two packages: `lib` and
`main`.

Take a look at the `lib/BUILD` file:

```bazel
cc_library(
    name = "hello-time",
    srcs = ["hello-time.cc"],
    hdrs = ["hello-time.h"],
    visibility = ["//main:__pkg__"],
)
```

And at the `main/BUILD` file:

```bazel
cc_library(
    name = "hello-greet",
    srcs = ["hello-greet.cc"],
    hdrs = ["hello-greet.h"],
)

cc_binary(
    name = "hello-world",
    srcs = ["hello-world.cc"],
    deps = [
        ":hello-greet",
        "//lib:hello-time",
    ],
)
```

The `hello-world` target in the main package depends on the` hello-time` target
in the `lib` package (hence the target label `//lib:hello-time`) - Bazel knows
this through the `deps` attribute. You can see this reflected in the dependency
graph:

![Dependency graph for `hello-world` displays how the target in the main package
depends on the target in the `lib`
package.](/docs/images/cpp-tutorial-stage3.png "Dependency graph for
`hello-world` displays how the target in the main package depends on the target
in the `lib` package.")

For the build to succeed, you make the `//lib:hello-time` target in `lib/BUILD`
explicitly visible to targets in `main/BUILD` using the visibility attribute.
This is because by default targets are only visible to other targets in the same
`BUILD` file. Bazel uses target visibility to prevent issues such as libraries
containing implementation details leaking into public APIs.

Now build this final version of the project. Switch to the `cpp-tutorial/stage3`
directory by running:

```posix-terminal
cd  ../stage3
```

Once again, run the following command:

```posix-terminal
bazel build //main:hello-world
```

Bazel produces something that looks like this:

```none
INFO: Found 1 target...
Target //main:hello-world up-to-date:
  bazel-bin/main/hello-world
INFO: Elapsed time: 0.167s, Critical Path: 0.00s
```

Now test the last binary of this tutorial for a final `Hello world` message:

```posix-terminal
bazel-bin/main/hello-world
```

### Summary: stage 3

You've now built the project as two packages with three targets and understand
the dependencies between them, which equips you to go forth and build future
projects with Bazel. In the next section, take a look at how to continue your
Bazel journey.

## Next steps

You've now completed your first basic build with Bazel, but this is just the
start. Here are some more resources to continue learning with Bazel:

*   To keep focusing on C++, read about common [C++ build use
    cases](https://bazel.build/tutorials/cpp-use-cases).
*   To get started with building other applications with Bazel, see the
    tutorials for [Java](https://bazel.build/start/java), [Android
    application](https://bazel.build/start/android-app), or [iOS
    application](https://bazel.build/start/ios-app).
*   To learn more about working with local and remote repositories, read about
    [external dependencies](https://bazel.build/docs/external).
*   To learn more about Bazel's other rules, see this [reference
    guide](https://bazel.build/rules).

Happy building!

---

## Bazel Tutorial: Build a Go Project
- URL: https://bazel.build/start/go
- Source: start/go.mdx
- Slug: /start/go

This tutorial introduces you to the basics of Bazel by showing you how to build
a Go (Golang) project. You'll learn how to set up your workspace, build a small
program, import a library, and run its test. Along the way, you'll learn key
Bazel concepts, such as targets and `BUILD` files.

Estimated completion time: 30 minutes

## Before you begin

### Install Bazel

Before you get started, first [install bazel](/install) if you haven't done so
already.

You can check if Bazel is installed by running `bazel version` in any directory.

### Install Go (optional)

You don't need to [install Go](https://go.dev/doc/install) to build Go projects
with Bazel. The Bazel Go rule set automatically downloads and uses a Go
toolchain instead of using the toolchain installed on your machine. This ensures
all developers on a project build with same version of Go.

However, you may still want to install a Go toolchain to run commands like `go
get` and `go mod tidy`.

You can check if Go is installed by running `go version` in any directory.

### Get the sample project

The Bazel examples are stored in a Git repository, so you'll need to [install
Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) if you
haven't already. To download the examples repository, run this command:

```posix-terminal
git clone https://github.com/bazelbuild/examples
```

The sample project for this tutorial is in the `examples/go-tutorial` directory.
See what it contains:

```none
go-tutorial/
└── stage1
└── stage2
└── stage3
```

There are three subdirectories (`stage1`, `stage2`, and `stage3`), each for a
different section of this tutorial. Each stage builds on the previous one.

## Build with Bazel

Start in the `stage1` directory, where we'll find a program. We can
build it with `bazel build`, then run it:

```posix-shell
$ cd go-tutorial/stage1/
$ bazel build //:hello
INFO: Analyzed target //:hello (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //:hello up-to-date:
  bazel-bin/hello_/hello
INFO: Elapsed time: 0.473s, Critical Path: 0.25s
INFO: 3 processes: 1 internal, 2 darwin-sandbox.
INFO: Build completed successfully, 3 total actions

$ bazel-bin/hello_/hello
Hello, Bazel! 💚
```

We can also build run the program with a single `bazel run` command:

```posix-shell
$ bazel run //:hello
bazel run //:hello
INFO: Analyzed target //:hello (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //:hello up-to-date:
  bazel-bin/hello_/hello
INFO: Elapsed time: 0.128s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/hello_/hello
Hello, Bazel! 💚
```

### Understanding project structure

Take a look at the project we just built.

`hello.go` contains the Go source code for the program.

```go
package main

import "fmt"

func main() {
	fmt.Println("Hello, Bazel! 💚")
}
```

`BUILD` contains some instructions for Bazel, telling it what we want to build.
You'll typically write a file like this in each directory. For this project, we
have a single `go_binary` target that builds our program from `hello.go`.

```bazel
load("@rules_go//go:def.bzl", "go_binary")

go_binary(
    name = "hello",
    srcs = ["hello.go"],
)
```

`MODULE.bazel` tracks your project's dependencies. It also marks your project's
root directory, so you'll only write one `MODULE.bazel` file per project. It
serves a similar purpose to Go's `go.mod` file. You don't actually need a
`go.mod` file in a Bazel project, but it may still be useful to have one so that
you can continue using `go get` and `go mod tidy` for dependency management. The
Bazel Go rule set can import dependencies from `go.mod`, but we'll cover that in
another tutorial.

Our `MODULE.bazel` file contains a single dependency on
[rules_go](https://github.com/bazel-contrib/rules_go), the Go rule set. We need
this dependency because Bazel doesn't have built-in support for Go.

```bazel
bazel_dep(
    name = "rules_go",
    version = "0.50.1",
)
```

Finally, `MODULE.bazel.lock` is a file generated by Bazel that contains hashes
and other metadata about our dependencies. It includes implicit dependencies
added by Bazel itself, so it's quite long, and we won't show it here. Just like
`go.sum`, you should commit your `MODULE.bazel.lock` file to source control to
ensure everyone on your project gets the same version of each dependency. You
shouldn't need to edit `MODULE.bazel.lock` manually.

### Understand the BUILD file

Most of your interaction with Bazel will be through `BUILD` files (or
equivalently, `BUILD.bazel` files), so it's important to understand what they
do.

`BUILD` files are written in a scripting language called
[Starlark](https://bazel.build/rules/language), a limited subset of Python.

A `BUILD` file contains a list of
[targets](https://bazel.build/reference/glossary#target). A target is something
Bazel can build, like a binary, library, or test.

A target calls a rule function with a list of
[attributes](https://bazel.build/reference/glossary#attribute) to describe what
should be built. Our example has two attributes: `name` identifies the target on
the command line, and `srcs` is a list of source file paths (slash-separated,
relative to the directory containing the `BUILD` file).

A [rule](https://bazel.build/reference/glossary#rule) tells Bazel how to build a
target. In our example, we used the
[`go_binary`](https://github.com/bazel-contrib/rules_go/blob/master/docs/go/core/rules.md#go_binary)
rule. Each rule defines [actions](https://bazel.build/reference/glossary#action)
(commands) that generate a set of output files. For example, `go_binary` defines
Go compile and link actions that produce an executable output file.

Bazel has built-in rules for a few languages like Java and C++. You can find
their [documentation in the Build
Encyclopedia](https://bazel.build/reference/be/overview#rules). You can find
rule sets for many other languages and tools on the [Bazel Central Registry
(BCR)](https://registry.bazel.build/).

## Add a library

Move onto the `stage2` directory, where we'll build a new program that
prints your fortune. This program uses a separate Go package as a library that
selects a fortune from a predefined list of messages.

```none
go-tutorial/stage2
├── BUILD
├── MODULE.bazel
├── MODULE.bazel.lock
├── fortune
│   ├── BUILD
│   └── fortune.go
└── print_fortune.go
```

`fortune.go` is the source file for the library. The `fortune` library is a
separate Go package, so its source files are in a separate directory. Bazel
doesn't require you to keep Go packages in separate directories, but it's a
strong convention in the Go ecosystem, and following it will help you stay
compatible with other Go tools.

```go
package fortune

import "math/rand"

var fortunes = []string{
	"Your build will complete quickly.",
	"Your dependencies will be free of bugs.",
	"Your tests will pass.",
}

func Get() string {
	return fortunes[rand.Intn(len(fortunes))]
}
```

The `fortune` directory has its own `BUILD` file that tells Bazel how to build
this package. We use `go_library` here instead of `go_binary`.

We also need to set the `importpath` attribute to a string with which the
library can be imported into other Go source files. This name should be the
repository path (or module path) concatenated with the directory within the
repository.

Finally, we need to set the `visibility` attribute to `["//visibility:public"]`.
[`visibility`](https://bazel.build/concepts/visibility) may be set on any
target. It determines which Bazel packages may depend on this target. In our
case, we want any target to be able to depend on this library, so we use the
special value `//visibility:public`.

```bazel
load("@rules_go//go:def.bzl", "go_library")

go_library(
    name = "fortune",
    srcs = ["fortune.go"],
    importpath = "github.com/bazelbuild/examples/go-tutorial/stage2/fortune",
    visibility = ["//visibility:public"],
)
```

You can build this library with:

```posix-shell
$ bazel build //fortune
```

Next, see how `print_fortune.go` uses this package.

```go
package main

import (
	"fmt"

	"github.com/bazelbuild/examples/go-tutorial/stage2/fortune"
)

func main() {
	fmt.Println(fortune.Get())
}
```

`print_fortune.go` imports the package using the same string declared in the
`importpath` attribute of the `fortune` library.

We also need to declare this dependency to Bazel. Here's the `BUILD` file in the
`stage2` directory.

```bazel
load("@rules_go//go:def.bzl", "go_binary")

go_binary(
    name = "print_fortune",
    srcs = ["print_fortune.go"],
    deps = ["//fortune"],
)
```

You can run this with the command below.

```posix-shell
bazel run //:print_fortune
```

The `print_fortune` target has a `deps` attribute, a list of other targets that
it depends on. It contains `"//fortune"`, a label string referring to the target
in the `fortune` directory named `fortune`.

Bazel requires that all targets declare their dependencies explicitly with
attributes like `deps`. This may seem cumbersome since dependencies are *also*
specified in source files, but Bazel's explictness gives it an advantage. Bazel
builds an [action graph](https://bazel.build/reference/glossary#action-graph)
containing all commands, inputs, and outputs before running any commands,
without reading any source files. Bazel can then cache action results or send
actions for [remote execution](https://bazel.build/remote/rbe) without built-in
language-specific logic.

### Understanding labels

A [label](https://bazel.build/reference/glossary#label) is a string Bazel uses
to identify a target or a file. Labels are used in command line arguments and in
`BUILD` file attributes like `deps`. We've seen a few already, like `//fortune`,
`//:print-fortune`, and `@rules_go//go:def.bzl`.

A label has three parts: a repository name, a package name, and a target (or
file) name.

The repository name is written between `@` and `//` and is used to refer to a
target from a different Bazel module (for historical reasons, *module* and
*repository* are sometimes used synonymously). In the label,
`@rules_go//go:def.bzl`, the repository name is `rules_go`. The repository name
can be omitted when referring to targets in the same repository.

The package name is written between `//` and `:` and is used to refer to a
target in from a different Bazel package. In the label `@rules_go//go:def.bzl`,
the package name is `go`. A Bazel
[package](https://bazel.build/reference/glossary#package) is a set of files and
targets defined by a `BUILD` or `BUILD.bazel` file in its top-level directory.
Its package name is a slash-separated path from the module root directory
(containing `MODULE.bazel`) to the directory containing the `BUILD` file. A
package may include subdirectories, but only if they don't also contain `BUILD`
files defining their own packages.

Most Go projects have one `BUILD` file per directory and one Go package per
`BUILD` file. The package name in a label may be omitted when referring to
targets in the same directory.

The target name is written after `:` and refers to a target within a package.
The target name may be omitted if it's the same as the last component of the
package name (so `//a/b/c:c` is the same as `//a/b/c`; `//fortune:fortune` is
the same as `//fortune`).

On the command-line, you can use `...` as a wildcard to refer to all the targets
within a package. This is useful for building or testing all the targets in a
repository.

```posix-shell
# Build everything
$ bazel build //...
```

## Test your project

Next, move to the `stage3` directory, where we'll add a test.

```none
go-tutorial/stage3
├── BUILD
├── MODULE.bazel
├── MODULE.bazel.lock
├── fortune
│   ├── BUILD
│   ├── fortune.go
│   └── fortune_test.go
└── print-fortune.go
```

`fortune/fortune_test.go` is our new test source file.

```go
package fortune

import (
	"slices"
	"testing"
)

// TestGet checks that Get returns one of the strings from fortunes.
func TestGet(t *testing.T) {
	msg := Get()
	if i := slices.Index(fortunes, msg); i < 0 {
		t.Errorf("Get returned %q, not one the expected messages", msg)
	}
}
```

This file uses the unexported `fortunes` variable, so it needs to be compiled
into the same Go package as `fortune.go`. Look at the `BUILD` file to see
how that works:

```bazel
load("@rules_go//go:def.bzl", "go_library", "go_test")

go_library(
    name = "fortune",
    srcs = ["fortune.go"],
    importpath = "github.com/bazelbuild/examples/go-tutorial/stage3/fortune",
    visibility = ["//visibility:public"],
)

go_test(
    name = "fortune_test",
    srcs = ["fortune_test.go"],
    embed = [":fortune"],
)
```

We have a new `fortune_test` target that uses the `go_test` rule to compile and
link a test executable. `go_test` needs to compile `fortune.go` and
`fortune_test.go` together with the same command, so we use the `embed`
attribute here to incorporate the attributes of the `fortune` target into
`fortune_test`. `embed` is most commonly used with `go_test` and `go_binary`,
but it also works with `go_library`, which is sometimes useful for generated
code.

You may be wondering if the `embed` attribute is related to Go's
[`embed`](https://pkg.go.dev/embed) package, which is used to access data files
copied into an executable. This is an unfortunate name collision: rules_go's
`embed` attribute was introduced before Go's `embed` package. Instead, rules_go
uses the `embedsrcs` to list files that can be loaded with the `embed` package.

Try running our test with `bazel test`:

```posix-shell
$ bazel test //fortune:fortune_test
INFO: Analyzed target //fortune:fortune_test (0 packages loaded, 0 targets configured).
INFO: Found 1 test target...
Target //fortune:fortune_test up-to-date:
  bazel-bin/fortune/fortune_test_/fortune_test
INFO: Elapsed time: 0.168s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
//fortune:fortune_test                                          PASSED in 0.3s

Executed 0 out of 1 test: 1 test passes.
There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.
```

You can use the `...` wildcard to run all tests. Bazel will also build targets
that aren't tests, so this can catch compile errors even in packages that don't
have tests.

```posix-shell
$ bazel test //...
```

## Conclusion and further reading

In this tutorial, we built and tested a small Go project with Bazel, and we
learned some core Bazel concepts along the way.

-   To get started building other applications with Bazel, see the tutorials for
    [C++](/start/cpp), [Java](/start/java), [Android](/start/android-app), and
    [iOS](/start/ios-app).
-   You can also check the list of [recommended rules](/rules) for other
    languages.
-   For more information on Go, see the
    [rules_go](https://github.com/bazel-contrib/rules_go) module, especially the
    [Core Go
    rules](https://github.com/bazel-contrib/rules_go/blob/master/docs/go/core/rules.md)
    documentation.
-   To learn more about working with Bazel modules outside your project, see
    [external dependencies](/docs/external). In particular, for information on
    how to depend on Go modules and toolchains through Bazel's module system,
    see [Go with
    bzlmod](https://github.com/bazel-contrib/rules_go/tree/master/docs/go/core/bzlmod.md).

---

## Bazel Tutorial: Build an iOS App
- URL: https://bazel.build/start/ios-app
- Source: start/ios-app.mdx
- Slug: /start/ios-app

This tutorial has been moved into the [bazelbuild/rules_apple](https://github.com/bazelbuild/rules_apple/blob/master/doc/tutorials/ios-app.md) repository.

---

## Bazel Tutorial: Build a Java Project
- URL: https://bazel.build/start/java
- Source: start/java.mdx
- Slug: /start/java

This tutorial covers the basics of building Java applications with
Bazel. You will set up your workspace and build a simple Java project that
illustrates key Bazel concepts, such as targets and `BUILD` files.

Estimated completion time: 30 minutes.

## What you'll learn

In this tutorial you learn how to:

*  Build a target
*  Visualize the project's dependencies
*  Split the project into multiple targets and packages
*  Control target visibility across packages
*  Reference targets through labels
*  Deploy a target

## Before you begin

### Install Bazel

To prepare for the tutorial, first [Install Bazel](/install) if
you don't have it installed already.

### Install the JDK

1.  Install Java JDK (preferred version is 11, however versions between 8 and 15 are supported).

2.  Set the JAVA\_HOME environment variable to point to the JDK.
    *   On Linux/macOS:

            export JAVA_HOME="$(dirname $(dirname $(realpath $(which javac))))"
    *   On Windows:
        1.  Open Control Panel.
        2.  Go to "System&nbsp;and&nbsp;Security" &gt; "System" &gt; "Advanced&nbsp;System&nbsp;Settings" &gt; "Advanced"&nbsp;tab &gt; "Environment&nbsp;Variables..." .
        3.  Under the "User&nbsp;variables" list (the one on the top), click "New...".
        4.  In the "Variable&nbsp;name" field, enter `JAVA_HOME`.
        5.  Click "Browse&nbsp;Directory...".
        6.  Navigate to the JDK directory (for example `C:\Program Files\Java\jdk1.8.0_152`).
        7.  Click "OK" on all dialog windows.

### Get the sample project

Retrieve the sample project from Bazel's GitHub repository:

```posix-terminal
git clone https://github.com/bazelbuild/examples
```

The sample project for this tutorial is in the `examples/java-tutorial`
directory and is structured as follows:

```
java-tutorial
├── BUILD
├── src
│   └── main
│       └── java
│           └── com
│               └── example
│                   ├── cmdline
│                   │   ├── BUILD
│                   │   └── Runner.java
│                   ├── Greeting.java
│                   └── ProjectRunner.java
└── MODULE.bazel
```

## Build with Bazel

### Set up the workspace

Before you can build a project, you need to set up its workspace. A workspace is
a directory that holds your project's source files and Bazel's build outputs. It
also contains files that Bazel recognizes as special:

*  The `MODULE.bazel` file, which identifies the directory and its contents as a
   Bazel workspace and lives at the root of the project's directory structure,

*  One or more `BUILD` files, which tell Bazel how to build different parts of
   the project. (A directory within the workspace that contains a `BUILD` file
   is a *package*. You will learn about packages later in this tutorial.)

To designate a directory as a Bazel workspace, create an empty file named
`MODULE.bazel` in that directory.

When Bazel builds the project, all inputs and dependencies must be in the same
workspace. Files residing in different workspaces are independent of one
another unless linked, which is beyond the scope of this tutorial.

### Understand the BUILD file

A `BUILD` file contains several different types of instructions for Bazel.
The most important type is the *build rule*, which tells Bazel how to build the
desired outputs, such as executable binaries or libraries. Each instance
of a build rule in the `BUILD` file is called a *target* and points to a
specific set of source files and dependencies. A target can also point to other
targets.

Take a look at the `java-tutorial/BUILD` file:

```python
java_binary(
    name = "ProjectRunner",
    srcs = glob(["src/main/java/com/example/*.java"]),
)
```

In our example, the `ProjectRunner` target instantiates Bazel's built-in
[`java_binary` rule](/reference/be/java#java_binary). The rule tells Bazel to
build a `.jar` file and a wrapper shell script (both named after the target).

The attributes in the target explicitly state its dependencies and options.
While the `name` attribute is mandatory, many are optional. For example, in the
`ProjectRunner` rule target, `name` is the name of the target, `srcs` specifies
the source files that Bazel uses to build the target, and `main_class` specifies
the class that contains the main method. (You may have noticed that our example
uses [glob](/reference/be/functions#glob) to pass a set of source files to Bazel
instead of listing them one by one.)

### Build the project

To build your sample project, navigate to the `java-tutorial` directory
and run:

```posix-terminal
bazel build //:ProjectRunner
```
In the target label, the `//` part is the location of the `BUILD` file
relative to the root of the workspace (in this case, the root itself),
and `ProjectRunner` is the target name in the `BUILD` file. (You will
learn about target labels in more detail at the end of this tutorial.)

Bazel produces output similar to the following:

```bash
   INFO: Found 1 target...
   Target //:ProjectRunner up-to-date:
      bazel-bin/ProjectRunner.jar
      bazel-bin/ProjectRunner
   INFO: Elapsed time: 1.021s, Critical Path: 0.83s
```

Congratulations, you just built your first Bazel target! Bazel places build
outputs in the `bazel-bin` directory at the root of the workspace. Browse
through its contents to get an idea for Bazel's output structure.

Now test your freshly built binary:

```posix-terminal
bazel-bin/ProjectRunner
```

### Review the dependency graph

Bazel requires build dependencies to be explicitly declared in BUILD files.
Bazel uses those statements to create the project's dependency graph, which
enables accurate incremental builds.

To visualize the sample project's dependencies, you can generate a text
representation of the dependency graph by running this command at the
workspace root:

```posix-terminal
bazel query  --notool_deps --noimplicit_deps "deps(//:ProjectRunner)" --output graph
```

The above command tells Bazel to look for all dependencies for the target
`//:ProjectRunner` (excluding host and implicit dependencies) and format the
output as a graph.

Then, paste the text into [GraphViz](http://www.webgraphviz.com/).

As you can see, the project has a single target that build two source files with
no additional dependencies:

![Dependency graph of the target 'ProjectRunner'](/docs/images/tutorial_java_01.svg)

After you set up your workspace, build your project, and examine its
dependencies, then you can add some complexity.

## Refine your Bazel build

While a single target is sufficient for small projects, you may want to split
larger projects into multiple targets and packages to allow for fast incremental
builds (that is, only rebuild what's changed) and to speed up your builds by
building multiple parts of a project at once.

### Specify multiple build targets

You can split the sample project build into two targets. Replace the contents of
the `java-tutorial/BUILD` file with the following:

```python
java_binary(
    name = "ProjectRunner",
    srcs = ["src/main/java/com/example/ProjectRunner.java"],
    main_class = "com.example.ProjectRunner",
    deps = [":greeter"],
)

java_library(
    name = "greeter",
    srcs = ["src/main/java/com/example/Greeting.java"],
)
```

With this configuration, Bazel first builds the `greeter` library, then the
`ProjectRunner` binary. The `deps` attribute in `java_binary` tells Bazel that
the `greeter` library is required to build the `ProjectRunner` binary.

To build this new version of the project, run the following command:

```posix-terminal
bazel build //:ProjectRunner
```

Bazel produces output similar to the following:

```
INFO: Found 1 target...
Target //:ProjectRunner up-to-date:
  bazel-bin/ProjectRunner.jar
  bazel-bin/ProjectRunner
INFO: Elapsed time: 2.454s, Critical Path: 1.58s
```

Now test your freshly built binary:

```posix-terminal
bazel-bin/ProjectRunner
```

If you now modify `ProjectRunner.java` and rebuild the project, Bazel only
recompiles that file.

Looking at the dependency graph, you can see that `ProjectRunner` depends on the
same inputs as it did before, but the structure of the build is different:

![Dependency graph of the target 'ProjectRunner' after adding a dependency](
/docs/images/tutorial_java_02.svg)

You've now built the project with two targets. The `ProjectRunner` target builds
one source files and depends on one other target (`:greeter`), which builds
one additional source file.

### Use multiple packages

Let’s now split the project into multiple packages. If you take a look at the
`src/main/java/com/example/cmdline` directory, you can see that it also contains
a `BUILD` file, plus some source files. Therefore, to Bazel, the workspace now
contains two packages, `//src/main/java/com/example/cmdline` and `//` (since
there is a `BUILD` file at the root of the workspace).

Take a look at the `src/main/java/com/example/cmdline/BUILD` file:

```python
java_binary(
    name = "runner",
    srcs = ["Runner.java"],
    main_class = "com.example.cmdline.Runner",
    deps = ["//:greeter"],
)
```

The `runner` target depends on the `greeter` target in the `//` package (hence
the target label `//:greeter`) - Bazel knows this through the `deps` attribute.
Take a look at the dependency graph:

![Dependency graph of the target 'runner'](/docs/images/tutorial_java_03.svg)

However, for the build to succeed, you must explicitly give the `runner` target
in `//src/main/java/com/example/cmdline/BUILD` visibility to targets in
`//BUILD` using the `visibility` attribute. This is because by default targets
are only visible to other targets in the same `BUILD` file. (Bazel uses target
visibility to prevent issues such as libraries containing implementation details
leaking into public APIs.)

To do this, add the `visibility` attribute to the `greeter` target in
`java-tutorial/BUILD` as shown below:

```python
java_library(
    name = "greeter",
    srcs = ["src/main/java/com/example/Greeting.java"],
    visibility = ["//src/main/java/com/example/cmdline:__pkg__"],
)
```

Now you can build the new package by running the following command at the root
of the workspace:

```posix-terminal
bazel build //src/main/java/com/example/cmdline:runner
```

Bazel produces output similar to the following:

```
INFO: Found 1 target...
Target //src/main/java/com/example/cmdline:runner up-to-date:
  bazel-bin/src/main/java/com/example/cmdline/runner.jar
  bazel-bin/src/main/java/com/example/cmdline/runner
  INFO: Elapsed time: 1.576s, Critical Path: 0.81s
```

Now test your freshly built binary:

```posix-terminal
./bazel-bin/src/main/java/com/example/cmdline/runner
```

You've now modified the project to build as two packages, each containing one
target, and understand the dependencies between them.


## Use labels to reference targets

In `BUILD` files and at the command line, Bazel uses target labels to reference
targets - for example, `//:ProjectRunner` or
`//src/main/java/com/example/cmdline:runner`. Their syntax is as follows:

```
//path/to/package:target-name
```

If the target is a rule target, then `path/to/package` is the path to the
directory containing the `BUILD` file, and `target-name` is what you named the
target in the `BUILD` file (the `name` attribute). If the target is a file
target, then `path/to/package` is the path to the root of the package, and
`target-name` is the name of the target file, including its full path.

When referencing targets at the repository root, the package path is empty,
just use `//:target-name`. When referencing targets within the same `BUILD`
file, you can even skip the `//` workspace root identifier and just use
`:target-name`.

For example, for targets in the `java-tutorial/BUILD` file, you did not have to
specify a package path, since the workspace root is itself a package (`//`), and
your two target labels were simply `//:ProjectRunner` and `//:greeter`.

However, for targets in the `//src/main/java/com/example/cmdline/BUILD` file you
had to specify the full package path of `//src/main/java/com/example/cmdline`
and your target label was `//src/main/java/com/example/cmdline:runner`.

## Package a Java target for deployment

Let’s now package a Java target for deployment by building the binary with all
of its runtime dependencies. This lets you run the binary outside of your
development environment.

As you remember, the [java_binary](/reference/be/java#java_binary) build rule
produces a `.jar` and a wrapper shell script. Take a look at the contents of
`runner.jar` using this command:

```posix-terminal
jar tf bazel-bin/src/main/java/com/example/cmdline/runner.jar
```

The contents are:

```
META-INF/
META-INF/MANIFEST.MF
com/
com/example/
com/example/cmdline/
com/example/cmdline/Runner.class
```
As you can see, `runner.jar` contains `Runner.class`, but not its dependency,
`Greeting.class`. The `runner` script that Bazel generates adds `greeter.jar`
to the classpath, so if you leave it like this, it will run locally, but it
won't run standalone on another machine. Fortunately, the `java_binary` rule
allows you to build a self-contained, deployable binary. To build it, append
`_deploy.jar` to the target name:

```posix-terminal
bazel build //src/main/java/com/example/cmdline:runner_deploy.jar
```

Bazel produces output similar to the following:

```
INFO: Found 1 target...
Target //src/main/java/com/example/cmdline:runner_deploy.jar up-to-date:
  bazel-bin/src/main/java/com/example/cmdline/runner_deploy.jar
INFO: Elapsed time: 1.700s, Critical Path: 0.23s
```
You have just built `runner_deploy.jar`, which you can run standalone away from
your development environment since it contains the required runtime
dependencies. Take a look at the contents of this standalone JAR using the
same command as before:

```posix-terminal
jar tf bazel-bin/src/main/java/com/example/cmdline/runner_deploy.jar
```

The contents include all of the necessary classes to run:

```
META-INF/
META-INF/MANIFEST.MF
build-data.properties
com/
com/example/
com/example/cmdline/
com/example/cmdline/Runner.class
com/example/Greeting.class
```

## Further reading

For more details, see:

*  [rules_jvm_external](https://github.com/bazelbuild/rules_jvm_external) for
   rules to manage transitive Maven dependencies.

*  [External Dependencies](/docs/external) to learn more about working with
   local and remote repositories.

*  The [other rules](/rules) to learn more about Bazel.

*  The [C++ build tutorial](/start/cpp) to get started with building
   C++ projects with Bazel.

*  The [Android application tutorial](/start/android-app ) and
   [iOS application tutorial](/start/ios-app)) to get started with
   building mobile applications for Android and iOS with Bazel.

Happy building!

---

## Bazel Tutorial: Configure C++ Toolchains
- URL: https://bazel.build/tutorials/ccp-toolchain-config
- Source: tutorials/ccp-toolchain-config.mdx
- Slug: /tutorials/ccp-toolchain-config

This tutorial uses an example scenario to describe how to configure C++
toolchains for a project.

## What you'll learn 

In this tutorial you learn how to:

*   Set up the build environment
*   Use `--toolchain_resolution_debug` to debug toolchain resolution
*   Configure the C++ toolchain
*   Create a Starlark rule that provides additional configuration for the
    `cc_toolchain` so that Bazel can build the application with `clang`
*   Build the C++ binary by running `bazel build //main:hello-world` on a
    Linux machine
*   Cross-compile the binary for android by running `bazel build
    //main:hello-world --platforms=//:android_x86_64`

## Before you begin 

This tutorial assumes you are on Linux and have successfully built C++
applications and installed the appropriate tooling and libraries. The tutorial
uses `clang version 19`, which you can install on your system.

### Set up the build environment 

Set up your build environment as follows:

1.  If you have not already done so, [download and install Bazel
    7.0.2](https://bazel.build/install) or later.

2.  Add an empty `MODULE.bazel` file at the root folder.

3.  Add the following `cc_binary` target to the `main/BUILD` file:

    ```python
    cc_binary(
        name = "hello-world",
        srcs = ["hello-world.cc"],
    )
    ```

    Because Bazel uses many internal tools written in C++ during the build, such
    as `process-wrapper`, the pre-existing default C++ toolchain is specified
    for the host platform. This enables these internal tools to build using that
    toolchain of the one created in this tutorial. Hence, the `cc_binary` target
    is also built with the default toolchain.

4.  Run the build with the following command:

    ```bash
    bazel build //main:hello-world
    ```

    The build succeeds without any toolchain registered in `MODULE.bazel`.

    To further see what's under the hood, run:

    ```bash
    bazel build //main:hello-world --toolchain_resolution_debug='@bazel_tools//tools/cpp:toolchain_type'

    INFO: ToolchainResolution: Target platform @@platforms//host:host: Selected execution platform @@platforms//host:host, type @@bazel_tools//tools/cpp:toolchain_type -> toolchain @@bazel_tools+cc_configure_extension+local_config_cc//:cc-compiler-k8
    ```

    Without specifying `--platforms`, Bazel builds the target for
    `@platforms//host` using
    `@bazel_tools+cc_configure_extension+local_config_cc//:cc-compiler-k8`.

## Configure the C++ toolchain 

To configure the C++ toolchain, repeatedly build the application and eliminate
each error one by one as described as following.

Note: This tutorial assumes you're using Bazel 7.0.2 or later. If you're using
an older release of Bazel, use `--incompatible_enable_cc_toolchain_resolution`
flag to enable C++ toolchain resolution.

It also assumes `clang version 9.0.1`, although the details should only change
slightly between different versions of clang.

1.  Add `toolchain/BUILD` with

    ```python
    filegroup(name = "empty")

    cc_toolchain(
        name = "linux_x86_64_toolchain",
        toolchain_identifier = "linux_x86_64-toolchain",
        toolchain_config = ":linux_x86_64_toolchain_config",
        all_files = ":empty",
        compiler_files = ":empty",
        dwp_files = ":empty",
        linker_files = ":empty",
        objcopy_files = ":empty",
        strip_files = ":empty",
        supports_param_files = 0,
    )

    toolchain(
        name = "cc_toolchain_for_linux_x86_64",
        toolchain = ":linux_x86_64_toolchain",
        toolchain_type = "@bazel_tools//tools/cpp:toolchain_type",
        exec_compatible_with = [
            "@platforms//cpu:x86_64",
            "@platforms//os:linux",
        ],
        target_compatible_with = [
            "@platforms//cpu:x86_64",
            "@platforms//os:linux",
        ],
    )
    ```

    Then add appropriate dependencies and register the toolchain with
    `MODULE.bazel` with

    ```python
    bazel_dep(name = "platforms", version = "0.0.10")
    register_toolchains(
        "//toolchain:cc_toolchain_for_linux_x86_64"
    )
    ```

    This step defines a `cc_toolchain` and binds it to a `toolchain` target for
    the host configuration.

2.  Run the build again. Because the `toolchain` package doesn't yet define the
    `linux_x86_64_toolchain_config` target, Bazel throws the following error:

    ```bash
    ERROR: toolchain/BUILD:4:13: in toolchain_config attribute of cc_toolchain rule //toolchain:linux_x86_64_toolchain: rule '//toolchain:linux_x86_64_toolchain_config' does not exist.
    ```

3.  In the `toolchain/BUILD` file, define an empty filegroup as follows:

    ```python
    package(default_visibility = ["//visibility:public"])

    filegroup(name = "linux_x86_64_toolchain_config")
    ```

4.  Run the build again. Bazel throws the following error:

    ```bash
    '//toolchain:linux_x86_64_toolchain_config' does not have mandatory providers: 'CcToolchainConfigInfo'.
    ```

    `CcToolchainConfigInfo` is a provider that you use to configure your C++
    toolchains. To fix this error, create a Starlark rule that provides
    `CcToolchainConfigInfo` to Bazel by making a
    `toolchain/cc_toolchain_config.bzl` file with the following content:

    ```python
    def _impl(ctx):
        return cc_common.create_cc_toolchain_config_info(
            ctx = ctx,
            toolchain_identifier = "k8-toolchain",
            host_system_name = "local",
            target_system_name = "local",
            target_cpu = "k8",
            target_libc = "unknown",
            compiler = "clang",
            abi_version = "unknown",
            abi_libc_version = "unknown",
        )

    cc_toolchain_config = rule(
        implementation = _impl,
        attrs = {},
        provides = [CcToolchainConfigInfo],
    )
    ```

    `cc_common.create_cc_toolchain_config_info()` creates the needed provider
    `CcToolchainConfigInfo`. To use the `cc_toolchain_config` rule, add a load
    statement to `toolchain/BUILD` right below the package statement:

    ```python
    load(":cc_toolchain_config.bzl", "cc_toolchain_config")
    ```

    And replace the "linux_x86_64_toolchain_config" filegroup with a declaration
    of a `cc_toolchain_config` rule:

    ```python
    cc_toolchain_config(name = "linux_x86_64_toolchain_config")
    ```

5.  Run the build again. Bazel throws the following error:

    ```bash
    .../BUILD:1:1: C++ compilation of rule '//:hello-world' failed (Exit 1)
    src/main/tools/linux-sandbox-pid1.cc:421:
    "execvp(toolchain/DUMMY_GCC_TOOL, 0x11f20e0)": No such file or directory
    Target //:hello-world failed to build`
    ```

    At this point, Bazel has enough information to attempt building the code but
    it still does not know what tools to use to complete the required build
    actions. You will modify the Starlark rule implementation to tell Bazel what
    tools to use. For that, you need the `tool_path()` constructor from
    [`@bazel_tools//tools/cpp:cc_toolchain_config_lib.bzl`](https://source.bazel.build/bazel/+/4eea5c62a566d21832c93e4c18ec559e75d5c1ce:tools/cpp/cc_toolchain_config_lib.bzl;l=400):

    ```python
    # toolchain/cc_toolchain_config.bzl:
    # NEW
    load("@bazel_tools//tools/cpp:cc_toolchain_config_lib.bzl", "tool_path")

    def _impl(ctx):
        tool_paths = [ # NEW
            tool_path(
                name = "gcc",  # Compiler is referenced by the name "gcc" for historic reasons.
                path = "/usr/bin/clang",
            ),
            tool_path(
                name = "ld",
                path = "/usr/bin/ld",
            ),
            tool_path(
                name = "ar",
                path = "/usr/bin/ar",
            ),
            tool_path(
                name = "cpp",
                path = "/bin/false",
            ),
            tool_path(
                name = "gcov",
                path = "/bin/false",
            ),
            tool_path(
                name = "nm",
                path = "/bin/false",
            ),
            tool_path(
                name = "objdump",
                path = "/bin/false",
            ),
            tool_path(
                name = "strip",
                path = "/bin/false",
            ),
        ]

        return cc_common.create_cc_toolchain_config_info(
            ctx = ctx,
            toolchain_identifier = "local",
            host_system_name = "local",
            target_system_name = "local",
            target_cpu = "k8",
            target_libc = "unknown",
            compiler = "clang",
            abi_version = "unknown",
            abi_libc_version = "unknown",
            tool_paths = tool_paths, # NEW
        )
    ```

    Make sure that `/usr/bin/clang` and `/usr/bin/ld` are the correct paths for
    your system. Note that the compiler is referenced by the name "gcc" for
    historic reasons.

6.  Run the build again. Bazel throws the following error:

    ```bash
    ERROR: main/BUILD:3:10: Compiling main/hello-world.cc failed: absolute path inclusion(s) found in rule '//main:hello-world':
    the source file 'main/hello-world.cc' includes the following non-builtin files with absolute paths (if these are builtin files, make sure these paths are in your toolchain):
      '/usr/include/c++/13/ctime'
      '/usr/include/x86_64-linux-gnu/c++/13/bits/c++config.h'
      '/usr/include/x86_64-linux-gnu/c++/13/bits/os_defines.h'
      ...
    ```

    Bazel needs to know where to search for included headers. There are multiple
    ways to solve this, such as using the `includes` attribute of `cc_binary`,
    but here this is solved at the toolchain level with the
    [`cxx_builtin_include_directories`](/rules/lib/toplevel/cc_common#create_cc_toolchain_config_info)
    parameter of `cc_common.create_cc_toolchain_config_info`. Beware that if you
    are using a different version of `clang`, the include path will be
    different. These paths may also be different depending on the distribution.

    Modify the return value in `toolchain/cc_toolchain_config.bzl` to look like
    this:

    ```python
    return cc_common.create_cc_toolchain_config_info(
        ctx = ctx,
        cxx_builtin_include_directories = [ # NEW
            "/usr/lib/llvm-19/lib/clang/19/include",
            "/usr/include",
        ],
        toolchain_identifier = "local",
        host_system_name = "local",
        target_system_name = "local",
        target_cpu = "k8",
        target_libc = "unknown",
        compiler = "clang",
        abi_version = "unknown",
        abi_libc_version = "unknown",
        tool_paths = tool_paths,
    )
    ```

7.  Run the build command again, you will see an error like:

    ```bash
    /usr/bin/ld: bazel-out/k8-fastbuild/bin/main/_objs/hello-world/hello-world.o: in function `print_localtime()':
    hello-world.cc:(.text+0x68): undefined reference to `std::cout'
    ```

    The reason for this is because the linker is missing the C++ standard
    library and it can't find its symbols. There are many ways to solve this,
    such as using the `linkopts` attribute of `cc_binary`. Here it is solved by
    making sure that any target using the toolchain doesn't have to specify this
    flag.

    Copy the following code to `toolchain/cc_toolchain_config.bzl`:

    ```python
    # NEW
    load("@bazel_tools//tools/build_defs/cc:action_names.bzl", "ACTION_NAMES")
    # NEW
    load(
        "@bazel_tools//tools/cpp:cc_toolchain_config_lib.bzl",
        "feature",    # NEW
        "flag_group", # NEW
        "flag_set",   # NEW
        "tool_path",
    )

    all_link_actions = [ # NEW
        ACTION_NAMES.cpp_link_executable,
        ACTION_NAMES.cpp_link_dynamic_library,
        ACTION_NAMES.cpp_link_nodeps_dynamic_library,
    ]

    def _impl(ctx):
        tool_paths = [
            tool_path(
                name = "gcc",  # Compiler is referenced by the name "gcc" for historic reasons.
                path = "/usr/bin/clang",
            ),
            tool_path(
                name = "ld",
                path = "/usr/bin/ld",
            ),
            tool_path(
                name = "ar",
                path = "/bin/false",
            ),
            tool_path(
                name = "cpp",
                path = "/bin/false",
            ),
            tool_path(
                name = "gcov",
                path = "/bin/false",
            ),
            tool_path(
                name = "nm",
                path = "/bin/false",
            ),
            tool_path(
                name = "objdump",
                path = "/bin/false",
            ),
            tool_path(
                name = "strip",
                path = "/bin/false",
            ),
        ]

        features = [ # NEW
            feature(
                name = "default_linker_flags",
                enabled = True,
                flag_sets = [
                    flag_set(
                        actions = all_link_actions,
                        flag_groups = ([
                            flag_group(
                                flags = [
                                    "-lstdc++",
                                ],
                            ),
                        ]),
                    ),
                ],
            ),
        ]

        return cc_common.create_cc_toolchain_config_info(
            ctx = ctx,
            features = features, # NEW
            cxx_builtin_include_directories = [
                "/usr/lib/llvm-19/lib/clang/19/include",
                "/usr/include",
            ],
            toolchain_identifier = "local",
            host_system_name = "local",
            target_system_name = "local",
            target_cpu = "k8",
            target_libc = "unknown",
            compiler = "clang",
            abi_version = "unknown",
            abi_libc_version = "unknown",
            tool_paths = tool_paths,
        )

    cc_toolchain_config = rule(
        implementation = _impl,
        attrs = {},
        provides = [CcToolchainConfigInfo],
    )
    ```

    Note that this code uses the GNU C++ library libstdc++. If you want to use
    the LLVM C++ library, use "-lc++" instead of "-lstdc++".

8.  Running `bazel build //main:hello-world`, it should finally build the binary
    successfully for host.

9.  In `toolchain/BUILD`, copy the `cc_toolchain_config`, `cc_toolchain`, and
    `toolchain` targets and replace `linux_x86_64` with `android_x86_64`in
    target names.

    In `MODULE.bazel`, register the toolchain for android

    ```python
    register_toolchains(
        "//toolchain:cc_toolchain_for_linux_x86_64",
        "//toolchain:cc_toolchain_for_android_x86_64"
    )
    ```

10.  Run `bazel build //main:hello-world
    --android_platforms=//toolchain:android_x86_64` to build the binary for
    Android.

In practice, Linux and Android should have different C++ toolchain configs. You
can either modify the existing `cc_toolchain_config` for the differences or
create a separate rules (i.e. `CcToolchainConfigInfo` provider) for separate
platforms.

## Review your work 

In this tutorial you learned how to configure a basic C++ toolchain, but
toolchains are more powerful than this example.

The key takeaways are:

-   You need to specify a matching `platforms` flag in the command line for
    Bazel to resolve to the toolchain for the same constraint values on the
    platform. The documentation holds more [information about language specific
    configuration flags](/concepts/platforms).
-   You have to let the toolchain know where the tools live. In this tutorial
    there is a simplified version where you access the tools from the system. If
    you are interested in a more self-contained approach, you can read about
    [external dependencies](/external/overview). Your tools could come from a
    different module and you would have to make their files available to the
    `cc_toolchain` with target dependencies on attributes, such as
    `compiler_files`. The `tool_paths` would need to be changed as well.
-   You can create features to customize which flags should be passed to
    different actions, be it linking or any other type of action.

## Further reading 

For more details, see [C++ toolchain
configuration](/docs/cc-toolchain-config-reference)

---

## Review the dependency graph
- URL: https://bazel.build/tutorials/cpp-dependency
- Source: tutorials/cpp-dependency.mdx
- Slug: /tutorials/cpp-dependency

A successful build has all of its dependencies explicitly stated in the `BUILD`
file. Bazel uses those statements to create the project's dependency graph,
which enables accurate incremental builds.

To visualize the sample project's dependencies, you can generate a text
representation of the dependency graph by running this command at the
workspace root:

```
bazel query --notool_deps --noimplicit_deps "deps(//main:hello-world)" \
  --output graph
```

The above command tells Bazel to look for all dependencies for the target
`//main:hello-world` (excluding host and implicit dependencies) and format the
output as a graph.

Then, paste the text into [GraphViz](http://www.webgraphviz.com/).

On Ubuntu, you can view the graph locally by installing GraphViz and the xdot
Dot Viewer:

```
sudo apt update && sudo apt install graphviz xdot
```

Then you can generate and view the graph by piping the text output above
straight to xdot:

```
xdot <(bazel query --notool_deps --noimplicit_deps "deps(//main:hello-world)" \
  --output graph)
```

As you can see, the first stage of the sample project has a single target
that builds a single source file with no additional dependencies:

![Dependency graph for 'hello-world'](/docs/images/cpp-tutorial-stage1.png "Dependency graph")

**Figure 1.** Dependency graph for `hello-world` displays a single target with a single
source file.

After you set up your workspace, build your project, and examine its
dependencies, then you can add some complexity.

---

## Use labels to reference targets
- URL: https://bazel.build/tutorials/cpp-labels
- Source: tutorials/cpp-labels.mdx
- Slug: /tutorials/cpp-labels

In `BUILD` files and at the command line, Bazel uses *labels* to reference
targets - for example, `//main:hello-world` or `//lib:hello-time`. Their syntax
is:

```
//path/to/package:target-name
```

If the target is a rule target, then `path/to/package` is the path from the
workspace root (the directory containing the `MODULE.bazel` file) to the directory
containing the `BUILD` file, and `target-name` is what you named the target
in the `BUILD` file (the `name` attribute). If the target is a file target,
then `path/to/package` is the path to the root of the package, and
`target-name` is the name of the target file, including its full
path relative to the root of the package (the directory containing the
package's `BUILD` file).

When referencing targets at the repository root, the package path is empty,
just use `//:target-name`. When referencing targets within the same `BUILD`
file, you can even skip the `//` workspace root identifier and just use
`:target-name`.

---

## Common C++ Build Use Cases
- URL: https://bazel.build/tutorials/cpp-use-cases
- Source: tutorials/cpp-use-cases.mdx
- Slug: /tutorials/cpp-use-cases

Here you will find some of the most common use cases for building C++ projects
with Bazel. If you have not done so already, get started with building C++
projects with Bazel by completing the tutorial
[Introduction to Bazel: Build a C++ Project](/start/cpp).

For information on cc_library and hdrs header files, see
<a href="/reference/be/c-cpp#cc_library">cc_library</a>.

## Including multiple files in a target

You can include multiple files in a single target with
<a href="/reference/be/functions#glob">glob</a>.
For example:

```python
cc_library(
    name = "build-all-the-files",
    srcs = glob(["*.cc"]),
    hdrs = glob(["*.h"]),
)
```

With this target, Bazel will build all the `.cc` and `.h` files it finds in the
same directory as the `BUILD` file that contains this target (excluding
subdirectories).

## Using transitive includes

If a file includes a header, then any rule with that file as a source (that is,
having that file in the `srcs`, `hdrs`, or `textual_hdrs` attribute) should
depend on the included header's library rule. Conversely, only direct
dependencies need to be specified as dependencies. For example, suppose
`sandwich.h` includes `bread.h` and `bread.h` includes `flour.h`. `sandwich.h`
doesn't include `flour.h` (who wants flour in their sandwich?), so the `BUILD`
file would look like this:

```python
cc_library(
    name = "sandwich",
    srcs = ["sandwich.cc"],
    hdrs = ["sandwich.h"],
    deps = [":bread"],
)

cc_library(
    name = "bread",
    srcs = ["bread.cc"],
    hdrs = ["bread.h"],
    deps = [":flour"],
)

cc_library(
    name = "flour",
    srcs = ["flour.cc"],
    hdrs = ["flour.h"],
)
```

Here, the `sandwich` library depends on the `bread` library, which depends
on the `flour` library.

## Adding include paths

Sometimes you cannot (or do not want to) root include paths at the workspace
root. Existing libraries might already have an include directory that doesn't
match its path in your workspace. For example, suppose you have the following
directory structure:

```
└── my-project
    ├── legacy
    │   └── some_lib
    │       ├── BUILD
    │       ├── include
    │       │   └── some_lib.h
    │       └── some_lib.cc
    └── MODULE.bazel
```

Bazel will expect `some_lib.h` to be included as
`legacy/some_lib/include/some_lib.h`, but suppose `some_lib.cc` includes
`"some_lib.h"`. To make that include path valid,
`legacy/some_lib/BUILD` will need to specify that the `some_lib/include`
directory is an include directory:

```python
cc_library(
    name = "some_lib",
    srcs = ["some_lib.cc"],
    hdrs = ["include/some_lib.h"],
    copts = ["-Ilegacy/some_lib/include"],
)
```

This is especially useful for external dependencies, as their header files
must otherwise be included with a `/` prefix.

## Include external libraries

Suppose you are using [Google Test](https://github.com/google/googletest)
.
You can add a dependency on it in the `MODULE.bazel` file to
download Google Test and make it available in your repository:

```python
bazel_dep(name = "googletest", version = "1.15.2")
```

## Writing and running C++ tests

For example, you could create a test `./test/hello-test.cc`, such as:

```cpp
#include "gtest/gtest.h"
#include "main/hello-greet.h"

TEST(HelloTest, GetGreet) {
  EXPECT_EQ(get_greet("Bazel"), "Hello Bazel");
}
```

Then create `./test/BUILD` file for your tests:

```python
cc_test(
    name = "hello-test",
    srcs = ["hello-test.cc"],
    copts = [
      "-Iexternal/gtest/googletest/include",
      "-Iexternal/gtest/googletest",
    ],
    deps = [
        "@googletest//:gtest_main",
        "//main:hello-greet",
    ],
)
```

To make `hello-greet` visible to `hello-test`, you must add
`"//test:__pkg__",` to the `visibility` attribute in `./main/BUILD`.

Now you can use `bazel test` to run the test.

```
bazel test test:hello-test
```

This produces the following output:

```
INFO: Found 1 test target...
Target //test:hello-test up-to-date:
  bazel-bin/test/hello-test
INFO: Elapsed time: 4.497s, Critical Path: 2.53s
//test:hello-test PASSED in 0.3s

Executed 1 out of 1 tests: 1 test passes.
```


## Adding dependencies on precompiled libraries

If you want to use a library of which you only have a compiled version (for
example, headers and a `.so` file) wrap it in a `cc_library` rule:

```python
cc_library(
    name = "mylib",
    srcs = ["mylib.so"],
    hdrs = ["mylib.h"],
)
```

This way, other C++ targets in your workspace can depend on this rule.

---

## Documentation Versions
- URL: https://bazel.build/versions
- Source: versions/index.mdx
- Slug: /versions

The default documentation on this website represents the latest version at HEAD.
Each major and minor supported release will have a snapshot of the narrative and
reference documentation that follows the lifecycle of Bazel's version support.

To see documentation for stable Bazel versions, use the "Versioned docs"
drop-down.

To see documentation for older Bazel versions prior to Feb 2022, go to
[docs.bazel.build](https://docs.bazel.build/).

---
